diff -ruN marc_original/arclib/eval.py marc/arclib/eval.py
--- marc_original/arclib/eval.py	2025-02-28 19:58:37.898437737 -0500
+++ marc/arclib/eval.py	2025-02-28 17:08:53.467658960 -0500
@@ -73,6 +73,7 @@
 
 def evaluate(
     data_file: str, solution_file: str, submission_file: str, task_info_file: Optional[str] = None, mistakes: bool = False, subpoints=False,
+    used_ttt_adapters: bool = False
 ):
     tasks = read_tasks_from_single_file(data_file, solution_file=solution_file, test=False)
 
Binary files marc_original/arclib/__pycache__/arc.cpython-312.pyc and marc/arclib/__pycache__/arc.cpython-312.pyc differ
Binary files marc_original/arclib/__pycache__/augmenters.cpython-312.pyc and marc/arclib/__pycache__/augmenters.cpython-312.pyc differ
Binary files marc_original/arclib/__pycache__/eval.cpython-312.pyc and marc/arclib/__pycache__/eval.cpython-312.pyc differ
Binary files marc_original/arclib/__pycache__/__init__.cpython-312.pyc and marc/arclib/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/arclib/__pycache__/messagers.cpython-312.pyc and marc/arclib/__pycache__/messagers.cpython-312.pyc differ
Binary files marc_original/arclib/__pycache__/representers.cpython-312.pyc and marc/arclib/__pycache__/representers.cpython-312.pyc differ
Binary files marc_original/arclib/__pycache__/voting.cpython-312.pyc and marc/arclib/__pycache__/voting.cpython-312.pyc differ
diff -ruN marc_original/arclib/voting.py marc/arclib/voting.py
--- marc_original/arclib/voting.py	2025-02-28 19:58:37.898437737 -0500
+++ marc/arclib/voting.py	2025-02-28 17:34:53.027638800 -0500
@@ -69,7 +69,13 @@
     return most_common, second_most_common, third_most_common
 
 
-def vote(outputs):
+def vote(outputs, augmentations=['all', 'identity', 'Transpose()', 'Flip(0)', 'Flip(1)', 'Rotate(180)', 'Rotate(270)']):
+    """
+    Vote on the outputs based on the augmentations. Uses hierarchical voting to vote within augmentation category before voting across categories.
+    outputs: list of dictionaries created from all_predictions.json
+    augmentations: list of augmentations to vote on (use ['all'] only for flat voting)
+    """
+
     # TODO: simplify the categories based on iverters.
     # The categories should've been assinged outside of this function.
     # This function just needs to vote based on the categories.
@@ -92,15 +98,7 @@
     row_first = False
 
     candidates = []
-    for key in [
-        "all",
-        "identity",
-        "Transpose()",
-        "Flip(0)",
-        "Flip(1)",
-        "Rotate(180)",
-        "Rotate(270)",
-    ]:
+    for key in augmentations:
         if key in outputs_by_category:
             category_candidates = get_all_type_of_votingsv2(
                 outputs_by_category[key], row_first=row_first
@@ -121,6 +119,7 @@
             else:
                 C2 = C3
     except:
-        print("Error in ", id)
+        #print("Error in ", id)
+        pass
 
     return (C1, C2)
diff -ruN marc_original/configs/pretrain/1B_full.yaml marc/configs/pretrain/1B_full.yaml
--- marc_original/configs/pretrain/1B_full.yaml	2025-02-28 19:58:37.902437737 -0500
+++ marc/configs/pretrain/1B_full.yaml	1969-12-31 19:00:00.000000000 -0500
@@ -1,101 +0,0 @@
-# Config for multi-device full finetuning in full_finetune_distributed.py
-# using a Llama3 8B Instruct model
-#
-# This config assumes that you've run the following command before launching
-# this run:
-#   tune download meta-llama/Meta-Llama-3.2-8B-Instruct --output-dir /tmp/Meta-Llama-3.2-8B-Instruct --hf-token <HF_TOKEN>
-#
-# To launch on 4 devices, run the following command from root:
-#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full
-#
-# You can add specific overrides through the command line. For example
-# to override the checkpointer directory while launching training
-# you can run:
-#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
-#
-# This config works best when the model is being fine-tuned on 2+ GPUs.
-# Single device full finetuning requires more memory optimizations. It's
-# best to use 8B_full_single_device.yaml for those cases
-
-
-# Tokenizer
-tokenizer:
-  _component_: torchtune.models.llama3.llama3_tokenizer
-  path: checkpoints/base/Llama-3.2-1B-Instruct/original/tokenizer.model
-  max_seq_len: null
-
-# Dataset
-dataset:
-  _component_: torchtune.datasets.arc_dataset
-  source: /raid/lingo/akyurek/git/arc/data/tasks/all_in_pm_fix_30/
-  train_on_input: False
-  unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
-
-seed: 58
-shuffle: True
-
-# Model Arguments
-model:
-  _component_: torchtune.models.llama3_2.llama3_2_1b
-
-checkpointer:
-  _component_: torchtune.training.FullModelHFCheckpointer
-  checkpoint_dir: checkpoints/base/Llama-3.2.2-1B-Instruct
-  checkpoint_files: [
-    #  hf_model_0001_1.pt,
-    #  hf_model_0002_1.pt,
-    #  hf_model_0003_1.pt,
-    #  hf_model_0004_1.pt,
-    # "hf_model_0001_0.pt",
-    "model.safetensors",
-    #  model-00001-of-00004.safetensors,
-    #  model-00002-of-00004.safetensors,
-    #  model-00003-of-00004.safetensors,
-    #  model-00004-of-00004.safetensors,
-     # pytorch_model-0001-of-0004.bin,
-     # pytorch_model-0002-of-0004.bin,
-     # pytorch_model-0003-of-0004.bin,
-     # pytorch_model-0004-of-0004.bin,
-  ]
-  recipe_checkpoint: null
-  output_dir: experiments/pretrain/tasks_v1/
-  model_type: LLAMA3
-resume_from_checkpoint: False
-
-# Fine-tuning arguments
-batch_size: 4
-epochs: 4
-
-optimizer:
-  _component_: torch.optim.AdamW
-  lr: 2e-5
-  fused: True
-loss:
-  _component_: torch.nn.CrossEntropyLoss
-max_steps_per_epoch: null
-gradient_accumulation_steps: 4
-
-lr_scheduler:
-  _component_: torchtune.modules.get_cosine_schedule_with_warmup
-  num_warmup_steps: 2000
-# Training env
-device: cuda
-
-# Memory management
-enable_activation_checkpointing: True
-
-# Reduced precision
-dtype: bf16
-
-# Logging
-# Logging
-metric_logger:
-  _component_: torchtune.training.metric_logging.WandBLogger
-  project: ARCPT
-  entity: akyurek
-  job_type: PT
-  group: pretrain
-
-output_dir: experiments/pretrain/just_grids/
-log_every_n_steps: 1
-log_peak_memory_stats: False
diff -ruN marc_original/configs/pretrain/3B_full.yaml marc/configs/pretrain/3B_full.yaml
--- marc_original/configs/pretrain/3B_full.yaml	2025-02-28 19:58:37.902437737 -0500
+++ marc/configs/pretrain/3B_full.yaml	1969-12-31 19:00:00.000000000 -0500
@@ -1,99 +0,0 @@
-# Config for multi-device full finetuning in full_finetune_distributed.py
-# using a Llama3 8B Instruct model
-#
-# This config assumes that you've run the following command before launching
-# this run:
-#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
-#
-# To launch on 4 devices, run the following command from root:
-#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full
-#
-# You can add specific overrides through the command line. For example
-# to override the checkpointer directory while launching training
-# you can run:
-#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
-#
-# This config works best when the model is being fine-tuned on 2+ GPUs.
-# Single device full finetuning requires more memory optimizations. It's
-# best to use 8B_full_single_device.yaml for those cases
-
-
-# Tokenizer
-tokenizer:
-  _component_: torchtune.models.llama3.llama3_tokenizer
-  path: checkpoints/base/Llama-3.2-3B-Instruct/original/tokenizer.model
-  max_seq_len: null
-
-# Dataset
-dataset:
-  _component_: torchtune.datasets.arc_dataset
-  source: /raid/lingo/akyurek/git/arc/data/tasks/all_in_pm_fix_30/
-  train_on_input: False
-  unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
-
-seed: 57
-shuffle: True
-
-# Model Arguments
-model:
-  _component_: torchtune.models.llama3_2.llama3_2_3b
-
-checkpointer:
-  _component_: torchtune.training.FullModelHFCheckpointer
-  checkpoint_dir: checkpoints/base/Llama-3.2-3B-Instruct/
-  checkpoint_files: [
-    #  hf_model_0001_1.pt,
-    #  hf_model_0002_1.pt,
-    #  hf_model_0003_1.pt,
-    #  hf_model_0004_1.pt,
-     model-00001-of-00002.safetensors,
-     model-00002-of-00002.safetensors,
-    #  model-00003-of-00004.safetensors,
-    #  model-00004-of-00004.safetensors,
-     # pytorch_model-0001-of-0004.bin,
-     # pytorch_model-0002-of-0004.bin,
-     # pytorch_model-0003-of-0004.bin,
-     # pytorch_model-0004-of-0004.bin,
-  ]
-  recipe_checkpoint: null
-  output_dir: experiments/pretrain/debug/
-  model_type: LLAMA3
-resume_from_checkpoint: False
-
-# Fine-tuning arguments
-batch_size: 4
-epochs: 4
-
-optimizer:
-  _component_: torch.optim.AdamW
-  lr: 2e-5
-  fused: True
-loss:
-  _component_: torch.nn.CrossEntropyLoss
-max_steps_per_epoch: null
-gradient_accumulation_steps: 2
-
-lr_scheduler:
-  _component_: torchtune.modules.get_cosine_schedule_with_warmup
-  num_warmup_steps: 2000
-# Training env
-device: cuda
-
-# Memory management
-enable_activation_checkpointing: True
-
-# Reduced precision
-dtype: bf16
-
-# Logging
-# Logging
-metric_logger:
-  _component_: torchtune.training.metric_logging.WandBLogger
-  project: ARCPT
-  entity: akyurek
-  job_type: PT
-  group: pretrain
-
-output_dir: experiments/pretrain/debug/
-log_every_n_steps: 1
-log_peak_memory_stats: False
diff -ruN marc_original/configs/pretrain/8B_full.yaml marc/configs/pretrain/8B_full.yaml
--- marc_original/configs/pretrain/8B_full.yaml	2025-02-28 19:58:37.902437737 -0500
+++ marc/configs/pretrain/8B_full.yaml	1969-12-31 19:00:00.000000000 -0500
@@ -1,99 +0,0 @@
-# Config for multi-device full finetuning in full_finetune_distributed.py
-# using a Llama3 8B Instruct model
-#
-# This config assumes that you've run the following command before launching
-# this run:
-#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
-#
-# To launch on 4 devices, run the following command from root:
-#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full
-#
-# You can add specific overrides through the command line. For example
-# to override the checkpointer directory while launching training
-# you can run:
-#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
-#
-# This config works best when the model is being fine-tuned on 2+ GPUs.
-# Single device full finetuning requires more memory optimizations. It's
-# best to use 8B_full_single_device.yaml for those cases
-
-
-# Tokenizer
-tokenizer:
-  _component_: torchtune.models.llama3.llama3_tokenizer
-  path: checkpoints/base/Meta-Llama-3-8B-Instruct/original/tokenizer.model
-  max_seq_len: null
-
-# Dataset
-dataset:
-  _component_: torchtune.datasets.arc_dataset
-  source: /raid/lingo/akyurek/git/arc/data/tasks/all_in_pm_fix_30/
-  train_on_input: False
-  unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
-
-seed: 59
-shuffle: True
-
-# Model Arguments
-model:
-  _component_: torchtune.models.llama3.llama3_8b
-
-checkpointer:
-  _component_: torchtune.training.FullModelHFCheckpointer
-  checkpoint_dir: checkpoints/base/Meta-Llama-3-8B-Instruct/
-  checkpoint_files: [
-    #  hf_model_0001_0.pt,
-    #  hf_model_0002_0.pt,
-    #  hf_model_0003_0.pt,
-    #  hf_model_0004_0.pt,
-     model-00001-of-00004.safetensors,
-     model-00002-of-00004.safetensors,
-     model-00003-of-00004.safetensors,
-     model-00004-of-00004.safetensors,
-     # pytorch_model-0001-of-0004.bin,
-     # pytorch_model-0002-of-0004.bin,
-     # pytorch_model-0003-of-0004.bin,
-     # pytorch_model-0004-of-0004.bin,
-  ]
-  recipe_checkpoint: recipe_state.pt
-  output_dir: experiments/pretrain/debug/
-  model_type: LLAMA3
-resume_from_checkpoint: False
-
-# Fine-tuning arguments
-batch_size: 2
-epochs: 4
-
-optimizer:
-  _component_: torch.optim.AdamW
-  lr: 2e-5
-  fused: True
-loss:
-  _component_: torch.nn.CrossEntropyLoss
-max_steps_per_epoch: null
-gradient_accumulation_steps: 4
-
-lr_scheduler:
-  _component_: torchtune.modules.get_cosine_schedule_with_warmup
-  num_warmup_steps: 2000
-# Training env
-device: cuda
-
-# Memory management
-enable_activation_checkpointing: True
-
-# Reduced precision
-dtype: bf16
-
-# Logging
-# Logging
-metric_logger:
-  _component_: torchtune.training.metric_logging.WandBLogger
-  project: ARCPT
-  entity: akyurek
-  job_type: PT
-  group: pretrain
-
-output_dir: experiments/pretrain/debug/
-log_every_n_steps: 1
-log_peak_memory_stats: False
diff -ruN marc_original/configs/ttt/1B_lora_single_device.yaml marc/configs/ttt/1B_lora_single_device.yaml
--- marc_original/configs/ttt/1B_lora_single_device.yaml	2025-02-28 19:58:37.902437737 -0500
+++ marc/configs/ttt/1B_lora_single_device.yaml	2025-02-28 20:21:31.586219964 -0500
@@ -29,17 +29,17 @@
 # Tokenizer
 tokenizer:
   _component_: torchtune.models.llama3.llama3_tokenizer
-  path: checkpoints/base/Llama-3.2-1B-Instruct/original/tokenizer.model
+  path: checkpoints/acc_rd_ttt-Llama-3.2-1B-Instruct/original/tokenizer.model
   max_seq_len: null
 
 checkpointer:
   _component_: torchtune.training.FullModelHFCheckpointer
-  checkpoint_dir: /raid/lingo/akyurek/git/arc/weights/hf_model_last/
+  checkpoint_dir: weights/hf_model_last/
   checkpoint_files: [
      hf_model_0001.pt,
   ]
   recipe_checkpoint: # recipe_state.pt
-  output_dir: /raid/lingo/akyurek/git/arc/experiments/lora/model_weights/
+  output_dir: experiments/lora/model_weights/
   model_type: LLAMA3
 
 resume_from_checkpoint: False
@@ -77,7 +77,7 @@
 compile: False
 
 # Logging
-output_dir: /raid/lingo/akyurek/git/arc/experiments/lora
+output_dir: experiments/lora
 metric_logger:
   _component_: torchtune.training.metric_logging.DiskLogger
   log_dir: ${output_dir}
@@ -89,7 +89,7 @@
 dtype: bf16
 
 # Activations Memory
-enable_activation_checkpointing: True
+enable_activation_checkpointing: False
 enable_activation_offloading: False
 
 # Profiler (disabled)
diff -ruN marc_original/configs/ttt/1B_qlora_single_device.yaml marc/configs/ttt/1B_qlora_single_device.yaml
--- marc_original/configs/ttt/1B_qlora_single_device.yaml	2025-02-28 19:58:37.902437737 -0500
+++ marc/configs/ttt/1B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
@@ -1,118 +0,0 @@
-# Config for single device LoRA finetuning in lora_finetune_single_device.py
-# using a Llama3 1b Instruct model
-#
-# This config assumes that you've run the following command before launching
-# this run:
-#   tune download meta-llama/Meta-Llama-3-1b-Instruct --output-dir /tmp/Meta-Llama-3-1b-Instruct --hf-token <HF_TOKEN>
-#
-# To launch on a single device, run the following command from root:
-#   tune run lora_finetune_single_device --config llama3/1b_lora_single_device
-#
-# You can add specific overrides through the command line. For example
-# to override the checkpointer directory while launching training
-# you can run:
-#   tune run lora_finetune_single_device --config llama3/1b_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
-#
-# This config works only for training on single device.
-
-
-# Model Arguments
-model:
-  _component_: torchtune.models.llama3_2.qlora_llama3_2_1b
-  lora_attn_modules: ['q_proj', 'v_proj']
-  apply_lora_to_mlp: True
-  # apply_lora_to_output: False
-  lora_rank: 64
-  lora_alpha: 16
-  lora_dropout: 0.0
-
-# Tokenizer
-tokenizer:
-  _component_: torchtune.models.llama3.llama3_tokenizer
-  path: checkpoints/base/Llama-3.2-1B-Instruct/original/tokenizer.model
-  max_seq_len: null
-
-checkpointer:
-  _component_: torchtune.training.FullModelHFCheckpointer
-  checkpoint_dir: /raid/lingo/akyurek/git/arc/weights/hf_model_last/
-  checkpoint_files: [
-     hf_model_0001.pt,
-  ]
-  recipe_checkpoint: # recipe_state.pt
-  output_dir: /raid/lingo/akyurek/git/arc/experiments/lora/model_weights/
-  model_type: LLAMA3
-
-resume_from_checkpoint: False
-save_adapter_weights_only: True
-
-# Dataset and Sampler
-dataset:
-   _component_: torchtune.datasets.arc_dataset
-   source: data/dummy/
-   train_on_input: False
-   unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
-
-seed: 57
-shuffle: True
-batch_size: 2
-
-# Optimizer and Scheduler
-optimizer:
-  _component_: torch.optim.AdamW
-  fused: True
-  weight_decay: 0.01
-  lr: 3e-4
-
-lr_scheduler:
-  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
-  num_warmup_steps: 5
-
-loss:
-  _component_: torch.nn.CrossEntropyLoss
-
-# Training
-epochs: 3
-max_steps_per_epoch: null
-gradient_accumulation_steps: 16
-compile: False
-
-# Logging
-output_dir: /raid/lingo/akyurek/git/arc/experiments/lora
-metric_logger:
-  _component_: torchtune.training.metric_logging.DiskLogger
-  log_dir: ${output_dir}
-log_every_n_steps: 1
-log_peak_memory_stats: False
-
-# Environment
-device: cuda
-dtype: bf16
-
-# Activations Memory
-enable_activation_checkpointing: True
-enable_activation_offloading: False
-
-# Profiler (disabled)
-profiler:
-  _component_: torchtune.training.setup_torch_profiler
-  enabled: False
-
-  #Output directory of trace artifacts
-  output_dir: ${output_dir}/profiling_outputs
-
-  #`torch.profiler.ProfilerActivity` types to trace
-  cpu: True
-  cuda: True
-
-  #trace options passed to `torch.profiler.profile`
-  profile_memory: False
-  with_stack: False
-  record_shapes: True
-  with_flops: False
-
-  # `torch.profiler.schedule` options:
-  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
-  wait_steps: 5
-  warmup_steps: 5
-  active_steps: 2
-  num_cycles: 1
diff -ruN marc_original/configs/ttt/3B_lora_single_device.yaml marc/configs/ttt/3B_lora_single_device.yaml
--- marc_original/configs/ttt/3B_lora_single_device.yaml	2025-02-28 19:58:37.902437737 -0500
+++ marc/configs/ttt/3B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
@@ -1,119 +0,0 @@
-# Config for single device LoRA finetuning in lora_finetune_single_device.py
-# using a Llama3 3b Instruct model
-#
-# This config assumes that you've run the following command before launching
-# this run:
-#   tune download meta-llama/Meta-Llama-3-3b-Instruct --output-dir /tmp/Meta-Llama-3-3b-Instruct --hf-token <HF_TOKEN>
-#
-# To launch on a single device, run the following command from root:
-#   tune run lora_finetune_single_device --config llama3/3b_lora_single_device
-#
-# You can add specific overrides through the command line. For example
-# to override the checkpointer directory while launching training
-# you can run:
-#   tune run lora_finetune_single_device --config llama3/3b_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
-#
-# This config works only for training on single device.
-
-
-# Model Arguments
-model:
-  _component_: torchtune.models.llama3_2.lora_llama3_2_3b
-  lora_attn_modules: ['q_proj', 'v_proj']
-  apply_lora_to_mlp: True
-  # apply_lora_to_output: False
-  lora_rank: 64
-  lora_alpha: 16
-  lora_dropout: 0.0
-
-# Tokenizer
-tokenizer:
-  _component_: torchtune.models.llama3.llama3_tokenizer
-  path: checkpoints/base/Llama-3.2-3B-Instruct/original/tokenizer.model
-  max_seq_len: null
-
-checkpointer:
-  _component_: torchtune.training.FullModelHFCheckpointer
-  checkpoint_dir: /raid/lingo/akyurek/git/arc/weights/hf_model_last/
-  checkpoint_files: [
-    pytorch_model-0001-of-0002.bin,
-    pytorch_model-0002-of-0002.bin
-  ]
-  recipe_checkpoint: # recipe_state.pt
-  output_dir: /raid/lingo/akyurek/git/arc/experiments/lora/model_weights/
-  model_type: LLAMA3
-
-resume_from_checkpoint: False
-save_adapter_weights_only: True
-
-# Dataset and Sampler
-dataset:
-   _component_: torchtune.datasets.arc_dataset
-   source: data/dummy/
-   train_on_input: False
-   unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
-
-seed: 57
-shuffle: True
-batch_size: 2
-
-# Optimizer and Scheduler
-optimizer:
-  _component_: torch.optim.AdamW
-  fused: True
-  weight_decay: 0.01
-  lr: 3e-4
-
-lr_scheduler:
-  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
-  num_warmup_steps: 5
-
-loss:
-  _component_: torch.nn.CrossEntropyLoss
-
-# Training
-epochs: 3
-max_steps_per_epoch: null
-gradient_accumulation_steps: 16
-compile: False
-
-# Logging
-output_dir: /raid/lingo/akyurek/git/arc/experiments/lora
-metric_logger:
-  _component_: torchtune.training.metric_logging.DiskLogger
-  log_dir: ${output_dir}
-log_every_n_steps: 1
-log_peak_memory_stats: False
-
-# Environment
-device: cuda
-dtype: bf16
-
-# Activations Memory
-enable_activation_checkpointing: True
-enable_activation_offloading: False
-
-# Profiler (disabled)
-profiler:
-  _component_: torchtune.training.setup_torch_profiler
-  enabled: False
-
-  #Output directory of trace artifacts
-  output_dir: ${output_dir}/profiling_outputs
-
-  #`torch.profiler.ProfilerActivity` types to trace
-  cpu: True
-  cuda: True
-
-  #trace options passed to `torch.profiler.profile`
-  profile_memory: False
-  with_stack: False
-  record_shapes: True
-  with_flops: False
-
-  # `torch.profiler.schedule` options:
-  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
-  wait_steps: 5
-  warmup_steps: 5
-  active_steps: 2
-  num_cycles: 1
diff -ruN marc_original/configs/ttt/8.1B_lora_single_device.yaml marc/configs/ttt/8.1B_lora_single_device.yaml
--- marc_original/configs/ttt/8.1B_lora_single_device.yaml	2025-02-28 19:58:37.902437737 -0500
+++ marc/configs/ttt/8.1B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
@@ -1,121 +0,0 @@
-# Config for single device LoRA finetuning in lora_finetune_single_device.py
-# using a Llama3 8B Instruct model
-#
-# This config assumes that you've run the following command before launching
-# this run:
-#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
-#
-# To launch on a single device, run the following command from root:
-#   tune run lora_finetune_single_device --config llama3/8B_lora_single_device
-#
-# You can add specific overrides through the command line. For example
-# to override the checkpointer directory while launching training
-# you can run:
-#   tune run lora_finetune_single_device --config llama3/8B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
-#
-# This config works only for training on single device.
-
-
-# Model Arguments
-model:
-  _component_: torchtune.models.llama3_1.lora_llama3_1_8b
-  lora_attn_modules: ['q_proj', 'v_proj']
-  apply_lora_to_mlp: True
-  apply_lora_to_output: False
-  lora_rank: 64
-  lora_alpha: 16
-  lora_dropout: 0.0
-
-# Tokenizer
-tokenizer:
-  _component_: torchtune.models.llama3.llama3_tokenizer
-  path: checkpoints/base/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model
-  max_seq_len: null
-
-checkpointer:
-  _component_: torchtune.training.FullModelHFCheckpointer
-  checkpoint_dir: /raid/lingo/akyurek/git/arc/weights/hf_model_last/
-  checkpoint_files: [
-     model-00001-of-00004.safetensors,
-     model-00002-of-00004.safetensors,
-     model-00003-of-00004.safetensors,
-     model-00004-of-00004.safetensors,
-  ]
-  recipe_checkpoint: # recipe_state.pt
-  output_dir: /raid/lingo/akyurek/git/arc/experiments/lora/model_weights/
-  model_type: LLAMA3
-
-resume_from_checkpoint: False
-save_adapter_weights_only: True
-
-# Dataset and Sampler
-dataset:
-   _component_: torchtune.datasets.arc_dataset
-   source: data/dummy/
-   train_on_input: False
-   unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
-
-seed: 57
-shuffle: True
-batch_size: 2
-
-# Optimizer and Scheduler
-optimizer:
-  _component_: torch.optim.AdamW
-  fused: True
-  weight_decay: 0.01
-  lr: 3e-4
-
-lr_scheduler:
-  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
-  num_warmup_steps: 5
-
-loss:
-  _component_: torch.nn.CrossEntropyLoss
-
-# Training
-epochs: 3
-max_steps_per_epoch: null
-gradient_accumulation_steps: 16
-compile: False
-
-# Logging
-output_dir: /raid/lingo/akyurek/git/arc/experiments/lora
-metric_logger:
-  _component_: torchtune.training.metric_logging.DiskLogger
-  log_dir: ${output_dir}
-log_every_n_steps: 1
-log_peak_memory_stats: False
-
-# Environment
-device: cuda
-dtype: bf16
-
-# Activations Memory
-enable_activation_checkpointing: True
-enable_activation_offloading: False
-
-# Profiler (disabled)
-profiler:
-  _component_: torchtune.training.setup_torch_profiler
-  enabled: False
-
-  #Output directory of trace artifacts
-  output_dir: ${output_dir}/profiling_outputs
-
-  #`torch.profiler.ProfilerActivity` types to trace
-  cpu: True
-  cuda: True
-
-  #trace options passed to `torch.profiler.profile`
-  profile_memory: False
-  with_stack: False
-  record_shapes: True
-  with_flops: False
-
-  # `torch.profiler.schedule` options:
-  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
-  wait_steps: 5
-  warmup_steps: 5
-  active_steps: 2
-  num_cycles: 1
diff -ruN marc_original/configs/ttt/8B_lora_single_device.yaml marc/configs/ttt/8B_lora_single_device.yaml
--- marc_original/configs/ttt/8B_lora_single_device.yaml	2025-02-28 19:58:37.902437737 -0500
+++ marc/configs/ttt/8B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
@@ -1,121 +0,0 @@
-# Config for single device LoRA finetuning in lora_finetune_single_device.py
-# using a Llama3 8B Instruct model
-#
-# This config assumes that you've run the following command before launching
-# this run:
-#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
-#
-# To launch on a single device, run the following command from root:
-#   tune run lora_finetune_single_device --config llama3/8B_lora_single_device
-#
-# You can add specific overrides through the command line. For example
-# to override the checkpointer directory while launching training
-# you can run:
-#   tune run lora_finetune_single_device --config llama3/8B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
-#
-# This config works only for training on single device.
-
-
-# Model Arguments
-model:
-  _component_: torchtune.models.llama3.lora_llama3_8b
-  lora_attn_modules: ['q_proj', 'v_proj']
-  apply_lora_to_mlp: True
-  apply_lora_to_output: False
-  lora_rank: 64
-  lora_alpha: 16
-  lora_dropout: 0.0
-
-# Tokenizer
-tokenizer:
-  _component_: torchtune.models.llama3.llama3_tokenizer
-  path: checkpoints/base/Meta-Llama-3-8B-Instruct/original/tokenizer.model
-  max_seq_len: null
-
-checkpointer:
-  _component_: torchtune.training.FullModelHFCheckpointer
-  checkpoint_dir: /raid/lingo/akyurek/git/arc/weights/hf_model_last/
-  checkpoint_files: [
-     pytorch_model-0001-of-0004.bin,
-     pytorch_model-0002-of-0004.bin,
-     pytorch_model-0003-of-0004.bin,
-     pytorch_model-0004-of-0004.bin,
-  ]
-  recipe_checkpoint: # recipe_state.pt
-  output_dir: /raid/lingo/akyurek/git/arc/experiments/lora/model_weights/
-  model_type: LLAMA3
-
-resume_from_checkpoint: False
-save_adapter_weights_only: True
-
-# Dataset and Sampler
-dataset:
-   _component_: torchtune.datasets.arc_dataset
-   source: data/dummy/
-   train_on_input: False
-   unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
-
-seed: 57
-shuffle: True
-batch_size: 2
-
-# Optimizer and Scheduler
-optimizer:
-  _component_: torch.optim.AdamW
-  fused: True
-  weight_decay: 0.01
-  lr: 3e-4
-
-lr_scheduler:
-  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
-  num_warmup_steps: 5
-
-loss:
-  _component_: torch.nn.CrossEntropyLoss
-
-# Training
-epochs: 3
-max_steps_per_epoch: null
-gradient_accumulation_steps: 16
-compile: False
-
-# Logging
-output_dir: /raid/lingo/akyurek/git/arc/experiments/lora
-metric_logger:
-  _component_: torchtune.training.metric_logging.DiskLogger
-  log_dir: ${output_dir}
-log_every_n_steps: 1
-log_peak_memory_stats: False
-
-# Environment
-device: cuda
-dtype: bf16
-
-# Activations Memory
-enable_activation_checkpointing: True
-enable_activation_offloading: False
-
-# Profiler (disabled)
-profiler:
-  _component_: torchtune.training.setup_torch_profiler
-  enabled: False
-
-  #Output directory of trace artifacts
-  output_dir: ${output_dir}/profiling_outputs
-
-  #`torch.profiler.ProfilerActivity` types to trace
-  cpu: True
-  cuda: True
-
-  #trace options passed to `torch.profiler.profile`
-  profile_memory: False
-  with_stack: False
-  record_shapes: True
-  with_flops: False
-
-  # `torch.profiler.schedule` options:
-  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
-  wait_steps: 5
-  warmup_steps: 5
-  active_steps: 2
-  num_cycles: 1
diff -ruN marc_original/configs/ttt/8B_qlora_single_device.yaml marc/configs/ttt/8B_qlora_single_device.yaml
--- marc_original/configs/ttt/8B_qlora_single_device.yaml	2025-02-28 19:58:37.902437737 -0500
+++ marc/configs/ttt/8B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
@@ -1,121 +0,0 @@
-# Config for single device qlora finetuning in qlora_finetune_single_device.py
-# using a Llama3 8B Instruct model
-#
-# This config assumes that you've run the following command before launching
-# this run:
-#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
-#
-# To launch on a single device, run the following command from root:
-#   tune run qlora_finetune_single_device --config llama3/8B_qlora_single_device
-#
-# You can add specific overrides through the command line. For example
-# to override the checkpointer directory while launching training
-# you can run:
-#   tune run qlora_finetune_single_device --config llama3/8B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
-#
-# This config works only for training on single device.
-
-
-# Model Arguments
-model:
-  _component_: torchtune.models.llama3.qlora_llama3_8b
-  qlora_attn_modules: ['q_proj', 'v_proj']
-  apply_qlora_to_mlp: True
-  apply_qlora_to_output: False
-  qlora_rank: 64
-  qlora_alpha: 16
-  qlora_dropout: 0.0
-
-# Tokenizer
-tokenizer:
-  _component_: torchtune.models.llama3.llama3_tokenizer
-  path: checkpoints/base/Meta-Llama-3-8B-Instruct/original/tokenizer.model
-  max_seq_len: null
-
-checkpointer:
-  _component_: torchtune.training.FullModelHFCheckpointer
-  checkpoint_dir: /raid/lingo/akyurek/git/arc/weights/hf_model_last/
-  checkpoint_files: [
-     pytorch_model-0001-of-0004.bin,
-     pytorch_model-0002-of-0004.bin,
-     pytorch_model-0003-of-0004.bin,
-     pytorch_model-0004-of-0004.bin,
-  ]
-  recipe_checkpoint: # recipe_state.pt
-  output_dir: /raid/lingo/akyurek/git/arc/experiments/qlora/model_weights/
-  model_type: LLAMA3
-
-resume_from_checkpoint: False
-save_adapter_weights_only: True
-
-# Dataset and Sampler
-dataset:
-   _component_: torchtune.datasets.arc_dataset
-   source: data/dummy/
-   train_on_input: False
-   unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
-
-seed: 57
-shuffle: True
-batch_size: 2
-
-# Optimizer and Scheduler
-optimizer:
-  _component_: torch.optim.AdamW
-  fused: True
-  weight_decay: 0.01
-  lr: 3e-4
-
-lr_scheduler:
-  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
-  num_warmup_steps: 5
-
-loss:
-  _component_: torch.nn.CrossEntropyLoss
-
-# Training
-epochs: 3
-max_steps_per_epoch: null
-gradient_accumulation_steps: 16
-compile: False
-
-# Logging
-output_dir: /raid/lingo/akyurek/git/arc/experiments/qlora
-metric_logger:
-  _component_: torchtune.training.metric_logging.DiskLogger
-  log_dir: ${output_dir}
-log_every_n_steps: 1
-log_peak_memory_stats: False
-
-# Environment
-device: cuda
-dtype: bf16
-
-# Activations Memory
-enable_activation_checkpointing: True
-enable_activation_offloading: False
-
-# Profiler (disabled)
-profiler:
-  _component_: torchtune.training.setup_torch_profiler
-  enabled: False
-
-  #Output directory of trace artifacts
-  output_dir: ${output_dir}/profiling_outputs
-
-  #`torch.profiler.ProfilerActivity` types to trace
-  cpu: True
-  cuda: True
-
-  #trace options passed to `torch.profiler.profile`
-  profile_memory: False
-  with_stack: False
-  record_shapes: True
-  with_flops: False
-
-  # `torch.profiler.schedule` options:
-  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
-  wait_steps: 5
-  warmup_steps: 5
-  active_steps: 2
-  num_cycles: 1
diff -ruN marc_original/download_models.sh marc/download_models.sh
--- marc_original/download_models.sh	1969-12-31 19:00:00.000000000 -0500
+++ marc/download_models.sh	2025-02-28 20:14:37.986312423 -0500
@@ -0,0 +1,8 @@
+# Download Llama-3.2-1B-Instruct model
+huggingface-cli download stewy33/acc_rd_ttt-Llama-3.2-1B-Instruct --local-dir checkpoints/acc_rd_ttt-Llama-3.2-1B-Instruct --local-dir-use-symlinks False
+
+# Download finetuned 1B model
+huggingface-cli download stewy33/acc_rd_ttt-finetuned-1B-model --local-dir checkpoints/acc_rd_ttt-finetuned-1B-model --local-dir-use-symlinks False
+
+# Download experiments folder (LoRA adapters and prediction results)
+huggingface-cli download stewy33/acc_rd_ttt-experiments_folder --local-dir experiments --local-dir-use-symlinks False
\ No newline at end of file
diff -ruN marc_original/.git/config marc/.git/config
--- marc_original/.git/config	2025-02-28 19:58:37.898437737 -0500
+++ marc/.git/config	1969-12-31 19:00:00.000000000 -0500
@@ -1,11 +0,0 @@
-[core]
-	repositoryformatversion = 0
-	filemode = true
-	bare = false
-	logallrefupdates = true
-[remote "origin"]
-	url = https://github.com/ekinakyurek/marc.git
-	fetch = +refs/heads/*:refs/remotes/origin/*
-[branch "main"]
-	remote = origin
-	merge = refs/heads/main
diff -ruN marc_original/.git/description marc/.git/description
--- marc_original/.git/description	2025-02-28 19:58:37.694437758 -0500
+++ marc/.git/description	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-Unnamed repository; edit this file 'description' to name the repository.
diff -ruN marc_original/.git/HEAD marc/.git/HEAD
--- marc_original/.git/HEAD	2025-02-28 19:58:44.198437085 -0500
+++ marc/.git/HEAD	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-95b334872d435d5639135b32039577e2853a706b
diff -ruN marc_original/.git/hooks/applypatch-msg.sample marc/.git/hooks/applypatch-msg.sample
--- marc_original/.git/hooks/applypatch-msg.sample	2025-02-28 19:58:37.694437758 -0500
+++ marc/.git/hooks/applypatch-msg.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,15 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to check the commit log message taken by
-# applypatch from an e-mail message.
-#
-# The hook should exit with non-zero status after issuing an
-# appropriate message if it wants to stop the commit.  The hook is
-# allowed to edit the commit message file.
-#
-# To enable this hook, rename this file to "applypatch-msg".
-
-. git-sh-setup
-commitmsg="$(git rev-parse --git-path hooks/commit-msg)"
-test -x "$commitmsg" && exec "$commitmsg" ${1+"$@"}
-:
diff -ruN marc_original/.git/hooks/commit-msg.sample marc/.git/hooks/commit-msg.sample
--- marc_original/.git/hooks/commit-msg.sample	2025-02-28 19:58:37.690437759 -0500
+++ marc/.git/hooks/commit-msg.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to check the commit log message.
-# Called by "git commit" with one argument, the name of the file
-# that has the commit message.  The hook should exit with non-zero
-# status after issuing an appropriate message if it wants to stop the
-# commit.  The hook is allowed to edit the commit message file.
-#
-# To enable this hook, rename this file to "commit-msg".
-
-# Uncomment the below to add a Signed-off-by line to the message.
-# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
-# hook is more suited to it.
-#
-# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
-# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"
-
-# This example catches duplicate Signed-off-by lines.
-
-test "" = "$(grep '^Signed-off-by: ' "$1" |
-	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')" || {
-	echo >&2 Duplicate Signed-off-by lines.
-	exit 1
-}
diff -ruN marc_original/.git/hooks/fsmonitor-watchman.sample marc/.git/hooks/fsmonitor-watchman.sample
--- marc_original/.git/hooks/fsmonitor-watchman.sample	2025-02-28 19:58:37.690437759 -0500
+++ marc/.git/hooks/fsmonitor-watchman.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,173 +0,0 @@
-#!/usr/bin/perl
-
-use strict;
-use warnings;
-use IPC::Open2;
-
-# An example hook script to integrate Watchman
-# (https://facebook.github.io/watchman/) with git to speed up detecting
-# new and modified files.
-#
-# The hook is passed a version (currently 2) and last update token
-# formatted as a string and outputs to stdout a new update token and
-# all files that have been modified since the update token. Paths must
-# be relative to the root of the working tree and separated by a single NUL.
-#
-# To enable this hook, rename this file to "query-watchman" and set
-# 'git config core.fsmonitor .git/hooks/query-watchman'
-#
-my ($version, $last_update_token) = @ARGV;
-
-# Uncomment for debugging
-# print STDERR "$0 $version $last_update_token\n";
-
-# Check the hook interface version
-if ($version ne 2) {
-	die "Unsupported query-fsmonitor hook version '$version'.\n" .
-	    "Falling back to scanning...\n";
-}
-
-my $git_work_tree = get_working_dir();
-
-my $retry = 1;
-
-my $json_pkg;
-eval {
-	require JSON::XS;
-	$json_pkg = "JSON::XS";
-	1;
-} or do {
-	require JSON::PP;
-	$json_pkg = "JSON::PP";
-};
-
-launch_watchman();
-
-sub launch_watchman {
-	my $o = watchman_query();
-	if (is_work_tree_watched($o)) {
-		output_result($o->{clock}, @{$o->{files}});
-	}
-}
-
-sub output_result {
-	my ($clockid, @files) = @_;
-
-	# Uncomment for debugging watchman output
-	# open (my $fh, ">", ".git/watchman-output.out");
-	# binmode $fh, ":utf8";
-	# print $fh "$clockid\n@files\n";
-	# close $fh;
-
-	binmode STDOUT, ":utf8";
-	print $clockid;
-	print "\0";
-	local $, = "\0";
-	print @files;
-}
-
-sub watchman_clock {
-	my $response = qx/watchman clock "$git_work_tree"/;
-	die "Failed to get clock id on '$git_work_tree'.\n" .
-		"Falling back to scanning...\n" if $? != 0;
-
-	return $json_pkg->new->utf8->decode($response);
-}
-
-sub watchman_query {
-	my $pid = open2(\*CHLD_OUT, \*CHLD_IN, 'watchman -j --no-pretty')
-	or die "open2() failed: $!\n" .
-	"Falling back to scanning...\n";
-
-	# In the query expression below we're asking for names of files that
-	# changed since $last_update_token but not from the .git folder.
-	#
-	# To accomplish this, we're using the "since" generator to use the
-	# recency index to select candidate nodes and "fields" to limit the
-	# output to file names only. Then we're using the "expression" term to
-	# further constrain the results.
-	if (substr($last_update_token, 0, 1) eq "c") {
-		$last_update_token = "\"$last_update_token\"";
-	}
-	my $query = <<"	END";
-		["query", "$git_work_tree", {
-			"since": $last_update_token,
-			"fields": ["name"],
-			"expression": ["not", ["dirname", ".git"]]
-		}]
-	END
-
-	# Uncomment for debugging the watchman query
-	# open (my $fh, ">", ".git/watchman-query.json");
-	# print $fh $query;
-	# close $fh;
-
-	print CHLD_IN $query;
-	close CHLD_IN;
-	my $response = do {local $/; <CHLD_OUT>};
-
-	# Uncomment for debugging the watch response
-	# open ($fh, ">", ".git/watchman-response.json");
-	# print $fh $response;
-	# close $fh;
-
-	die "Watchman: command returned no output.\n" .
-	"Falling back to scanning...\n" if $response eq "";
-	die "Watchman: command returned invalid output: $response\n" .
-	"Falling back to scanning...\n" unless $response =~ /^\{/;
-
-	return $json_pkg->new->utf8->decode($response);
-}
-
-sub is_work_tree_watched {
-	my ($output) = @_;
-	my $error = $output->{error};
-	if ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {
-		$retry--;
-		my $response = qx/watchman watch "$git_work_tree"/;
-		die "Failed to make watchman watch '$git_work_tree'.\n" .
-		    "Falling back to scanning...\n" if $? != 0;
-		$output = $json_pkg->new->utf8->decode($response);
-		$error = $output->{error};
-		die "Watchman: $error.\n" .
-		"Falling back to scanning...\n" if $error;
-
-		# Uncomment for debugging watchman output
-		# open (my $fh, ">", ".git/watchman-output.out");
-		# close $fh;
-
-		# Watchman will always return all files on the first query so
-		# return the fast "everything is dirty" flag to git and do the
-		# Watchman query just to get it over with now so we won't pay
-		# the cost in git to look up each individual file.
-		my $o = watchman_clock();
-		$error = $output->{error};
-
-		die "Watchman: $error.\n" .
-		"Falling back to scanning...\n" if $error;
-
-		output_result($o->{clock}, ("/"));
-		$last_update_token = $o->{clock};
-
-		eval { launch_watchman() };
-		return 0;
-	}
-
-	die "Watchman: $error.\n" .
-	"Falling back to scanning...\n" if $error;
-
-	return 1;
-}
-
-sub get_working_dir {
-	my $working_dir;
-	if ($^O =~ 'msys' || $^O =~ 'cygwin') {
-		$working_dir = Win32::GetCwd();
-		$working_dir =~ tr/\\/\//;
-	} else {
-		require Cwd;
-		$working_dir = Cwd::cwd();
-	}
-
-	return $working_dir;
-}
diff -ruN marc_original/.git/hooks/post-update.sample marc/.git/hooks/post-update.sample
--- marc_original/.git/hooks/post-update.sample	2025-02-28 19:58:37.690437759 -0500
+++ marc/.git/hooks/post-update.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,8 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to prepare a packed repository for use over
-# dumb transports.
-#
-# To enable this hook, rename this file to "post-update".
-
-exec git update-server-info
diff -ruN marc_original/.git/hooks/pre-applypatch.sample marc/.git/hooks/pre-applypatch.sample
--- marc_original/.git/hooks/pre-applypatch.sample	2025-02-28 19:58:37.690437759 -0500
+++ marc/.git/hooks/pre-applypatch.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,14 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to verify what is about to be committed
-# by applypatch from an e-mail message.
-#
-# The hook should exit with non-zero status after issuing an
-# appropriate message if it wants to stop the commit.
-#
-# To enable this hook, rename this file to "pre-applypatch".
-
-. git-sh-setup
-precommit="$(git rev-parse --git-path hooks/pre-commit)"
-test -x "$precommit" && exec "$precommit" ${1+"$@"}
-:
diff -ruN marc_original/.git/hooks/pre-commit.sample marc/.git/hooks/pre-commit.sample
--- marc_original/.git/hooks/pre-commit.sample	2025-02-28 19:58:37.694437758 -0500
+++ marc/.git/hooks/pre-commit.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,49 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to verify what is about to be committed.
-# Called by "git commit" with no arguments.  The hook should
-# exit with non-zero status after issuing an appropriate message if
-# it wants to stop the commit.
-#
-# To enable this hook, rename this file to "pre-commit".
-
-if git rev-parse --verify HEAD >/dev/null 2>&1
-then
-	against=HEAD
-else
-	# Initial commit: diff against an empty tree object
-	against=$(git hash-object -t tree /dev/null)
-fi
-
-# If you want to allow non-ASCII filenames set this variable to true.
-allownonascii=$(git config --type=bool hooks.allownonascii)
-
-# Redirect output to stderr.
-exec 1>&2
-
-# Cross platform projects tend to avoid non-ASCII filenames; prevent
-# them from being added to the repository. We exploit the fact that the
-# printable range starts at the space character and ends with tilde.
-if [ "$allownonascii" != "true" ] &&
-	# Note that the use of brackets around a tr range is ok here, (it's
-	# even required, for portability to Solaris 10's /usr/bin/tr), since
-	# the square bracket bytes happen to fall in the designated range.
-	test $(git diff --cached --name-only --diff-filter=A -z $against |
-	  LC_ALL=C tr -d '[ -~]\0' | wc -c) != 0
-then
-	cat <<\EOF
-Error: Attempt to add a non-ASCII file name.
-
-This can cause problems if you want to work with people on other platforms.
-
-To be portable it is advisable to rename the file.
-
-If you know what you are doing you can disable this check using:
-
-  git config hooks.allownonascii true
-EOF
-	exit 1
-fi
-
-# If there are whitespace errors, print the offending file names and fail.
-exec git diff-index --check --cached $against --
diff -ruN marc_original/.git/hooks/pre-merge-commit.sample marc/.git/hooks/pre-merge-commit.sample
--- marc_original/.git/hooks/pre-merge-commit.sample	2025-02-28 19:58:37.694437758 -0500
+++ marc/.git/hooks/pre-merge-commit.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,13 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to verify what is about to be committed.
-# Called by "git merge" with no arguments.  The hook should
-# exit with non-zero status after issuing an appropriate message to
-# stderr if it wants to stop the merge commit.
-#
-# To enable this hook, rename this file to "pre-merge-commit".
-
-. git-sh-setup
-test -x "$GIT_DIR/hooks/pre-commit" &&
-        exec "$GIT_DIR/hooks/pre-commit"
-:
diff -ruN marc_original/.git/hooks/prepare-commit-msg.sample marc/.git/hooks/prepare-commit-msg.sample
--- marc_original/.git/hooks/prepare-commit-msg.sample	2025-02-28 19:58:37.694437758 -0500
+++ marc/.git/hooks/prepare-commit-msg.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,42 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to prepare the commit log message.
-# Called by "git commit" with the name of the file that has the
-# commit message, followed by the description of the commit
-# message's source.  The hook's purpose is to edit the commit
-# message file.  If the hook fails with a non-zero status,
-# the commit is aborted.
-#
-# To enable this hook, rename this file to "prepare-commit-msg".
-
-# This hook includes three examples. The first one removes the
-# "# Please enter the commit message..." help message.
-#
-# The second includes the output of "git diff --name-status -r"
-# into the message, just before the "git status" output.  It is
-# commented because it doesn't cope with --amend or with squashed
-# commits.
-#
-# The third example adds a Signed-off-by line to the message, that can
-# still be edited.  This is rarely a good idea.
-
-COMMIT_MSG_FILE=$1
-COMMIT_SOURCE=$2
-SHA1=$3
-
-/usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' "$COMMIT_MSG_FILE"
-
-# case "$COMMIT_SOURCE,$SHA1" in
-#  ,|template,)
-#    /usr/bin/perl -i.bak -pe '
-#       print "\n" . `git diff --cached --name-status -r`
-# 	 if /^#/ && $first++ == 0' "$COMMIT_MSG_FILE" ;;
-#  *) ;;
-# esac
-
-# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
-# git interpret-trailers --in-place --trailer "$SOB" "$COMMIT_MSG_FILE"
-# if test -z "$COMMIT_SOURCE"
-# then
-#   /usr/bin/perl -i.bak -pe 'print "\n" if !$first_line++' "$COMMIT_MSG_FILE"
-# fi
diff -ruN marc_original/.git/hooks/pre-push.sample marc/.git/hooks/pre-push.sample
--- marc_original/.git/hooks/pre-push.sample	2025-02-28 19:58:37.690437759 -0500
+++ marc/.git/hooks/pre-push.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,53 +0,0 @@
-#!/bin/sh
-
-# An example hook script to verify what is about to be pushed.  Called by "git
-# push" after it has checked the remote status, but before anything has been
-# pushed.  If this script exits with a non-zero status nothing will be pushed.
-#
-# This hook is called with the following parameters:
-#
-# $1 -- Name of the remote to which the push is being done
-# $2 -- URL to which the push is being done
-#
-# If pushing without using a named remote those arguments will be equal.
-#
-# Information about the commits which are being pushed is supplied as lines to
-# the standard input in the form:
-#
-#   <local ref> <local oid> <remote ref> <remote oid>
-#
-# This sample shows how to prevent push of commits where the log message starts
-# with "WIP" (work in progress).
-
-remote="$1"
-url="$2"
-
-zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
-
-while read local_ref local_oid remote_ref remote_oid
-do
-	if test "$local_oid" = "$zero"
-	then
-		# Handle delete
-		:
-	else
-		if test "$remote_oid" = "$zero"
-		then
-			# New branch, examine all commits
-			range="$local_oid"
-		else
-			# Update to existing branch, examine new commits
-			range="$remote_oid..$local_oid"
-		fi
-
-		# Check for WIP commit
-		commit=$(git rev-list -n 1 --grep '^WIP' "$range")
-		if test -n "$commit"
-		then
-			echo >&2 "Found WIP commit in $local_ref, not pushing"
-			exit 1
-		fi
-	fi
-done
-
-exit 0
diff -ruN marc_original/.git/hooks/pre-rebase.sample marc/.git/hooks/pre-rebase.sample
--- marc_original/.git/hooks/pre-rebase.sample	2025-02-28 19:58:37.694437758 -0500
+++ marc/.git/hooks/pre-rebase.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,169 +0,0 @@
-#!/bin/sh
-#
-# Copyright (c) 2006, 2008 Junio C Hamano
-#
-# The "pre-rebase" hook is run just before "git rebase" starts doing
-# its job, and can prevent the command from running by exiting with
-# non-zero status.
-#
-# The hook is called with the following parameters:
-#
-# $1 -- the upstream the series was forked from.
-# $2 -- the branch being rebased (or empty when rebasing the current branch).
-#
-# This sample shows how to prevent topic branches that are already
-# merged to 'next' branch from getting rebased, because allowing it
-# would result in rebasing already published history.
-
-publish=next
-basebranch="$1"
-if test "$#" = 2
-then
-	topic="refs/heads/$2"
-else
-	topic=`git symbolic-ref HEAD` ||
-	exit 0 ;# we do not interrupt rebasing detached HEAD
-fi
-
-case "$topic" in
-refs/heads/??/*)
-	;;
-*)
-	exit 0 ;# we do not interrupt others.
-	;;
-esac
-
-# Now we are dealing with a topic branch being rebased
-# on top of master.  Is it OK to rebase it?
-
-# Does the topic really exist?
-git show-ref -q "$topic" || {
-	echo >&2 "No such branch $topic"
-	exit 1
-}
-
-# Is topic fully merged to master?
-not_in_master=`git rev-list --pretty=oneline ^master "$topic"`
-if test -z "$not_in_master"
-then
-	echo >&2 "$topic is fully merged to master; better remove it."
-	exit 1 ;# we could allow it, but there is no point.
-fi
-
-# Is topic ever merged to next?  If so you should not be rebasing it.
-only_next_1=`git rev-list ^master "^$topic" ${publish} | sort`
-only_next_2=`git rev-list ^master           ${publish} | sort`
-if test "$only_next_1" = "$only_next_2"
-then
-	not_in_topic=`git rev-list "^$topic" master`
-	if test -z "$not_in_topic"
-	then
-		echo >&2 "$topic is already up to date with master"
-		exit 1 ;# we could allow it, but there is no point.
-	else
-		exit 0
-	fi
-else
-	not_in_next=`git rev-list --pretty=oneline ^${publish} "$topic"`
-	/usr/bin/perl -e '
-		my $topic = $ARGV[0];
-		my $msg = "* $topic has commits already merged to public branch:\n";
-		my (%not_in_next) = map {
-			/^([0-9a-f]+) /;
-			($1 => 1);
-		} split(/\n/, $ARGV[1]);
-		for my $elem (map {
-				/^([0-9a-f]+) (.*)$/;
-				[$1 => $2];
-			} split(/\n/, $ARGV[2])) {
-			if (!exists $not_in_next{$elem->[0]}) {
-				if ($msg) {
-					print STDERR $msg;
-					undef $msg;
-				}
-				print STDERR " $elem->[1]\n";
-			}
-		}
-	' "$topic" "$not_in_next" "$not_in_master"
-	exit 1
-fi
-
-<<\DOC_END
-
-This sample hook safeguards topic branches that have been
-published from being rewound.
-
-The workflow assumed here is:
-
- * Once a topic branch forks from "master", "master" is never
-   merged into it again (either directly or indirectly).
-
- * Once a topic branch is fully cooked and merged into "master",
-   it is deleted.  If you need to build on top of it to correct
-   earlier mistakes, a new topic branch is created by forking at
-   the tip of the "master".  This is not strictly necessary, but
-   it makes it easier to keep your history simple.
-
- * Whenever you need to test or publish your changes to topic
-   branches, merge them into "next" branch.
-
-The script, being an example, hardcodes the publish branch name
-to be "next", but it is trivial to make it configurable via
-$GIT_DIR/config mechanism.
-
-With this workflow, you would want to know:
-
-(1) ... if a topic branch has ever been merged to "next".  Young
-    topic branches can have stupid mistakes you would rather
-    clean up before publishing, and things that have not been
-    merged into other branches can be easily rebased without
-    affecting other people.  But once it is published, you would
-    not want to rewind it.
-
-(2) ... if a topic branch has been fully merged to "master".
-    Then you can delete it.  More importantly, you should not
-    build on top of it -- other people may already want to
-    change things related to the topic as patches against your
-    "master", so if you need further changes, it is better to
-    fork the topic (perhaps with the same name) afresh from the
-    tip of "master".
-
-Let's look at this example:
-
-		   o---o---o---o---o---o---o---o---o---o "next"
-		  /       /           /           /
-		 /   a---a---b A     /           /
-		/   /               /           /
-	       /   /   c---c---c---c B         /
-	      /   /   /             \         /
-	     /   /   /   b---b C     \       /
-	    /   /   /   /             \     /
-    ---o---o---o---o---o---o---o---o---o---o---o "master"
-
-
-A, B and C are topic branches.
-
- * A has one fix since it was merged up to "next".
-
- * B has finished.  It has been fully merged up to "master" and "next",
-   and is ready to be deleted.
-
- * C has not merged to "next" at all.
-
-We would want to allow C to be rebased, refuse A, and encourage
-B to be deleted.
-
-To compute (1):
-
-	git rev-list ^master ^topic next
-	git rev-list ^master        next
-
-	if these match, topic has not merged in next at all.
-
-To compute (2):
-
-	git rev-list master..topic
-
-	if this is empty, it is fully merged to "master".
-
-DOC_END
diff -ruN marc_original/.git/hooks/pre-receive.sample marc/.git/hooks/pre-receive.sample
--- marc_original/.git/hooks/pre-receive.sample	2025-02-28 19:58:37.694437758 -0500
+++ marc/.git/hooks/pre-receive.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to make use of push options.
-# The example simply echoes all push options that start with 'echoback='
-# and rejects all pushes when the "reject" push option is used.
-#
-# To enable this hook, rename this file to "pre-receive".
-
-if test -n "$GIT_PUSH_OPTION_COUNT"
-then
-	i=0
-	while test "$i" -lt "$GIT_PUSH_OPTION_COUNT"
-	do
-		eval "value=\$GIT_PUSH_OPTION_$i"
-		case "$value" in
-		echoback=*)
-			echo "echo from the pre-receive-hook: ${value#*=}" >&2
-			;;
-		reject)
-			exit 1
-		esac
-		i=$((i + 1))
-	done
-fi
diff -ruN marc_original/.git/hooks/push-to-checkout.sample marc/.git/hooks/push-to-checkout.sample
--- marc_original/.git/hooks/push-to-checkout.sample	2025-02-28 19:58:37.694437758 -0500
+++ marc/.git/hooks/push-to-checkout.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,78 +0,0 @@
-#!/bin/sh
-
-# An example hook script to update a checked-out tree on a git push.
-#
-# This hook is invoked by git-receive-pack(1) when it reacts to git
-# push and updates reference(s) in its repository, and when the push
-# tries to update the branch that is currently checked out and the
-# receive.denyCurrentBranch configuration variable is set to
-# updateInstead.
-#
-# By default, such a push is refused if the working tree and the index
-# of the remote repository has any difference from the currently
-# checked out commit; when both the working tree and the index match
-# the current commit, they are updated to match the newly pushed tip
-# of the branch. This hook is to be used to override the default
-# behaviour; however the code below reimplements the default behaviour
-# as a starting point for convenient modification.
-#
-# The hook receives the commit with which the tip of the current
-# branch is going to be updated:
-commit=$1
-
-# It can exit with a non-zero status to refuse the push (when it does
-# so, it must not modify the index or the working tree).
-die () {
-	echo >&2 "$*"
-	exit 1
-}
-
-# Or it can make any necessary changes to the working tree and to the
-# index to bring them to the desired state when the tip of the current
-# branch is updated to the new commit, and exit with a zero status.
-#
-# For example, the hook can simply run git read-tree -u -m HEAD "$1"
-# in order to emulate git fetch that is run in the reverse direction
-# with git push, as the two-tree form of git read-tree -u -m is
-# essentially the same as git switch or git checkout that switches
-# branches while keeping the local changes in the working tree that do
-# not interfere with the difference between the branches.
-
-# The below is a more-or-less exact translation to shell of the C code
-# for the default behaviour for git's push-to-checkout hook defined in
-# the push_to_deploy() function in builtin/receive-pack.c.
-#
-# Note that the hook will be executed from the repository directory,
-# not from the working tree, so if you want to perform operations on
-# the working tree, you will have to adapt your code accordingly, e.g.
-# by adding "cd .." or using relative paths.
-
-if ! git update-index -q --ignore-submodules --refresh
-then
-	die "Up-to-date check failed"
-fi
-
-if ! git diff-files --quiet --ignore-submodules --
-then
-	die "Working directory has unstaged changes"
-fi
-
-# This is a rough translation of:
-#
-#   head_has_history() ? "HEAD" : EMPTY_TREE_SHA1_HEX
-if git cat-file -e HEAD 2>/dev/null
-then
-	head=HEAD
-else
-	head=$(git hash-object -t tree --stdin </dev/null)
-fi
-
-if ! git diff-index --quiet --cached --ignore-submodules $head --
-then
-	die "Working directory has staged changes"
-fi
-
-if ! git read-tree -u -m "$commit"
-then
-	die "Could not update working tree to new HEAD"
-fi
diff -ruN marc_original/.git/hooks/update.sample marc/.git/hooks/update.sample
--- marc_original/.git/hooks/update.sample	2025-02-28 19:58:37.690437759 -0500
+++ marc/.git/hooks/update.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,128 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to block unannotated tags from entering.
-# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
-#
-# To enable this hook, rename this file to "update".
-#
-# Config
-# ------
-# hooks.allowunannotated
-#   This boolean sets whether unannotated tags will be allowed into the
-#   repository.  By default they won't be.
-# hooks.allowdeletetag
-#   This boolean sets whether deleting tags will be allowed in the
-#   repository.  By default they won't be.
-# hooks.allowmodifytag
-#   This boolean sets whether a tag may be modified after creation. By default
-#   it won't be.
-# hooks.allowdeletebranch
-#   This boolean sets whether deleting branches will be allowed in the
-#   repository.  By default they won't be.
-# hooks.denycreatebranch
-#   This boolean sets whether remotely creating branches will be denied
-#   in the repository.  By default this is allowed.
-#
-
-# --- Command line
-refname="$1"
-oldrev="$2"
-newrev="$3"
-
-# --- Safety check
-if [ -z "$GIT_DIR" ]; then
-	echo "Don't run this script from the command line." >&2
-	echo " (if you want, you could supply GIT_DIR then run" >&2
-	echo "  $0 <ref> <oldrev> <newrev>)" >&2
-	exit 1
-fi
-
-if [ -z "$refname" -o -z "$oldrev" -o -z "$newrev" ]; then
-	echo "usage: $0 <ref> <oldrev> <newrev>" >&2
-	exit 1
-fi
-
-# --- Config
-allowunannotated=$(git config --type=bool hooks.allowunannotated)
-allowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)
-denycreatebranch=$(git config --type=bool hooks.denycreatebranch)
-allowdeletetag=$(git config --type=bool hooks.allowdeletetag)
-allowmodifytag=$(git config --type=bool hooks.allowmodifytag)
-
-# check for no description
-projectdesc=$(sed -e '1q' "$GIT_DIR/description")
-case "$projectdesc" in
-"Unnamed repository"* | "")
-	echo "*** Project description file hasn't been set" >&2
-	exit 1
-	;;
-esac
-
-# --- Check types
-# if $newrev is 0000...0000, it's a commit to delete a ref.
-zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
-if [ "$newrev" = "$zero" ]; then
-	newrev_type=delete
-else
-	newrev_type=$(git cat-file -t $newrev)
-fi
-
-case "$refname","$newrev_type" in
-	refs/tags/*,commit)
-		# un-annotated tag
-		short_refname=${refname##refs/tags/}
-		if [ "$allowunannotated" != "true" ]; then
-			echo "*** The un-annotated tag, $short_refname, is not allowed in this repository" >&2
-			echo "*** Use 'git tag [ -a | -s ]' for tags you want to propagate." >&2
-			exit 1
-		fi
-		;;
-	refs/tags/*,delete)
-		# delete tag
-		if [ "$allowdeletetag" != "true" ]; then
-			echo "*** Deleting a tag is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	refs/tags/*,tag)
-		# annotated tag
-		if [ "$allowmodifytag" != "true" ] && git rev-parse $refname > /dev/null 2>&1
-		then
-			echo "*** Tag '$refname' already exists." >&2
-			echo "*** Modifying a tag is not allowed in this repository." >&2
-			exit 1
-		fi
-		;;
-	refs/heads/*,commit)
-		# branch
-		if [ "$oldrev" = "$zero" -a "$denycreatebranch" = "true" ]; then
-			echo "*** Creating a branch is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	refs/heads/*,delete)
-		# delete branch
-		if [ "$allowdeletebranch" != "true" ]; then
-			echo "*** Deleting a branch is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	refs/remotes/*,commit)
-		# tracking branch
-		;;
-	refs/remotes/*,delete)
-		# delete tracking branch
-		if [ "$allowdeletebranch" != "true" ]; then
-			echo "*** Deleting a tracking branch is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	*)
-		# Anything else (is there anything else?)
-		echo "*** Update hook: unknown type of update to ref $refname of type $newrev_type" >&2
-		exit 1
-		;;
-esac
-
-# --- Finished
-exit 0
Binary files marc_original/.git/index and marc/.git/index differ
diff -ruN marc_original/.git/info/exclude marc/.git/info/exclude
--- marc_original/.git/info/exclude	2025-02-28 19:58:37.694437758 -0500
+++ marc/.git/info/exclude	1969-12-31 19:00:00.000000000 -0500
@@ -1,6 +0,0 @@
-# git ls-files --others --exclude-from=.git/info/exclude
-# Lines that start with '#' are comments.
-# For a project mostly in C, the following would be a good set of
-# exclude patterns (uncomment them if you want to use them):
-# *.[oa]
-# *~
diff -ruN marc_original/.git/logs/HEAD marc/.git/logs/HEAD
--- marc_original/.git/logs/HEAD	2025-02-28 19:58:44.198437085 -0500
+++ marc/.git/logs/HEAD	1969-12-31 19:00:00.000000000 -0500
@@ -1,2 +0,0 @@
-0000000000000000000000000000000000000000 95b334872d435d5639135b32039577e2853a706b Stewy Slocum <slocumstewy@gmail.com> 1740790717 -0500	clone: from https://github.com/ekinakyurek/marc.git
-95b334872d435d5639135b32039577e2853a706b 95b334872d435d5639135b32039577e2853a706b Stewy Slocum <slocumstewy@gmail.com> 1740790724 -0500	checkout: moving from main to 95b334872d435d5639135b32039577e2853a706b
diff -ruN marc_original/.git/logs/refs/heads/main marc/.git/logs/refs/heads/main
--- marc_original/.git/logs/refs/heads/main	2025-02-28 19:58:37.898437737 -0500
+++ marc/.git/logs/refs/heads/main	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 95b334872d435d5639135b32039577e2853a706b Stewy Slocum <slocumstewy@gmail.com> 1740790717 -0500	clone: from https://github.com/ekinakyurek/marc.git
diff -ruN marc_original/.git/logs/refs/remotes/origin/HEAD marc/.git/logs/refs/remotes/origin/HEAD
--- marc_original/.git/logs/refs/remotes/origin/HEAD	2025-02-28 19:58:37.894437737 -0500
+++ marc/.git/logs/refs/remotes/origin/HEAD	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 95b334872d435d5639135b32039577e2853a706b Stewy Slocum <slocumstewy@gmail.com> 1740790717 -0500	clone: from https://github.com/ekinakyurek/marc.git
Binary files marc_original/.git/objects/pack/pack-9adec99cb5e13fddadbb7a993dc208cbb59337ed.idx and marc/.git/objects/pack/pack-9adec99cb5e13fddadbb7a993dc208cbb59337ed.idx differ
Binary files marc_original/.git/objects/pack/pack-9adec99cb5e13fddadbb7a993dc208cbb59337ed.pack and marc/.git/objects/pack/pack-9adec99cb5e13fddadbb7a993dc208cbb59337ed.pack differ
diff -ruN marc_original/.git/packed-refs marc/.git/packed-refs
--- marc_original/.git/packed-refs	2025-02-28 19:58:37.894437737 -0500
+++ marc/.git/packed-refs	1969-12-31 19:00:00.000000000 -0500
@@ -1,3 +0,0 @@
-# pack-refs with: peeled fully-peeled sorted 
-95b334872d435d5639135b32039577e2853a706b refs/remotes/origin/main
-efa56504aef4cd5a3af6e4db70e5db05323e1364 refs/remotes/origin/modal
diff -ruN marc_original/.git/refs/heads/main marc/.git/refs/heads/main
--- marc_original/.git/refs/heads/main	2025-02-28 19:58:37.894437737 -0500
+++ marc/.git/refs/heads/main	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-95b334872d435d5639135b32039577e2853a706b
diff -ruN marc_original/.git/refs/remotes/origin/HEAD marc/.git/refs/remotes/origin/HEAD
--- marc_original/.git/refs/remotes/origin/HEAD	2025-02-28 19:58:37.894437737 -0500
+++ marc/.git/refs/remotes/origin/HEAD	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-ref: refs/remotes/origin/main
diff -ruN marc_original/inference/engine.py marc/inference/engine.py
--- marc_original/inference/engine.py	2025-02-28 19:58:37.902437737 -0500
+++ marc/inference/engine.py	2025-02-28 15:20:26.354778190 -0500
@@ -1,5 +1,6 @@
 from typing import Dict, List, Optional, Tuple
 
+import tqdm
 from transformers import PreTrainedTokenizer
 from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams
 from vllm.lora.request import LoRARequest
@@ -10,12 +11,14 @@
     num_tokens: int,
     max_tokens: int,
     temperature: float = 0.0,
+    top_p: float = 1.0,
     n: int = 1,
 ) -> SamplingParams:
     max_new_tokens = max_tokens - num_tokens
     return SamplingParams(
         max_tokens=max_new_tokens,
         temperature=temperature,
+        top_p=top_p,
         n=n,
         stop=[tokenizer.eos_token, "<|eot_id|>"],
         # best_of=10,
@@ -40,7 +43,7 @@
         max_lora_rank=max_lora_rank,
         enforce_eager=enforce_eager,
         quantization=quantization,
-        lora_target_modules=lora_target_modules,
+        #lora_target_modules=lora_target_modules,
         load_format="bitsandbytes" if quantization else "auto",
         max_model_len=8192,
         gpu_memory_utilization=0.9
@@ -54,6 +57,10 @@
 ) -> Dict[str, List[str]]:
     """Continuously process a list of prompts and handle the outputs."""
     all_outputs: Dict[str, List[str]] = {}
+
+    # Create progress bar
+    pbar = tqdm.tqdm(total=len(test_prompts), desc="Processing requests")
+
     while test_prompts or engine.has_unfinished_requests():
         if test_prompts:
             prompt, sampling_param, lora_request, idx = test_prompts.pop(0)
@@ -67,5 +74,7 @@
             if request_output.finished:
                 texts = [output.text for output in request_output.outputs]
                 all_outputs[str(request_output.request_id)] = texts
+                pbar.update(1)
 
+    pbar.close()
     return all_outputs
Binary files marc_original/inference/__pycache__/engine.cpython-312.pyc and marc/inference/__pycache__/engine.cpython-312.pyc differ
Binary files marc_original/inference/__pycache__/__init__.cpython-312.pyc and marc/inference/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/inference/__pycache__/preprocess.cpython-312.pyc and marc/inference/__pycache__/preprocess.cpython-312.pyc differ
diff -ruN marc_original/inference.sh marc/inference.sh
--- marc_original/inference.sh	1969-12-31 19:00:00.000000000 -0500
+++ marc/inference.sh	2025-02-28 20:31:45.374100524 -0500
@@ -0,0 +1,33 @@
+# Specify data path
+data_file=../kaggle-arc-data/arc-agi_evaluation_challenges.json
+# Tell where your Finetuned (named as base) and TTT checkpoints are
+base_checkpoint_dir=checkpoints/acc_rd_ttt-finetuned-1B-model # checkpoints/acc_rd_ttt-Llama-3.2-1B-Instruct for Llama3.2 base model
+ttt_folder=experiments/ttt_models_original  # 'none' to use the base model
+
+# if solution file is given predict will evaluate the model
+solution_file=../kaggle-arc-data/arc-agi_evaluation_solutions.json
+
+
+temperature=0.0
+top_p=1.0
+n_sample=1
+
+# this should be same as your ttt
+max_lora_rank=128
+# You need to tell where predictions and submissions should be saved
+tti_folder=experiments/ttt_predictions
+mkdir -p $tti_folder
+
+
+python predict.py \
+--experiment_folder=$tti_folder \
+--pretrained_checkpoint=$base_checkpoint_dir \
+--lora_checkpoints_folder=$ttt_folder \
+--temperature=$temperature \
+--top_p=$top_p \
+--n_sample=$n_sample \
+--data_file=$data_file \
+--solution_file=$solution_file \
+--max_lora_rank=$max_lora_rank \
+--include_n=1 \
+--new_format
\ No newline at end of file
diff -ruN marc_original/plots.ipynb marc/plots.ipynb
--- marc_original/plots.ipynb	1969-12-31 19:00:00.000000000 -0500
+++ marc/plots.ipynb	2025-02-28 18:13:29.795518684 -0500
@@ -0,0 +1,386 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "%load_ext autoreload\n",
+    "%autoreload 2"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import json\n",
+    "import os\n",
+    "\n",
+    "import matplotlib.pyplot as plt\n",
+    "import numpy as np\n",
+    "\n",
+    "from arclib.voting import vote\n",
+    "from arclib.arc import to_list, make_submission, read_tasks_from_single_file\n",
+    "from arclib.eval import evaluate"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 36,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def load_predictions(path: str):\n",
+    "    # Load predictions from a specific inference.sh run\n",
+    "    with open(f\"{path}/submission.json\", 'r') as f:\n",
+    "        submission = json.load(f)\n",
+    "    with open(f\"{path}/all_predictions.json\", 'r') as f:\n",
+    "        all_predictions = json.load(f)\n",
+    "    return submission, all_predictions\n",
+    "\n",
+    "def vote_and_evaluate(path: str, oracle_mode: bool = False, flat_voting: bool = False, augmentations: list[str] = ['all', 'identity', 'Transpose()', 'Flip(0)', 'Flip(1)', 'Rotate(180)', 'Rotate(270)']):\n",
+    "    \"\"\"\n",
+    "    Apply voting to get final predictions. Prints accuracy at pass@2.\n",
+    "    path: path to the inference.sh run\n",
+    "    oracle_mode: if True, use oracle mode (correct if any of the predictions are correct)\n",
+    "    flat_voting: if True, use flat voting (no hierarchical voting)\n",
+    "    augmentations: list of augmentations to use for voting (default uses all, like optimal setup in paper)\n",
+    "    \"\"\"\n",
+    "    submission, all_predictions = load_predictions(path)\n",
+    "    if flat_voting:\n",
+    "        augmentations = ['all']\n",
+    "\n",
+    "    outputs = {}\n",
+    "\n",
+    "    tasks = read_tasks_from_single_file(\"../kaggle-arc-data/arc-agi_evaluation_challenges.json\", solution_file=\"../kaggle-arc-data/arc-agi_evaluation_solutions.json\", test=True)\n",
+    "    for task in tasks:\n",
+    "        name = task.name\n",
+    "\n",
+    "        to_vote = [out for key, out in all_predictions.items() if name in key]\n",
+    "        to_vote = [out for sublist in to_vote for out in sublist]\n",
+    "\n",
+    "        if oracle_mode:\n",
+    "            if any(\n",
+    "                (np.array(x['output']).shape == task.test_example.output.shape\n",
+    "                and (np.array(x['output']) == task.test_example.output).all()) for x in to_vote\n",
+    "            ):\n",
+    "                outputs[name] = [task.test_example.output] * 2\n",
+    "            else:\n",
+    "                outputs[name] = [[[0]], [[0]]]\n",
+    "        elif len(to_vote) == 0:\n",
+    "            outputs[name] = [[[0]], [[0]]]\n",
+    "        else:\n",
+    "            # Use different augmentations for voting (see voting.py for details)\n",
+    "            attempt_1, attempt_2 = vote(to_vote, augmentations=augmentations)\n",
+    "            outputs[name] = [to_list(attempt_1), to_list(attempt_2)]\n",
+    "\n",
+    "    predictions = [outputs[task.name] for task in tasks]\n",
+    "    make_submission(tasks, predictions, \"tmp_submission.json\", number_of_attempts=2)\n",
+    "\n",
+    "    # Evaluate predictions\n",
+    "    data_file=\"../kaggle-arc-data/arc-agi_evaluation_challenges.json\"\n",
+    "    solution_file=\"../kaggle-arc-data/arc-agi_evaluation_solutions.json\"\n",
+    "    submission_file=\"tmp_submission.json\"\n",
+    "\n",
+    "    evaluate(data_file, solution_file, submission_file)\n",
+    "\n",
+    "    # Remove temporary submission file\n",
+    "    os.remove(\"tmp_submission.json\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 37,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "==================== \n",
+      "Llama-3.2-1B-Instruct:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 0 / 419 = 0.0\n",
+      "Per Prediction Hamming Distance: 286.4077904711215 / 419 = 0.6835508125802422\n",
+      "Competition Accuracy: 0 / 400 = 0.0\n",
+      "==================== \n",
+      "FT:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 33 / 419 = 0.07875894988066826\n",
+      "Per Prediction Hamming Distance: 124.21835894809055 / 419 = 0.29646386383792495\n",
+      "Competition Accuracy: 27 / 400 = 0.0675\n",
+      "==================== \n",
+      "FT+TTT: (only 100 adapters computed, so multiply accuracy by 4)\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 42 / 419 = 0.10023866348448687\n",
+      "Per Prediction Hamming Distance: 330.79335735313236 / 419 = 0.7894829531101011\n",
+      "Competition Accuracy: 38 / 400 = 0.095\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Analagous to Figure 1 (left) in the TTT paper.\n",
+    "\n",
+    "# For TTT runs, we only train 100 adapters. For problems without adapters, the eval function returns a dummy incorrect solution. So we multiply TTT accuracies by 4 to get the true accuracy. Note that for non-TTT runs, this is not needed, and we can use the accuracy reported by the eval function directly.\n",
+    "print(\"=\"*20, \"\\nLlama-3.2-1B-Instruct:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_base\")\n",
+    "print(\"=\"*20, \"\\nFT:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_ft\")\n",
+    "print(\"=\"*20, \"\\nFT+TTT: (only 100 adapters computed, so multiply accuracy by 4)\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_top-p-0.9\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "image/png": "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",
+      "text/plain": [
+       "<Figure size 640x480 with 1 Axes>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "heights = [0.0, 0.0675, 0.38]\n",
+    "\n",
+    "plt.title('ARC-AGI Accuracy w/ Different Training Strategies')\n",
+    "plt.bar(['Base Model', 'FT', 'FT+TTT'], heights)\n",
+    "plt.ylabel('Accuracy')\n",
+    "\n",
+    "# Add labels above bars\n",
+    "for i, v in enumerate(heights):\n",
+    "    plt.text(i, v + 0.005, f'{v * 100:.1f}%', ha='center')\n",
+    "\n",
+    "plt.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "==================== \n",
+      "No Data Augmentation:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 16 / 419 = 0.03818615751789976\n",
+      "Per Prediction Hamming Distance: 338.49624383698466 / 419 = 0.8078669303985314\n",
+      "Competition Accuracy: 15 / 400 = 0.0375\n",
+      "==================== \n",
+      "No Demonstration Loss:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 38 / 419 = 0.09069212410501193\n",
+      "Per Prediction Hamming Distance: 329.20221876826145 / 419 = 0.785685486320433\n",
+      "Competition Accuracy: 34 / 400 = 0.085\n",
+      "==================== \n",
+      "Optimal:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 42 / 419 = 0.10023866348448687\n",
+      "Per Prediction Hamming Distance: 330.79335735313236 / 419 = 0.7894829531101011\n",
+      "Competition Accuracy: 38 / 400 = 0.095\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Analagous to Figure 3 in the TTT paper.\n",
+    "# Recall again we multiply TTT accuracies by 4 to get the true accuracy.\n",
+    "print(\"=\"*20, \"\\nNo Data Augmentation:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_no_transform\")\n",
+    "print(\"=\"*20, \"\\nNo Demonstration Loss:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_no_demonstration_loss\")\n",
+    "print(\"=\"*20, \"\\nOptimal:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_top-p-0.9\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "image/png": "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",
+      "text/plain": [
+       "<Figure size 640x480 with 1 Axes>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "heights = [0.15, 0.34, 0.38]\n",
+    "\n",
+    "plt.title('Accuracy on ARC-AGI w/ Different TTT Training Configurations')\n",
+    "plt.bar(['No Data Augmentation', 'No Demonstration Loss', 'Optimal'], heights)\n",
+    "plt.ylabel('Accuracy')\n",
+    "plt.xticks(rotation=15, ha='center')\n",
+    "\n",
+    "# Add labels above bars\n",
+    "for i, v in enumerate(heights):\n",
+    "    plt.text(i, v + 0.005, f'{v * 100:.1f}%', ha='center')\n",
+    "\n",
+    "plt.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 32,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "==================== \n",
+      "No Data Augmentation (Identity/no data augmentation used at inference-time to create a pool of inputs for voting):\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 29 / 419 = 0.06921241050119331\n",
+      "Per Prediction Hamming Distance: 329.71556196854254 / 419 = 0.7869106490896004\n",
+      "Competition Accuracy: 26 / 400 = 0.065\n",
+      "==================== \n",
+      "Transpose:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 33 / 419 = 0.07875894988066826\n",
+      "Per Prediction Hamming Distance: 328.92160871477455 / 419 = 0.7850157725889607\n",
+      "Competition Accuracy: 29 / 400 = 0.0725\n",
+      "==================== \n",
+      "Flip:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 36 / 419 = 0.08591885441527446\n",
+      "Per Prediction Hamming Distance: 328.13389160233925 / 419 = 0.7831357794805233\n",
+      "Competition Accuracy: 32 / 400 = 0.08\n",
+      "==================== \n",
+      "Rotate:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 35 / 419 = 0.08353221957040573\n",
+      "Per Prediction Hamming Distance: 329.15641678317047 / 419 = 0.7855761737068507\n",
+      "Competition Accuracy: 32 / 400 = 0.08\n",
+      "\n",
+      "==================== \n",
+      "Flattened:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 35 / 419 = 0.08353221957040573\n",
+      "Per Prediction Hamming Distance: 330.1181580643335 / 419 = 0.7878714989602231\n",
+      "Competition Accuracy: 31 / 400 = 0.0775\n",
+      "==================== \n",
+      "Hierarchical Greedy:\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 40 / 419 = 0.0954653937947494\n",
+      "Per Prediction Hamming Distance: 328.2428168521567 / 419 = 0.7833957442772236\n",
+      "Competition Accuracy: 36 / 400 = 0.09\n",
+      "==================== \n",
+      ":Hierarchical Top-p\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 42 / 419 = 0.10023866348448687\n",
+      "Per Prediction Hamming Distance: 330.79335735313236 / 419 = 0.7894829531101011\n",
+      "Competition Accuracy: 38 / 400 = 0.095\n",
+      "==================== \n",
+      ":Oracle\n",
+      "Attempted tasks:  400\n",
+      "Per Prediction Accuracy: 50 / 419 = 0.11933174224343675\n",
+      "Per Prediction Hamming Distance: 366.41639330747137 / 419 = 0.8745021319987384\n",
+      "Competition Accuracy: 46 / 400 = 0.115\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Analagous to Figure 5 in the TTT paper.\n",
+    "# We note that greedy decoding did better in general, except in the singular case of hierarchical voting across all categories. Thus, we use greedy decoding where it performs best, and top-p when that performs better.\n",
+    "print(\"=\"*20, \"\\nNo Data Augmentation (Identity/no data augmentation used at inference-time to create a pool of inputs for voting):\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_greedy\", augmentations=['identity'])\n",
+    "print(\"=\"*20, \"\\nTranspose:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_greedy\", augmentations=['identity', 'Transpose()'])\n",
+    "print(\"=\"*20, \"\\nFlip:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_greedy\", augmentations=['identity', 'Flip(0)', 'Flip(1)'])\n",
+    "print(\"=\"*20, \"\\nRotate:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_greedy\", augmentations=['identity', 'Rotate(180)', 'Rotate(270)'])\n",
+    "\n",
+    "print()\n",
+    "\n",
+    "print(\"=\"*20, \"\\nFlattened:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_greedy\", augmentations=['all'])\n",
+    "print(\"=\"*20, \"\\nHierarchical Greedy:\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_greedy\")\n",
+    "print(\"=\"*20, \"\\n:Hierarchical Top-p\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_top-p-0.9\")\n",
+    "print(\"=\"*20, \"\\n:Oracle\")\n",
+    "vote_and_evaluate(\"experiments/ttt_predictions_greedy\", oracle_mode=True)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 33,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "image/png": "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",
+      "text/plain": [
+       "<Figure size 640x480 with 1 Axes>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "heights = [0.26, 0.29, 0.32, 0.32, 0.31, 0.36, 0.38, 0.46]\n",
+    "\n",
+    "plt.title('Accuracy on ARC-AGI w/ Different Voting Strategies')\n",
+    "plt.bar(['No Augmentations\\n(Identity)', 'Transpose', 'Flip', 'Rotate', 'Flattened', 'Hierarchical\\nGreedy', 'Hierarchical\\nTop-p', 'Oracle'], heights)\n",
+    "plt.ylabel('Accuracy')\n",
+    "plt.xticks(rotation=45, ha='center')\n",
+    "plt.axvline(x=3.5, color='black', linestyle='--', linewidth=0.75)\n",
+    "plt.text(1, 0.43, 'Individual', ha='center', va='bottom', fontsize=12)\n",
+    "plt.text(5, 0.43, 'Aggregated', ha='center', va='bottom', fontsize=12)\n",
+    "\n",
+    "# Add labels above bars\n",
+    "for i, v in enumerate(heights):\n",
+    "    plt.text(i, v + 0.005, f'{v * 100:.1f}%', ha='center')\n",
+    "\n",
+    "plt.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "ttt_replication",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.12.2"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff -ruN marc_original/predict.py marc/predict.py
--- marc_original/predict.py	2025-02-28 19:58:37.902437737 -0500
+++ marc/predict.py	2025-02-28 15:19:55.866772497 -0500
@@ -69,6 +69,7 @@
 )
 parser.add_argument("--max_tokens", type=int, default=8192, help="Max tokens")
 parser.add_argument("--temperature", type=float, default=0.0, help="Temperature for sampling")
+parser.add_argument("--top_p", type=float, default=1.0, help="Top-p for sampling")
 parser.add_argument(
     "--n_sample", type=int, default=1, help="Number of samples to generate per input"
 )
@@ -116,6 +117,8 @@
 )
 
 args = parser.parse_args()
+if args.lora_checkpoints_folder.lower() == 'none':
+    args.lora_checkpoints_folder = None
 
 # set seed
 np.random.seed(args.seed)
@@ -263,6 +266,7 @@
             input_token_length,
             args.max_tokens,
             temperature=args.temperature,
+            top_p=args.top_p,
             n=args.n_sample,
         )
         inputs_to_the_engine.append(
Binary files marc_original/__pycache__/predict.cpython-312.pyc and marc/__pycache__/predict.cpython-312.pyc differ
diff -ruN marc_original/requirements.txt marc/requirements.txt
--- marc_original/requirements.txt	2025-02-28 19:58:37.902437737 -0500
+++ marc/requirements.txt	2025-02-28 20:35:09.566075614 -0500
@@ -3,5 +3,11 @@
 matplotlib
 litellm
 tqdm
-# torchtune @ git+https://github.com/ekinakyurek/torchtune.git@ekin/currentrb
-# vllm @ git@github.com:ekinakyurek/vllm.git@ekin/torchtunecompat
\ No newline at end of file
+torchvision
+gpustat
+ipykernel
+torchao==0.8.0
+# These weren't necessary for our setup, but I'm leaving this here in case they are needed for your setup
+# vllm@git+https://github.com/ekinakyurek/vllm.git@ekin/newvllm
+# torchtune@git+https://github.com/ekinakyurek/torchtune.git@ekin/currentrb
+# vllm@git@github.com:ekinakyurek/vllm.git@ekin/torchtunecompat
\ No newline at end of file
diff -ruN marc_original/test_time_train.py marc/test_time_train.py
--- marc_original/test_time_train.py	2025-02-28 19:58:37.902437737 -0500
+++ marc/test_time_train.py	2025-02-28 01:34:41.446927263 -0500
@@ -144,7 +144,7 @@
 )
 
 parser.add_argument(
-    "--no_transform", action="store_true", help="Whether to use the new format or not"
+    "--no_transform", action="store_true", help="Whether to use data augmentations or not"
 )
 
 
diff -ruN marc_original/third_party/torchtune/CITATION.cff marc/third_party/torchtune/CITATION.cff
--- marc_original/third_party/torchtune/CITATION.cff	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/CITATION.cff	2025-02-20 17:49:28.746022924 -0500
@@ -0,0 +1,9 @@
+cff-version: 1.2.0
+title: "torchtune: PyTorch's finetuning library"
+message: "If you use this software, please cite it as below."
+type: software
+authors:
+  - given-names: "torchtune maintainers and contributors"
+url: "https//github.com/pytorch/torchtune"
+license: "BSD-3-Clause"
+date-released: "2024-04-14"
diff -ruN marc_original/third_party/torchtune/CODE_OF_CONDUCT.md marc/third_party/torchtune/CODE_OF_CONDUCT.md
--- marc_original/third_party/torchtune/CODE_OF_CONDUCT.md	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/CODE_OF_CONDUCT.md	2025-02-20 17:49:28.746022924 -0500
@@ -0,0 +1,80 @@
+# Code of Conduct
+
+## Our Pledge
+
+In the interest of fostering an open and welcoming environment, we as
+contributors and maintainers pledge to make participation in our project and
+our community a harassment-free experience for everyone, regardless of age, body
+size, disability, ethnicity, sex characteristics, gender identity and expression,
+level of experience, education, socio-economic status, nationality, personal
+appearance, race, religion, or sexual identity and orientation.
+
+## Our Standards
+
+Examples of behavior that contributes to creating a positive environment
+include:
+
+* Using welcoming and inclusive language
+* Being respectful of differing viewpoints and experiences
+* Gracefully accepting constructive criticism
+* Focusing on what is best for the community
+* Showing empathy towards other community members
+
+Examples of unacceptable behavior by participants include:
+
+* The use of sexualized language or imagery and unwelcome sexual attention or
+advances
+* Trolling, insulting/derogatory comments, and personal or political attacks
+* Public or private harassment
+* Publishing others' private information, such as a physical or electronic
+address, without explicit permission
+* Other conduct which could reasonably be considered inappropriate in a
+professional setting
+
+## Our Responsibilities
+
+Project maintainers are responsible for clarifying the standards of acceptable
+behavior and are expected to take appropriate and fair corrective action in
+response to any instances of unacceptable behavior.
+
+Project maintainers have the right and responsibility to remove, edit, or
+reject comments, commits, code, wiki edits, issues, and other contributions
+that are not aligned to this Code of Conduct, or to ban temporarily or
+permanently any contributor for other behaviors that they deem inappropriate,
+threatening, offensive, or harmful.
+
+## Scope
+
+This Code of Conduct applies within all project spaces, and it also applies when
+an individual is representing the project or its community in public spaces.
+Examples of representing a project or community include using an official
+project e-mail address, posting via an official social media account, or acting
+as an appointed representative at an online or offline event. Representation of
+a project may be further defined and clarified by project maintainers.
+
+This Code of Conduct also applies outside the project spaces when there is a
+reasonable belief that an individual's behavior may have a negative impact on
+the project or its community.
+
+## Enforcement
+
+Instances of abusive, harassing, or otherwise unacceptable behavior may be
+reported by contacting the project team at <opensource-conduct@meta.com>. All
+complaints will be reviewed and investigated and will result in a response that
+is deemed necessary and appropriate to the circumstances. The project team is
+obligated to maintain confidentiality with regard to the reporter of an incident.
+Further details of specific enforcement policies may be posted separately.
+
+Project maintainers who do not follow or enforce the Code of Conduct in good
+faith may face temporary or permanent repercussions as determined by other
+members of the project's leadership.
+
+## Attribution
+
+This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
+available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html
+
+[homepage]: https://www.contributor-covenant.org
+
+For answers to common questions about this code of conduct, see
+https://www.contributor-covenant.org/faq
diff -ruN marc_original/third_party/torchtune/CONTRIBUTING.md marc/third_party/torchtune/CONTRIBUTING.md
--- marc_original/third_party/torchtune/CONTRIBUTING.md	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/CONTRIBUTING.md	2025-02-20 17:49:28.750022931 -0500
@@ -0,0 +1,189 @@
+# Contributing to torchtune
+We want to make contributing to this project as easy and transparent as possible.
+
+&nbsp;
+
+## Dev install
+You should first [fork](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo) the torchtune repository
+and then clone your forked repository. Make sure to keep your fork in sync with the torchtune repository over time.
+
+```git clone https://github.com/<YOUR_GITHUB_USER>/torchtune.git```
+
+Then navigate into the newly cloned repo and install dependencies needed for development.
+
+**Step 1:** [Install PyTorch](https://pytorch.org/get-started/locally/). torchtune is tested with the latest stable PyTorch release as well as the preview nightly version.
+
+
+**Step 2:** Install all the additional dependencies and dev dependencies in the local repo:
+
+```
+cd torchtune
+pip install -e ".[dev]"
+```
+
+&nbsp;
+
+## Contributing workflow
+We actively welcome your pull requests.
+
+1. Create your new branch from `main` in your forked repo, with a name describing the work you're completing e.g. `add-feature-x`.
+2. If you've added code that should be tested, add tests. Ensure all tests pass. See the [testing section](#testing) for more information.
+3. If you've changed APIs, [update the documentation](#updating-documentation).
+4. Make sure your [code lints](#coding-style).
+5. If you haven't already, complete the [Contributor License Agreement ("CLA")](#contributor-license-agreement-cla)
+
+&nbsp;
+
+## Testing
+torchtune contains three different types of tests: unit tests, recipe tests, and regression tests. These tests are distinguished by their complexity and the resources they require to run. Recipe tests and regression tests are explicitly marked via pytest.mark decorators and both require S3 access to download the requisite assets.
+
+- **Unit tests**
+  - These should be minimal tests runnable without remote access. (No large models, no downloading weights). Unit tests should be under [tests/torchtune](https://github.com/pytorch/torchtune/tree/main/tests/torchtune).
+  - All unit tests can be run via ```pytest tests```.
+- **Recipe tests**
+  - These are relatively small-scale integration tests for running our recipes. These include
+  both single-device recipes and distributed recipes. In the latter case, tests should be marked with the `@gpu_test` decorator to indicate how many GPUs they need to run.
+  - Recipe tests require remote access as (small) model weights will be downloaded from S3 to run them.
+  - Recipe tests are found under [tests/recipes](https://github.com/pytorch/torchtune/tree/main/tests/recipes) and should be marked with the `@pytest.mark.integration_test` decorator.
+  - To run only recipe tests, you can run `pytest tests -m integration_test`.
+- **Regression tests**
+  - These are the most heavyweight tests in the repo. They involve building a full model (i.e. 7B size or larger), then running some finetune and/or evaluation via a combination of tune CLI commands. Whereas an individual recipe test runtime is generally still O(seconds), integration tests should be O(minutes) or greater. Like recipe tests, regression tests also require S3 access.
+  - Regression tests are found under [tests/regression_tests](https://github.com/pytorch/torchtune/tree/main/tests/regression_tests) and should be marked with the `@pytest.mark.slow_integration_test` decorator.
+  - To run only regression tests, you can use the command `pytest tests -m slow_integration_test`.
+
+Whenever running tests in torchtune, favor using the command line flags as much as possible (e.g. run `pytest tests -m integration_test` over `pytest tests/recipes`). This is because (a) the default behavior is to run unit tests only (so you will miss recipe tests without the flag), and (b) using the flags ensures pytest will automatically download any remote assets needed for your test run.
+
+Note that the above flags can be combined with other pytest flags, so e.g. `pytest tests -m integration_test -k 'test_loss'` will run only recipe tests matching the substring `test_loss`.
+
+&nbsp;
+
+## Updating documentation
+Each API and class should be clearly documented. Well-documented code is easier to review and understand/extend. All documentation is contained in the [docs directory](docs/source):
+
+* All files following the pattern `api_ref_*` document top-level APIs.
+* All files under the [deep dives directory](docs/source/deep_dives) contain "deep-dive" tutorials
+* All files under the [tutorials directory](docs/source/tutorials) contain regular tutorials
+
+Documentation is written in [RST](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html) format.
+
+### Adding a new class/method to the API References
+Once you've added an API that is meant to be exposed publically, you should add it to the appropriate rst file. For example, any new API within the [configs/](torchtune/configs)
+directory should be added to `api_ref_configs.rst`, [data/](torchtune/data) should be added to `api_ref_data.rst`, [datasets](torchtune/datasets) should be added to
+`api_ref_datasets.rst`, and so on. To add, it's as simple as adding the name of the exposed API somewhere in the appropriate RST file.
+
+All code written within the docstring of the class or method will be correctly rendered there.
+
+> Note: Our RST theme expects code to be specified using double backticks instead of single. Eg: ``hidden_dim``. Single backticks will be rendered as italics instead of as "code".
+
+### Adding documentation for a recipe
+
+If you've contributed a new recipe, or you're interesting in adding documentation for an existing recipe, you can add a new page in [the recipes directory](docs/source/recipes). Please refer to existing recipe docpages to understand the format of these documentation pages. Broadly speaking:
+
+- Recipe documentation pages are like beefed up API references for recipes.
+- They should have a low noise/information ratio, i.e. information in the recipe documentation page should mostly be relevant for using that recipe.
+- Relevant information could include:
+  - A cookbook/manual-style description of all the ways in which the recipe can be modified. For instance, does it support different loss functions? If so, describe those loss functions and help a user understand when they might want to use them.
+  - Example commands for using and customizing the recipe, particularly w.r.t the specific knobs and levers unique to the recipe.
+  - Pre-requisites for the recipe including models and datasets.
+  - Reference outputs for a recipe to help a user understand what successful training looks like e.g. loss curves, eval results, generations, etc.
+  - References to the appropriate [memory optimization](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html) features which can be used in the recipe. If you've contributed new memory optimization features which could be used across other recipes, consider adding them to the overview!
+
+
+Finally, make sure you update the [recipe overview page](docs/source/recipes/recipes_overview.rst), and the [index sidebar](docs/source/index.rst).
+
+### Building docs
+
+All documentation is built for each PR and contains a preview on the PR. However, this takes awhile (~8 minutes) and you should first build docs from your local machine.
+
+From the [docs/](docs) directory:
+
+1. Install dependencies:
+
+```
+pip install -r requirements.txt
+```
+
+2. Run make command:
+
+```
+make html
+# Now open build/html/index.html
+```
+
+To avoid building the examples (which execute python code and can take time) you
+can use `make html-noplot`. To build a subset of specific examples instead of
+all of them, you can use a regex like `EXAMPLES_PATTERN="plot_the_best_example*"
+make html`.
+
+If the doc build starts failing for a weird reason, try `make clean`.
+
+#### Serving docs locally (if building from a GPU env)
+
+If you're developing locally, you can just open the generated `index.html` file in your browser.
+
+If instead you're using a remote machine, you can use a combination of a simple python HTTP server and port forwarding to serve the docs locally. This allows you to iterate on the documentation much more quickly than relying on PR previews.
+
+To do so, after following the above doc build steps, run the following from the `docs/build/html` folder:
+
+```
+python -m http.server 8000 # or any free port
+```
+
+This will open up a simple HTTP server serving the files in the build directory. If this is done on a remote machine, you can set up port forwarding from your local machine to access the server, for example:
+
+```
+ssh -L 9000:localhost:8000 $REMOTE_DEV_HOST
+```
+
+Now, you can navigate to `localhost:9000` on your local machine to view the rendered documentation.
+
+&nbsp;
+
+## Coding Style
+`torchtune` uses pre-commit hooks to ensure style consistency and prevent common mistakes. Enable it by:
+
+```
+pre-commit install
+```
+
+After this pre-commit hooks will be run before every commit.
+
+You can also run this manually on every file using:
+
+```
+pre-commit run --all-files
+```
+
+&nbsp;
+
+## Best Practices
+
+This section captures some best practices for contributing code to torchtune. Following these will make PR reviews easier.
+
+- **Modular Blocks instead of Monolithic Classes**. Stuffing all of the logic into a single class limits readability and makes it hard to reuse logic. Think about breaking the implementation into self-contained blocks which can be used independently from a given model. For example, attention mechanisms, embedding classes, transformer layers etc.
+- **Say no to Implementation Inheritance**. You really dont need it AND it makes the code much harder to understand or refactor since the logic is spread across many files/classes. Where needed, consider using Protocols.
+- **Clean Interfaces**. Theres nothing more challenging than reading through functions/constructors with ~100 parameters. Think carefully about what needs to be exposed to the user and dont hesitate to hard-code parameters until there is a need to make them configurable.
+- **Intrusive Configs**. Config objects should not intrude into the class implementation. Configs should interact with these classes through cleanly defined builder functions which convert the config into flat parameters needed to instantiate an object.
+- **Limit Generalization**. Attempting to generalize code before this is needed unnecessarily complicates implementations - you are anticipating use cases you dont know a lot about. When you actually need to generalize a component, think about whether its worth it to complicate a given interface to stuff in more functionality. Dont be afraid of code duplication if it makes things easier to read.
+- **Value Checks and Asserts**. Dont check values in higher level modules - defer the checks to the modules where the values are actually used. This helps reduce the number of raise statements in code which generally hurts readability, but are critical for correctness.
+
+&nbsp;
+
+## Issues
+We use GitHub issues to track public bugs. Please ensure your description is clear and has sufficient instructions to be able to reproduce the issue.
+
+Meta has a [bounty program](https://www.facebook.com/whitehat/) for the safe disclosure of security bugs. In those cases, please go through the process outlined on that page and do not file a public issue.
+
+&nbsp;
+
+## License
+By contributing to torchtune, you agree that your contributions will be licensed under the LICENSE file in the root directory of this source tree.
+
+&nbsp;
+
+## Contributor License Agreement ("CLA")
+In order to accept your pull request, we need you to submit a CLA. You only need to do this once to work on any of Meta's open source projects.
+
+Complete your CLA here: <https://code.facebook.com/cla>
+
+&nbsp;
diff -ruN marc_original/third_party/torchtune/docs/license_header.txt marc/third_party/torchtune/docs/license_header.txt
--- marc_original/third_party/torchtune/docs/license_header.txt	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/license_header.txt	2025-02-20 17:49:28.778022977 -0500
@@ -0,0 +1,5 @@
+Copyright (c) Meta Platforms, Inc. and affiliates.
+All rights reserved.
+
+This source code is licensed under the BSD-style license found in the
+LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/docs/Makefile marc/third_party/torchtune/docs/Makefile
--- marc_original/third_party/torchtune/docs/Makefile	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/Makefile	2025-02-20 17:49:28.774022970 -0500
@@ -0,0 +1,43 @@
+# Minimal makefile for Sphinx documentation
+#
+
+ifneq ($(EXAMPLES_PATTERN),)
+    EXAMPLES_PATTERN_OPTS := -D sphinx_gallery_conf.filename_pattern="$(EXAMPLES_PATTERN)"
+endif
+
+# You can set these variables from the command line.
+SPHINXOPTS    = -W -j auto -T -v $(EXAMPLES_PATTERN_OPTS)
+SPHINXBUILD   = sphinx-build
+SPHINXPROJ    = torchtune
+SOURCEDIR     = source
+BUILDDIR      = build
+
+# Put it first so that "make" without argument is like "make help".
+help:
+	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
+
+docset: html
+	doc2dash --name $(SPHINXPROJ) --icon $(SOURCEDIR)/_static/img/pytorch-logo-flame.png --enable-js --online-redirect-url http://pytorch.org/vision/ --force $(BUILDDIR)/html/
+
+	# Manually fix because Zeal doesn't deal well with `icon.png`-only at 2x resolution.
+	cp $(SPHINXPROJ).docset/icon.png $(SPHINXPROJ).docset/icon@2x.png
+	convert $(SPHINXPROJ).docset/icon@2x.png -resize 16x16 $(SPHINXPROJ).docset/icon.png
+
+html-noplot:  # Avoids running the gallery examples, which may take time
+	$(SPHINXBUILD) -D plot_gallery=0 -b html "${SOURCEDIR}" "$(BUILDDIR)"/html
+	@echo
+	@echo "Build finished. The HTML pages are in $(BUILDDIR)/html."
+
+clean:
+	rm -rf $(BUILDDIR)/*
+	rm -rf $(SOURCEDIR)/generated_examples/  # sphinx-gallery
+	rm -rf $(SOURCEDIR)/gen_modules/  # sphinx-gallery
+	rm -rf $(SOURCEDIR)/sg_execution_times.rst  # sphinx-gallery
+	rm -rf $(SOURCEDIR)/generated/  # autosummary
+
+.PHONY: help Makefile docset
+
+# Catch-all target: route all unknown targets to Sphinx using the new
+# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
+%: Makefile
+	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
diff -ruN marc_original/third_party/torchtune/docs/requirements.txt marc/third_party/torchtune/docs/requirements.txt
--- marc_original/third_party/torchtune/docs/requirements.txt	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/requirements.txt	2025-02-20 17:49:28.786022990 -0500
@@ -0,0 +1,7 @@
+sphinx-gallery>0.11
+sphinx==5.0.0
+sphinx_design
+sphinx_copybutton
+sphinx-tabs
+matplotlib
+-e git+https://github.com/pytorch/pytorch_sphinx_theme.git#egg=pytorch_sphinx_theme
diff -ruN marc_original/third_party/torchtune/docs/source/api_ref_config.rst marc/third_party/torchtune/docs/source/api_ref_config.rst
--- marc_original/third_party/torchtune/docs/source/api_ref_config.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/api_ref_config.rst	2025-02-20 17:49:28.942023246 -0500
@@ -0,0 +1,16 @@
+.. _config:
+
+================
+torchtune.config
+================
+
+.. currentmodule:: torchtune.config
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    instantiate
+    parse
+    validate
+    log_config
diff -ruN marc_original/third_party/torchtune/docs/source/api_ref_data.rst marc/third_party/torchtune/docs/source/api_ref_data.rst
--- marc_original/third_party/torchtune/docs/source/api_ref_data.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/api_ref_data.rst	2025-02-20 17:49:28.946023253 -0500
@@ -0,0 +1,96 @@
+.. _data:
+
+==============
+torchtune.data
+==============
+
+.. currentmodule:: torchtune.data
+
+.. _chat_formats:
+
+Text templates
+--------------
+
+Templates for instruct prompts and chat prompts. Includes some specific formatting for difference datasets
+and models.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    InstructTemplate
+    GrammarErrorCorrectionTemplate
+    SummarizeTemplate
+    QuestionAnswerTemplate
+    PromptTemplate
+    PromptTemplateInterface
+    ChatMLTemplate
+    ChatFormat
+
+Types
+-----
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    Message
+    Role
+
+Converters
+----------
+
+Converts data from common JSON formats into a torchtune :class:`Message`.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    get_sharegpt_messages
+    get_openai_messages
+
+.. _message_transforms_ref:
+
+Message transforms
+------------------
+
+Converts data from common schema and conversation JSON formats into a list of torchtune :class:`Message`.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    InputOutputToMessages
+    ShareGPTToMessages
+    OpenAIToMessages
+    ChosenRejectedToMessages
+    AlpacaToMessages
+
+Collaters
+---------
+
+Collaters used to collect samples into batches and handle any padding.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    padded_collate
+    padded_collate_tiled_images_and_mask
+    padded_collate_sft
+    padded_collate_dpo
+    left_pad_sequence
+
+Helper functions
+----------------
+
+Miscellaneous helper functions used in modifying data.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    validate_messages
+    truncate
+    load_image
+    format_content_with_images
diff -ruN marc_original/third_party/torchtune/docs/source/api_ref_datasets.rst marc/third_party/torchtune/docs/source/api_ref_datasets.rst
--- marc_original/third_party/torchtune/docs/source/api_ref_datasets.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/api_ref_datasets.rst	2025-02-20 17:49:28.946023253 -0500
@@ -0,0 +1,71 @@
+.. _datasets:
+
+==================
+torchtune.datasets
+==================
+
+.. currentmodule:: torchtune.datasets
+
+For a detailed general usage guide, please see our :ref:`datasets tutorial <dataset_tutorial_label>`.
+
+
+Text datasets
+------------------
+
+torchtune supports several widely used text-only datasets to help quickly bootstrap your fine-tuning.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    alpaca_dataset
+    alpaca_cleaned_dataset
+    grammar_dataset
+    hh_rlhf_helpful_dataset
+    samsum_dataset
+    slimorca_dataset
+    stack_exchange_paired_dataset
+    cnn_dailymail_articles_dataset
+    wikitext_dataset
+
+Image + Text datasets
+---------------------
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    multimodal.llava_instruct_dataset
+    multimodal.the_cauldron_dataset
+
+.. _dataset_builders:
+
+Generic dataset builders
+------------------------
+
+torchtune also supports generic dataset builders for common formats like chat models and instruct models.
+These are especially useful for specifying from a YAML config.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    instruct_dataset
+    chat_dataset
+    preference_dataset
+    text_completion_dataset
+
+Generic dataset classes
+-----------------------
+
+Class representations for the above dataset builders.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    TextCompletionDataset
+    ConcatDataset
+    PackedDataset
+    PreferenceDataset
+    SFTDataset
diff -ruN marc_original/third_party/torchtune/docs/source/api_ref_generation.rst marc/third_party/torchtune/docs/source/api_ref_generation.rst
--- marc_original/third_party/torchtune/docs/source/api_ref_generation.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/api_ref_generation.rst	2025-02-20 17:49:28.950023259 -0500
@@ -0,0 +1,17 @@
+.. _generation:
+
+====================
+torchtune.generation
+====================
+
+.. currentmodule:: torchtune.generation
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    generate
+    generate_next_token
+    sample
+    get_causal_mask_from_padding_mask
+    get_position_ids_from_padding_mask
diff -ruN marc_original/third_party/torchtune/docs/source/api_ref_models.rst marc/third_party/torchtune/docs/source/api_ref_models.rst
--- marc_original/third_party/torchtune/docs/source/api_ref_models.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/api_ref_models.rst	2025-02-20 17:49:28.954023266 -0500
@@ -0,0 +1,335 @@
+.. _models:
+
+================
+torchtune.models
+================
+
+.. currentmodule:: torchtune.models
+
+llama3.2
+--------
+
+Text-only models from the 3.2 version of `Llama3 family <https://llama.meta.com/llama3/>`_.
+
+Important: You need to request access on `Hugging Face <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct>`__ before downloading it.
+
+To download the Llama-3.2-1B-Instruct model:
+
+.. code-block:: bash
+
+    tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns "original/consolidated.00.pth" --hf-token <HF_TOKEN>
+
+To download the Llama-3.2-3B-Instruct model:
+
+.. code-block:: bash
+
+    tune download meta-llama/Llama-3.2-3B-Instruct --output-dir /tmp/Llama-3.2-3B-Instruct --ignore-patterns "original/consolidated*" --hf-token <HF_TOKEN>
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    llama3_2.llama3_2_1b
+    llama3_2.llama3_2_3b
+    llama3_2.lora_llama3_2_1b
+    llama3_2.lora_llama3_2_3b
+    llama3_2.qlora_llama3_2_1b
+    llama3_2.qlora_llama3_2_3b
+
+.. note::
+
+    The Llama3.2 tokenizer reuses the :class:`~torchtune.models.llama3.llama3_tokenizer` class.
+
+llama3.2 Vision
+---------------
+
+Vision-Language Models from the 3.2 version of `Llama3 family <https://llama.meta.com/llama3/>`_.
+
+Important: You need to request access on `Hugging Face <https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct>`__ before downloading it.
+
+To download the Llama-3.2-11B-Instruct model:
+
+.. code-block:: bash
+
+    tune download meta-llama/Llama-3.2-11B-Vision-Instruct --output-dir /tmp/Llama-3.2-11B-Vision-Instruct --hf-token <HF_TOKEN>
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    llama3_2_vision.llama3_2_vision_11b
+    llama3_2_vision.llama3_2_vision_transform
+    llama3_2_vision.lora_llama3_2_vision_11b
+    llama3_2_vision.qlora_llama3_2_vision_11b
+    llama3_2_vision.llama3_2_vision_decoder
+    llama3_2_vision.llama3_2_vision_encoder
+    llama3_2_vision.lora_llama3_2_vision_decoder
+    llama3_2_vision.lora_llama3_2_vision_encoder
+    llama3_2_vision.Llama3VisionEncoder
+    llama3_2_vision.Llama3VisionProjectionHead
+    llama3_2_vision.Llama3VisionTransform
+
+.. note::
+
+    The Llama3.2 tokenizer reuses the :class:`~torchtune.models.llama3.llama3_tokenizer` class.
+
+llama3 & llama3.1
+-----------------
+
+Models 3 and 3.1 from the `Llama3 family <https://llama.meta.com/llama3/>`_.
+
+Important: You need to request access on `Hugging Face <https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct>`__ before downloading it.
+
+To download the Llama3.1-8B-Instruct model:
+
+.. code-block:: bash
+
+    tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth" --hf-token <HF_TOKEN>
+
+To download the Llama3.1-70B-Instruct model:
+
+.. code-block:: bash
+
+    tune download meta-llama/Meta-Llama-3.1-70B-Instruct --output-dir /tmp/Meta-Llama-3.1-70B-Instruct --ignore-patterns "original/consolidated*" --hf-token <HF_TOKEN>
+
+To download the Llama3.1-405B-Instruct model:
+
+.. code-block:: bash
+
+    tune download meta-llama/Meta-Llama-3.1-405B-Instruct --ignore-patterns "original/consolidated*" --hf-token <HF_TOKEN>
+
+To download the Llama3 weights of the above models, you can instead download from `Meta-Llama-3-8B-Instruct` and
+`Meta-Llama-3-70B-Instruct`.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    llama3.llama3
+    llama3.lora_llama3
+    llama3.llama3_8b
+    llama3.lora_llama3_8b
+    llama3.qlora_llama3_8b
+    llama3.llama3_70b
+    llama3.lora_llama3_70b
+    llama3.qlora_llama3_70b
+    llama3.llama3_tokenizer
+
+    |
+
+    llama3_1.llama3_1
+    llama3_1.lora_llama3_1
+    llama3_1.llama3_1_8b
+    llama3_1.lora_llama3_1_8b
+    llama3_1.qlora_llama3_1_8b
+    llama3_1.llama3_1_70b
+    llama3_1.lora_llama3_1_70b
+    llama3_1.qlora_llama3_1_70b
+    llama3_1.llama3_1_405b
+    llama3_1.lora_llama3_1_405b
+    llama3_1.qlora_llama3_1_405b
+
+
+.. note::
+
+    The Llama3.1 tokenizer reuses the `llama3.llama3_tokenizer` builder class.
+
+llama2
+------
+
+All models from the `Llama2 family <https://llama.meta.com/llama2/>`_.
+
+Important: You need to request access on `Hugging Face <https://huggingface.co/meta-llama/Llama-2-7b-hf>`__ before downloading it.
+
+To download the Llama2-7B model:
+
+.. code-block:: bash
+
+   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+
+To download the Llama2-13B model:
+
+.. code-block:: bash
+
+    tune download meta-llama/Llama-2-13b-hf --output-dir /tmp/Llama-2-13b-hf --hf-token <HF_TOKEN>
+
+To download the Llama2-70B model:
+
+.. code-block:: bash
+
+    tune download meta-llama/Llama-2-70b-hf --output-dir /tmp/Llama-2-70b-hf --hf-token <HF_TOKEN>
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    llama2.llama2
+    llama2.lora_llama2
+    llama2.llama2_7b
+    llama2.lora_llama2_7b
+    llama2.qlora_llama2_7b
+    llama2.llama2_13b
+    llama2.lora_llama2_13b
+    llama2.qlora_llama2_13b
+    llama2.llama2_70b
+    llama2.lora_llama2_70b
+    llama2.qlora_llama2_70b
+    llama2.llama2_tokenizer
+    llama2.llama2_reward_7b
+    llama2.lora_llama2_reward_7b
+    llama2.qlora_llama2_reward_7b
+    llama2.Llama2ChatTemplate
+
+
+code llama
+----------
+
+Models from the `Code Llama family <https://arxiv.org/pdf/2308.12950>`_.
+
+Important: You need to request access on `Hugging Face <https://huggingface.co/meta-llama/CodeLlama-7b-hf>`__ before downloading it.
+
+To download the CodeLlama-7B model:
+
+.. code-block:: bash
+
+    tune download meta-llama/CodeLlama-7b-hf --output-dir /tmp/CodeLlama-7b-hf --hf-token <HF_TOKEN>
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    code_llama2.code_llama2_7b
+    code_llama2.lora_code_llama2_7b
+    code_llama2.qlora_code_llama2_7b
+    code_llama2.code_llama2_13b
+    code_llama2.lora_code_llama2_13b
+    code_llama2.qlora_code_llama2_13b
+    code_llama2.code_llama2_70b
+    code_llama2.lora_code_llama2_70b
+    code_llama2.qlora_code_llama2_70b
+
+qwen-2
+------
+
+Models of size 0.5B, 1.5B, and 7B from the `Qwen2 family <https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f>`_.
+
+To download the Qwen2 1.5B model, for example:
+
+.. code-block:: bash
+
+    tune download Qwen/Qwen2-1.5B-Instruct --output-dir /tmp/Qwen2-1.5B-Instruct --ignore-patterns None
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    qwen2.qwen2
+    qwen2.lora_qwen2
+    qwen2.qwen2_7b
+    qwen2.qwen2_0_5b
+    qwen2.qwen2_1_5b
+    qwen2.lora_qwen2_7b
+    qwen2.lora_qwen2_0_5b
+    qwen2.lora_qwen2_1_5b
+    qwen2.qwen2_tokenizer
+
+phi-3
+-----
+
+Models from the `Phi-3 mini family <https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/>`_.
+
+To download the Phi-3 Mini 4k instruct model:
+
+.. code-block:: bash
+
+    tune download microsoft/Phi-3-mini-4k-instruct --output-dir /tmp/Phi-3-mini-4k-instruct --ignore-patterns None --hf-token <HF_TOKEN>
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    phi3.phi3
+    phi3.lora_phi3
+    phi3.phi3_mini
+    phi3.lora_phi3_mini
+    phi3.qlora_phi3_mini
+    phi3.phi3_mini_tokenizer
+
+mistral
+-------
+
+All models from `Mistral AI family <https://mistral.ai/technology/#models>`_.
+
+Important: You need to request access on `Hugging Face <https://huggingface.co/mistralai/Mistral-7B-v0.1>`__ to download this model.
+
+To download the Mistral 7B v0.1 model:
+
+.. code-block:: bash
+
+    tune download mistralai/Mistral-7B-v0.1 --output-dir /tmp/Mistral-7B-v0.1 --hf-token <HF_TOKEN>
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    mistral.mistral
+    mistral.lora_mistral
+    mistral.mistral_classifier
+    mistral.lora_mistral_classifier
+    mistral.mistral_7b
+    mistral.lora_mistral_7b
+    mistral.qlora_mistral_7b
+    mistral.mistral_reward_7b
+    mistral.lora_mistral_reward_7b
+    mistral.qlora_mistral_reward_7b
+    mistral.mistral_tokenizer
+    mistral.MistralChatTemplate
+
+
+gemma
+-----
+
+Models of size 2B and 7B from the `Gemma family <https://blog.google/technology/developers/gemma-open-models/>`_.
+
+Important: You need to request access on `Hugging Face <https://huggingface.co/google/gemma-2b>`__ to use this model.
+
+To download the Gemma 2B model (not Gemma2):
+
+.. code-block:: bash
+
+    tune download google/gemma-2b --ignore-patterns "gemma-2b.gguf"  --hf-token <HF_TOKEN>
+
+To download the Gemma 7B model:
+
+.. code-block:: bash
+
+    tune download google/gemma-7b --ignore-patterns "gemma-7b.gguf"  --hf-token <HF_TOKEN>
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    gemma.gemma
+    gemma.lora_gemma
+    gemma.gemma_2b
+    gemma.lora_gemma_2b
+    gemma.qlora_gemma_2b
+    gemma.gemma_7b
+    gemma.lora_gemma_7b
+    gemma.qlora_gemma_7b
+    gemma.gemma_tokenizer
+
+
+clip
+-----
+
+Vision components to support multimodality using `CLIP encoder <https://arxiv.org/abs/2103.00020>`_.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    clip.clip_vision_encoder
+    clip.TokenPositionalEmbedding
+    clip.TiledTokenPositionalEmbedding
+    clip.TilePositionalEmbedding
diff -ruN marc_original/third_party/torchtune/docs/source/api_ref_modules.rst marc/third_party/torchtune/docs/source/api_ref_modules.rst
--- marc_original/third_party/torchtune/docs/source/api_ref_modules.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/api_ref_modules.rst	2025-02-20 17:49:28.958023273 -0500
@@ -0,0 +1,117 @@
+=================
+torchtune.modules
+=================
+
+.. currentmodule:: torchtune.modules
+
+Modeling Components and Building Blocks
+---------------------------------------
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    MultiHeadAttention
+    FeedForward
+    KVCache
+    RotaryPositionalEmbeddings
+    RMSNorm
+    Fp32LayerNorm
+    TanhGate
+    TiedLinear
+    TransformerSelfAttentionLayer
+    TransformerCrossAttentionLayer
+    TransformerDecoder
+    VisionTransformer
+
+Losses
+------
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    loss.CEWithChunkedOutputLoss
+    loss.ForwardKLLoss
+    loss.ForwardKLWithChunkedOutputLoss
+
+Base Tokenizers
+---------------
+Base tokenizers are tokenizer models that perform the direct encoding of text
+into token IDs and decoding of token IDs into text. These are typically `byte pair
+encodings <https://en.wikipedia.org/wiki/Byte_pair_encoding>`_ that underlie the
+model specific tokenizers.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    tokenizers.SentencePieceBaseTokenizer
+    tokenizers.TikTokenBaseTokenizer
+    tokenizers.ModelTokenizer
+    tokenizers.BaseTokenizer
+
+Tokenizer Utilities
+-------------------
+These are helper methods that can be used by any tokenizer.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    tokenizers.tokenize_messages_no_special_tokens
+    tokenizers.parse_hf_tokenizer_json
+
+
+PEFT Components
+---------------
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    peft.LoRALinear
+    peft.AdapterModule
+    peft.get_adapter_params
+    peft.set_trainable_params
+    peft.validate_missing_and_unexpected_for_lora
+    peft.validate_state_dict_for_lora
+    peft.disable_adapter
+
+
+Fusion Components
+-----------------
+Components for building models that are a fusion of two+ pre-trained models.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    model_fusion.DeepFusionModel
+    model_fusion.FusionLayer
+    model_fusion.FusionEmbedding
+    model_fusion.register_fusion_module
+    model_fusion.get_fusion_params
+
+
+Module Utilities
+------------------
+These are utilities that are common to and can be used by all modules.
+
+.. autosummary::
+   :toctree: generated/
+   :nosignatures:
+
+   common_utils.reparametrize_as_dtype_state_dict_post_hook
+
+
+Vision Transforms
+------------------
+Functions used for preprocessing images.
+
+.. autosummary::
+   :toctree: generated/
+   :nosignatures:
+
+    transforms.Transform
+    transforms.VisionCrossAttentionMask
diff -ruN marc_original/third_party/torchtune/docs/source/api_ref_rlhf.rst marc/third_party/torchtune/docs/source/api_ref_rlhf.rst
--- marc_original/third_party/torchtune/docs/source/api_ref_rlhf.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/api_ref_rlhf.rst	2025-02-20 17:49:28.962023280 -0500
@@ -0,0 +1,19 @@
+===============
+torchtune.rlhf
+===============
+
+.. currentmodule:: torchtune.rlhf
+
+Components and losses for RLHF algorithms like PPO and DPO.
+
+.. autosummary::
+   :toctree: generated/
+   :nosignatures:
+
+    estimate_advantages
+    get_rewards_ppo
+    truncate_sequence_at_first_stop_token
+    loss.PPOLoss
+    loss.DPOLoss
+    loss.RSOLoss
+    loss.SimPOLoss
diff -ruN marc_original/third_party/torchtune/docs/source/api_ref_training.rst marc/third_party/torchtune/docs/source/api_ref_training.rst
--- marc_original/third_party/torchtune/docs/source/api_ref_training.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/api_ref_training.rst	2025-02-20 17:49:28.966023285 -0500
@@ -0,0 +1,131 @@
+==================
+torchtune.training
+==================
+
+.. currentmodule:: torchtune.training
+
+.. _checkpointing_label:
+
+Checkpointing
+-------------
+
+torchtune offers checkpointers to allow seamless transitioning between checkpoint formats for training and interoperability with the rest of the ecosystem. For a comprehensive overview of
+checkpointing, please see the :ref:`checkpointing deep-dive <understand_checkpointer>`.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    FullModelHFCheckpointer
+    FullModelMetaCheckpointer
+    FullModelTorchTuneCheckpointer
+    ModelType
+    FormattedCheckpointFiles
+    update_state_dict_for_classifier
+
+.. _mp_label:
+
+Reduced Precision
+------------------
+
+Utilities for working in a reduced precision setting.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    get_dtype
+    set_default_dtype
+    validate_expected_param_dtype
+    get_quantizer_mode
+
+.. _dist_label:
+
+Distributed
+-----------
+
+Utilities for enabling and working with distributed training.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    FSDPPolicyType
+    init_distributed
+    is_distributed
+    get_world_size_and_rank
+    get_full_finetune_fsdp_wrap_policy
+    lora_fsdp_wrap_policy
+
+.. _ac_label:
+
+Memory Management
+-----------------
+
+Utilities to reduce memory consumption during training.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    apply_selective_activation_checkpointing
+    set_activation_checkpointing
+    OptimizerInBackwardWrapper
+    create_optim_in_bwd_wrapper
+    register_optim_in_bwd_hooks
+
+.. _lr_scheduler_label:
+
+Schedulers
+----------
+
+Utilities to control lr during the training process.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    get_cosine_schedule_with_warmup
+
+.. _metric_logging_label:
+
+Metric Logging
+--------------
+
+Various logging utilities.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    metric_logging.CometLogger
+    metric_logging.WandBLogger
+    metric_logging.TensorBoardLogger
+    metric_logging.StdoutLogger
+    metric_logging.DiskLogger
+
+.. _perf_profiling_label:
+
+Performance and Profiling
+-------------------------
+
+torchtune provides utilities to profile and debug the memory and performance
+of your finetuning job.
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    get_memory_stats
+    log_memory_stats
+    setup_torch_profiler
+
+Miscellaneous
+-------------
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    get_unmasked_sequence_lengths
+    set_seed
diff -ruN marc_original/third_party/torchtune/docs/source/api_ref_utilities.rst marc/third_party/torchtune/docs/source/api_ref_utilities.rst
--- marc_original/third_party/torchtune/docs/source/api_ref_utilities.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/api_ref_utilities.rst	2025-02-20 17:49:28.970023292 -0500
@@ -0,0 +1,20 @@
+===============
+torchtune.utils
+===============
+
+.. currentmodule:: torchtune.utils
+
+
+.. _gen_label:
+
+Miscellaneous
+-------------
+
+.. autosummary::
+    :toctree: generated/
+    :nosignatures:
+
+    batch_to_device
+    get_device
+    get_logger
+    torch_version_ge
diff -ruN marc_original/third_party/torchtune/docs/source/basics/chat_datasets.rst marc/third_party/torchtune/docs/source/basics/chat_datasets.rst
--- marc_original/third_party/torchtune/docs/source/basics/chat_datasets.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/chat_datasets.rst	2025-02-20 17:49:28.974023299 -0500
@@ -0,0 +1,361 @@
+.. _chat_dataset_usage_label:
+
+=============
+Chat Datasets
+=============
+
+Chat datasets involve multi-turn conversations (multiple back-and-forths) between user and assistant.
+
+.. code-block:: python
+
+    [
+        {"role": "user", "content": "What is the answer to the ultimate question of life?"},
+        {"role": "assistant", "content": "The answer is 42."},
+        {"role": "user", "content": "That's ridiculous"},
+        {"role": "assistant", "content": "Oh I know."},
+    ]
+
+This is more structured than freeform text association that models are typically pre-trained with,
+where they learn to simply predict the next token instead of responding accurately to the user.
+
+The primary entry point for fine-tuning with chat datasets in torchtune is the :func:`~torchtune.datasets.chat_dataset`
+builder. This lets you specify a local or Hugging Face dataset that follows the chat data format
+directly from the config and train your LLM on it.
+
+.. _example_chat:
+
+Example chat dataset
+--------------------
+
+.. code-block:: python
+
+    # data/my_data.json
+    [
+        {
+            "conversations": [
+                {
+                    "from": "human",
+                    "value": "What is the answer to life?"
+                },
+                {
+                    "from": "gpt",
+                    "value": "The answer is 42."
+                },
+                {
+                    "from": "human",
+                    "value": "That's ridiculous"
+                },
+                {
+                    "from": "gpt",
+                    "value": "Oh I know."
+                }
+            ]
+        }
+    ]
+
+.. code-block:: python
+
+    from torchtune.models.mistral import mistral_tokenizer
+    from torchtune.datasets import chat_dataset
+
+    m_tokenizer = mistral_tokenizer(
+        path="/tmp/Mistral-7B-v0.1/tokenizer.model",
+        prompt_template="torchtune.models.mistral.MistralChatTemplate",
+        max_seq_len=8192,
+    )
+    ds = chat_dataset(
+        tokenizer=m_tokenizer,
+        source="json",
+        data_files="data/my_data.json",
+        split="train",
+        conversation_column="conversations",
+        conversation_style="sharegpt",
+        # By default, user prompt is ignored in loss. Set to True to include it
+        train_on_input=True,
+        new_system_prompt=None,
+    )
+    tokenized_dict = ds[0]
+    tokens, labels = tokenized_dict["tokens"], tokenized_dict["labels"]
+    print(m_tokenizer.decode(tokens))
+    # [INST] What is the answer to life?  [/INST] The answer is 42. [INST] That's ridiculous  [/INST] Oh I know.
+    print(labels)
+    # [1, 733, 16289, 28793, 1824, 349, 272, 4372, ...]
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.mistral.mistral_tokenizer
+      path: /tmp/Mistral-7B-v0.1/tokenizer.model
+      prompt_template: torchtune.models.mistral.MistralChatTemplate
+      max_seq_len: 8192
+
+    dataset:
+      _component_: torchtune.datasets.chat_dataset
+      source: json
+      data_files: data/my_data.json
+      split: train
+      conversation_column: conversations
+      conversation_style: sharegpt
+      train_on_input: True
+      new_system_prompt: null
+
+Chat dataset format
+-------------------
+
+Chat datasets typically have a single column named "conversations" or "messages" that contains a list of messages on a single topic
+per sample. The list of messages could include a system prompt, multiple turns between user and assistant, and tool calls/returns.
+
+.. code-block:: text
+
+    |  conversations                                               |
+    |--------------------------------------------------------------|
+    | [{"role": "user", "content": "What day is today?"},          |
+    |  {"role": "assistant", "content": "It is Tuesday."}]         |
+    | [{"role": "user", "content": "What about tomorrow?"},        |
+    |  {"role": "assistant", "content": "Tomorrow is Wednesday."}] |
+
+As an example, you can see the schema of the `SlimOrca dataset <https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup>`_.
+
+Loading chat datasets from Hugging Face
+---------------------------------------
+
+You need to pass in the dataset repo name to ``source``, select one of the conversation styles in ``conversation_style``, and specify the ``conversation_column``.
+For most HF datasets, you will also need to specify the ``split``.
+
+.. code-block:: python
+
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import chat_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = chat_dataset(
+        tokenizer=g_tokenizer,
+        source="Open-Orca/SlimOrca-Dedup",
+        conversation_column="conversations",
+        conversation_style="sharegpt",
+        split="train",
+    )
+
+.. code-block:: yaml
+
+    # Tokenizer is passed into the dataset in the recipe
+    dataset:
+      _component_: torchtune.datasets.chat_dataset
+      source: Open-Orca/SlimOrca-Dedup
+      conversation_column: conversations
+      conversation_style: sharegpt
+      split: train
+
+
+Loading local and remote chat datasets
+--------------------------------------
+
+To load in a local or remote dataset via https that has conversational data, you need to additionally specify the ``data_files`` and ``split``
+arguments. See Hugging Face's ``load_dataset`` `documentation <https://huggingface.co/docs/datasets/main/en/loading#local-and-remote-files>`_
+for more details on loading local or remote files.
+
+.. code-block:: python
+
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import chat_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = chat_dataset(
+        tokenizer=g_tokenizer,
+        source="json",
+        conversation_column="conversations",
+        conversation_style="sharegpt",
+        data_files="data/my_data.json",
+        split="train",
+    )
+
+.. code-block:: yaml
+
+    # Tokenizer is passed into the dataset in the recipe
+    dataset:
+      _component_: torchtune.datasets.chat_dataset
+      source: json
+      conversation_column: conversations
+      conversation_style: sharegpt
+      data_files: data/my_data.json
+      split: train
+
+Specifying conversation style
+-----------------------------
+
+The structure of the conversation in the raw dataset can vary widely with different role names and different fields
+indicating the message content name. There are a few standardized formats that are common across many datasets.
+We have built-in converters to convert these standardized formats into a list of torchtune :class:`~torchtune.data.Message`
+that follows this format:
+
+.. code-block:: python
+
+    [
+        {
+            "role": "system" | "user" | "assistant" | "ipython",
+            "content": <message>,
+        },
+        ...
+    ]
+
+.. _sharegpt:
+
+``"sharegpt"``
+^^^^^^^^^^^^^^
+The associated message transform is :class:`~torchtune.data.ShareGPTToMessages`. The expected format is:
+
+.. code-block:: python
+
+    {
+        "conversations": [
+            {
+                "from": "system" | "human" | "gpt",
+                "value": <message>,
+            },
+            ...
+        ]
+    }
+
+You can specify ``conversation_style=sharegpt`` in code or config:
+
+.. code-block:: python
+
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import chat_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = chat_dataset(
+        tokenizer=g_tokenizer,
+        source="json",
+        conversation_column="conversations",
+        conversation_style="sharegpt",
+        data_files="data/my_data.json",
+        split="train",
+    )
+
+.. code-block:: yaml
+
+    # Tokenizer is passed into the dataset in the recipe
+    dataset:
+      _component_: torchtune.datasets.chat_dataset
+      source: json
+      conversation_column: conversations
+      conversation_style: sharegpt
+      data_files: data/my_data.json
+      split: train
+
+``"openai"``
+^^^^^^^^^^^^
+The associated message transform is :class:`~torchtune.data.OpenAIToMessages`. The expected format is:
+
+.. code-block:: python
+
+    {
+        "messages": [
+            {
+                "role": "system" | "user" | "assistant",
+                "content": <message>,
+            },
+            ...
+        ]
+    }
+
+You can specify ``conversation_style=openai`` in code or config:
+
+.. code-block:: python
+
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import chat_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = chat_dataset(
+        tokenizer=g_tokenizer,
+        source="json",
+        conversation_column="conversations",
+        conversation_style="openai",
+        data_files="data/my_data.json",
+        split="train",
+    )
+
+.. code-block:: yaml
+
+    # Tokenizer is passed into the dataset in the recipe
+    dataset:
+      _component_: torchtune.datasets.chat_dataset
+      source: json
+      conversation_column: conversations
+      conversation_style: openai
+      data_files: data/my_data.json
+      split: train
+
+If your dataset does not fit one of the above conversation styles, then you will need to create a custom message transform.
+
+
+Renaming columns
+----------------
+
+To specify the column that contains your conversation data, use ``conversation_column``.
+
+.. code-block:: python
+
+    # data/my_data.json
+    [
+        {
+            "dialogue": [
+                {
+                    "from": "human",
+                    "value": "What is the answer to life?"
+                },
+                {
+                    "from": "gpt",
+                    "value": "The answer is 42."
+                },
+                {
+                    "from": "human",
+                    "value": "That's ridiculous"
+                },
+                {
+                    "from": "gpt",
+                    "value": "Oh I know."
+                }
+            ]
+        }
+    ]
+
+.. code-block:: python
+
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import chat_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = chat_dataset(
+        tokenizer=g_tokenizer,
+        source="json",
+        conversation_column="dialogue",
+        conversation_style="sharegpt",
+        data_files="data/my_data.json",
+        split="train",
+    )
+
+.. code-block:: yaml
+
+    # Tokenizer is passed into the dataset in the recipe
+    dataset:
+      _component_: torchtune.datasets.chat_dataset
+      source: json
+      conversation_column: dialogue
+      conversation_style: sharegpt
+      data_files: data/my_data.json
+      split: train
+
+
+Chat templates
+--------------
+
+Chat templates are defined the same way as instruct templates in :func:`~torchtune.datasets.instruct_dataset`. See :ref:`instruct_template` for more info.
+
+
+Built-in chat datasets
+----------------------
+- :class:`~torchtune.datasets.slimorca_dataset`
diff -ruN marc_original/third_party/torchtune/docs/source/basics/datasets_overview.rst marc/third_party/torchtune/docs/source/basics/datasets_overview.rst
--- marc_original/third_party/torchtune/docs/source/basics/datasets_overview.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/datasets_overview.rst	2025-02-20 17:49:28.978023305 -0500
@@ -0,0 +1,35 @@
+.. _datasets_overview:
+
+=================
+Datasets Overview
+=================
+torchtune lets you fine-tune LLMs and VLMs using any dataset found on Hugging Face Hub, downloaded locally,
+or on a remote url. We provide built-in dataset builders to help you quickly bootstrap your fine-tuning project
+for workflows including instruct tuning, preference alignment, continued pretraining, and more. Beyond those, torchtune
+enables full customizability on your dataset pipeline, letting you train on any data format or schema.
+
+The following tasks are supported:
+
+- Text supervised fine-tuning
+    - :ref:`instruct_dataset_usage_label`
+    - :ref:`chat_dataset_usage_label`
+- Multimodal supervised fine-tuning
+    - :ref:`multimodal_dataset_usage_label`
+- RLHF
+    - :ref:`preference_dataset_usage_label`
+- Continued pre-training
+    - :ref:`text_completion_dataset_usage_label`
+
+Data pipeline
+-------------
+.. image:: /_static/img/torchtune_datasets.svg
+
+From raw data samples to the model inputs in the training recipe, all torchtune datasets follow
+the same pipeline:
+
+1. Raw data is queried one sample at a time from a Hugging Face dataset, local file, or remote file
+2. :ref:`message_transform_usage_label` convert the raw sample which can take any format into a list of torchtune
+   :ref:`messages_usage_label`. Images are contained in the message object they are associated with.
+3. :ref:`model_transform_usage_label` applies model-specific transforms to the messages, including tokenization (see :ref:`tokenizers_usage_label`),
+   prompt templating (see :ref:`prompt_templates_usage_label`), image transforms, and anything else required for that particular model.
+4. The collater packages the processed samples together in a batch and the batch is passed into the model during training.
diff -ruN marc_original/third_party/torchtune/docs/source/basics/instruct_datasets.rst marc/third_party/torchtune/docs/source/basics/instruct_datasets.rst
--- marc_original/third_party/torchtune/docs/source/basics/instruct_datasets.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/instruct_datasets.rst	2025-02-20 17:49:28.982023312 -0500
@@ -0,0 +1,225 @@
+.. _instruct_dataset_usage_label:
+
+=================
+Instruct Datasets
+=================
+
+Instruction tuning involves training an LLM to perform specific task(s). This typically takes the form
+of a user command or prompt and the assistant's response, along with an optional system prompt that
+describes the task at hand. This is more structured than freeform text association that models are
+typically pre-trained with, where they learn to specifically predict the next token instead of completing
+the task.
+
+The primary entry point for fine-tuning with instruct datasets in torchtune is the :func:`~torchtune.datasets.instruct_dataset`
+builder. This lets you specify a local or Hugging Face dataset that follows the instruct data format
+directly from the config and train your LLM on it.
+
+.. _example_instruct:
+
+Example instruct dataset
+------------------------
+
+Here is an example of an instruct dataset to fine-tune for a grammar correction task.
+
+.. code-block:: bash
+
+    head data/my_data.csv
+    # incorrect,correct
+    # This are a cat,This is a cat.
+
+.. code-block:: python
+
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import instruct_dataset
+
+    g_tokenizer = gemma_tokenizer(
+        path="/tmp/gemma-7b/tokenizer.model",
+        prompt_template="torchtune.data.GrammarErrorCorrectionTemplate",
+        max_seq_len=8192,
+    )
+    ds = instruct_dataset(
+        tokenizer=g_tokenizer,
+        source="csv",
+        data_files="data/my_data.csv",
+        split="train",
+        # By default, user prompt is ignored in loss. Set to True to include it
+        train_on_input=True,
+        # Prepend a system message to every sample
+        new_system_prompt="You are an AI assistant. ",
+        # Use columns in our dataset instead of default
+        column_map={"input": "incorrect", "output": "correct"},
+    )
+    tokenized_dict = ds[0]
+    tokens, labels = tokenized_dict["tokens"], tokenized_dict["labels"]
+    print(g_tokenizer.decode(tokens))
+    # You are an AI assistant. Correct this to standard English:This are a cat---\nCorrected:This is a cat.
+    print(labels)  # System message is masked out, but not user message
+    # [-100, -100, -100, -100, -100, -100, 27957, 736, 577, ...]
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.gemma.gemma_tokenizer
+      path: /tmp/gemma-7b/tokenizer.model
+      prompt_template: torchtune.data.GrammarErrorCorrectionTemplate
+      max_seq_len: 8192
+
+    dataset:
+      source: csv
+      data_files: data/my_data.csv
+      split: train
+      train_on_input: True
+      new_system_prompt: You are an AI assistant.
+      column_map:
+        input: incorrect
+        output: correct
+
+Instruct dataset format
+-----------------------
+
+Instruct datasets are expected to follow an input-output format, where the user prompt is in one column
+and the assistant prompt is in another column.
+
+.. code-block:: text
+
+    |  input          |  output          |
+    |-----------------|------------------|
+    | "user prompt"   | "model response" |
+
+As an example, you can see the schema of the `C4 200M dataset <https://huggingface.co/datasets/liweili/c4_200m>`_.
+
+
+Loading instruct datasets from Hugging Face
+-------------------------------------------
+
+You simply need to pass in the dataset repo name to ``source``, which is then passed into Hugging Face's ``load_dataset``.
+For most datasets, you will also need to specify the ``split``.
+
+.. code-block:: python
+
+    # In code
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import instruct_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = instruct_dataset(
+        tokenizer=g_tokenizer,
+        source="liweili/c4_200m",
+        split="train"
+    )
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.gemma.gemma_tokenizer
+      path: /tmp/gemma-7b/tokenizer.model
+
+    # Tokenizer is passed into the dataset in the recipe
+    dataset:
+      _component_: torchtune.datasets.instruct_dataset
+      source: liweili/c4_200m
+      split: train
+
+This will use the default column names "input" and "output". To change the column names, use the ``column_map`` argument (see :ref:`column_map`).
+
+Loading local and remote instruct datasets
+------------------------------------------
+
+To load in a local or remote dataset via https that follows the instruct format, you need to specify the ``source``, ``data_files`` and ``split``
+arguments. See Hugging Face's ``load_dataset`` `documentation <https://huggingface.co/docs/datasets/main/en/loading#local-and-remote-files>`_
+for more details on loading local or remote files.
+
+.. code-block:: python
+
+    # In code
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import instruct_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = instruct_dataset(
+        tokenizer=g_tokenizer,
+        source="json",
+        data_files="data/my_data.json",
+        split="train",
+    )
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.gemma.gemma_tokenizer
+      path: /tmp/gemma-7b/tokenizer.model
+
+    # Tokenizer is passed into the dataset in the recipe
+    dataset:
+      _component_: torchtune.datasets.instruct_dataset
+      source: json
+      data_files: data/my_data.json
+      split: train
+
+.. _column_map:
+
+Renaming columns
+----------------
+
+You can remap the default column names to the column names in your dataset by specifying
+``column_map`` as ``{"<default column>": "<column in your dataset>"}``. The default column names
+are detailed in each of the dataset builders (see :func:`~torchtune.datasets.instruct_dataset` and
+:func:`~torchtune.datasets.chat_dataset` as examples).
+
+For example, if the default column names are "input", "output" and you need to change them to something else,
+such as "prompt", "response", then ``column_map = {"input": "prompt", "output": "response"}``.
+
+.. code-block:: python
+
+    # data/my_data.json
+    [
+        {"prompt": "hello world", "response": "bye world"},
+        {"prompt": "are you a robot", "response": "no, I am an AI assistant"},
+        ...
+    ]
+
+.. code-block:: python
+
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import instruct_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = instruct_dataset(
+        tokenizer=g_tokenizer,
+        source="json",
+        data_files="data/my_data.json",
+        split="train",
+        column_map={"input": "prompt", "output": "response"},
+    )
+
+.. code-block:: yaml
+
+    # Tokenizer is passed into the dataset in the recipe
+    dataset:
+      _component_: torchtune.datasets.instruct_dataset
+      source: json
+      data_files: data/my_data.json
+      split: train
+      column_map:
+        input: prompt
+        output: response
+
+.. _instruct_template:
+
+Instruct templates
+------------------
+
+Typically for instruct datasets, you will want to add a :class:`~torchtune.data.PromptTemplate` to provide task-relevant
+information. For example, for a grammar correction task, we may want to use a prompt template like :class:`~torchtune.data.GrammarErrorCorrectionTemplate`
+to structure each of our samples. Prompt templates are passed into the tokenizer and automatically applied to the dataset
+you are fine-tuning on. See :ref:`using_prompt_templates` for more details.
+
+
+Built-in instruct datasets
+--------------------------
+- :class:`~torchtune.datasets.alpaca_dataset`
+- :class:`~torchtune.datasets.grammar_dataset`
+- :class:`~torchtune.datasets.samsum_dataset`
diff -ruN marc_original/third_party/torchtune/docs/source/basics/messages.rst marc/third_party/torchtune/docs/source/basics/messages.rst
--- marc_original/third_party/torchtune/docs/source/basics/messages.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/messages.rst	2025-02-20 17:49:28.986023319 -0500
@@ -0,0 +1,240 @@
+.. _messages_usage_label:
+
+========
+Messages
+========
+
+Messages are a core component in torchtune that govern how text and multimodal content is tokenized. It serves as the common interface
+for all tokenizer and datasets APIs to operate on. Messages contain information about the text content, which role is sending the text
+content, and other information relevant for special tokens in model tokenizers. For more information about the individual parameters
+for Messages, see the API ref for :class:`~torchtune.data.Message`.
+
+.. _creating_messages:
+
+Creating Messages
+-----------------
+
+Messages can be created via the standard class constructor or directly from a dictionary.
+
+.. code-block:: python
+
+    from torchtune.data import Message
+
+    msg = Message(
+        role="user",
+        content="Hello world!",
+        masked=True,
+        eot=True,
+        ipython=False,
+    )
+    # This is identical
+    msg = Message.from_dict(
+        {
+            "role": "user",
+            "content": "Hello world!",
+            "masked": True,
+            "eot": True,
+            "ipython": False,
+        },
+    )
+    print(msg.content)
+    # [{'type': 'text', 'content': 'Hello world!'}]
+
+Content is formatted as a list of dictionaries. This is because Messages can also contain multimodal content, such as images.
+
+Images in Messages
+^^^^^^^^^^^^^^^^^^
+For multimodal datasets, you need to add the image as a :class:`~PIL.Image.Image` to the corresponding :class:`~torchtune.data.Message`.
+To add it to the beginning of the message, simply prepend it to the content list.
+
+.. code-block:: python
+
+    import PIL
+    from torchtune.data import Message
+
+    img_msg = Message(
+        role="user",
+        content=[
+            {
+                "type": "image",
+                # Place your image here
+                "content": PIL.Image.new(mode="RGB", size=(4, 4)),
+            },
+            {"type": "text", "content": "What's in this image?"},
+        ],
+    )
+
+This will indicate to the model tokenizers where to add the image special token and will be processed by the model transform
+appropriately.
+
+In many cases, you will have an image path instead of a raw :class:`~PIL.Image.Image`. You can use the :func:`~torchtune.data.load_image`
+utility for both local paths and remote paths.
+
+.. code-block:: python
+
+    import PIL
+    from torchtune.data import Message, load_image
+
+    image_path = "path/to/image.jpg"
+    img_msg = Message(
+        role="user",
+        content=[
+            {
+                "type": "image",
+                # Place your image here
+                "content": load_image(image_path),
+            },
+            {"type": "text", "content": "What's in this image?"},
+        ],
+    )
+
+If your dataset contain image tags, or placeholder text to indicate where in the text the image should be inserted,
+you can use the :func:`~torchtune.data.format_content_with_images` to split the text into the correct content list
+that you can pass into the content field of Message.
+
+.. code-block:: python
+
+    import PIL
+    from torchtune.data import format_content_with_images
+
+    content = format_content_with_images(
+        "<|image|>hello <|image|>world",
+        image_tag="<|image|>",
+        images=[PIL.Image.new(mode="RGB", size=(4, 4)), PIL.Image.new(mode="RGB", size=(4, 4))]
+    )
+    print(content)
+    # [
+    #     {"type": "image", "content": <PIL.Image.Image>},
+    #     {"type": "text", "content": "hello "},
+    #     {"type": "image", "content": <PIL.Image.Image>},
+    #     {"type": "text", "content": "world"}
+    # ]
+
+Message transforms
+^^^^^^^^^^^^^^^^^^
+Message transforms are convenient utilities to format raw data into a list of torchtune :class:`~torchtune.data.Message`
+objects.
+
+.. code-block:: python
+
+    from torchtune.data import InputOutputToMessages
+
+    sample = {
+        "input": "What is your name?",
+        "output": "I am an AI assistant, I don't have a name."
+    }
+    transform = InputOutputToMessages()
+    output = transform(sample)
+    for message in output["messages"]:
+        print(message.role, message.text_content)
+    # user What is your name?
+    # assistant I am an AI assistant, I don't have a name.
+
+See :ref:`message_transform_usage_label` for more discussion.
+
+
+Formatting messages with prompt templates
+-----------------------------------------
+
+Prompt templates provide a way to format messages into a structured text template. You can simply call any class that inherits
+from :class:`~torchtune.data.PromptTemplateInterface` on a list of Messages and it will add the appropriate text to the content
+list.
+
+.. code-block:: python
+
+    from torchtune.models.mistral import MistralChatTemplate
+    from torchtune.data import Message
+
+    msg = Message(
+        role="user",
+        content="Hello world!",
+        masked=True,
+        eot=True,
+        ipython=False,
+    )
+    template = MistralChatTemplate()
+    templated_msg = template([msg])
+    print(templated_msg[0].content)
+    # [{'type': 'text', 'content': '[INST] '},
+    # {'type': 'text', 'content': 'Hello world!'},
+    # {'type': 'text', 'content': ' [/INST] '}]
+
+Accessing text content in messages
+----------------------------------
+.. code-block:: python
+
+    from torchtune.models.mistral import MistralChatTemplate
+    from torchtune.data import Message
+
+    msg = Message(
+        role="user",
+        content="Hello world!",
+        masked=True,
+        eot=True,
+        ipython=False,
+    )
+    template = MistralChatTemplate()
+    templated_msg = template([msg])
+    print(templated_msg[0].text_content)
+    # [INST] Hello world! [/INST]
+
+Accessing images in messages
+----------------------------
+.. code-block:: python
+
+    from torchtune.data import Message
+    import PIL
+
+    msg = Message(
+        role="user",
+        content=[
+            {
+                "type": "image",
+                # Place your image here
+                "content": PIL.Image.new(mode="RGB", size=(4, 4)),
+            },
+            {"type": "text", "content": "What's in this image?"},
+        ],
+    )
+    if msg.contains_media:
+        print(msg.get_media())
+    # [<PIL.Image.Image image mode=RGB size=4x4 at 0x7F8D27E72740>]
+
+Tokenizing messages
+-------------------
+All model tokenizers have a ``tokenize_messsages`` method that converts a list of
+:class:`~torchtune.data.Message` objects into token IDs and a loss mask.
+
+.. code-block:: python
+
+    from torchtune.models.mistral import mistral_tokenizer
+    from torchtune.data import Message
+
+    m_tokenizer = mistral_tokenizer(
+        path="/tmp/Mistral-7B-v0.1/tokenizer.model",
+        prompt_template="torchtune.models.mistral.MistralChatTemplate",
+        max_seq_len=8192,
+    )
+    msgs = [
+        Message(
+            role="user",
+            content="Hello world!",
+            masked=True,
+            eot=True,
+            ipython=False,
+        ),
+        Message(
+            role="assistant",
+            content="Hi, I am an AI assistant.",
+            masked=False,
+            eot=True,
+            ipython=False,
+        )
+    ]
+    tokens, mask = m_tokenizer.tokenize_messages(msgs)
+    print(tokens)
+    # [1, 733, 16289, 28793, 22557, 1526, 28808, 28705, 733, 28748, 16289, 28793, 15359, 28725, 315, 837, 396, 16107, 13892, 28723, 2]
+    print(mask)  # User message is masked from the loss
+    # [True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False]
+    print(m_tokenizer.decode(tokens))
+    # [INST] Hello world!  [/INST] Hi, I am an AI assistant.
diff -ruN marc_original/third_party/torchtune/docs/source/basics/message_transforms.rst marc/third_party/torchtune/docs/source/basics/message_transforms.rst
--- marc_original/third_party/torchtune/docs/source/basics/message_transforms.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/message_transforms.rst	2025-02-20 17:49:28.986023319 -0500
@@ -0,0 +1,104 @@
+.. _message_transform_usage_label:
+
+==================
+Message Transforms
+==================
+
+Message transforms perform the conversion of raw sample dictionaries from your dataset into torchtune's
+:class:`~torchtune.data.Message` structure. Once you data is represented as Messages, torchtune will handle
+tokenization and preparing it for the model.
+
+.. TODO (rafiayub): place an image here to depict overall pipeline
+
+
+Configuring message transforms
+------------------------------
+Most of our built-in message transforms contain parameters for controlling input masking (``train_on_input``),
+adding a system prompt (``new_system_prompt``), and changing the expected column names (``column_map``).
+These are exposed in our dataset builders :func:`~torchtune.datasets.instruct_dataset` and :func:`~torchtune.datasets.chat_dataset`
+so you don't have to worry about the message transform itself and can configure this directly from the config.
+You can see :ref:`example_instruct` or :ref:`example_chat` for more details.
+
+.. _custom_message_transform:
+
+Custom message transforms
+-------------------------
+If our built-in message transforms do not configure for your particular dataset well,
+you can create your own class with full flexibility. Simply inherit from the :class:`~torchtune.modules.transforms.Transform`
+class and add your code in the ``__call__`` method.
+
+A simple contrived example would be to take one column from the dataset as the user message and another
+column as the model response. Indeed, this is quite similar to :class:`~torchtune.data.InputOutputToMessages`.
+
+.. code-block:: python
+
+    from torchtune.modules.transforms import Transform
+    from torchtune.data import Message
+    from typing import Any, Mapping
+
+    class MessageTransform(Transform):
+        def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+            return [
+                Message(
+                    role="user",
+                    content=sample["input"],
+                    masked=True,
+                    eot=True,
+                ),
+                Message(
+                    role="assistant",
+                    content=sample["output"],
+                    masked=False,
+                    eot=True,
+                ),
+            ]
+
+    sample = {"input": "hello world", "output": "bye world"}
+    transform = MessageTransform()
+    messages = transform(sample)
+    print(messages)
+    # [<torchtune.data._messages.Message at 0x7fb0a10094e0>,
+    # <torchtune.data._messages.Message at 0x7fb0a100a290>]
+    for msg in messages:
+        print(msg.role, msg.text_content)
+    # user hello world
+    # assistant bye world
+
+See :ref:`creating_messages` for more details on how to manipulate :class:`~torchtune.data.Message` objects.
+
+To use this for your dataset, you must create a custom dataset builder that uses the underlying
+dataset class, :class:`~torchtune.datasets.SFTDataset`.
+
+.. code-block:: python
+
+    # In data/dataset.py
+    from torchtune.datasets import SFTDataset
+
+    def custom_dataset(tokenizer, **load_dataset_kwargs) -> SFTDataset:
+        message_transform = MyMessageTransform()
+        return SFTDataset(
+            source="json",
+            data_files="data/my_data.json",
+            split="train",
+            message_transform=message_transform,
+            model_transform=tokenizer,
+            **load_dataset_kwargs,
+        )
+
+This can be used directly from the config.
+
+.. code-block:: yaml
+
+    dataset:
+      _component_: data.dataset.custom_dataset
+
+
+Example message transforms
+--------------------------
+- Instruct
+    - :class:`~torchtune.data.InputOutputToMessages`
+- Chat
+    - :class:`~torchtune.data.ShareGPTToMessages`
+    - :class:`~torchtune.data.OpenAIToMessages`
+- Preference
+    - :class:`~torchtune.data.ChosenRejectedToMessages`
diff -ruN marc_original/third_party/torchtune/docs/source/basics/model_transforms.rst marc/third_party/torchtune/docs/source/basics/model_transforms.rst
--- marc_original/third_party/torchtune/docs/source/basics/model_transforms.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/model_transforms.rst	2025-02-20 17:49:28.990023326 -0500
@@ -0,0 +1,170 @@
+.. _model_transform_usage_label:
+
+=====================
+Multimodal Transforms
+=====================
+
+Multimodal model transforms apply model-specific data transforms to each modality and prepares :class:`~torchtune.data.Message`
+objects to be input into the model. torchtune currently supports text + image model transforms.
+These are intended to be drop-in replacements for tokenizers in multimodal datasets and support the standard
+``encode``, ``decode``, and ``tokenize_messages``.
+
+.. code-block:: python
+
+    # torchtune.models.llama3_2_vision.Llama3VisionTransform
+    class Llama3VisionTransform(ModelTokenizer, Transform):
+        def __init__(...):
+            # Text transform - standard tokenization
+            self.tokenizer = llama3_tokenizer(...)
+            # Image transforms
+            self.transform_image = CLIPImageTransform(...)
+            self.xattn_mask = VisionCrossAttentionMask(...)
+
+
+.. code-block:: python
+
+    from torchtune.models.llama3_2_vision import Llama3VisionTransform
+    from torchtune.data import Message
+    from PIL import Image
+
+    sample = {
+        "messages": [
+            Message(
+                role="user",
+                content=[
+                    {"type": "image", "content": Image.new(mode="RGB", size=(224, 224))},
+                    {"type": "image", "content": Image.new(mode="RGB", size=(224, 224))},
+                    {"type": "text", "content": "What is common in these two images?"},
+                ],
+            ),
+            Message(
+                role="assistant",
+                content="A robot is in both images.",
+            ),
+        ],
+    }
+    transform = Llama3VisionTransform(
+        path="/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model",
+        tile_size=224,
+        patch_size=14,
+    )
+    tokenized_dict = transform(sample)
+    print(transform.decode(tokenized_dict["tokens"], skip_special_tokens=False))
+    # '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n<|image|><|image|>What is common in these two images?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nA robot is in both images.<|eot_id|>'
+    print(tokenized_dict["encoder_input"]["images"][0].shape)  # (num_tiles, num_channels, tile_height, tile_width)
+    # torch.Size([4, 3, 224, 224])
+
+
+Using model transforms
+----------------------
+You can pass them into any multimodal dataset builder just as you would a model tokenizer.
+
+.. code-block:: python
+
+    from torchtune.datasets.multimodal import the_cauldron_dataset
+    from torchtune.models.llama3_2_vision import Llama3VisionTransform
+
+    transform = Llama3VisionTransform(
+        path="/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model",
+        tile_size=224,
+        patch_size=14,
+    )
+    ds = the_cauldron_dataset(
+        model_transform=transform,
+        subset="ai2d",
+    )
+    tokenized_dict = ds[0]
+    print(transform.decode(tokenized_dict["tokens"], skip_special_tokens=False))
+    # <|begin_of_text|><|start_header_id|>user<|end_header_id|>
+    #
+    # <|image|>Question: What do respiration and combustion give out
+    # Choices:
+    # A. Oxygen
+    # B. Carbon dioxide
+    # C. Nitrogen
+    # D. Heat
+    # Answer with the letter.<|eot_id|><|start_header_id|>assistant<|end_header_id|>
+    #
+    # Answer: B<|eot_id|>
+    print(tokenized_dict["encoder_input"]["images"][0].shape)  # (num_tiles, num_channels, tile_height, tile_width)
+    # torch.Size([4, 3, 224, 224])
+
+Creating model transforms
+-------------------------
+Model transforms are expected to process both text and images in the sample dictionary.
+Both should be contained in the ``"messages"`` field of the sample.
+
+The following methods are required on the model transform:
+
+- ``tokenize_messages``
+- ``__call__``
+
+.. code-block:: python
+
+    from torchtune.modules.tokenizers import ModelTokenizer
+    from torchtune.modules.transforms import Transform
+
+    class MyMultimodalTransform(ModelTokenizer, Transform):
+        def __init__(...):
+            self.tokenizer = my_tokenizer_builder(...)
+            self.transform_image = MyImageTransform(...)
+
+        def tokenize_messages(
+            self,
+            messages: List[Message],
+            add_eos: bool = True,
+        ) -> Tuple[List[int], List[bool]]:
+            # Any other custom logic here
+            ...
+
+            return self.tokenizer.tokenize_messages(
+                messages=messages,
+                add_eos=add_eos,
+            )
+
+        def __call__(
+            self, sample: Mapping[str, Any], inference: bool = False
+        ) -> Mapping[str, Any]:
+            # Expected input parameters for vision encoder
+            encoder_input = {"images": [], "aspect_ratio": []}
+            messages = sample["messages"]
+
+            # Transform all images in sample
+            for message in messages:
+                for image in message.get_media():
+                    out = self.transform_image({"image": image}, inference=inference)
+                    encoder_input["images"].append(out["image"])
+                    encoder_input["aspect_ratio"].append(out["aspect_ratio"])
+            sample["encoder_input"] = encoder_input
+
+            # Transform all text - returns same dictionary with additional keys "tokens" and "mask"
+            sample = self.tokenizer(sample, inference=inference)
+
+            return sample
+
+    transform = MyMultimodalTransform(...)
+    sample = {
+        "messages": [
+            Message(
+                role="user",
+                content=[
+                    {"type": "image", "content": Image.new(mode="RGB", size=(224, 224))},
+                    {"type": "image", "content": Image.new(mode="RGB", size=(224, 224))},
+                    {"type": "text", "content": "What is common in these two images?"},
+                ],
+            ),
+            Message(
+                role="assistant",
+                content="A robot is in both images.",
+            ),
+        ],
+    }
+    tokenized_dict = transform(sample)
+    print(tokenized_dict)
+    # {'encoder_input': {'images': ..., 'aspect_ratio': ...}, 'tokens': ..., 'mask': ...}
+
+
+Example model transforms
+------------------------
+- Llama 3.2 Vision
+    - :class:`~torchtune.models.llama3_2_vision.Llama3VisionTransform`
diff -ruN marc_original/third_party/torchtune/docs/source/basics/multimodal_datasets.rst marc/third_party/torchtune/docs/source/basics/multimodal_datasets.rst
--- marc_original/third_party/torchtune/docs/source/basics/multimodal_datasets.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/multimodal_datasets.rst	2025-02-20 17:49:28.994023331 -0500
@@ -0,0 +1,287 @@
+.. _multimodal_dataset_usage_label:
+
+===================
+Multimodal Datasets
+===================
+
+Multimodal datasets include more than one data modality, e.g. text + image, and can be used to train transformer-based models.
+torchtune currently only supports multimodal text+image chat datasets for Vision-Language Models (VLMs).
+
+The primary entry point for fine-tuning with multimodal datasets in torchtune is the :func:`~torchtune.datasets.multimodal.multimodal_chat_dataset`
+builder. This lets you specify a local or Hugging Face dataset that follows the multimodal chat data format
+directly from the config and train your VLM on it.
+
+.. _example_multimodal:
+
+Example multimodal dataset
+--------------------------
+
+Here is an example of a multimodal chat dataset for a visual question-answering task. Note that there is a placeholder
+in the text, ``"<image>"`` for where to place the image tokens. This will get replaced by the image special token
+``<|image|>`` in the example below.
+
+.. code-block:: python
+
+    # data/my_data.json
+    [
+        {
+            "dialogue": [
+                {
+                    "from": "human",
+                    "value": "<image>What time is it on the clock?",
+                },
+                {
+                    "from": "gpt",
+                    "value": "It is 10:00 AM.",
+                },
+            ],
+            "image_path": "images/clock.jpg",
+        },
+        ...,
+    ]
+
+.. code-block:: python
+
+    from torchtune.models.llama3_2_vision import llama3_2_vision_transform
+    from torchtune.datasets.multimodal import multimodal_chat_dataset
+
+    model_transform = Llama3VisionTransform(
+        path="/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model",
+        prompt_template="torchtune.data.QuestionAnswerTemplate",
+        max_seq_len=8192,
+        image_size=560,
+    )
+    ds = multimodal_chat_dataset(
+        model_transform=model_transform,
+        source="json",
+        data_files="data/my_data.json",
+        column_map={
+            "dialogue": "conversations",
+            "image_path": "image",
+        },
+        image_dir="/home/user/dataset/",  # /home/user/dataset/images/clock.jpg
+        image_tag="<image>",
+        split="train",
+    )
+    tokenized_dict = ds[0]
+    print(model_transform.decode(tokenized_dict["tokens"], skip_special_tokens=False))
+    # '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nQuestion:<|image|>What time is it on the clock?Answer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nIt is 10:00AM.<|eot_id|>'
+    print(tokenized_dict["encoder_input"]["images"][0].shape)  # (num_tiles, num_channels, tile_height, tile_width)
+    # torch.Size([4, 3, 224, 224])
+
+.. code-block:: yaml
+
+    # In config - model_transforms takes the place of the tokenizer
+    model_transform:
+      _component_: torchtune.models.llama3_2_vision_transform
+      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+      prompt_template: torchtune.data.QuestionAnswerTemplate
+      max_seq_len: 8192
+
+    dataset:
+      _component_: torchtune.datasets.multimodal.multimodal_chat_dataset
+      source: json
+      data_files: data/my_data.json
+      split: train
+      column_map:
+        dialogue: conversations
+        image_path: image
+      image_dir: /home/user/dataset/
+      image_tag: "<image>"
+      split: train
+
+Multimodal dataset format
+-------------------------
+
+Multimodal datasets are currently expected to follow the :ref:`sharegpt` chat format, where the image paths are in one column
+and the user-assistant conversations are in another column.
+
+.. code-block:: text
+
+    |  conversations                     | image        |
+    |------------------------------------|--------------|
+    | [{"from": "human", "value": "Q1"}, | images/1.jpg |
+    |  {"from": "gpt", "value": "A1"}]   |              |
+
+As an example, you can see the schema of the `ShareGPT4V dataset <https://huggingface.co/datasets/Lin-Chen/ShareGPT4V>`_.
+
+Currently, :func:`~torchtune.datasets.multimodal.multimodal_chat_dataset` only supports a single image path per conversation sample.
+
+
+Loading multimodal datasets from Hugging Face
+---------------------------------------------
+
+You simply need to pass in the dataset repo name to ``source``, which is then passed into Hugging Face's ``load_dataset``.
+For most datasets, you will also need to specify the ``split`` and/or the subset via ``name``.
+
+.. code-block:: python
+
+    # In code
+    from torchtune.models.llama3_2_vision import llama3_2_vision_transform
+    from torchtune.datasets.multimodal import multimodal_chat_dataset
+
+    model_transform = llama3_2_vision_transform(
+        path="/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model",
+        max_seq_len=8192,
+        image_size=560,
+    )
+    ds = multimodal_chat_dataset(
+        model_transform=model_transform,
+        source="Lin-Chen/ShareGPT4V",
+        split="train",
+        name="ShareGPT4V",
+        image_dir="/home/user/dataset/",
+        image_tag="<image>",
+    )
+
+.. code-block:: yaml
+
+    # In config
+    model_transform:
+      _component_: torchtune.models.llama3_2_vision.llama3_2_vision_transform
+      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+      max_seq_len: 8192
+      image_size: 560
+
+    # Tokenizer is passed into the dataset in the recipe
+    dataset:
+      _component_: torchtune.datasets.multimodal.multimodal_chat_dataset
+      source: Lin-Chen/ShareGPT4V
+      split: train
+      name: ShareGPT4V
+      image_dir: /home/user/dataset/
+      image_tag: "<image>"
+
+This will use the default column names "conversations" and "image". To change the column names, use the ``column_map`` argument (see :ref:`column_map`).
+
+Loading local and remote multimodal datasets
+--------------------------------------------
+
+To load in a local or remote dataset via https that follows the instruct format, you need to specify the ``source``, ``data_files`` and ``split``
+arguments. See Hugging Face's ``load_dataset`` `documentation <https://huggingface.co/docs/datasets/main/en/loading#local-and-remote-files>`_
+for more details on loading local or remote files. See :ref:`example_multimodal` above.
+
+Loading images
+--------------
+In many cases, your dataset will contain paths to the images instead of the raw images themselves. :func:`~torchtune.datasets.multimodal.multimodal_chat_dataset`
+will automatically handle this for you, but if you are writing a custom message transform for a custom multimodal dataset
+(see :ref:`custom_message_transform`), you can use the :func:`~torchtune.data.load_image` utility directly.
+
+.. code-block:: python
+
+    from torchtune.data import load_image
+    from pathlib import Path
+
+    sample = {
+        "conversations": [
+            {
+                "from": "human",
+                "value": "What time is it on the clock?",
+            },
+            {
+                "from": "gpt",
+                "value": "It is 10:00 AM.",
+            },
+        ],
+        "image": "images/clock.jpg",
+    }
+    image_dir = "/home/user/dataset/"
+    pil_image = load_image(Path(image_dir) / Path(sample["image"]))
+    print(pil_image)
+    # <PIL.Image.Image>
+
+Then, you can add the PIL image directly to the content of the related message. Only PIL images are supported as image content
+in :class:`~torchtune.data.Message`, not image paths or urls.
+
+.. code-block:: python
+
+    from torchtune.data import Message
+
+    user_message = None
+    for msg in sample["conversations"]:
+        if msg["from"] == "human":
+            user_message = Message(
+                role="user",
+                content=[
+                    {"type": "image", "content": pil_image},
+                    {"type": "text", "content": msg["value"]},
+                ]
+            )
+    print(user_message.contains_media)
+    # True
+    print(user_message.get_media())
+    # [<PIL.Image.Image>]
+    print(user_message.text_content)
+    # What time is it on the clock?
+
+If the image paths in your dataset are relative paths, you can use the ``image_dir`` parameter in :func:`~torchtune.datasets.multimodal.multimodal_chat_dataset`
+to prepend the full path where your images are downloaded locally.
+
+Interleaving images in text
+---------------------------
+torchtune supports adding multiple images in any locations in the text, as long as your model supports it.
+
+.. code-block:: python
+
+    import PIL
+    from torchtune.data import Message
+
+    image_dog = PIL.Image.new(mode="RGB", size=(4, 4))
+    image_cat = PIL.Image.new(mode="RGB", size=(4, 4))
+    image_bird = PIL.Image.new(mode="RGB", size=(4, 4))
+
+    user_message = Message(
+        role="user",
+        content=[
+            {"type": "image", "content": image_dog},
+            {"type": "text", "content": "This is an image of a dog. "},
+            {"type": "image", "content": image_cat},
+            {"type": "text", "content": "This is an image of a cat. "},
+            {"type": "image", "content": image_bird},
+            {"type": "text", "content": "This is a bird, the best pet of the three."},
+        ]
+    )
+    print(user_message.contains_media)
+    # True
+    print(user_message.get_media())
+    # [<PIL.Image.Image>, <PIL.Image.Image>, <PIL.Image.Image>]
+    print(user_message.text_content)
+    # This is an image of a dog. This is an image of a cat. This is a bird, the best pet of the three.
+
+Your dataset may contain image placeholder tags which indicate where in the text the image should be referenced
+As an example, see `ShareGPT4V <https://huggingface.co/datasets/Lin-Chen/ShareGPT4V>`, which uses ``"<image>"``.
+You can easily create the interleaved message content similar to above with the utility :func:`~torchtune.data.format_content_with_images`,
+which replaces the image placeholder tags with the passed in images.
+
+.. code-block:: python
+
+    import PIL
+    from torchtune.data import Message, format_content_with_images
+
+    image_dog = PIL.Image.new(mode="RGB", size=(4, 4))
+    image_cat = PIL.Image.new(mode="RGB", size=(4, 4))
+    image_bird = PIL.Image.new(mode="RGB", size=(4, 4))
+
+    text = "[img]This is an image of a dog. [img]This is an image of a cat. [img]This is a bird, the best pet of the three."
+    user_message = Message(
+        role="user",
+        content=format_content_with_images(
+            content=text,
+            image_tag="[img]",
+            images=[image_dog, image_cat, image_bird],
+        ),
+    )
+    print(user_message.contains_media)
+    # True
+    print(user_message.get_media())
+    # [<PIL.Image.Image>,<PIL.Image.Image>, <PIL.Image.Image>]
+    print(user_message.text_content)
+    # This is an image of a dog. This is an image of a cat. This is a bird, the best pet of the three.
+
+This is handled automatically for you in :func:`~torchtune.datasets.multimodal.multimodal_chat_dataset` when you pass in
+``image_tag``.
+
+Built-in multimodal datasets
+----------------------------
+- :class:`~torchtune.datasets.multimodal.the_cauldron_dataset`
+- :class:`~torchtune.datasets.multimodal.llava_instruct_dataset`
diff -ruN marc_original/third_party/torchtune/docs/source/basics/preference_datasets.rst marc/third_party/torchtune/docs/source/basics/preference_datasets.rst
--- marc_original/third_party/torchtune/docs/source/basics/preference_datasets.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/preference_datasets.rst	2025-02-20 17:49:28.998023338 -0500
@@ -0,0 +1,164 @@
+.. _preference_dataset_usage_label:
+
+===================
+Preference Datasets
+===================
+
+
+Preference datasets are used for reward modelling, where the downstream task is to fine-tune a base model
+to capture some underlying human preferences. Currently, these datasets are used in torchtune with the
+Direct Preference Optimization (DPO) `recipe <https://github.com/pytorch/torchtune/blob/main/recipes/lora_dpo_single_device.py>`_.
+
+The ground-truth in preference datasets is usually the outcome of a binary comparison between two completions for the same prompt,
+and where a human annotator has indicated that one completion is more preferable than the other, according to some pre-set criterion.
+These prompt-completion pairs could be instruct style (single-turn, optionally with a single prompt), chat style (multi-turn), or
+some other set of interactions between a user and model (e.g. free-form text completion).
+
+The primary entry point for fine-tuning with preference datasets in torchtune with the DPO recipe is :func:`~torchtune.datasets.preference_dataset`.
+
+
+Example local preference dataset
+--------------------------------
+
+.. code-block:: bash
+
+   # my_preference_dataset.json
+   [
+       {
+           "chosen_conversations": [
+               {
+                   "content": "What do I do when I have a hole in my trousers?",
+                   "role": "user"
+               },
+               { "content": "Fix the hole.", "role": "assistant" }
+           ],
+           "rejected_conversations": [
+               {
+                   "content": "What do I do when I have a hole in my trousers?",
+                   "role": "user"
+               },
+               { "content": "Take them off.", "role": "assistant" }
+           ]
+       }
+   ]
+
+
+.. code-block:: python
+
+   from torchtune.models.mistral import mistral_tokenizer
+   from torchtune.datasets import preference_dataset
+
+    m_tokenizer = mistral_tokenizer(
+        path="/tmp/Mistral-7B-v0.1/tokenizer.model",
+        prompt_template="torchtune.models.mistral.MistralChatTemplate",
+        max_seq_len=8192,
+    )
+   column_map = {
+       "chosen": "chosen_conversations",
+       "rejected": "rejected_conversations"
+   }
+   ds = preference_dataset(
+       tokenizer=tokenizer,
+       source="json",
+       column_map=column_map,
+       data_files="my_preference_dataset.json",
+       train_on_input=False,
+       split="train",
+   )
+   tokenized_dict = ds[0]
+   print(m_tokenizer.decode(tokenized_dict["rejected_input_ids"]))
+   # user\n\nWhat do I do when I have a hole in my trousers?assistant\n\nTake them off.
+   print(tokenized_dict["rejected_labels"])
+   # [-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100, -100,-100,\
+   # -100,-100,-100,-100,-100,128006,78191,128007,271,18293,1124,1022,13,128009,-100]
+
+
+This can also be accomplished via the yaml config:
+
+.. code-block:: yaml
+
+   # In config
+   tokenizer:
+     _component_: torchtune.models.mistral.mistral_tokenizer
+     path: /tmp/Mistral-7B-v0.1/tokenizer.model
+     prompt_template: torchtune.models.mistral.MistralChatTemplate
+     max_seq_len: 8192
+
+   dataset:
+     _component_: torchtune.datasets.preference_dataset
+     source: json
+     data_files: my_preference_dataset.json
+     column_map:
+       chosen: chosen_conversations
+       rejected: rejected_conversations
+     train_on_input: False
+     split: train
+
+In this example, we've also shown how `column_map` can be used when the "chosen" and/or "rejected" column names differ from the corresponding columns in your dataset.
+
+Preference dataset format
+-------------------------
+
+Preference datasets are expected to have two columns: *"chosen"*, which indicates the human annotator's preferred response, and *"rejected"*, indicating
+the human annotator's dis-preferred response. Each of these columns should contain a list of messages with an identical prompt.
+The list of messages could include a system prompt, an instruction, multiple turns between user and assistant, or tool calls/returns. Let's take a look at
+Anthropic's helpfulness/harmlessness dataset `on Hugging Face <https://huggingface.co/datasets/RLHFlow/HH-RLHF-Helpful-standard>`_ as an example of a multi-turn
+chat-style format:
+
+.. code-block:: text
+
+    | chosen                                | rejected                              |
+    |---------------------------------------|---------------------------------------|
+    |[{                                     |[{                                     |
+    | "role": "user",                       | "role": "user",                       |
+    | "content": "helping my granny with her| "content": "helping my granny with her|
+    | mobile phone issue"                   | mobile phone issue"                   |
+    | },                                    | },                                    |
+    | {                                     | {                                     |
+    | "role": "assistant",                  | "role": "assistant",                  |
+    | "content": "I see you are chatting    | "content": "Well, the best choice here|
+    | with your grandmother about an issue  | could be helping with so-called 'self-|
+    | with her mobile phone. How can I      | management behaviors'. These are      |
+    | help?"                                | things your grandma can do on her own |
+    | },                                    | to help her feel more in control."    |
+    | {                                     | }]                                    |
+    | "role": "user",                       |                                       |
+    | "content": "her phone is not turning  |                                       |
+    | on"                                   |                                       |
+    | },                                    |                                       |
+    | {...},                                |                                       |
+    |]                                      |                                       |
+
+Currently, only JSON-format conversations are supported, as shown in the example above.
+You can use this dataset out-of-the-box in torchtune through :func:`~torchtune.datasets.hh_rlhf_helpful_dataset`.
+
+Loading preference datasets from Hugging Face
+---------------------------------------------
+
+To load in preference datasets from Hugging Face you'll need to pass in the dataset repo name to ``source``. For most HF datasets, you will also need to specify the ``split``.
+
+.. code-block:: python
+
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import preference_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = chat_dataset(
+        tokenizer=g_tokenizer,
+        source="hendrydong/preference_700K",
+        split="train",
+    )
+
+.. code-block:: yaml
+
+    # Tokenizer is passed into the dataset in the recipe so we don't need it here
+    dataset:
+      _component_: torchtune.datasets.preference_dataset
+      source: hendrydong/preference_700K
+      split: train
+
+
+Built-in preference datasets
+----------------------------
+- :func:`~torchtune.datasets.hh_rlhf_helpful_dataset`
+- :func:`~torchtune.datasets.stack_exchange_paired_dataset`
diff -ruN marc_original/third_party/torchtune/docs/source/basics/prompt_templates.rst marc/third_party/torchtune/docs/source/basics/prompt_templates.rst
--- marc_original/third_party/torchtune/docs/source/basics/prompt_templates.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/prompt_templates.rst	2025-02-20 17:49:29.002023345 -0500
@@ -0,0 +1,268 @@
+.. _prompt_templates_usage_label:
+
+================
+Prompt Templates
+================
+
+Prompt templates are structured text templates which are used to format user prompts
+to optimize model performance on specific tasks. They can serve many purposes:
+
+1. Model-specific templates that are required whenever the model is prompted, such as the [INST]
+   tags in the instruct-tuned Llama2 and Mistral models. These models were pre-trained with these tags and using them
+   in inference can help ensure optimal performance.
+2. Task-specific templates to gear models for a particular task that it will expect after training.
+   Example include grammar correction (:class:`~torchtune.data.GrammarErrorCorrectionTemplate`),
+   summarization (:class:`~torchtune.data.SummarizeTemplate`), question answering (:class:`~torchtune.data.QuestionAnswerTemplate`),
+   and more.
+3. Community standardized templates, such as :class:`~torchtune.data.ChatMLTemplate`
+
+For example, if I wanted to fine-tune a model to perform a grammar correction task, I could use the :class:`~torchtune.data.GrammarErrorCorrectionTemplate`
+to add the text "Correct this to standard English: {prompt} --- Corrected: {response}" to all my data samples.
+
+.. code-block:: python
+
+    from torchtune.data import GrammarErrorCorrectionTemplate, Message
+
+    sample = {
+        "incorrect": "This are a cat",
+        "correct": "This is a cat.",
+    }
+    msgs = [
+        Message(role="user", content=sample["incorrect"]),
+        Message(role="assistant", content=sample["correct"]),
+    ]
+
+    gec_template = GrammarErrorCorrectionTemplate()
+    templated_msgs = gec_template(msgs)
+    for msg in templated_msgs:
+        print(msg.text_content)
+    # Correct this to standard English: This are a cat
+    # ---
+    # Corrected:
+    # This is a cat.
+
+
+The added text is different from special tokens that are added by the model tokenizer. For an extended
+discussion on the different between prompt templates and special tokens, see :ref:`prompt_template_vs_special_tokens`.
+
+.. _using_prompt_templates:
+
+Using prompt templates
+----------------------
+Prompt templates are passed into the tokenizer and will be automatically applied for the dataset you are fine-tuning on. You can pass it in two ways:
+
+- A string dotpath to a prompt template class, i.e., "torchtune.models.mistral.MistralChatTemplate" or "path.to.my.CustomPromptTemplate"
+- A dictionary that maps role to a tuple of strings indicating the text to add before and after the message content
+
+
+Defining via dotpath string
+^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+.. code-block:: python
+
+    # In code
+    from torchtune.models.mistral import mistral_tokenizer
+
+    m_tokenizer = mistral_tokenizer(
+        path="/tmp/Mistral-7B-v0.1/tokenizer.model"
+        prompt_template="torchtune.models.mistral.MistralChatTemplate"
+    )
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.mistral.mistral_tokenizer
+      path: /tmp/Mistral-7B-v0.1/tokenizer.model
+      prompt_template: torchtune.models.mistral.MistralChatTemplate
+
+
+Defining via dictionary
+^^^^^^^^^^^^^^^^^^^^^^^
+
+For example to achieve the following prompt template:
+
+.. code-block:: text
+
+    System: {content}\\n
+    User: {content}\\n
+    Assistant: {content}\\n
+    Tool: {content}\\n
+
+You need to pass in a tuple for each role, where ``PREPEND_TAG`` is the string
+added before the text content and ``APPEND_TAG`` is the string added after.
+
+.. code-block:: python
+
+    template = {role: (PREPEND_TAG, APPEND_TAG)}
+
+Thus, the template would be defined as follows:
+
+.. code-block:: python
+
+    template = {
+        "system": ("System: ", "\\n"),
+        "user": ("User: ", "\\n"),
+        "assistant": ("Assistant: ", "\\n"),
+        "ipython": ("Tool: ", "\\n"),
+    }
+
+Now we can pass it into the tokenizer as a dictionary:
+
+.. code-block:: python
+
+    # In code
+    from torchtune.models.mistral import mistral_tokenizer
+
+    template = {
+        "system": ("System: ", "\\n"),
+        "user": ("User: ", "\\n"),
+        "assistant": ("Assistant: ", "\\n"),
+        "ipython": ("Tool: ", "\\n"),
+    }
+    m_tokenizer = mistral_tokenizer(
+        path="/tmp/Mistral-7B-v0.1/tokenizer.model"
+        prompt_template=template,
+    )
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.mistral.mistral_tokenizer
+      path: /tmp/Mistral-7B-v0.1/tokenizer.model
+      prompt_template:
+        system:
+          - "System: "
+          - "\\n"
+        user:
+          - "User: "
+          - "\\n"
+        assistant:
+          - "Assistant: "
+          - "\\n"
+        ipython:
+          - "Tool: "
+          - "\\n"
+
+If you don't want to add a prepend/append tag to a role, you can just pass in an empty string "" where needed.
+
+Using the :class:`~torchtune.data.PromptTemplate` class
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+A template dictionary can also be passed into :class:`~torchtune.data.PromptTemplate` so you can use it as a standalone custom
+prompt template class.
+
+.. code-block:: python
+
+    from torchtune.data import PromptTemplate
+
+    def my_custom_template() -> PromptTemplate:
+        return PromptTemplate(
+            template={
+                "user": ("User: ", "\\n"),
+                "assistant": ("Assistant: ", "\\n"),
+            },
+        )
+
+    template = my_custom_template()
+    msgs = [
+        Message(role="user", content="Hello world!"),
+        Message(role="assistant", content="Is AI overhyped?"),
+    ]
+    templated_msgs = template(msgs)
+    for msg in templated_msgs:
+        print(msg.role, msg.text_content)
+    # user, User: Hello world!
+    #
+    # assistant, Assistant: Is AI overhyped?
+    #
+
+.. TODO (RdoubleA) add a section on how to define prompt templates for inference once generate script is finalized
+
+Custom prompt templates
+-----------------------
+
+For more advanced configuration that doesn't neatly fall into the ``PREPEND_TAG content APPEND_TAG``
+pattern, you can create a new class that inherits from :class:`~torchtune.data.PromptTemplateInterface`
+and implements the ``__call__`` method.
+
+.. code-block:: python
+
+    from torchtune.data import Message
+
+    class PromptTemplateInterface(Protocol):
+        def __call__(
+            self,
+            messages: List[Message],
+            inference: bool = False,
+        ) -> List[Message]:
+            """
+            Format each role's message(s) according to the prompt template
+
+            Args:
+                messages (List[Message]): a single conversation, structured as a list
+                    of :class:`~torchtune.data.Message` objects
+                inference (bool): Whether the template is being used for inference or not.
+
+            Returns:
+                The formatted list of messages
+            """
+            pass
+
+    # Contrived example - make all assistant prompts say "Eureka!"
+    class EurekaTemplate(PromptTemplateInterface):
+        def __call__(
+            self,
+            messages: List[Message],
+            inference: bool = False,
+        ) -> List[Message]:
+            formatted_dialogue = []
+            for message in messages:
+                if message.role == "assistant":
+                    content = "Eureka!"
+                else:
+                    content = message.content
+                formatted_dialogue.append(
+                    Message(
+                        role=message.role,
+                        content=content,
+                        masked=message.masked,
+                        ipython=message.ipython,
+                        eot=message.eot,
+                    ),
+                )
+            return formatted_dialogue
+
+    template = EurekaTemplate()
+    msgs = [
+        Message(role="user", content="Hello world!"),
+        Message(role="assistant", content="Is AI overhyped?"),
+    ]
+    templated_msgs = template(msgs)
+    for msg in templated_msgs:
+        print(msg.role, msg.text_content)
+    # user, Hello world!
+    # assistant, Eureka!
+
+For more examples, you can look at :class:`~torchtune.models.mistral.MistralChatTemplate` or
+:class:`~torchtune.models.llama2.Llama2ChatTemplate`.
+
+To use this custom template in the tokenizer, you can pass it in via dotpath string:
+
+.. code-block:: python
+
+    # In code
+    from torchtune.models.mistral import mistral_tokenizer
+
+    m_tokenizer = mistral_tokenizer(
+        path="/tmp/Mistral-7B-v0.1/tokenizer.model",
+        prompt_template="path.to.template.EurekaTemplate",
+    )
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.mistral.mistral_tokenizer
+      path: /tmp/Mistral-7B-v0.1/tokenizer.model
+      prompt_template: path.to.template.EurekaTemplate
diff -ruN marc_original/third_party/torchtune/docs/source/basics/text_completion_datasets.rst marc/third_party/torchtune/docs/source/basics/text_completion_datasets.rst
--- marc_original/third_party/torchtune/docs/source/basics/text_completion_datasets.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/text_completion_datasets.rst	2025-02-20 17:49:29.006023351 -0500
@@ -0,0 +1,155 @@
+.. _text_completion_dataset_usage_label:
+
+========================
+Text-completion Datasets
+========================
+
+
+Text-completion datasets are typically used for continued pre-training paradigms which involve
+fine-tuning a base model on an unstructured, unlabelled dataset in a self-supervised manner.
+
+The primary entry point for fine-tuning with text completion datasets in torchtune :func:`~torchtune.datasets.text_completion`.
+Text completion datasets are simply expected to contain a column, "text", which contains the text for each sample.
+
+
+Example local text completion datasets
+--------------------------------------
+
+``.json`` format
+^^^^^^^^^^^^^^^^
+
+.. code-block:: bash
+
+   # odyssey.json
+   [
+       {
+           "input": "After we were clear of the river Oceanus, and had got out into the open sea, we went on till we reached the Aeaean island where there is dawn and sunrise as in other places. We then drew our ship on to the sands and got out of her on to the shore, where we went to sleep and waited till day should break."
+       },
+       {
+           "input": "Then, when the child of morning, rosy-fingered Dawn, appeared, I sent some men to Circe's house to fetch the body of Elpenor. We cut firewood from a wood where the headland jutted out into the sea, and after we had wept over him and lamented him we performed his funeral rites. When his body and armour had been burned to ashes, we raised a cairn, set a stone over it, and at the top of the cairn we fixed the oar that he had been used to row with."
+       }
+   ]
+
+.. code-block:: python
+
+   from torchtune.models.llama3 import llama3_tokenizer
+   from torchtune.datasets import text_completion_dataset
+
+   m_tokenizer = llama3_tokenizer(
+       path="/tmp/Meta-Llama-3.1-8B/original/tokenizer.model",
+       max_seq_len=8192
+   )
+
+   ds = text_completion_dataset(
+       tokenizer=m_tokenizer,
+       source="json",
+       column="input",
+       data_files="odyssey.json",
+       split="train",
+   )
+   tokenized_dict = ds[0]
+   print(m_tokenizer.decode(tokenized_dict["tokens"]))
+   # After we were clear of the river Oceanus, and had got out into the open sea,\
+   # we went on till we reached the Aeaean island where there is dawn and sunrise \
+   # as in other places. We then drew our ship on to the sands and got out of her on \
+   # to the shore, where we went to sleep and waited till day should break.
+   print(tokenized_dict["labels"])
+   # [128000, 6153, 584, 1051, 2867, 315, 279, 15140, 22302, 355, 11, 323, 1047, \
+   # 2751, 704, 1139, 279, 1825, 9581, 11, 584, 4024, 389, 12222, 584, 8813, 279, \
+   # 362, 12791, 5420, 13218, 1405, 1070, 374, 39493, 323, 64919, 439, 304, 1023, \
+   # 7634, 13, 1226, 1243, 24465, 1057, 8448, 389, 311, 279, 70163, 323, 2751, 704, \
+   # 315, 1077, 389, 311, 279, 31284, 11, 1405, 584, 4024, 311, 6212, 323, 30315, \
+   # 12222, 1938, 1288, 1464, 13, 128001]
+
+
+This can also be accomplished via the yaml config:
+
+.. code-block:: yaml
+
+   # In config
+   tokenizer:
+     _component_: torchtune.models.llama3.llama3_tokenizer
+     path: /tmp/Meta-Llama-3.1-8B/original/tokenizer.model
+     max_seq_len: 8192
+
+   dataset:
+     _component_: torchtune.datasets.text_completion_dataset
+     source: json
+     data_files: odyssey.json
+     column: input
+     split: train
+
+``.txt`` format
+^^^^^^^^^^^^^^^
+
+.. code-block:: text
+
+   # odyssey.txt
+
+   After we were clear of the river Oceanus, and had got out into the open sea, we went on till we reached the Aeaean island where there is dawn and sunrise as in other places. We then drew our ship on to the sands and got out of her on to the shore, where we went to sleep and waited till day should break.
+   Then, when the child of morning, rosy-fingered Dawn, appeared, I sent some men to Circe's house to fetch the body of Elpenor. We cut firewood from a wood where the headland jutted out into the sea, and after we had wept over him and lamented him we performed his funeral rites. When his body and armour had been burned to ashes, we raised a cairn, set a stone over it, and at the top of the cairn we fixed the oar that he had been used to row with.
+
+
+.. code-block:: python
+
+   from torchtune.models.llama3 import llama3_tokenizer
+   from torchtune.datasets import text_completion_dataset
+
+   m_tokenizer = llama3_tokenizer(
+       path="/tmp/Meta-Llama-3.1-8B/original/tokenizer.model",
+       max_seq_len=8192
+   )
+
+   ds = text_completion_dataset(
+       tokenizer=m_tokenizer,
+       source="text",
+       data_files="odyssey.txt",
+       split="train",
+   )
+   # the outputs here are identical to above
+
+Similarly, this can also be accomplished via the yaml config:
+
+.. code-block:: yaml
+
+   # In config
+   tokenizer:
+     _component_: torchtune.models.llama3.llama3_tokenizer
+     path: /tmp/Meta-Llama-3.1-8B/original/tokenizer.model
+     max_seq_len: 8192
+
+   dataset:
+     _component_: torchtune.datasets.text_completion_dataset
+     source: text
+     data_files: odyssey.txt
+     split: train
+
+Loading text completion datasets from Hugging Face
+--------------------------------------------------
+
+To load in a text completion dataset from Hugging Face you'll need to pass in the dataset repo name to ``source``. For most HF datasets, you will also need to specify the ``split``.
+
+.. code-block:: python
+
+    from torchtune.models.gemma import gemma_tokenizer
+    from torchtune.datasets import text_completion_dataset
+
+    g_tokenizer = gemma_tokenizer("/tmp/gemma-7b/tokenizer.model")
+    ds = text_completion_dataset(
+        tokenizer=g_tokenizer,
+        source="wikimedia/wikipedia",
+        split="train",
+    )
+
+.. code-block:: yaml
+
+    # Tokenizer is passed into the dataset in the recipe so we don't need it here
+    dataset:
+      _component_: torchtune.datasets.text_completion_dataset
+      source: wikimedia/wikipedia
+      split: train
+
+
+Built-in text completion datasets
+---------------------------------
+- :func:`~torchtune.datasets.cnn_dailymail_articles_dataset`
diff -ruN marc_original/third_party/torchtune/docs/source/basics/tokenizers.rst marc/third_party/torchtune/docs/source/basics/tokenizers.rst
--- marc_original/third_party/torchtune/docs/source/basics/tokenizers.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/basics/tokenizers.rst	2025-02-20 17:49:29.006023351 -0500
@@ -0,0 +1,280 @@
+.. _tokenizers_usage_label:
+
+==========
+Tokenizers
+==========
+
+Tokenizers are a key component of any LLM. They convert raw text into token IDs, which index into embedding vectors that are
+understood by the model.
+
+In torchtune, tokenizers play the role of converting :class:`~torchtune.data.Message` objects into token IDs and any necessary model-specific special tokens.
+
+.. code-block:: python
+
+    from torchtune.data import Message
+    from torchtune.models.phi3 import phi3_mini_tokenizer
+
+    sample = {
+        "input": "user prompt",
+        "output": "model response",
+    }
+
+    msgs = [
+        Message(role="user", content=sample["input"]),
+        Message(role="assistant", content=sample["output"])
+    ]
+
+    p_tokenizer = phi3_mini_tokenizer("/tmp/Phi-3-mini-4k-instruct/tokenizer.model")
+    tokens, mask = p_tokenizer.tokenize_messages(msgs)
+    print(tokens)
+    # [1, 32010, 29871, 13, 1792, 9508, 32007, 29871, 13, 32001, 29871, 13, 4299, 2933, 32007, 29871, 13]
+    print(p_tokenizer.decode(tokens))
+    # '\nuser prompt \n \nmodel response \n'
+
+Model tokenizers are usually based on an underlying byte-pair encoding algorithm, such as SentencePiece or TikToken, which are both
+supported in torchtune.
+
+Downloading tokenizers from Hugging Face
+----------------------------------------
+
+Models hosted on Hugging Face are also distributed with the tokenizers they were trained with. These are automatically downloaded alongside
+model weights when using ``tune download``. For example, this command downloads the Mistral-7B model weights and tokenizer:
+
+.. code-block:: bash
+
+    tune download mistralai/Mistral-7B-v0.1 --output-dir /tmp/Mistral-7B-v0.1 --hf-token <HF_TOKEN>
+    cd /tmp/Mistral-7B-v0.1/
+    ls tokenizer.model
+    # tokenizer.model
+
+Loading tokenizers from file
+----------------------------
+
+Once you've downloaded the tokenizer file, you can load it into the corresponding tokenizer class by pointing
+to the file path of the tokenizer model in your config or in the constructor. You can also pass in a custom file path if you've already
+downloaded it to a different location.
+
+.. code-block:: python
+
+    # In code
+    from torchtune.models.mistral import mistral_tokenizer
+
+    m_tokenizer = mistral_tokenizer("/tmp/Mistral-7B-v0.1/tokenizer.model")
+    type(m_tokenizer)
+    # <class 'torchtune.models.mistral._tokenizer.MistralTokenizer'>
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.mistral.mistral_tokenizer
+      path: /tmp/Mistral-7B-v0.1/tokenizer.model
+
+Setting max sequence length
+---------------------------
+
+Setting max sequence length can give you control over memory usage and adhere to model specifications.
+
+.. code-block:: python
+
+    # In code
+    from torchtune.models.mistral import mistral_tokenizer
+
+    m_tokenizer = mistral_tokenizer("/tmp/Mistral-7B-v0.1/tokenizer.model", max_seq_len=8192)
+
+    # Set an arbitrarily small seq len for demonstration
+    from torchtune.data import Message
+
+    m_tokenizer = mistral_tokenizer("/tmp/Mistral-7B-v0.1/tokenizer.model", max_seq_len=7)
+    msg = Message(role="user", content="hello world")
+    tokens, mask = m_tokenizer.tokenize_messages([msg])
+    print(len(tokens))
+    # 7
+    print(tokens)
+    # [1, 733, 16289, 28793, 6312, 28709, 2]
+    print(m_tokenizer.decode(tokens))
+    # '[INST] hello'
+
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.mistral.mistral_tokenizer
+      path: /tmp/Mistral-7B-v0.1/tokenizer.model
+      max_seq_len: 8192
+
+
+Prompt templates
+----------------
+
+Prompt templates are enabled by passing it into any model tokenizer. See :ref:`prompt_templates_usage_label` for more details.
+
+Special tokens
+--------------
+
+Special tokens are model-specific tags that are required to prompt the model. They are different from prompt templates
+because they are assigned their own unique token IDs. For an extended discussion on the difference between special tokens
+and prompt templates, see :ref:`prompt_templates_usage_label`.
+
+Special tokens are automatically added to your data by the model tokenizer and do not require any additional configuration
+from you. You also have the ability to customize the special tokens for experimentation by passing in a file path to
+the new special tokens mapping in a JSON file. This will NOT modify the underlying ``tokenizer.model`` to support the new
+special token ids - it is your responsibility to ensure that the tokenizer file encodes it correctly. Note also that
+some models require the presence of certain special tokens for proper usage, such as the ``"<|eot_id|>"`` in Llama3 Instruct.
+
+For example, here we change the ``"<|begin_of_text|>"`` and ``"<|end_of_text|>"`` token IDs in Llama3 Instruct:
+
+.. code-block:: python
+
+    # tokenizer/special_tokens.json
+    {
+        "added_tokens": [
+            {
+                "id": 128257,
+                "content": "<|begin_of_text|>",
+            },
+            {
+                "id": 128258,
+                "content": "<|end_of_text|>",
+            },
+            # Remaining required special tokens
+            ...
+        ]
+    }
+
+.. code-block:: python
+
+    # In code
+    from torchtune.models.llama3 import llama3_tokenizer
+
+    tokenizer = llama3_tokenizer(
+        path="/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model",
+        special_tokens_path="tokenizer/special_tokens.json",
+    )
+    print(tokenizer.special_tokens)
+    # {'<|begin_of_text|>': 128257, '<|end_of_text|>': 128258, ...}
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.llama3.llama3_tokenizer
+      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+      special_tokens_path: tokenizer/special_tokens.json
+
+.. _base_tokenizers:
+
+Base tokenizers
+---------------
+
+:class:`~torchtune.modules.tokenizers.BaseTokenizer` are the underlying byte-pair encoding modules that perform the actual raw string to token ID conversion and back.
+In torchtune, they are required to implement ``encode`` and ``decode`` methods, which are called by the :ref:`model_tokenizers` to convert
+between raw text and token IDs.
+
+.. code-block:: python
+
+    class BaseTokenizer(Protocol):
+
+        def encode(self, text: str, **kwargs: Dict[str, Any]) -> List[int]:
+            """
+            Given a string, return the encoded list of token ids.
+
+            Args:
+                text (str): The text to encode.
+                **kwargs (Dict[str, Any]): kwargs.
+
+            Returns:
+                List[int]: The encoded list of token ids.
+            """
+            pass
+
+        def decode(self, token_ids: List[int], **kwargs: Dict[str, Any]) -> str:
+            """
+            Given a list of token ids, return the decoded text, optionally including special tokens.
+
+            Args:
+                token_ids (List[int]): The list of token ids to decode.
+                **kwargs (Dict[str, Any]): kwargs.
+
+            Returns:
+                str: The decoded text.
+            """
+            pass
+
+If you load any :ref:`model_tokenizers`, you can see that it calls its underlying :class:`~torchtune.modules.tokenizers.BaseTokenizer`
+to do the actual encoding and decoding.
+
+.. code-block:: python
+
+    from torchtune.models.mistral import mistral_tokenizer
+    from torchtune.modules.tokenizers import SentencePieceBaseTokenizer
+
+    m_tokenizer = mistral_tokenizer("/tmp/Mistral-7B-v0.1/tokenizer.model")
+    # Mistral uses SentencePiece for its underlying BPE
+    sp_tokenizer = SentencePieceBaseTokenizer("/tmp/Mistral-7B-v0.1/tokenizer.model")
+
+    text = "hello world"
+
+    print(m_tokenizer.encode(text))
+    # [1, 6312, 28709, 1526, 2]
+
+    print(sp_tokenizer.encode(text))
+    # [1, 6312, 28709, 1526, 2]
+
+.. _model_tokenizers:
+
+Model tokenizers
+----------------
+
+:class:`~torchtune.modules.tokenizers.ModelTokenizer` are specific to a particular model. They are required to implement the ``tokenize_messages`` method,
+which converts a list of Messages into a list of token IDs.
+
+.. code-block:: python
+
+    class ModelTokenizer(Protocol):
+
+        special_tokens: Dict[str, int]
+        max_seq_len: Optional[int]
+
+        def tokenize_messages(
+            self, messages: List[Message], **kwargs: Dict[str, Any]
+        ) -> Tuple[List[int], List[bool]]:
+            """
+            Given a list of messages, return a list of tokens and list of masks for
+            the concatenated and formatted messages.
+
+            Args:
+                messages (List[Message]): The list of messages to tokenize.
+                **kwargs (Dict[str, Any]): kwargs.
+
+            Returns:
+                Tuple[List[int], List[bool]]: The list of token ids and the list of masks.
+            """
+            pass
+
+The reason they are model specific and different from :ref:`base_tokenizers`
+is because they add all the necessary special tokens or prompt templates required to prompt the model.
+
+.. code-block:: python
+
+    from torchtune.models.mistral import mistral_tokenizer
+    from torchtune.modules.tokenizers import SentencePieceBaseTokenizer
+    from torchtune.data import Message
+
+    m_tokenizer = mistral_tokenizer("/tmp/Mistral-7B-v0.1/tokenizer.model")
+    # Mistral uses SentencePiece for its underlying BPE
+    sp_tokenizer = SentencePieceBaseTokenizer("/tmp/Mistral-7B-v0.1/tokenizer.model")
+
+    text = "hello world"
+    msg = Message(role="user", content=text)
+
+    tokens, mask = m_tokenizer.tokenize_messages([msg])
+    print(tokens)
+    # [1, 733, 16289, 28793, 6312, 28709, 1526, 28705, 733, 28748, 16289, 28793]
+    print(sp_tokenizer.encode(text))
+    # [1, 6312, 28709, 1526, 2]
+    print(m_tokenizer.decode(tokens))
+    # [INST] hello world  [/INST]
+    print(sp_tokenizer.decode(sp_tokenizer.encode(text)))
+    # hello world
diff -ruN marc_original/third_party/torchtune/docs/source/conf.py marc/third_party/torchtune/docs/source/conf.py
--- marc_original/third_party/torchtune/docs/source/conf.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/conf.py	2025-02-20 17:49:29.010023358 -0500
@@ -0,0 +1,284 @@
+#!/usr/bin/env python3
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+#
+# PyTorch documentation build configuration file, created by
+# sphinx-quickstart on Fri Dec 23 13:31:47 2016.
+#
+# This file is execfile()d with the current directory set to its
+# containing dir.
+#
+# Note that not all possible configuration values are present in this
+# autogenerated file.
+#
+# All configuration values have a default; values that are commented out
+# serve to show the default.
+
+# If extensions (or modules to document with autodoc) are in another directory,
+# add these directories to sys.path here. If the directory is relative to the
+# documentation root, use os.path.abspath to make it absolute, like shown here.
+#
+# import os
+# import sys
+# sys.path.insert(0, os.path.abspath('.'))
+
+import os
+import sys
+
+import pytorch_sphinx_theme
+
+sys.path.append(os.path.abspath("."))
+
+# -- General configuration ------------------------------------------------
+
+# Required version of sphinx is set from docs/requirements.txt
+
+# Add any Sphinx extension module names here, as strings. They can be
+# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
+# ones.
+extensions = [
+    "sphinx.ext.autodoc",
+    "sphinx.ext.autosummary",
+    "sphinx.ext.doctest",
+    "sphinx.ext.intersphinx",
+    "sphinx.ext.todo",
+    "sphinx.ext.mathjax",
+    "sphinx.ext.napoleon",
+    "sphinx.ext.viewcode",
+    "sphinx.ext.duration",
+    "sphinx_tabs.tabs",
+    "sphinx_design",
+    "sphinx_gallery.gen_gallery",
+    "sphinx_copybutton",
+]
+
+sphinx_gallery_conf = {
+    "examples_dirs": "tutorials/",  # path to your sphinx-gallery examples
+    "gallery_dirs": "generated_examples",  # path to where to save shpinx-gallery generated output
+    "filename_pattern": "./*.py",  # any .py file in docs/source/examples will be built by sphinx-gallery
+    "backreferences_dir": "gen_modules/backreferences",  # path to store the backreferences
+    "doc_module": ("torchtune",),
+    "remove_config_comments": True,
+}
+
+napoleon_use_ivar = True
+napoleon_numpy_docstring = False
+napoleon_google_docstring = True
+
+
+# Add any paths that contain templates here, relative to this directory.
+templates_path = ["_templates"]
+
+# The suffix(es) of source filenames.
+# You can specify multiple suffix as a list of string:
+#
+source_suffix = [".rst"]
+
+# Get TORCHTUNE_VERSION_DOCS during the build.
+torchtune_version_docs = os.environ.get("TORCHTUNE_VERSION_DOCS", None)
+
+# Get TORCHTUNE_VERSION_DOCS during the build.
+torchtune_version_docs = os.environ.get("TORCHTUNE_VERSION_DOCS", None)
+print(f"torchtune_version_docs: {torchtune_version_docs}")
+project = "torchtune"
+
+# The code below will cut version displayed in the dropdown like this:
+# By default, set to "main".
+# If it's a tag like refs/tags/v1.2.3-rc4 or refs/tags/v1.2.3, or a
+#  refs/heads/release/1.2 then
+# cut to 1.2
+# the version varible is used in layout.html: https://github.com/pytorch/torchtune/blob/main/docs/source/_templates/layout.html#L29
+version = release = "main"
+if torchtune_version_docs:
+    if torchtune_version_docs.startswith("refs/tags/v"):
+        version = ".".join(
+            torchtune_version_docs.split("/")[-1]
+            .split("-")[0]
+            .lstrip("v")
+            .split(".")[:2]
+        )
+    elif torchtune_version_docs.startswith("refs/heads/release/"):
+        version = torchtune_version_docs.split("/")[-1]
+print(f"Version: {version}")
+html_title = " ".join((project, version, "documentation"))
+
+
+# The master toctree document.
+master_doc = "index"
+
+# General information about the project.
+copyright = "2023-present, torchtune Contributors"
+author = "Torch Contributors"
+
+# The language for content autogenerated by Sphinx. Refer to documentation
+# for a list of supported languages.
+#
+# This is also used if you do content translation via gettext catalogs.
+# Usually you set "language" from the command line for these cases.
+language = "en"
+
+# List of patterns, relative to source directory, that match files and
+# directories to ignore when looking for source files.
+# This patterns also effect to html_static_path and html_extra_path
+exclude_patterns = []
+
+# The name of the Pygments (syntax highlighting) style to use.
+pygments_style = "sphinx"
+
+# If true, `todo` and `todoList` produce output, else they produce nothing.
+todo_include_todos = True
+
+
+# -- Options for HTML output ----------------------------------------------
+
+# The theme to use for HTML and HTML Help pages.  See the documentation for
+# a list of builtin themes.
+#
+html_theme = "pytorch_sphinx_theme"
+html_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]
+
+# Theme options are theme-specific and customize the look and feel of a theme
+# further.  For a list of options available for each theme, see the
+# documentation.
+#
+html_theme_options = {
+    "collapse_navigation": False,
+    "display_version": True,
+    "logo_only": True,
+    "pytorch_project": "docs",
+    "navigation_with_keys": True,
+    "analytics_id": "GTM-T8XT4PS",
+}
+
+html_logo = "_static/img/pytorch-logo-dark.svg"
+
+html_css_files = ["css/custom_torchtune.css"]
+
+# Add any paths that contain custom static files (such as style sheets) here,
+# relative to this directory. They are copied after the builtin static files,
+# so a file named "default.css" will overwrite the builtin "default.css".
+html_static_path = ["_static"]
+
+# -- Options for HTMLHelp output ------------------------------------------
+
+# Output file base name for HTML help builder.
+htmlhelp_basename = "PyTorchdoc"
+
+
+autosummary_generate = True
+
+# Example configuration for intersphinx: refer to the Python standard library.
+intersphinx_mapping = {
+    "python": ("https://docs.python.org/3/", None),
+    "torch": ("https://pytorch.org/docs/stable/", None),
+    "numpy": ("https://numpy.org/doc/stable/", None),
+    "PIL": ("https://pillow.readthedocs.io/en/stable/", None),
+    "matplotlib": ("https://matplotlib.org/stable/", None),
+}
+
+# -- A patch that prevents Sphinx from cross-referencing ivar tags -------
+# See http://stackoverflow.com/a/41184353/3343043
+
+from docutils import nodes
+from sphinx import addnodes
+from sphinx.util.docfields import TypedField
+
+
+def patched_make_field(self, types, domain, items, **kw):
+    # `kw` catches `env=None` needed for newer sphinx while maintaining
+    #  backwards compatibility when passed along further down!
+
+    # type: (list, unicode, tuple) -> nodes.field  # noqa: F821
+    def handle_item(fieldarg, content):
+        par = nodes.paragraph()
+        par += addnodes.literal_strong("", fieldarg)  # Patch: this line added
+        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,
+        #                           addnodes.literal_strong))
+        if fieldarg in types:
+            par += nodes.Text(" (")
+            # NOTE: using .pop() here to prevent a single type node to be
+            # inserted twice into the doctree, which leads to
+            # inconsistencies later when references are resolved
+            fieldtype = types.pop(fieldarg)
+            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):
+                typename = "".join(n.astext() for n in fieldtype)
+                typename = typename.replace("int", "python:int")
+                typename = typename.replace("long", "python:long")
+                typename = typename.replace("float", "python:float")
+                typename = typename.replace("type", "python:type")
+                par.extend(
+                    self.make_xrefs(
+                        self.typerolename,
+                        domain,
+                        typename,
+                        addnodes.literal_emphasis,
+                        **kw,
+                    )
+                )
+            else:
+                par += fieldtype
+            par += nodes.Text(")")
+        par += nodes.Text(" -- ")
+        par += content
+        return par
+
+    fieldname = nodes.field_name("", self.label)
+    if len(items) == 1 and self.can_collapse:
+        fieldarg, content = items[0]
+        bodynode = handle_item(fieldarg, content)
+    else:
+        bodynode = self.list_type()
+        for fieldarg, content in items:
+            bodynode += nodes.list_item("", handle_item(fieldarg, content))
+    fieldbody = nodes.field_body("", bodynode)
+    return nodes.field("", fieldname, fieldbody)
+
+
+TypedField.make_field = patched_make_field
+
+
+def inject_minigalleries(app, what, name, obj, options, lines):
+    """Inject a minigallery into a docstring.
+
+    [This is 100% taken from torchvision]
+
+    This avoids having to manually write the .. minigallery directive for every item we want a minigallery for,
+    as it would be easy to miss some.
+
+    This callback is called after the .. auto directives (like ..autoclass) have been processed,
+    and modifies the lines parameter inplace to add the .. minigallery that will show which examples
+    are using which object.
+
+    It's a bit hacky, but not *that* hacky when you consider that the recommended way is to do pretty much the same,
+    but instead with templates using autosummary (which we don't want to use):
+    (https://sphinx-gallery.github.io/stable/configuration.html#auto-documenting-your-api-with-links-to-examples)
+
+    For docs on autodoc-process-docstring, see the autodoc docs:
+    https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html
+    """
+
+    if what in ("class", "function"):
+        lines.append(f".. minigallery:: {name}")
+        lines.append(f"    :add-heading: Examples using ``{name.split('.')[-1]}``:")
+        # avoid heading entirely to avoid warning. As a bonud it actually renders better
+        lines.append("    :heading-level: 9")
+        lines.append("\n")
+
+
+def setup(app):
+
+    app.connect("autodoc-process-docstring", inject_minigalleries)
+
+
+# Custom directives definitions to create cards on main torchtune page
+
+from custom_directives import CustomCardEnd, CustomCardItem, CustomCardStart
+from docutils.parsers import rst
+
+rst.directives.register_directive("customcardstart", CustomCardStart)
+rst.directives.register_directive("customcarditem", CustomCardItem)
+rst.directives.register_directive("customcardend", CustomCardEnd)
diff -ruN marc_original/third_party/torchtune/docs/source/custom_directives.py marc/third_party/torchtune/docs/source/custom_directives.py
--- marc_original/third_party/torchtune/docs/source/custom_directives.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/custom_directives.py	2025-02-20 17:49:29.014023365 -0500
@@ -0,0 +1,178 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import hashlib
+import os
+from pathlib import Path
+from typing import List
+from urllib.parse import quote, urlencode
+
+import requests
+from docutils import nodes
+from docutils.parsers.rst import Directive, directives
+from docutils.parsers.rst.directives.images import Image
+from docutils.statemachine import StringList
+from sphinx.util.docutils import SphinxDirective
+
+_THIS_DIR = Path(__file__).parent
+
+# Color palette from PyTorch Developer Day 2021 Presentation Template
+YELLOW = "F9DB78"
+GREEN = "70AD47"
+BLUE = "00B0F0"
+PINK = "FF71DA"
+ORANGE = "FF8300"
+TEAL = "00E5D1"
+GRAY = "7F7F7F"
+
+
+def _get_cache_path(key, ext):
+    filename = f"{hashlib.sha256(key).hexdigest()}{ext}"
+    cache_dir = _THIS_DIR / "gen_images"
+    cache_dir.mkdir(parents=True, exist_ok=True)
+    return cache_dir / filename
+
+
+def _download(url, path):
+    response = requests.get(url)
+    response.raise_for_status()
+    with open(path, "wb") as file:
+        file.write(response.content)
+
+
+def _fetch_image(url):
+    path = _get_cache_path(url.encode("utf-8"), ext=".svg")
+    if not path.exists():
+        _download(url, path)
+    return os.sep + str(path.relative_to(_THIS_DIR))
+
+
+def _get_relpath(target, base):
+    target = os.sep + target
+    base = os.sep + base
+    target_path, filename = os.path.split(target)
+    rel_path = os.path.relpath(target_path, os.path.dirname(base))
+    return os.path.normpath(os.path.join(rel_path, filename))
+
+
+class BaseShield(Image, SphinxDirective):
+    def run(self, params, alt, section) -> List[nodes.Node]:
+        url = f"https://img.shields.io/static/v1?{urlencode(params, quote_via=quote)}"
+        path = _fetch_image(url)
+        self.arguments = [path]
+        self.options["alt"] = alt
+        if "class" not in self.options:
+            self.options["class"] = []
+        self.options["class"].append("shield-badge")
+        target = _get_relpath("supported_features.html", self.env.docname)
+        self.options["target"] = f"{target}#{section}"
+        return super().run()
+
+
+_CARDLIST_START = """
+.. raw:: html
+
+   <div id="tutorial-cards-container">
+     <nav class="navbar navbar-expand-lg navbar-light tutorials-nav col-12">
+       <div class="tutorial-tags-container">
+         <div id="dropdown-filter-tags">
+           <div class="tutorial-filter-menu">
+             <div class="tutorial-filter filter-btn all-tag-selected" data-tag="all">All</div>
+           </div>
+         </div>
+       </div>
+     </nav>
+
+     <hr class="tutorials-hr">
+
+     <div class="row">
+       <div id="tutorial-cards">
+         <div class="list">
+"""
+
+_CARD_TEMPLATE = """
+.. raw:: html
+
+   <div class="col-md-12 tutorials-card-container" data-tags={tags}>
+     <div class="card tutorials-card">
+       <a href="{link}">
+         <div class="card-body">
+           <div class="card-title-container">
+             <h4>{header}</h4>
+           </div>
+           <p>Topics: <span class="tags">{tags}</span></p>
+           <p class="card-summary">{card_description}</p>
+           <div class="tutorials-image">{image}</div>
+         </div>
+       </a>
+     </div>
+   </div>
+"""
+
+_CARDLIST_END = """
+.. raw:: html
+
+         </div>
+         <div class="pagination d-flex justify-content-center"></div>
+       </div>
+     </div>
+   </div>
+"""
+
+
+class CustomCardStart(Directive):
+    def run(self):
+        para = nodes.paragraph()
+        self.state.nested_parse(
+            StringList(_CARDLIST_START.split("\n")), self.content_offset, para
+        )
+        return [para]
+
+
+class CustomCardItem(Directive):
+    option_spec = {
+        "header": directives.unchanged,
+        "image": directives.unchanged,
+        "link": directives.unchanged,
+        "card_description": directives.unchanged,
+        "tags": directives.unchanged,
+    }
+
+    def run(self):
+        for key in ["header", "card_description", "link"]:
+            if key not in self.options:
+                raise ValueError(f"Key: `{key}` is missing")
+
+        header = self.options["header"]
+        link = self.options["link"]
+        card_description = self.options["card_description"]
+        tags = self.options.get("tags", "")
+
+        if "image" in self.options:
+            image = "<img src='" + self.options["image"] + "'>"
+        else:
+            image = "_static/img/thumbnails/default.png"
+
+        card_rst = _CARD_TEMPLATE.format(
+            header=header,
+            image=image,
+            link=link,
+            card_description=card_description,
+            tags=tags,
+        )
+        card_list = StringList(card_rst.split("\n"))
+        card = nodes.paragraph()
+        self.state.nested_parse(card_list, self.content_offset, card)
+        return [card]
+
+
+class CustomCardEnd(Directive):
+    def run(self):
+        para = nodes.paragraph()
+        self.state.nested_parse(
+            StringList(_CARDLIST_END.split("\n")), self.content_offset, para
+        )
+        return [para]
diff -ruN marc_original/third_party/torchtune/docs/source/deep_dives/checkpointer.rst marc/third_party/torchtune/docs/source/deep_dives/checkpointer.rst
--- marc_original/third_party/torchtune/docs/source/deep_dives/checkpointer.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/deep_dives/checkpointer.rst	2025-02-20 17:49:29.022023378 -0500
@@ -0,0 +1,476 @@
+.. _understand_checkpointer:
+
+==========================
+Checkpointing in torchtune
+==========================
+
+This deep-dive will walk you through the design and behavior of the checkpointer and
+associated utilities.
+
+.. grid:: 1
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What this deep-dive will cover:
+
+      * Checkpointer design for torchtune
+      * Checkpoint formats and how we handle them
+      * Checkpointing scenarios: Intermediate vs Final and LoRA vs Full-finetune
+
+
+Overview
+--------
+
+torchtune checkpointers are designed to be composable components which can be plugged
+into any recipe - training, evaluation or generation. Each checkpointer supports a
+set of models and scenarios making these easy to understand, debug and extend.
+
+Before we dive into the checkpointer in torchtune, let's define some concepts.
+
+|
+
+Checkpoint Format
+^^^^^^^^^^^^^^^^^
+
+In this deep-dive, we'll talk about different checkpoint formats and how torchtune handles them.
+Let's take a close look at these different formats.
+
+Very simply put, the format of a checkpoint is dictated by the state_dict and how this is stored
+in files on disk. Each weight is associated with a string key that identifies it in the state dict.
+If the string identifier of the keys in the stored checkpoints don't match up
+exactly with those in the model definition, you'll either run into explicit errors (loading the
+state dict will raise an exception) or worse - silent errors (loading will succeed but training or
+inference will not work as expected). In addition to the keys lining up, you also need the shapes
+of the weights (values in the state_dict) to match up exactly with those expected by the model
+definition.
+
+Let's look at the two popular formats for Llama2.
+
+**Meta Format**
+
+This is the format supported by the official Llama2 implementation. When you download the Llama2 7B model
+from the `meta-llama website <https://llama.meta.com/llama-downloads>`_, you'll get access to a single
+``.pth`` checkpoint file. You can inspect the contents of this checkpoint easily with ``torch.load``
+
+.. code-block:: python
+
+    >>> import torch
+    >>> state_dict = torch.load('consolidated.00.pth', mmap=True, weights_only=True, map_location='cpu')
+    >>> # inspect the keys and the shapes of the associated tensors
+    >>> for key, value in state_dict.items():
+    >>>    print(f'{key}: {value.shape}')
+
+    tok_embeddings.weight: torch.Size([32000, 4096])
+    ...
+    ...
+    >>> print(len(state_dict.keys()))
+    292
+
+The state_dict contains 292 keys, including an input embedding table called ``tok_embeddings``. The
+model definition for this state_dict expects an embedding layer with ``32000`` tokens each having a
+embedding with dim of ``4096``.
+
+
+**HF Format**
+
+This is the most popular format within the Hugging Face Model Hub and is
+the default format in every torchtune config. This is also the format you get when you download the
+llama2 model from the `Llama-2-7b-hf <https://huggingface.co/meta-llama/Llama-2-7b-hf>`_ repo.
+
+The first big difference is that the state_dict is split across two ``.bin`` files. To correctly
+load the checkpoint, you'll need to piece these files together. Let's inspect one of the files.
+
+.. code-block:: python
+
+    >>> import torch
+    >>> state_dict = torch.load('pytorch_model-00001-of-00002.bin', mmap=True, weights_only=True, map_location='cpu')
+    >>> # inspect the keys and the shapes of the associated tensors
+    >>> for key, value in state_dict.items():
+    >>>     print(f'{key}: {value.shape}')
+
+    model.embed_tokens.weight: torch.Size([32000, 4096])
+    ...
+    ...
+    >>> print(len(state_dict.keys()))
+    241
+
+Not only does the state_dict contain fewer keys (expected since this is one of two files), but
+the embedding table is called ``model.embed_tokens`` instead of ``tok_embeddings``. This mismatch
+in names will cause an exception when you try to load the state_dict. The size of this layer is the
+same between the two, which is as expected.
+
+|
+
+As you can see, if you're not careful you'll likely end up making a number of errors just during
+checkpoint load and save. The torchtune checkpointer makes this less error-prone by managing state dicts
+for you. torchtune is designed to be "state-dict invariant".
+
+- When loading, torchtune accepts checkpoints from multiple sources in multiple formats.
+  You don't have to worry about explicitly converting checkpoints every time you run a recipe.
+
+- When saving, torchtune produces checkpoints in the same format as the source. This includes
+  converting the state_dict back into the original form and splitting the keys and weights
+  across the same number of files.
+
+One big advantage of being "state-dict invariant" is that you should be able to use
+fine-tuned checkpoints from torchtune with any post-training tool (quantization, eval, inference)
+which supports the source format, without any code changes OR conversion scripts. This is one of the
+ways in which torchtune interoperates with the surrounding ecosystem.
+
+.. note::
+
+  To be state-dict "invariant" in this way, the ``load_checkpoint`` and ``save_checkpoint`` methods of each checkpointer
+  make use of weight converters which correctly map weights between checkpoint formats. For example, when loading weights
+  from Hugging Face, we apply a permutation to certain weights on load and save to ensure checkpoints behave exactly the same.
+  To further illustrate this, the Llama family of models uses a
+  `generic weight converter function <https://github.com/pytorch/torchtune/blob/898670f0eb58f956b5228e5a55ccac4ea0efaff8/torchtune/models/convert_weights.py#L113>`_
+  whilst some other models like Phi3 have their own `conversion functions <https://github.com/pytorch/torchtune/blob/main/torchtune/models/phi3/_convert_weights.py>`_
+  which can be found within their model folders.
+
+|
+
+Handling different Checkpoint Formats
+-------------------------------------
+
+torchtune supports three different
+:ref:`checkpointers<checkpointing_label>`,
+each of which supports a different checkpoint format.
+
+
+:class:`HFCheckpointer <torchtune.training.FullModelHFCheckpointer>`
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+This checkpointer reads and writes checkpoints in a format which is compatible with the transformers
+framework from Hugging Face. As mentioned above, this is the most popular format within the Hugging Face
+Model Hub and is the default format in every torchtune config.
+
+For this checkpointer to work correctly, we assume that ``checkpoint_dir`` contains the necessary checkpoint
+and json files. The easiest way to make sure everything works correctly is to use the following flow:
+
+- Download the model from the HF repo using tune download. By default, this will ignore the "safetensors"
+  files.
+
+    |
+
+    .. code-block:: bash
+
+        tune download meta-llama/Llama-2-7b-hf \
+        --output-dir <checkpoint_dir> \
+        --hf-token <hf-token>
+
+- Use ``output_dir`` specified here as the ``checkpoint_dir`` argument for the checkpointer.
+
+|
+
+The following snippet explains how the HFCheckpointer is setup in torchtune config files.
+
+.. code-block:: yaml
+
+    checkpointer:
+
+        # checkpointer to use
+        _component_: torchtune.training.FullModelHFCheckpointer
+
+        # directory with the checkpoint files
+        # this should match the output_dir above
+        checkpoint_dir: <checkpoint_dir>
+
+        # checkpoint files. For the llama2-7b-hf model we have
+        # 2 .bin files. The checkpointer takes care of sorting
+        # by id and so the order here does not matter
+        checkpoint_files: [
+            pytorch_model-00001-of-00002.bin,
+            pytorch_model-00002-of-00002.bin,
+        ]
+
+        # if we're restarting a previous run, we need to specify
+        # the file with the checkpoint state. More on this in the
+        # next section
+        recipe_checkpoint: null
+
+        # dir for saving the output checkpoints. Usually set
+        # to be the same as checkpoint_dir
+        output_dir: <checkpoint_dir>
+
+        # model_type which specifies how to convert the state_dict
+        # into a format which torchtune understands
+        model_type: LLAMA2
+
+    # set to True if restarting training
+    resume_from_checkpoint: False
+
+.. note::
+    Checkpoint conversion to and from HF's format requires access to model params which are
+    read directly from the ``config.json`` file. This helps ensure we either load the weights
+    correctly or error out in case of discrepancy between the HF checkpoint file and torchtune's
+    model implementations. This json file is downloaded from the hub along with the model checkpoints.
+
+|
+
+:class:`MetaCheckpointer <torchtune.training.FullModelMetaCheckpointer>`
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+This checkpointer reads and writes checkpoints in a format which is compatible with the original meta-llama
+github repository.
+
+
+For this checkpointer to work correctly, we assume that ``checkpoint_dir`` contains the necessary checkpoint
+and json files. The easiest way to make sure everything works correctly is to use the following flow:
+
+- Download the model from the HF repo using tune download. By default, this will ignore the "safetensors"
+  files.
+
+    |
+
+    .. code-block:: bash
+
+        tune download meta-llama/Llama-2-7b \
+        --output-dir <checkpoint_dir> \
+        --hf-token <hf-token>
+
+- Use ``output_dir`` above as the ``checkpoint_dir`` for the checkpointer.
+
+|
+
+The following snippet explains how the MetaCheckpointer is setup in torchtune config files.
+
+.. code-block:: yaml
+
+    checkpointer:
+
+        # checkpointer to use
+        _component_: torchtune.training.FullModelMetaCheckpointer
+
+        # directory with the checkpoint files
+        # this should match the output_dir above
+        checkpoint_dir: <checkpoint_dir>
+
+        # checkpoint files. For the llama2-7b model we have
+        # a single .pth file
+        checkpoint_files: [consolidated.00.pth]
+
+        # if we're restarting a previous run, we need to specify
+        # the file with the checkpoint state. More on this in the
+        # next section
+        recipe_checkpoint: null
+
+        # dir for saving the output checkpoints. Usually set
+        # to be the same as checkpoint_dir
+        output_dir: <checkpoint_dir>
+
+        # model_type which specifies how to convert the state_dict
+        # into a format which torchtune understands
+        model_type: LLAMA2
+
+    # set to True if restarting training
+    resume_from_checkpoint: False
+
+|
+
+:class:`TorchTuneCheckpointer <torchtune.training.FullModelTorchTuneCheckpointer>`
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+This checkpointer reads and writes checkpoints in a format that is compatible with torchtune's
+model definition. This does not perform any state_dict conversions and is currently used either
+for testing or for loading quantized models for generation.
+
+|
+
+
+Intermediate vs Final Checkpoints
+---------------------------------
+
+torchtune Checkpointers support two checkpointing scenarios:
+
+**End-of-training Checkpointing**
+
+The model weights at the end of a completed training
+run are written out to file. The checkpointer ensures that the output checkpoint
+files have the same keys as the input checkpoint file used to begin training. The
+checkpointer also ensures that the keys are partitioned across the same number of
+files as the original checkpoint. The output state dict has the following
+standard format:
+
+  .. code-block:: python
+
+            {
+                "key_1": weight_1,
+                "key_2": weight_2,
+                ...
+            }
+
+
+**Mid-training Chekpointing**.
+
+If checkpointing in the middle of training, the output checkpoint needs to store additional
+information to ensure that subsequent training runs can be correctly restarted. In addition to
+the model checkpoint files, we output a ``recipe_state.pt`` file for intermediate
+checkpoints. These are currently output at the end of each epoch, and contain information
+such as optimizer state, number of epochs completed etc.
+
+To prevent us from flooding ``output_dir`` with checkpoint files, the recipe state is
+overwritten at the end of each epoch.
+
+The output state dicts have the following formats:
+
+    .. code-block:: python
+
+        Model:
+            {
+                "key_1": weight_1,
+                "key_2": weight_2,
+                ...
+            }
+
+        Recipe State:
+            {
+                "optimizer": ...,
+                "epoch": ...,
+                ...
+            }
+
+To restart from a previous checkpoint file, you'll need to make the following changes
+to the config file
+
+.. code-block:: yaml
+
+    checkpointer:
+
+        # checkpointer to use
+        _component_: torchtune.training.FullModelHFCheckpointer
+
+        checkpoint_dir: <checkpoint_dir>
+
+        # checkpoint files. Note that you will need to update this
+        # section of the config with the intermediate checkpoint files
+        checkpoint_files: [
+            hf_model_0001_0.pt,
+            hf_model_0002_0.pt,
+        ]
+
+        # if we're restarting a previous run, we need to specify
+        # the file with the checkpoint state
+        recipe_checkpoint: recipe_state.pt
+
+        # dir for saving the output checkpoints. Usually set
+        # to be the same as checkpoint_dir
+        output_dir: <checkpoint_dir>
+
+        # model_type which specifies how to convert the state_dict
+        # into a format which torchtune understands
+        model_type: LLAMA2
+
+    # set to True if restarting training
+    resume_from_checkpoint: True
+
+
+Checkpointing for LoRA
+----------------------
+
+In torchtune, we output both the adapter weights and the full model "merged" weights
+for LoRA. The "merged" checkpoint can be used just like you would use the source
+checkpoint with any post-training tools. For more details, take a look at our
+:ref:`LoRA Finetuning Tutorial <lora_finetune_label>`.Additionally, by setting the option "save_adapter_weights_only" to True when saving a checkpoint, you can choose to save only the adapter weights.
+
+The primary difference between the two use cases is when you want to resume training
+from a checkpoint. In this case, the checkpointer needs access to both the initial frozen
+base model weights as well as the learnt adapter weights. The config for this scenario
+looks something like this:
+
+
+.. code-block:: yaml
+
+    checkpointer:
+
+        # checkpointer to use
+        _component_: torchtune.training.FullModelHFCheckpointer
+
+        # directory with the checkpoint files
+        # this should match the output_dir above
+        checkpoint_dir: <checkpoint_dir>
+
+        # checkpoint files. This is the ORIGINAL frozen checkpoint
+        # and NOT the merged checkpoint output during training
+        checkpoint_files: [
+            pytorch_model-00001-of-00002.bin,
+            pytorch_model-00002-of-00002.bin,
+        ]
+
+        # this refers to the adapter weights learnt during training
+        adapter_checkpoint: adapter_0.pt
+
+        # the file with the checkpoint state
+        recipe_checkpoint: recipe_state.pt
+
+        # dir for saving the output checkpoints. Usually set
+        # to be the same as checkpoint_dir
+        output_dir: <checkpoint_dir>
+
+        # model_type which specifies how to convert the state_dict
+        # into a format which torchtune understands
+        model_type: LLAMA2
+
+    # set to True if restarting training
+    resume_from_checkpoint: True
+
+    # Set to True to save only the adapter weights
+    save_adapter_weights_only: False
+
+|
+
+Putting this all together
+-------------------------
+
+Let's now put all of this knowledge together! We'll load some checkpoints,
+create some models and run a simple forward.
+
+For this section we'll use the Llama2 13B model in HF format.
+
+.. code-block:: python
+
+    import torch
+    from torchtune.training import FullModelHFCheckpointer, ModelType
+    from torchtune.models.llama2 import llama2_13b
+
+    # Set the right directory and files
+    checkpoint_dir = 'Llama-2-13b-hf/'
+    pytorch_files = [
+        'pytorch_model-00001-of-00003.bin',
+        'pytorch_model-00002-of-00003.bin',
+        'pytorch_model-00003-of-00003.bin'
+    ]
+
+    # Set up the checkpointer and load state dict
+    checkpointer = FullModelHFCheckpointer(
+        checkpoint_dir=checkpoint_dir,
+        checkpoint_files=pytorch_files,
+        output_dir=checkpoint_dir,
+        model_type=ModelType.LLAMA2
+    )
+    torchtune_sd = checkpointer.load_checkpoint()
+
+    # Setup the model and the input
+    model = llama2_13b()
+
+    # Model weights are stored with the key="model"
+    model.load_state_dict(torchtune_sd["model"])
+    <All keys matched successfully>
+
+    # We have 32000 vocab tokens; lets generate an input with 70 tokens
+    x = torch.randint(0, 32000, (1, 70))
+
+    with torch.no_grad():
+        model(x)
+
+    tensor([[[ -6.3989,  -9.0531,   3.2375,  ...,  -5.2822,  -4.4872,  -5.7469],
+        [ -8.6737, -11.0023,   6.8235,  ...,  -2.6819,  -4.2424,  -4.0109],
+        [ -4.6915,  -7.3618,   4.1628,  ...,  -2.8594,  -2.5857,  -3.1151],
+        ...,
+        [ -7.7808,  -8.2322,   2.8850,  ...,  -1.9604,  -4.7624,  -1.6040],
+        [ -7.3159,  -8.5849,   1.8039,  ...,  -0.9322,  -5.2010,  -1.6824],
+        [ -7.8929,  -8.8465,   3.3794,  ...,  -1.3500,  -4.6145,  -2.5931]]])
+
+
+You can do this with any model supported by torchtune. You can find a full list
+of models and model builders :ref:`here <models>`.
+
+We hope this deep-dive provided a deeper insight into the checkpointer and
+associated utilities in torchtune. Happy tuning!
diff -ruN marc_original/third_party/torchtune/docs/source/deep_dives/comet_logging.rst marc/third_party/torchtune/docs/source/deep_dives/comet_logging.rst
--- marc_original/third_party/torchtune/docs/source/deep_dives/comet_logging.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/deep_dives/comet_logging.rst	2025-02-20 17:49:29.026023384 -0500
@@ -0,0 +1,60 @@
+.. _comet_logging:
+
+================
+Logging to Comet
+================
+
+This deep-dive will guide you through how to set up logging to Comet in torchtune.
+
+.. grid:: 1
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What this deep-dive will cover
+
+      * How to get started with Comet
+      * How to use the :class:`~torchtune.training.metric_logging.CometLogger`
+      * How to log configs, metrics, and model checkpoints to Comet
+
+torchtune supports logging your training runs to `Comet <https://www.comet.com/site/?utm_source=torchtune&utm_medium=docs&utm_content=docs>`_.
+An example Comet workspace from a torchtune fine-tuning run can be seen in the screenshot below.
+
+.. image:: ../_static/img/comet_torchtune_project.png
+  :alt: torchtune workspace in Comet
+  :width: 100%
+  :align: center
+
+.. note::
+
+  You will need to install the :code:`comet_ml` package to use this feature.
+  You can install it via pip:
+
+  .. code-block:: bash
+
+    pip install comet_ml
+
+
+  You will also likely need to login to Comet in order to start logging data. You can do it through the command line with:
+
+  .. code-block:: bash
+
+    comet login
+
+Metric Logger
+-------------
+
+The only change you need to make is to add the metric logger to your config. Comet will log the metrics and model checkpoints for you.
+
+.. code-block:: yaml
+
+    # enable logging to the built-in CometLogger
+    metric_logger:
+      _component_: torchtune.training.metric_logging.CometLogger
+      # the Comet project to log to
+      project: comet-examples-torchtune
+      experiment_name: my-experiment-name
+
+We automatically grab the config from the recipe you are running and log it to Comet. You can find it in the Comet Hyperparameters tab and the actual file in the :code:`Assets & Artifacts` tab.
+
+.. note::
+
+  Click on this sample `Comet project to see the logged metrics after fine-tuning <https://www.comet.com/examples/comet-example-torchtune-mistral/>`_.
+  The config used to train the models can be found `here <https://www.comet.com/examples/comet-example-torchtune-mistral/fffb3036880e41b5af2df932db4d3578?experiment-tab=params>`_.
diff -ruN marc_original/third_party/torchtune/docs/source/deep_dives/configs.rst marc/third_party/torchtune/docs/source/deep_dives/configs.rst
--- marc_original/third_party/torchtune/docs/source/deep_dives/configs.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/deep_dives/configs.rst	2025-02-20 17:49:29.030023391 -0500
@@ -0,0 +1,269 @@
+.. _config_tutorial_label:
+
+=================
+All About Configs
+=================
+
+This deep-dive will guide you through writing configs for running recipes.
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What this deep-dive will cover
+
+      * How to write a YAML config and run a recipe with it
+      * How to use :code:`instantiate` and :code:`parse` APIs
+      * How to effectively use configs and CLI overrides for running recipes
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Be familiar with the :ref:`overview of torchtune<overview_label>`
+      * Make sure to :ref:`install torchtune<install_label>`
+      * Understand the :ref:`fundamentals of recipes<recipe_deepdive>`
+
+
+Where do parameters live?
+-------------------------
+
+There are two primary entry points for you to configure parameters: **configs** and
+**CLI overrides**. Configs are YAML files that define all the
+parameters needed to run a recipe within a single location. They are the single
+source of truth for reproducing a run. The config parameters can be overridden on the
+command-line using :code:`tune` for quick changes and experimentation without
+modifying the config.
+
+
+Writing configs
+---------------
+Configs serve as the primary entry point for running recipes in torchtune. They are
+expected to be YAML files and they simply list out values for parameters you want to define
+for a particular run.
+
+.. code-block:: yaml
+
+    seed: null
+    shuffle: True
+    device: cuda
+    dtype: fp32
+    enable_fsdp: True
+    ...
+
+Configuring components using :func:`instantiate<torchtune.config.instantiate>`
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+Many fields will require specifying torchtune objects with associated keyword
+arguments as parameters. Models, datasets, optimizers, and loss functions are
+common examples of this. You can easily do this using the :code:`_component_`
+subfield. In :code:`_component_`, you need to specify the dotpath of the object
+you wish to instantiate in the recipe. The dotpath is the exact path you would use
+to import the object normally in a Python file. For example, to specify the
+:class:`~torchtune.datasets.alpaca_dataset` in your config with custom
+arguments:
+
+.. code-block:: yaml
+
+    dataset:
+      _component_: torchtune.datasets.alpaca_dataset
+      train_on_input: False
+
+Here, we are changing the default value for :code:`train_on_input` from :code:`True`
+to :code:`False`.
+
+Once you've specified the :code:`_component_` in your config, you can create an
+instance of the specified object in your recipe's setup like so:
+
+.. code-block:: python
+
+    from torchtune import config
+
+    # Access the dataset field and create the object instance
+    dataset = config.instantiate(cfg.dataset)
+
+This will automatically use any keyword arguments specified in the fields under
+:code:`dataset`.
+
+As written, the preceding example will actually throw an error. If you look at the method for :class:`~torchtune.datasets.alpaca_dataset`,
+you'll notice that we're missing a required positional argument, the tokenizer.
+Since this is another configurable torchtune object, let's understand how to handle
+this by taking a look at the :func:`~torchtune.config.instantiate` API.
+
+.. code-block:: python
+
+    def instantiate(
+        config: DictConfig,
+        *args: Tuple[Any, ...],
+        **kwargs: Dict[str, Any],
+    )
+
+:func:`~torchtune.config.instantiate` also accepts positional arguments
+and keyword arguments and automatically uses that with the config when creating
+the object. This means we can not only pass in the tokenizer, but also add additional
+keyword arguments not specified in the config if we'd like:
+
+.. code-block:: yaml
+
+    # Tokenizer is needed for the dataset, configure it first
+    tokenizer:
+      _component_: torchtune.models.llama2.llama2_tokenizer
+      path: /tmp/tokenizer.model
+
+    dataset:
+      _component_: torchtune.datasets.alpaca_dataset
+
+.. code-block:: python
+
+    # Note the API of the tokenizer we specified - we need to pass in a path
+    def llama2_tokenizer(path: str) -> Llama2Tokenizer:
+
+    # Note the API of the dataset we specified - we need to pass in a model tokenizer
+    # and any optional keyword arguments
+    def alpaca_dataset(
+        tokenizer: ModelTokenizer,
+        train_on_input: bool = True,
+        max_seq_len: int = 512,
+    ) -> SFTDataset:
+
+    from torchtune import config
+
+    # Since we've already specified the path in the config, we don't need to pass
+    # it in
+    tokenizer = config.instantiate(cfg.tokenizer)
+    # We pass in the instantiated tokenizer as the first required argument, then
+    # we change an optional keyword argument
+    dataset = config.instantiate(
+        cfg.dataset,
+        tokenizer,
+        train_on_input=False,
+    )
+
+Note that additional keyword arguments will overwrite any duplicated keys in the
+config.
+
+Referencing other config fields with interpolations
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+Sometimes you need to use the same value more than once for multiple fields. You
+can use *interpolations* to reference another field, and :func:`~torchtune.config.instantiate`
+will automatically resolve it for you.
+
+.. code-block:: yaml
+
+    output_dir: /tmp/alpaca-llama2-finetune
+    metric_logger:
+      _component_: torchtune.training.metric_logging.DiskLogger
+      log_dir: ${output_dir}
+
+Validating your config
+^^^^^^^^^^^^^^^^^^^^^^
+We provide a convenient CLI utility, :ref:`tune validate<validate_cli_label>`, to quickly verify that
+your config is well-formed and all components can be instantiated properly. You
+can also pass in overrides if you want to test out the exact commands you will run
+your experiments with. If any parameters are not well-formed, :ref:`tune validate<validate_cli_label>`
+will list out all the locations where an error was found.
+
+.. code-block:: bash
+
+  tune cp llama2/7B_lora_single_device ./my_config.yaml
+  tune validate ./my_config.yaml
+
+Best practices for writing configs
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+Let's discuss some guidelines for writing configs to get the most out of them.
+
+Airtight configs
+""""""""""""""""
+While it may be tempting to put as much as you can in the config to give you
+maximum flexibility in switching parameters for your experiments, we encourage
+you to only include fields in the config that will be used or instantiated in the
+recipe. This ensures full clarity on the options a recipe was run with and will
+make it significantly easier to debug.
+
+.. code-block:: yaml
+
+    # dont do this
+    alpaca_dataset:
+      _component_: torchtune.datasets.alpaca_dataset
+    slimorca_dataset:
+      ...
+
+    # do this
+    dataset:
+      # change this in config or override when needed
+      _component_: torchtune.datasets.alpaca_dataset
+
+Use public APIs only
+""""""""""""""""""""
+If a component you wish to specify in a config is located in a private file, use
+the public dotpath in your config. These components are typically exposed in their
+parent module's :code:`__init__.py` file. This way, you can guarantee the stability
+of the API you are using in your config. There should be no underscores in your
+component dotpath.
+
+.. code-block:: yaml
+
+    # don't do this
+    dataset:
+      _component_: torchtune.datasets._alpaca.alpaca_dataset
+
+    # do this
+    dataset:
+      _component_: torchtune.datasets.alpaca_dataset
+
+.. _cli_override:
+
+Command-line overrides
+----------------------
+Configs are the primary location to collect all your parameters to run a recipe,
+but sometimes you may want to quickly try different values without having to update
+the config itself. To enable quick experimentation, you can specify override values
+to parameters in your config via the :code:`tune` command. These should be specified
+as key-value pairs :code:`k1=v1 k2=v2 ...`
+
+For example, to run the :ref:`LoRA single-device finetuning <lora_finetune_recipe_label>` recipe with custom model and tokenizer directories, you can provide overrides:
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device \
+    --config llama2/7B_lora_single_device \
+    checkpointer.checkpoint_dir=/home/my_model_checkpoint \
+    checkpointer.checkpoint_files=['file_1','file_2'] \
+    tokenizer.path=/home/my_tokenizer_path
+
+Overriding components
+^^^^^^^^^^^^^^^^^^^^^
+If you would like to override a class or function in the config that is instantiated
+via the :code:`_component_` field, you can do so by assigning to the parameter
+name directly. Any nested fields in the components can be overridden with dot notation.
+
+.. code-block:: yaml
+
+    dataset:
+      _component_: torchtune.datasets.alpaca_dataset
+
+.. code-block:: bash
+
+    # Change to slimorca_dataset and set train_on_input to True
+    tune run lora_finetune_single_device --config my_config.yaml \
+    dataset=torchtune.datasets.slimorca_dataset dataset.train_on_input=True
+
+Removing config fields
+^^^^^^^^^^^^^^^^^^^^^^
+You may need to remove certain parameters from the config when changing components
+through overrides that require different keyword arguments. You can do so by using
+the `~` flag and specify the dotpath of the config field you would like to remove.
+For example, if you want to override a built-in config and use the
+`bitsandbytes.optim.PagedAdamW8bit <https://huggingface.co/docs/bitsandbytes/main/en/reference/optim/adamw#bitsandbytes.optim.PagedAdamW8bit>`_
+optimizer, you may need to delete parameters like ``foreach`` which are
+specific to PyTorch optimizers. Note that this example requires that you have `bitsandbytes <https://github.com/bitsandbytes-foundation/bitsandbytes>`_
+installed.
+
+.. code-block:: yaml
+
+    # In configs/llama3/8B_full.yaml
+    optimizer:
+      _component_: torch.optim.AdamW
+      lr: 2e-5
+      foreach: False
+
+.. code-block:: bash
+
+    # Change to PagedAdamW8bit and remove fused, foreach
+    tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full \
+    optimizer=bitsandbytes.optim.PagedAdamW8bit ~optimizer.foreach
diff -ruN marc_original/third_party/torchtune/docs/source/deep_dives/recipe_deepdive.rst marc/third_party/torchtune/docs/source/deep_dives/recipe_deepdive.rst
--- marc_original/third_party/torchtune/docs/source/deep_dives/recipe_deepdive.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/deep_dives/recipe_deepdive.rst	2025-02-20 17:49:29.034023398 -0500
@@ -0,0 +1,231 @@
+.. _recipe_deepdive:
+
+=================
+What Are Recipes?
+=================
+
+This deep-dive will walk you through the design of training-recipes in torchtune.
+
+.. grid:: 1
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What this deep-dive will cover
+
+      * What are recipes?
+      * What are the core components that make up a recipe?
+      * How should I structure a new recipe?
+
+Recipes are the primary entry points for torchtune users. These can be thought of
+as "targeted" end-to-end pipelines for training and optionally evaluating LLMs.
+Each recipe implements a training method (eg: full fine-tuning) with a set of meaningful
+features (eg: FSDP + Activation Checkpointing + Gradient Accumulation + Mixed Precision
+training) applied to a given model family (eg: Llama2).
+
+As model training gets more and more complex, it becomes harder to anticipate new model
+architectures and training methodologies while also reasoning about every possible trade-off
+(eg: memory vs model quality). We believe a) users are best suited to make trade-offs
+specific to their use cases and b) there's no one-size-fits-all solution. As a result, recipes
+are meant to be easy to understand, extend and debug, *and not* generalized entry points for
+all possible settings.
+
+Depending on your use case and level of expertise, you will routinely find yourself modifying
+existing recipes (eg: adding new features) or writing new ones. torchtune makes writing recipes
+easy by providing well-tested modular components/building-blocks and general utilities
+(eg: :ref:`WandB Logging<metric_logging_label>` and :ref:`Checkpointing <checkpointing_label>`).
+
+|
+
+**Recipe Design**
+
+Recipes in torchtune are designed to be:
+
+- **Simple**. Written fully in native-PyTorch.
+- **Correct**. Numerical parity verification for every component and extensive comparisons with
+  reference implementations and benchmarks.
+- **Easy to Understand**. Each recipe provides a limited set of meaningful features, instead of
+  every possible feature hidden behind 100s of flags. Code duplication is preferred over unnecessary
+  abstractions.
+- **Easy to Extend**. No dependency on training frameworks and no implementation inheritance. Users
+  don't need to go through layers-upon-layers of abstractions to figure out how to extend core
+  functionality.
+- **Accessible to a spectrum of Users**. Users can decide how they want to interact with torchtune recipes:
+    - Start training models by modifying existing configs
+    - Modify existing recipes for custom cases
+    - Directly use available building blocks to write completely new recipes/training paradigms
+
+Each recipe consists of three components:
+
+- **Configurable parameters**, specified through yaml configs and command-line overrides
+- **Recipe Script**, entry-point which puts everything together including parsing and validating
+  configs, setting up the environment, and correctly using the recipe class
+- **Recipe Class**, core logic needed for training, exposed to users through a set of APIs
+
+In the following sections, we'll take a closer look at each of these components.
+For a complete working example, refer to the
+`full finetuning recipe <https://github.com/pytorch/torchtune/blob/main/recipes/full_finetune_distributed.py>`_
+in torchtune and the associated
+`config <https://github.com/pytorch/torchtune/blob/main/recipes/configs/7B_full.yaml>`_.
+
+.. TODO (SalmanMohammadi) ref to full finetune recipe doc
+
+|
+
+What Recipes are not?
+---------------------
+
+- **Monolithic Trainers.** A recipe is **not** a monolithic trainer meant to support every
+  possible feature through 100s of flags.
+- **Generalized entry-points.** A recipe is **not** meant to support every possible model
+  architecture or fine-tuning method.
+- **Wrappers around external frameworks.** A recipe is **not** meant to be a wrapper around
+  external frameworks. These are fully written in native-PyTorch using torchtune building blocks.
+  Dependencies are primarily in the form of additional utilities or interoperability with the
+  surrounding ecosystem (eg: EleutherAI's evaluation harness).
+
+|
+
+Recipe Script
+-------------
+
+This is the primary entry point for each recipe and provides the user with control over how
+the recipe is set up, how models are trained and how the subsequent checkpoints are used.
+This includes:
+
+- Setting up of the environment
+- Parsing and validating configs
+- Training the model
+- Setting up multi-stage training (eg: Distillation) using multiple recipe classes
+
+
+Scripts should generally structure operations in the following order:
+
+- Initialize the recipe class which in-turn initializes recipe state
+- Load and Validate checkpoint to update recipe state if resuming training
+- Initialize recipe components (model, tokenizer, optimizer, loss and dataloader)
+  from checkpoint (if applicable)
+- Train the model
+- Clean up recipe state after training is complete
+
+
+An example script looks something like this:
+
+.. code-block:: python
+
+    # Initialize the process group
+    init_process_group(backend="gloo" if cfg.device == "cpu" else "nccl")
+
+    # Setup the recipe and train the model
+    recipe = FullFinetuneRecipeDistributed(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+    # Other stuff to do after training is complete
+    ...
+
+
+Recipe Class
+------------
+
+The recipe class carries the core logic for training a model. Each class implements a relevant
+interface and exposes a set of APIs. For fine-tuning, the structure of this class is as follows:
+
+Initialize recipe state including seed, device, dtype, metric loggers, relevant flags etc:
+
+.. code-block:: python
+
+    def __init__(...):
+
+        self._device = utils.get_device(device=params.device)
+        self._dtype = training.get_dtype(dtype=params.dtype, device=self._device)
+        ...
+
+Load checkpoint, update recipe state from checkpoint, initialize components and load state dicts from checkpoint
+
+.. code-block:: python
+
+    def setup(self, cfg: DictConfig):
+
+        ckpt_dict = self.load_checkpoint(cfg.checkpointer)
+
+        # Setup the model, including FSDP wrapping, setting up activation checkpointing and
+        # loading the state dict
+        self._model = self._setup_model(...)
+        self._tokenizer = self._setup_tokenizer(...)
+
+        # Setup Optimizer, including transforming for FSDP when resuming training
+        self._optimizer = self._setup_optimizer(...)
+        self._loss_fn = self._setup_loss(...)
+        self._sampler, self._dataloader = self._setup_data(...)
+
+
+Run forward and backward across all epochs and save checkpoint at end of each epoch
+
+.. code-block:: python
+
+    def train(...):
+
+        self._optimizer.zero_grad()
+        for curr_epoch in range(self.epochs_run, self.total_epochs):
+
+            for idx, batch in enumerate(self._dataloader):
+                ...
+
+                with self._autocast:
+                    logits = self._model(...)
+                    ...
+                    loss = self._loss_fn(logits, labels)
+
+                if self.global_step % self._log_every_n_steps == 0:
+                    self._metric_logger.log_dict(...)
+
+                loss.backward()
+                self._optimizer.step()
+                self._optimizer.zero_grad()
+
+                # Update the number of steps when the weights are updated
+                self.global_step += 1
+
+            self.save_checkpoint(epoch=curr_epoch)
+
+
+Cleanup recipe state
+
+.. code-block:: python
+
+    def cleanup(...)
+
+        self.metric_loggers.close()
+        ...
+
+
+
+Running Recipes with Configs
+----------------------------
+
+To run a recipe with a set of user-defined parameters, you will need to write a config file.
+You can learn all about configs in our :ref:`config deep-dive<config_tutorial_label>`.
+
+Config and CLI parsing using :code:`parse`
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+We provide a convenient decorator :func:`~torchtune.config.parse` that wraps
+your recipe to enable running from the command-line with :ref:`tune <cli_label>` with config
+and CLI override parsing.
+
+.. code-block:: python
+
+    @config.parse
+    def recipe_main(cfg: DictConfig) -> None:
+        recipe = FullFinetuneRecipe(cfg=cfg)
+        recipe.setup(cfg=cfg)
+        recipe.train()
+        recipe.cleanup()
+
+
+Running your recipe
+^^^^^^^^^^^^^^^^^^^
+You should be able to run your recipe by providing the direct paths to your custom
+recipe and custom config using the :ref:`tune <cli_label>` command with any CLI overrides:
+
+.. code-block:: bash
+
+    tune run <path/to/recipe> --config <path/to/config> k1=v1 k2=v2 ...
diff -ruN marc_original/third_party/torchtune/docs/source/deep_dives/wandb_logging.rst marc/third_party/torchtune/docs/source/deep_dives/wandb_logging.rst
--- marc_original/third_party/torchtune/docs/source/deep_dives/wandb_logging.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/deep_dives/wandb_logging.rst	2025-02-20 17:49:29.038023404 -0500
@@ -0,0 +1,97 @@
+.. _wandb_logging:
+
+===========================
+Logging to Weights & Biases
+===========================
+
+This deep-dive will guide you through how to set up logging to Weights & Biases
+(W&B) in torchtune.
+
+.. grid:: 1
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What this deep-dive will cover
+
+      * How to get started with W&B
+      * How to use the :class:`~torchtune.training.metric_logging.WandBLogger`
+      * How to log configs, metrics, and model checkpoints to W&B
+
+torchtune supports logging your training runs to `Weights & Biases <https://wandb.ai)>`_.
+An example W&B workspace from a torchtune fine-tuning run can be seen in the screenshot below.
+
+.. image:: ../_static/img/torchtune_workspace.png
+  :alt: torchtune workspace in W&B
+  :width: 100%
+  :align: center
+
+.. note::
+
+  You will need to install the :code:`wandb` package to use this feature.
+  You can install it via pip:
+
+  .. code-block:: bash
+
+    pip install wandb
+
+  Then you need to login with your API key using the W&B CLI:
+
+  .. code-block:: bash
+
+    wandb login
+
+
+Metric Logger
+-------------
+
+The only change you need to make is to add the metric logger to your config. Weights & Biases will log the metrics and model checkpoints for you.
+
+.. code-block:: yaml
+
+    # enable logging to the built-in WandBLogger
+    metric_logger:
+      _component_: torchtune.training.metric_logging.WandBLogger
+      # the W&B project to log to
+      project: torchtune
+
+
+We automatically grab the config from the recipe you are running and log it to W&B. You can find it in the W&B overview tab and the actual file in the :code:`Files` tab.
+
+As a tip, you may see straggler `wandb` processes running in the background if your job crashes or otherwise exits without cleaning up resources. To kill these straggler processes, a command like ``ps
+-aux | grep wandb | awk '{ print $2 }' | xargs kill`` can be used.
+
+.. note::
+
+  Click on this sample `project to see the W&B workspace <https://wandb.ai/capecape/torchtune>`_.
+  The config used to train the models can be found `here <https://wandb.ai/capecape/torchtune/runs/6053ofw0/files/torchtune_config_j67sb73v.yaml>`_.
+
+Logging Model Checkpoints to W&B
+--------------------------------
+
+You can also log the model checkpoints to W&B by modifying the desired script :code:`save_checkpoint` method.
+
+A suggested approach would be something like this:
+
+.. code-block:: python
+
+    def save_checkpoint(self, epoch: int) -> None:
+        ...
+        ## Let's save the checkpoint to W&B
+        ## depending on the Checkpointer Class the file will be named differently
+        ## Here is an example for the full_finetune case
+        checkpoint_file = Path.joinpath(
+            self._checkpointer._output_dir, f"torchtune_model_{epoch}"
+        ).with_suffix(".pt")
+        wandb_at = wandb.Artifact(
+            name=f"torchtune_model_{epoch}",
+            type="model",
+            # description of the model checkpoint
+            description="Model checkpoint",
+            # you can add whatever metadata you want as a dict
+            metadata={
+                training.SEED_KEY: self.seed,
+                training.EPOCHS_KEY: self.epochs_run,
+                training.TOTAL_EPOCHS_KEY: self.total_epochs,
+                training.MAX_STEPS_KEY: self.max_steps_per_epoch,
+            }
+        )
+        wandb_at.add_file(checkpoint_file)
+        wandb.log_artifact(wandb_at)
diff -ruN marc_original/third_party/torchtune/docs/source/index.rst marc/third_party/torchtune/docs/source/index.rst
--- marc_original/third_party/torchtune/docs/source/index.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/index.rst	2025-02-20 17:49:29.042023411 -0500
@@ -0,0 +1,177 @@
+Welcome to the torchtune Documentation
+=======================================
+
+**torchtune** is a Native-PyTorch library for LLM fine-tuning.
+
+Getting Started
+~~~~~~~~~~~~~~~
+
+Topics in this section will help you get started with torchtune.
+
+.. grid:: 3
+
+     .. grid-item-card:: :octicon:`file-code;1em`
+        What is torchtune?
+        :img-top: _static/img/card-background.svg
+        :link: overview.html
+        :link-type: url
+
+        A gentle introduction to torchtune and how you can
+        use the library in your projects.
+
+     .. grid-item-card:: :octicon:`file-code;1em`
+        Installation instructions
+        :img-top: _static/img/card-background.svg
+        :link: install.html
+        :link-type: url
+
+        A step-by-step tutorial on how to install torchtune.
+
+     .. grid-item-card:: :octicon:`file-code;1em`
+         Finetune your first model
+         :img-top: _static/img/card-background.svg
+         :link: tutorials/first_finetune_tutorial.html
+         :link-type: url
+
+         Follow a simple tutorial to finetune Llama2 with torchtune.
+
+Tutorials
+~~~~~~~~~
+
+Ready to experiment? Check out some of the interactive
+torchtune tutorials.
+
+.. customcardstart::
+
+.. customcarditem::
+   :header: Llama3 in torchtune
+   :card_description:
+   :image: _static/img/generic-pytorch-logo.png
+   :link: tutorials/llama3.html
+   :tags: finetuning,llama3
+
+.. customcarditem::
+   :header: Finetuning with LoRA in torchtune
+   :card_description: Parameter-efficient finetuning of Llama2 using LoRA
+   :image: _static/img/generic-pytorch-logo.png
+   :link: tutorials/lora_finetune.html
+   :tags: finetuning,llama2,lora
+
+.. customcarditem::
+   :header: Understanding QLoRA in torchtune
+   :card_description: Using QLoRA to quantize base model weights and maximize memory savings
+   :image: _static/img/generic-pytorch-logo.png
+   :link: tutorials/qlora_finetune.html
+   :tags: finetuning,llama2,qlora
+
+.. customcarditem::
+   :header: Finetuning with QAT in torchtune
+   :card_description: Finetuning of Llama3 using QAT
+   :image: _static/img/generic-pytorch-logo.png
+   :link: tutorials/qat_finetune.html
+   :tags: finetuning,llama3,qat,quantization,evals
+
+.. customcarditem::
+   :header: End-to-End Workflow with torchtune
+   :card_description: Train, Evaluate, Quantize and then Generate with your LLM.
+   :image: _static/img/generic-pytorch-logo.png
+   :link: tutorials/e2e_flow.html
+   :tags: finetuning,quantization,inference,evals,llama2
+
+.. customcarditem::
+   :header: Distilling Llama3 8B into 1B
+   :card_description: Distilling Llama3 8B into 1B using Knowledge Distillation
+   :image: _static/img/generic-pytorch-logo.png
+   :link: tutorials/llama_kd_tutorial.html
+   :tags: finetuning,llama3,kd
+
+.. customcardend::
+
+
+.. ----------------------------------------------------------------------
+.. Below is the toctree i.e. it defines the content of the left sidebar.
+.. Each of the entry below corresponds to a file.rst in docs/source/.
+.. ----------------------------------------------------------------------
+
+.. toctree::
+   :glob:
+   :maxdepth: 1
+   :caption: Getting Started
+   :hidden:
+
+   overview
+   install
+   tutorials/first_finetune_tutorial
+   tune_cli
+
+.. toctree::
+   :glob:
+   :maxdepth: 1
+   :caption: Finetuning Recipes
+   :hidden:
+
+   recipes/recipes_overview
+   recipes/lora_finetune_single_device
+   recipes/qat_distributed
+
+.. toctree::
+   :glob:
+   :maxdepth: 1
+   :caption: Basics
+   :hidden:
+
+   basics/datasets_overview
+   basics/chat_datasets
+   basics/instruct_datasets
+   basics/multimodal_datasets
+   basics/preference_datasets
+   basics/text_completion_datasets
+   basics/model_transforms
+   basics/messages
+   basics/message_transforms
+   basics/tokenizers
+   basics/prompt_templates
+
+.. toctree::
+   :glob:
+   :maxdepth: 1
+   :caption: Tutorials
+   :hidden:
+
+   tutorials/llama3
+   tutorials/chat
+   tutorials/lora_finetune
+   tutorials/qlora_finetune
+   tutorials/qat_finetune
+   tutorials/e2e_flow
+   tutorials/datasets
+   tutorials/memory_optimizations
+   tutorials/llama_kd_tutorial
+
+.. toctree::
+   :glob:
+   :maxdepth: 1
+   :caption: Deep-Dives
+   :hidden:
+
+   deep_dives/checkpointer
+   deep_dives/configs
+   deep_dives/recipe_deepdive
+   deep_dives/comet_logging
+   deep_dives/wandb_logging
+
+.. toctree::
+   :glob:
+   :maxdepth: 1
+   :caption: API Reference
+   :hidden:
+
+   api_ref_config
+   api_ref_data
+   api_ref_datasets
+   api_ref_generation
+   api_ref_models
+   api_ref_modules
+   api_ref_rlhf
+   api_ref_training
+   api_ref_utilities
diff -ruN marc_original/third_party/torchtune/docs/source/install.rst marc/third_party/torchtune/docs/source/install.rst
--- marc_original/third_party/torchtune/docs/source/install.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/install.rst	2025-02-20 17:49:29.046023417 -0500
@@ -0,0 +1,91 @@
+.. _install_label:
+
+====================
+Install Instructions
+====================
+
+
+Pre-requisites
+--------------
+
+torchtune requires PyTorch, so please install for your proper host and environment
+using the `"Start Locally" <https://pytorch.org/get-started/locally/>`_ page. You should also install
+torchvision (for multimodal LLMs) and torchao (for quantization APIs). You can install either stable or
+nightly versions with the following commands:
+
+.. code-block:: bash
+
+    # Install stable version of PyTorch libraries using pip
+    pip install torch torchvision torchao
+
+    # Or nightly install for latest features
+    pip install --pre torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/cu121 # full options are cpu/cu118/cu121/cu124
+
+
+Install via PyPI
+----------------
+
+The latest stable version of torchtune is hosted on PyPI and can be downloaded
+with the following command:
+
+.. code-block:: bash
+
+    pip install torchtune
+
+To confirm that the package is installed correctly, you can run the following command:
+
+.. code-block:: bash
+
+    tune
+
+And should see the following output:
+
+::
+
+    usage: tune [-h] {download,ls,cp,run,validate} ...
+
+    Welcome to the torchtune CLI!
+
+    options:
+    -h, --help            show this help message and exit
+
+    ...
+
+|
+
+Install via ``git clone``
+-------------------------
+
+If you want the latest and greatest features from torchtune or if you want to `become a contributor <https://github.com/pytorch/torchtune/blob/main/CONTRIBUTING.md>`_,
+you can also install the package locally with the following command.
+
+.. code-block:: bash
+
+    git clone https://github.com/pytorch/torchtune.git
+    cd torchtune
+    pip install -e .
+
+    # or for a developer installation
+    pip install -e .["dev"]
+
+|
+
+Install nightly build
+---------------------
+
+torchtune gets built every evening with the latest commits to ``main`` branch. If you want the latest updates
+to the package *without* installing via ``git clone``, you can install with the following command:
+
+.. code-block:: bash
+
+    pip install --pre torchtune --extra-index-url https://download.pytorch.org/whl/nightly/cpu --no-cache-dir
+
+.. note::
+
+    ``--no-cache-dir`` will direct ``pip`` to not look for a cached version of torchtune, thereby overwriting
+    your existing torchtune installation.
+
+If you already have PyTorch installed, torchtune will default to using that version. However, if you want to
+use the nightly version of PyTorch, you can append the ``--force-reinstall`` option to the above command. If you
+opt for this install method, you will likely need to change the "cpu" suffix in the index url to match your CUDA
+version. For example, if you are running CUDA 12, your index url would be "https://download.pytorch.org/whl/nightly/cu121".
diff -ruN marc_original/third_party/torchtune/docs/source/overview.rst marc/third_party/torchtune/docs/source/overview.rst
--- marc_original/third_party/torchtune/docs/source/overview.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/overview.rst	2025-02-20 17:49:29.046023417 -0500
@@ -0,0 +1,82 @@
+.. _overview_label:
+
+==================
+torchtune Overview
+==================
+
+On this page, we'll walk through an overview of torchtune, including features, key concepts and additional pointers.
+
+What is torchtune?
+------------------
+
+torchtune is a PyTorch library for easily authoring, fine-tuning and experimenting with LLMs. The library emphasizes 4 key aspects:
+
+- **Simplicity and Extensibility**. Native-PyTorch, componentized design and easy-to-reuse abstractions
+- **Correctness**. High bar on proving the correctness of components and recipes
+- **Stability**. PyTorch just works. So should torchtune
+- **Democratizing LLM fine-tuning**. Works out-of-the-box on different hardware
+
+
+torchtune provides:
+
+- Modular native-PyTorch implementations of popular LLMs
+- Interoperability with popular model zoos through checkpoint-conversion utilities
+- Training recipes for a variety of fine-tuning techniques
+- Integration with `Hugging Face Datasets <https://huggingface.co/docs/datasets/en/index>`_ for training and `EleutherAI's Eval Harness <https://github.com/EleutherAI/lm-evaluation-harness>`_ for evaluation
+- Support for distributed training using `FSDP2 <https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md>`_
+- YAML configs for easily configuring training runs
+
+Excited? To get started, checkout some of our tutorials, including:
+
+- Our :ref:`quickstart guide <finetune_llama_label>` to finetune your first LLM using torchtune.
+- Our :ref:`LoRA tutorial <lora_finetune_label>` to learn about parameter-efficient finetuning with torchtune.
+- Our :ref:`QLoRA tutorial <qlora_finetune_label>` to attain maximal memory efficiency with torchtune.
+
+You can check out our :ref:`recipes overview<recipes_overview_label>` to see all the fine-tuning techniques we support.
+
+Key Concepts
+------------
+
+As you go through the tutorials and code, there are two concepts which will help you better understand and use torchtune.
+
+**Configs.** YAML files which help you configure training settings (dataset, model, checkpoint) and
+hyperparameters (batch size, learning rate) without modifying code.
+See the ":ref:`All About Configs<config_tutorial_label>`" deep-dive for more information.
+
+**Recipes.** Recipes can be thought of
+as targeted end-to-end pipelines for training and optionally evaluating LLMs.
+Each recipe implements a training method (eg: full fine-tuning) with a set of meaningful
+features (eg: FSDP2 + Activation Checkpointing + Gradient Accumulation + Reduced Precision training)
+applied to a given model family (eg: Llama3.1). See the ":ref:`What Are Recipes?<recipe_deepdive>`" deep-dive for more information.
+
+
+.. _design_principles_label:
+
+Design Principles
+-----------------
+
+torchtune embodies `PyTorchs design philosophy <https://pytorch.org/docs/stable/community/design.html>`_, especially "usability over everything else".
+
+**Native PyTorch**
+
+torchtune is a native-PyTorch library. While we provide integrations with the surrounding ecosystem (eg: `Hugging Face Datasets <https://huggingface.co/docs/datasets/en/index>`_,
+`EleutherAI's Eval Harness <https://github.com/EleutherAI/lm-evaluation-harness>`_), all of the core functionality is written in PyTorch.
+
+
+**Simplicity and Extensibility**
+
+torchtune is designed to be easy to understand, use and extend.
+
+- Composition over implementation inheritance - layers of inheritance for code re-use makes the code hard to read and extend
+- No training frameworks - explicitly outlining the training logic makes it easy to extend for custom use cases
+- Code duplication is prefered over unecessary abstractions
+- Modular building blocks over monolithic components
+
+
+**Correctness**
+
+torchtune provides well-tested components with a high bar on correctness. The library will never be the first to provide a feature, but available features will be thoroughly tested. We provide
+
+- Extensive unit tests to ensure component-level numerical parity with reference implementations
+- Checkpoint tests to ensure model-level numerical parity with reference implementations
+- Integration tests to ensure recipe-level performance parity with reference implementations on standard benchmarks
diff -ruN marc_original/third_party/torchtune/docs/source/recipes/lora_finetune_single_device.rst marc/third_party/torchtune/docs/source/recipes/lora_finetune_single_device.rst
--- marc_original/third_party/torchtune/docs/source/recipes/lora_finetune_single_device.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/recipes/lora_finetune_single_device.rst	2025-02-20 17:49:29.054023430 -0500
@@ -0,0 +1,57 @@
+.. _lora_finetune_recipe_label:
+
+=============================
+LoRA Single Device Finetuning
+=============================
+
+This recipe supports finetuning on next-token prediction tasks using parameter efficient fine-tuning techniques (PEFT)
+such as :ref:`glossary_lora` and :ref:`glossary_qlora`. These techniques
+significantly reduce memory consumption during training whilst still maintaining competitive performance.
+
+We provide configs which you can get up and running quickly. Here is an example with llama 3.1 8B:
+
+.. note::
+
+    You may need to be granted access to the Llama model you're interested in. See
+    :ref:`here <download_llama_label>` for details on accessing gated repositories.
+
+
+.. code-block:: bash
+
+    # download the model
+    tune download meta-llama/Meta-Llama-3.1-8B-Instruct \
+    --output-dir /tmp/Meta-Llama-3.1-8B-Instruct \
+    --ignore-patterns "original/consolidated.00.pth"
+
+    # run the recipe
+    tune run lora_finetune_single_device \
+    --config llama3_1/8B_lora_single_device
+
+You can customize this recipe through the :ref:`cli_label`. For example, when fine-tuning with LoRA, you can adjust the layers which LoRA are applied to:
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device \
+    --config llama3_1/8B_lora_single_device \
+    model.lora_attn_modules=[q_proj,k_proj,v_proj] \
+    model.apply_lora_to_mlp=True \
+    model.lora_rank=64 \
+    model.lora_alpha=128
+
+
+For a deeper understanding of the different levers you can pull when using this recipe,
+see our documentation for the different PEFT training paradigms we support:
+
+* :ref:`glossary_lora`
+* :ref:`glossary_qlora`
+
+Many of our other memory optimization features can be used in this recipe. You can learn more about all of our memory optimization features in our :ref:`memory optimization overview<memory_optimization_overview_label>`.
+
+Interested in seeing this recipe in action? Check out some of our tutorials to show off how it can be used:
+
+* :ref:`Finetuning Llama2 with LoRA<lora_finetune_label>`
+* :ref:`Finetuning Llama2 with QLoRA<qlora_finetune_label>`
+* :ref:`End-to-End Workflow with torchtune<dataset_tutorial_label>`
+* :ref:`Fine-tuning Llama3 with Chat Data<chat_tutorial_label>`
+* :ref:`Meta Llama3 in torchtune<llama3_label>`
+* :ref:`Fine-Tune Your First LLM<finetune_llama_label>`
diff -ruN marc_original/third_party/torchtune/docs/source/recipes/qat_distributed.rst marc/third_party/torchtune/docs/source/recipes/qat_distributed.rst
--- marc_original/third_party/torchtune/docs/source/recipes/qat_distributed.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/recipes/qat_distributed.rst	2025-02-20 17:49:29.058023437 -0500
@@ -0,0 +1,80 @@
+.. _qat_distributed_recipe_label:
+
+=============================================
+Distributed Quantization-Aware Training (QAT)
+=============================================
+
+QAT allows for taking advantage of memory-saving optimizations from quantization at inference time, without significantly
+degrading model performance. In torchtune, we use `torchao <https://github.com/pytorch/ao>`_ to implement QAT.
+This works by :ref:`simulating quantization numerics during fine-tuning <what_is_qat_label>`. While this may introduce memory and
+compute overheads during training, our tests found that QAT significantly reduced performance degradation in evaluations of
+quantized model, without compromising on model size reduction gains. Please see the `PyTorch blogpost <https://pytorch.org/blog/quantization-aware-training/>`_
+on QAT for a deeper dive on how the technique works.
+
+We provide pre-tested out-of-the-box configs which you can get up and running with the latest `Llama models <https://llama.meta.com/>`_
+in just two steps:
+
+.. code-block:: bash
+
+    tune download meta-llama/Meta-Llama-3-8B-Instruct  \
+    --output-dir /tmp/Meta-Llama-3-8B-Instruct \
+    --ignore-patterns "original/consolidated.00.pth" \
+    --HF_TOKEN <HF_TOKEN>
+
+    tune run --nproc_per_node 6 qat_distributed \
+    --config llama3/8B_qat_full
+
+.. note::
+  You may need to be granted access to the Llama model you're interested in. See
+  :ref:`here <download_llama_label>` for details on accessing gated repositories.
+  Also, this workload requires at least 6 GPUs, each with VRAM of at least 80GB e.g. A100s or H100s.
+
+Currently, the main lever you can pull for QAT is by using *delayed fake quantization*.
+Delayed fake quantization allows for control over the step after which fake quantization occurs.
+Empirically, allowing the model to finetune without fake quantization initially allows the
+weight and activation values to stabilize before fake quantizing them, potentially leading
+to improved quantized accuracy. This can be specified through ``fake_quant_after_n_steps``. To
+provide you with an idea of how to roughly configure this parameter, we've achieved best results with
+``fake_quant_after_n_steps ~= total_steps // 2``.
+
+In the future we plan to support different quantization strategies. For now, note that you'll need at least
+``torch>=2.4.0`` to use the `Int8DynActInt4WeightQATQuantizer <https://github.com/pytorch/ao/blob/08024c686fdd3f3dc2817094f817f54be7d3c4ac/torchao/quantization/prototype/qat/api.py#L35>`_
+strategy. Generally, the pipeline for training, quantizing, and evaluating a model using QAT is:
+
+#. Run the ``qat_distributed`` recipe using the above command, or by following the tutorial. By default, this will use ``Int8DynActInt4WeightQATQuantizer``.
+#. This produces an un-quantized model in the original data type. To get an actual quantized model, follow this with
+   ``tune run quantize`` while specifying the same quantizer in the config, e.g.
+
+   .. code-block:: yaml
+
+     # QAT specific args
+     quantizer:
+       _component_: torchtune.training.quantization.Int8DynActInt4WeightQATQuantizer
+       groupsize: 256
+
+#. :ref:`Evaluate<qat_eval_label>` or `run inference <https://github.com/pytorch/torchtune/blob/main/recipes/quantization.md#generate>`_
+   using your your quantized model by specifying the corresponding post-training quantizer:
+
+   .. code-block:: yaml
+
+     quantizer:
+       _component_: torchtune.training.quantization.Int8DynActInt4WeightQuantizer
+       groupsize: 256
+
+.. note::
+
+  We're using config files to show how to customize the recipe in these examples. Check out the
+  :ref:`configs tutorial <config_tutorial_label>` to learn more.
+
+Many of our other memory optimization features can be used in this recipe, too:
+
+* Adjust :ref:`model precision <glossary_precision>`.
+* Use :ref:`activation checkpointing <glossary_act_ckpt>`.
+* Enable :ref:`gradient accumulation <glossary_grad_accm>`.
+* Use :ref:`lower precision optimizers <glossary_low_precision_opt>`.
+
+You can learn more about all of our memory optimization features in our  :ref:`memory optimization overview<memory_optimization_overview_label>`.
+
+Interested in seeing this recipe in action? Check out some of our tutorials to show off how it can be used:
+
+* :ref:`qat_finetune_label`
diff -ruN marc_original/third_party/torchtune/docs/source/recipes/recipes_overview.rst marc/third_party/torchtune/docs/source/recipes/recipes_overview.rst
--- marc_original/third_party/torchtune/docs/source/recipes/recipes_overview.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/recipes/recipes_overview.rst	2025-02-20 17:49:29.058023437 -0500
@@ -0,0 +1,56 @@
+.. _recipes_overview_label:
+
+================
+Recipes Overview
+================
+
+Recipes are the primary entry points for torchtune users.
+These can be thought of as **hackable, singularly-focused scripts for interacting with LLMs** including fine-tuning,
+inference, evaluation, and quantization.
+
+Each recipe consists of three components:
+
+* **Configurable parameters**, specified through yaml configs and command-line overrides
+* **Recipe script**, entry-point which puts everything together including parsing and validating configs, setting up the environment, and correctly using the recipe class
+* **Recipe class**, core logic needed for fine-tuning, exposed through a set of APIs
+
+.. note::
+
+  To learn more about the concept of "recipes", check out our technical deep-dive: :ref:`recipe_deepdive`.
+
+
+Finetuning
+----------
+
+Our recipes include:
+
+* :ref:`Single-device LoRA fine-tuning <lora_finetune_recipe_label>`.
+* Single-device full fine-tuning
+* Distributed full fine-tuning
+* Distributed LoRA fine-tuning
+* Direct Preference Optimization (DPO)
+* Proximal Policy Optimization (PPO)
+* :ref:`Distributed Quantization-Aware Training (QAT)<qat_distributed_recipe_label>`.
+
+For a full list, please run:
+
+.. code-block:: bash
+
+    tune ls
+
+.. Alignment finetuning
+.. --------------------
+.. Interested in alignment fine-tuning? You've come to the right place! We support the following alignment techniques:
+
+.. Direct Preference Optimixation (DPO) Fine-Tuning
+.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+.. `Direct Preference Optimixation <https://arxiv.org/abs/2305.18290>`_ (DPO) stype techniques allow for aligning language models with respect
+.. to a reward model objective function without the use of reinforcement learning. We support DPO preference fine-tuning with:
+
+..   * :ref:`Single-device <lora_finetune_recipe_label>` and :ref:`multi-device <lora_finetune_recipe_label>` LoRA finetuning.
+
+.. note::
+
+  Our recipe documentation is currently in construction. Please feel free to follow the progress in our tracker
+  issue `here <https://github.com/pytorch/torchtune/issues/1408>`_.
diff -ruN marc_original/third_party/torchtune/docs/source/_static/css/custom_torchtune.css marc/third_party/torchtune/docs/source/_static/css/custom_torchtune.css
--- marc_original/third_party/torchtune/docs/source/_static/css/custom_torchtune.css	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/_static/css/custom_torchtune.css	2025-02-20 17:49:28.798023010 -0500
@@ -0,0 +1,191 @@
+/**
+ * Copyright (c) Meta Platforms, Inc. and affiliates.
+ * All rights reserved.
+ *
+ * This source code is licensed under the BSD-style license found in the
+ * LICENSE file in the root directory of this source tree.
+ */
+
+/* sphinx-design styles for cards/tabs */
+
+
+:root {
+    --sd-color-info: #ee4c2c;
+    --sd-color-primary: #6c6c6d;
+    --sd-color-primary-highlight: #f3f4f7;
+    --sd-color-card-border-hover: #ee4c2c;
+    --sd-color-card-border: #f3f4f7;
+    --sd-color-card-background: #fff;
+    --sd-color-card-text: inherit;
+    --sd-color-card-header: transparent;
+    --sd-color-card-footer: transparent;
+    --sd-color-tabs-label-active: #ee4c2c;
+    --sd-color-tabs-label-hover: #ee4c2c;
+    --sd-color-tabs-label-inactive: #6c6c6d;
+    --sd-color-tabs-underline-active: #ee4c2c;
+    --sd-color-tabs-underline-hover: #fabdbd;
+    --sd-color-tabs-underline-inactive: transparent;
+    --sd-color-tabs-overline: rgb(222, 222, 222);
+    --sd-color-tabs-underline: rgb(222, 222, 222);
+}
+
+.sd-text-info {
+    color: #ee4c2c;
+}
+
+.sd-card-img-top {
+    background: #ee4c2c;
+    height: 5px !important;
+}
+
+.sd-card {
+    position: relative;
+    background-color: #fff;
+    opacity: 1.0;
+    border-radius: 0px;
+    width: 30%;
+    border: none;
+    padding-bottom: 0px;
+}
+
+
+.sd-card-img:hover {
+    opacity: 1.0;
+    background-color: #f3f4f7;
+}
+
+
+.sd-card:after {
+    display: block;
+    opacity: 1;
+    content: '';
+    border-bottom: solid 1px #ee4c2c;
+    background-color: #fff;
+    transform: scaleX(0);
+    transition: transform .250s ease-in-out;
+    transform-origin:  0% 50%;
+}
+
+.sd-card:hover {
+    background-color: #fff;
+    opacity: 1;
+    border-top: 1px solid #f3f4f7;
+    border-left: 1px solid #f3f4f7;
+    border-right: 1px solid #f3f4f7;
+}
+
+.sd-card:hover:after {
+    transform: scaleX(1);
+}
+
+.card-prerequisites:hover {
+    transition: none;
+    border: none;
+}
+
+.card-prerequisites:hover:after {
+    transition: none;
+    transform: none;
+}
+
+.card-prerequisites:after {
+    display: block;
+    content: '';
+    border-bottom: none;
+    background-color: #fff;
+    transform: none;
+    transition: none;
+    transform-origin: none;
+}
+
+
+details.sd-dropdown {
+    font-weight: 300;
+    width: auto;
+}
+
+details.sd-dropdown:after {
+    border: none;
+    transition: none;
+}
+
+details.sd-dropdown:hover {
+    border: none;
+    transition: none;
+}
+
+details.sd-dropdown .sd-summary-content {
+    font-weight: 300;
+}
+
+details.sd-dropdown .highlight .n {
+    font-weight: normal;
+}
+
+.et-page-column1 {
+  float: left;
+  width: 70%;
+  font-size: 1rem;
+}
+
+.et-page-column2 {
+  float: right;
+  padding-top: 40px;
+  padding-left: 60px;
+  padding-right: 60px;
+  padding-bottom: 60px;
+  width: 30%;
+}
+
+.et-page-column-row:after {
+  content: "";
+  display: table;
+  clear: both;
+}
+
+/* For screens smaller than 768px (typical mobile devices) */
+@media screen and (max-width: 768px) {
+  .et-page-column1, .et-page-column2 {
+    float: none; /* Remove floats */
+    width: 100%; /* Full width for both columns */
+    padding: 0;
+    font-size: 1rem;
+  }
+
+  .et-page-column2 img {
+    display: none;
+  }
+  .et-page-column-row:after {
+    content: "";
+    display: table;
+    clear: both;
+  }
+}
+
+article.pytorch-article .class .method dt {
+    border-top: none;
+}
+
+article.pytorch-article .class .simple dt {
+    border-top: none;
+}
+
+article.pytorch-article .function dt.sig {
+    border-top: none;
+}
+
+/* Fix for Sphinx gallery thumbnails.
+See https://github.com/sphinx-gallery/sphinx-gallery/issues/990
+*/
+article.pytorch-article .sphx-glr-thumbnails .sphx-glr-thumbcontainer {
+    width: unset;
+    margin-right: 0;
+    margin-left: 0;
+}
+article.pytorch-article div.section div.wy-table-responsive tbody td {
+    width: 50%;
+}
+
+.MathJax {
+    font-size: 0.95em !important;
+}
diff -ruN marc_original/third_party/torchtune/docs/source/_static/img/card-background.svg marc/third_party/torchtune/docs/source/_static/img/card-background.svg
--- marc_original/third_party/torchtune/docs/source/_static/img/card-background.svg	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/_static/img/card-background.svg	2025-02-20 17:49:28.806023023 -0500
@@ -0,0 +1,13 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 202.43 51">
+  <defs>
+    <style>
+      .cls-1 {
+        fill: #ee4c2a;
+        stroke: #ee4c2a;
+        stroke-miterlimit: 10;
+      }
+    </style>
+  </defs>
+  <rect class="cls-1" x=".5" y=".5" width="201.43" height="50"/>
+</svg>
\ No newline at end of file
Binary files marc_original/third_party/torchtune/docs/source/_static/img/comet_torchtune_project.png and marc/third_party/torchtune/docs/source/_static/img/comet_torchtune_project.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/generic-pytorch-logo.png and marc/third_party/torchtune/docs/source/_static/img/generic-pytorch-logo.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/kd-finetune-student.png and marc/third_party/torchtune/docs/source/_static/img/kd-finetune-student.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/kd-finetune-teacher.png and marc/third_party/torchtune/docs/source/_static/img/kd-finetune-teacher.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/kd-hyperparam-kd-ratio.png and marc/third_party/torchtune/docs/source/_static/img/kd-hyperparam-kd-ratio.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/kd-hyperparam-lr.png and marc/third_party/torchtune/docs/source/_static/img/kd-hyperparam-lr.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/kd-qwen2-res.png and marc/third_party/torchtune/docs/source/_static/img/kd-qwen2-res.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/kd-simplified.png and marc/third_party/torchtune/docs/source/_static/img/kd-simplified.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/lora_diagram.png and marc/third_party/torchtune/docs/source/_static/img/lora_diagram.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/lora_experiment_loss_curves.png and marc/third_party/torchtune/docs/source/_static/img/lora_experiment_loss_curves.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/pytorch-logo-dark.png and marc/third_party/torchtune/docs/source/_static/img/pytorch-logo-dark.png differ
diff -ruN marc_original/third_party/torchtune/docs/source/_static/img/pytorch-logo-dark.svg marc/third_party/torchtune/docs/source/_static/img/pytorch-logo-dark.svg
--- marc_original/third_party/torchtune/docs/source/_static/img/pytorch-logo-dark.svg	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/_static/img/pytorch-logo-dark.svg	2025-02-20 17:49:28.890023160 -0500
@@ -0,0 +1,24 @@
+<?xml version="1.0" encoding="utf-8"?>
+<!-- Generator: Adobe Illustrator 21.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
+<svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
+	 viewBox="0 0 199.7 40.2" style="enable-background:new 0 0 199.7 40.2;" xml:space="preserve">
+<style type="text/css">
+	.st0{fill:#F05732;}
+	.st1{fill:#9E529F;}
+	.st2{fill:#333333;}
+</style>
+<path class="st0" d="M102.7,12.2c-1.3-1-1.8,3.9-4.4,3.9c-3,0-4-13-6.3-13c-0.7,0-0.8-0.4-7.9,21.3c-2.9,9,4.4,15.8,11.8,15.8
+	c4.6,0,12.3-3,12.3-12.6C108.2,20.5,104.7,13.7,102.7,12.2z M95.8,35.3c-3.7,0-6.7-3.1-6.7-7c0-3.9,3-7,6.7-7s6.7,3.1,6.7,7
+	C102.5,32.1,99.5,35.3,95.8,35.3z"/>
+<path class="st1" d="M99.8,0c-0.5,0-1.8,2.5-1.8,3.6c0,1.5,1,2,1.8,2c0.8,0,1.8-0.5,1.8-2C101.5,2.5,100.2,0,99.8,0z"/>
+<path class="st2" d="M0,39.5V14.9h11.5c5.3,0,8.3,3.6,8.3,7.9c0,4.3-3,7.9-8.3,7.9H5.2v8.8H0z M14.4,22.8c0-2.1-1.6-3.3-3.7-3.3H5.2
+	v6.6h5.5C12.8,26.1,14.4,24.8,14.4,22.8z"/>
+<path class="st2" d="M35.2,39.5V29.4l-9.4-14.5h6l6.1,9.8l6.1-9.8h5.9l-9.4,14.5v10.1H35.2z"/>
+<path class="st2" d="M63.3,39.5v-20h-7.2v-4.6h19.6v4.6h-7.2v20H63.3z"/>
+<path class="st2" d="M131.4,39.5l-4.8-8.7h-3.8v8.7h-5.2V14.9H129c5.1,0,8.3,3.4,8.3,7.9c0,4.3-2.8,6.7-5.4,7.3l5.6,9.4H131.4z
+	 M131.9,22.8c0-2-1.6-3.3-3.7-3.3h-5.5v6.6h5.5C130.3,26.1,131.9,24.9,131.9,22.8z"/>
+<path class="st2" d="M145.6,27.2c0-7.6,5.7-12.7,13.1-12.7c5.4,0,8.5,2.9,10.3,6l-4.5,2.2c-1-2-3.2-3.6-5.8-3.6
+	c-4.5,0-7.7,3.4-7.7,8.1c0,4.6,3.2,8.1,7.7,8.1c2.5,0,4.7-1.6,5.8-3.6l4.5,2.2c-1.7,3.1-4.9,6-10.3,6
+	C151.3,39.9,145.6,34.7,145.6,27.2z"/>
+<path class="st2" d="M194.5,39.5V29.1h-11.6v10.4h-5.2V14.9h5.2v9.7h11.6v-9.7h5.3v24.6H194.5z"/>
+</svg>
Binary files marc_original/third_party/torchtune/docs/source/_static/img/pytorch-logo-flame.png and marc/third_party/torchtune/docs/source/_static/img/pytorch-logo-flame.png differ
diff -ruN marc_original/third_party/torchtune/docs/source/_static/img/pytorch-logo-flame.svg marc/third_party/torchtune/docs/source/_static/img/pytorch-logo-flame.svg
--- marc_original/third_party/torchtune/docs/source/_static/img/pytorch-logo-flame.svg	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/_static/img/pytorch-logo-flame.svg	2025-02-20 17:49:28.898023174 -0500
@@ -0,0 +1,33 @@
+<?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<svg
+   xmlns:dc="http://purl.org/dc/elements/1.1/"
+   xmlns:cc="http://creativecommons.org/ns#"
+   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
+   xmlns:svg="http://www.w3.org/2000/svg"
+   xmlns="http://www.w3.org/2000/svg"
+   height="40.200001"
+   width="40.200001"
+   xml:space="preserve"
+   viewBox="0 0 40.200002 40.2"
+   y="0px"
+   x="0px"
+   id="Layer_1"
+   version="1.1"><metadata
+     id="metadata4717"><rdf:RDF><cc:Work
+         rdf:about=""><dc:format>image/svg+xml</dc:format><dc:type
+           rdf:resource="http://purl.org/dc/dcmitype/StillImage" /><dc:title></dc:title></cc:Work></rdf:RDF></metadata><defs
+     id="defs4715" /><style
+     id="style4694"
+     type="text/css">
+	.st0{fill:#F05732;}
+	.st1{fill:#9E529F;}
+	.st2{fill:#333333;}
+</style><path
+     style="fill:#f05732"
+     id="path4696"
+     d="m 26.975479,12.199999 c -1.3,-1 -1.8,3.9 -4.4,3.9 -3,0 -4,-12.9999998 -6.3,-12.9999998 -0.7,0 -0.8,-0.4 -7.9000003,21.2999998 -2.9000001,9 4.4000003,15.8 11.8000003,15.8 4.6,0 12.3,-3 12.3,-12.6 0,-7.1 -3.5,-13.9 -5.5,-15.4 z m -6.9,23.1 c -3.7,0 -6.7,-3.1 -6.7,-7 0,-3.9 3,-7 6.7,-7 3.7,0 6.7,3.1 6.7,7 0,3.8 -3,7 -6.7,7 z"
+     class="st0" /><path
+     style="fill:#9e529f"
+     id="path4698"
+     d="m 24.075479,-7.6293945e-7 c -0.5,0 -1.8,2.49999996293945 -1.8,3.59999996293945 0,1.5 1,2 1.8,2 0.8,0 1.8,-0.5 1.8,-2 -0.1,-1.1 -1.4,-3.59999996293945 -1.8,-3.59999996293945 z"
+     class="st1" /></svg>
Binary files marc_original/third_party/torchtune/docs/source/_static/img/qat_diagram.png and marc/third_party/torchtune/docs/source/_static/img/qat_diagram.png differ
Binary files marc_original/third_party/torchtune/docs/source/_static/img/qlora_exp.png and marc/third_party/torchtune/docs/source/_static/img/qlora_exp.png differ
diff -ruN marc_original/third_party/torchtune/docs/source/_static/img/torchtune_datasets.svg marc/third_party/torchtune/docs/source/_static/img/torchtune_datasets.svg
--- marc_original/third_party/torchtune/docs/source/_static/img/torchtune_datasets.svg	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/_static/img/torchtune_datasets.svg	2025-02-20 17:49:28.914023200 -0500
@@ -0,0 +1,4 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!-- Do not edit this file with editors other than draw.io -->
+<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
+<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="811px" height="647px" viewBox="-0.5 -0.5 811 647" content="&lt;mxfile host=&quot;app.diagrams.net&quot; agent=&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36&quot; version=&quot;24.7.12&quot; scale=&quot;1&quot; border=&quot;50&quot;&gt;&#10;  &lt;diagram name=&quot;Page-1&quot; id=&quot;XOIqh0cSQh_DR6o7WLQn&quot;&gt;&#10;    &lt;mxGraphModel dx=&quot;1018&quot; dy=&quot;683&quot; grid=&quot;1&quot; gridSize=&quot;10&quot; guides=&quot;1&quot; tooltips=&quot;1&quot; connect=&quot;1&quot; arrows=&quot;1&quot; fold=&quot;1&quot; page=&quot;1&quot; pageScale=&quot;1&quot; pageWidth=&quot;850&quot; pageHeight=&quot;1100&quot; math=&quot;0&quot; shadow=&quot;0&quot;&gt;&#10;      &lt;root&gt;&#10;        &lt;mxCell id=&quot;0&quot; /&gt;&#10;        &lt;mxCell id=&quot;1&quot; parent=&quot;0&quot; /&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-49&quot; value=&quot;&quot; style=&quot;edgeStyle=none;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;&quot; parent=&quot;1&quot; source=&quot;IdahOF6jb9ku1gB2LzXh-2&quot; target=&quot;IdahOF6jb9ku1gB2LzXh-17&quot; edge=&quot;1&quot;&gt;&#10;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-2&quot; value=&quot;Message transform&quot; style=&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=#d79b00;fontSize=17;shadow=0;&quot; parent=&quot;1&quot; vertex=&quot;1&quot;&gt;&#10;          &lt;mxGeometry x=&quot;70&quot; y=&quot;185&quot; width=&quot;140&quot; height=&quot;150&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-50&quot; value=&quot;&quot; style=&quot;edgeStyle=none;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;&quot; parent=&quot;1&quot; source=&quot;IdahOF6jb9ku1gB2LzXh-17&quot; target=&quot;IdahOF6jb9ku1gB2LzXh-28&quot; edge=&quot;1&quot;&gt;&#10;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-17&quot; value=&quot;Model transform&quot; style=&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=#6c8ebf;fontSize=17;shadow=0;&quot; parent=&quot;1&quot; vertex=&quot;1&quot;&gt;&#10;          &lt;mxGeometry x=&quot;70&quot; y=&quot;385&quot; width=&quot;140&quot; height=&quot;110&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-48&quot; value=&quot;&quot; style=&quot;edgeStyle=none;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;&quot; parent=&quot;1&quot; source=&quot;IdahOF6jb9ku1gB2LzXh-21&quot; target=&quot;IdahOF6jb9ku1gB2LzXh-2&quot; edge=&quot;1&quot;&gt;&#10;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-21&quot; value=&quot;Raw sample dict&quot; style=&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;fontSize=17;shadow=0;&quot; parent=&quot;1&quot; vertex=&quot;1&quot;&gt;&#10;          &lt;mxGeometry x=&quot;70&quot; y=&quot;40&quot; width=&quot;140&quot; height=&quot;95&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-28&quot; value=&quot;Model input&quot; style=&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#d5e8d4;strokeColor=#82b366;fontSize=17;shadow=0;&quot; parent=&quot;1&quot; vertex=&quot;1&quot;&gt;&#10;          &lt;mxGeometry x=&quot;70&quot; y=&quot;545&quot; width=&quot;140&quot; height=&quot;40&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-40&quot; value=&quot;Message(&amp;lt;div style=&amp;quot;font-size: 15px;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; role=&amp;quot;user&amp;quot;,&amp;amp;nbsp;&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;font-size: 15px;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; content=[&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;font-size: 15px;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; {&amp;quot;type&amp;quot;: &amp;quot;image&amp;quot;, &amp;quot;content&amp;quot;: &amp;amp;lt;PIL.Image.Image&amp;amp;gt;},&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;font-size: 15px;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; {&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What&amp;#39;s in this image?&amp;quot;},&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;font-size: 15px;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; ],&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;font-size: 15px;&amp;quot;&amp;gt;)&amp;lt;/div&amp;gt;&quot; style=&quot;text;strokeColor=#333333;align=left;fillColor=default;html=1;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=15;fontFamily=Courier New;fontStyle=1;spacing=7;fontColor=#333333;&quot; parent=&quot;1&quot; vertex=&quot;1&quot;&gt;&#10;          &lt;mxGeometry x=&quot;210&quot; y=&quot;185&quot; width=&quot;570&quot; height=&quot;150&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-42&quot; value=&quot;&amp;lt;span style=&amp;quot;color: rgb(0, 0, 0);&amp;quot;&amp;gt;{&amp;lt;/span&amp;gt;&amp;lt;div style=&amp;quot;color: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-weight: 400;&amp;quot;&amp;gt;&amp;lt;span style=&amp;quot;font-family: &amp;amp;quot;Courier New&amp;amp;quot;; font-size: 15px; font-weight: 700;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;quot;text&amp;quot;: &amp;quot;What&amp;#39;s in this image?&amp;quot;,&amp;amp;nbsp;&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;color: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-weight: 400;&amp;quot;&amp;gt;&amp;lt;span style=&amp;quot;font-family: &amp;amp;quot;Courier New&amp;amp;quot;; font-size: 15px; font-weight: 700;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;quot;image&amp;quot;: &amp;quot;bird.jpg&amp;quot;&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;color: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-weight: 400;&amp;quot;&amp;gt;&amp;lt;span style=&amp;quot;font-family: &amp;amp;quot;Courier New&amp;amp;quot;; font-size: 15px; font-weight: 700;&amp;quot;&amp;gt;}&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;&quot; style=&quot;text;strokeColor=#333333;align=left;fillColor=default;html=1;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=15;fontFamily=Courier New;fontStyle=1;spacing=7;fontColor=#333333;&quot; parent=&quot;1&quot; vertex=&quot;1&quot;&gt;&#10;          &lt;mxGeometry x=&quot;210&quot; y=&quot;40&quot; width=&quot;570&quot; height=&quot;95&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-44&quot; value=&quot;{&amp;lt;div style=&amp;quot;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;quot;tokens&amp;quot;: [&amp;lt;span style=&amp;quot;background-color: initial;&amp;quot;&amp;gt;128000,&amp;amp;nbsp;&amp;lt;/span&amp;gt;&amp;lt;span style=&amp;quot;background-color: initial;&amp;quot;&amp;gt;128006, ...]&amp;lt;/span&amp;gt;&amp;lt;span style=&amp;quot;background-color: initial;&amp;quot;&amp;gt;,&amp;amp;nbsp;&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;&amp;quot;&amp;gt;&amp;lt;span style=&amp;quot;background-color: initial;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;quot;mask&amp;quot;: [True, True, ...],&amp;amp;nbsp;&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;&amp;quot;&amp;gt;&amp;lt;span style=&amp;quot;background-color: initial;&amp;quot;&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;quot;encoder_input&amp;quot;: {...},&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div style=&amp;quot;&amp;quot;&amp;gt;&amp;lt;span style=&amp;quot;background-color: initial;&amp;quot;&amp;gt;}&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;&quot; style=&quot;text;strokeColor=#333333;align=left;fillColor=default;html=1;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=15;fontFamily=Courier New;fontStyle=1;spacing=7;fontColor=#333333;&quot; parent=&quot;1&quot; vertex=&quot;1&quot;&gt;&#10;          &lt;mxGeometry x=&quot;210&quot; y=&quot;385&quot; width=&quot;570&quot; height=&quot;110&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;        &lt;mxCell id=&quot;IdahOF6jb9ku1gB2LzXh-47&quot; value=&quot;logits = self._model(**batch)&quot; style=&quot;text;strokeColor=#333333;align=left;fillColor=default;html=1;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=15;fontFamily=Courier New;fontStyle=1;spacing=7;fontColor=#333333;&quot; parent=&quot;1&quot; vertex=&quot;1&quot;&gt;&#10;          &lt;mxGeometry x=&quot;210&quot; y=&quot;545&quot; width=&quot;570&quot; height=&quot;40&quot; as=&quot;geometry&quot; /&gt;&#10;        &lt;/mxCell&gt;&#10;      &lt;/root&gt;&#10;    &lt;/mxGraphModel&gt;&#10;  &lt;/diagram&gt;&#10;&lt;/mxfile&gt;&#10;" style="background-color: rgb(255, 255, 255);"><defs/><rect fill="#ffffff" width="100%" height="100%" x="0" y="0"/><g><g data-cell-id="0"><g data-cell-id="1"><g data-cell-id="IdahOF6jb9ku1gB2LzXh-49"><g><path d="M 120 345 L 120 388.63" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 120 393.88 L 116.5 386.88 L 120 388.63 L 123.5 386.88 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-2"><g><rect x="50" y="195" width="140" height="150" fill="#ffe6cc" stroke="#d79b00" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 138px; height: 1px; padding-top: 270px; margin-left: 51px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 17px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Message transform</div></div></div></foreignObject><text x="120" y="275" fill="rgb(0, 0, 0)" font-family="&quot;Helvetica&quot;" font-size="17px" text-anchor="middle">Message transform</text></switch></g></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-50"><g><path d="M 120 505 L 120 548.63" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 120 553.88 L 116.5 546.88 L 120 548.63 L 123.5 546.88 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-17"><g><rect x="50" y="395" width="140" height="110" fill="#dae8fc" stroke="#6c8ebf" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 138px; height: 1px; padding-top: 450px; margin-left: 51px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 17px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Model transform</div></div></div></foreignObject><text x="120" y="455" fill="rgb(0, 0, 0)" font-family="&quot;Helvetica&quot;" font-size="17px" text-anchor="middle">Model transform</text></switch></g></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-48"><g><path d="M 120 145 L 120 188.63" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 120 193.88 L 116.5 186.88 L 120 188.63 L 123.5 186.88 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-21"><g><rect x="50" y="50" width="140" height="95" fill="#fff2cc" stroke="#d6b656" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 138px; height: 1px; padding-top: 98px; margin-left: 51px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 17px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Raw sample dict</div></div></div></foreignObject><text x="120" y="103" fill="rgb(0, 0, 0)" font-family="&quot;Helvetica&quot;" font-size="17px" text-anchor="middle">Raw sample dict</text></switch></g></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-28"><g><rect x="50" y="555" width="140" height="40" fill="#d5e8d4" stroke="#82b366" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 138px; height: 1px; padding-top: 575px; margin-left: 51px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 17px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Model input</div></div></div></foreignObject><text x="120" y="580" fill="rgb(0, 0, 0)" font-family="&quot;Helvetica&quot;" font-size="17px" text-anchor="middle">Model input</text></switch></g></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-40"><g><rect x="190" y="195" width="570" height="150" fill="rgb(255, 255, 255)" stroke="#333333" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe flex-start; justify-content: unsafe flex-start; width: 558px; height: 1px; padding-top: 207px; margin-left: 197px;"><div data-drawio-colors="color: #333333; " style="box-sizing: border-box; font-size: 0px; text-align: left;"><div style="display: inline-block; font-size: 15px; font-family: &quot;Courier New&quot;; color: rgb(51, 51, 51); line-height: 1.2; pointer-events: all; font-weight: bold; white-space: normal; overflow-wrap: normal;">Message(<div style="font-size: 15px;">  role="user",</div><div style="font-size: 15px;">  content=[</div><div style="font-size: 15px;">    {"type": "image", "content": &lt;PIL.Image.Image&gt;},</div><div style="font-size: 15px;">    {"type": "text", "content": "What's in this image?"},</div><div style="font-size: 15px;">  ],</div><div style="font-size: 15px;">)</div></div></div></div></foreignObject><text x="197" y="222" fill="#333333" font-family="&quot;Courier New&quot;" font-size="15px" font-weight="bold">Message(...</text></switch></g></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-42"><g><rect x="190" y="50" width="570" height="95" fill="rgb(255, 255, 255)" stroke="#333333" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe flex-start; justify-content: unsafe flex-start; width: 558px; height: 1px; padding-top: 62px; margin-left: 197px;"><div data-drawio-colors="color: #333333; " style="box-sizing: border-box; font-size: 0px; text-align: left;"><div style="display: inline-block; font-size: 15px; font-family: &quot;Courier New&quot;; color: rgb(51, 51, 51); line-height: 1.2; pointer-events: all; font-weight: bold; white-space: normal; overflow-wrap: normal;"><span style="color: rgb(0, 0, 0);">{</span><div style="color: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-weight: 400;"><span style="font-family: &quot;Courier New&quot;; font-size: 15px; font-weight: 700;">  "text": "What's in this image?",</span></div><div style="color: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-weight: 400;"><span style="font-family: &quot;Courier New&quot;; font-size: 15px; font-weight: 700;">  "image": "bird.jpg"</span></div><div style="color: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-weight: 400;"><span style="font-family: &quot;Courier New&quot;; font-size: 15px; font-weight: 700;">}</span></div></div></div></div></foreignObject><text x="197" y="77" fill="#333333" font-family="&quot;Courier New&quot;" font-size="15px" font-weight="bold">{...</text></switch></g></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-44"><g><rect x="190" y="395" width="570" height="110" fill="rgb(255, 255, 255)" stroke="#333333" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe flex-start; justify-content: unsafe flex-start; width: 558px; height: 1px; padding-top: 407px; margin-left: 197px;"><div data-drawio-colors="color: #333333; " style="box-sizing: border-box; font-size: 0px; text-align: left;"><div style="display: inline-block; font-size: 15px; font-family: &quot;Courier New&quot;; color: rgb(51, 51, 51); line-height: 1.2; pointer-events: all; font-weight: bold; white-space: normal; overflow-wrap: normal;">{<div style="">  "tokens": [<span style="background-color: initial;">128000,</span><span style="background-color: initial;">128006, ...]</span><span style="background-color: initial;">,</span></div><div style=""><span style="background-color: initial;">  "mask": [True, True, ...],</span></div><div style=""><span style="background-color: initial;">  "encoder_input": {...},</span></div><div style=""><span style="background-color: initial;">}</span></div></div></div></div></foreignObject><text x="197" y="422" fill="#333333" font-family="&quot;Courier New&quot;" font-size="15px" font-weight="bold">{...</text></switch></g></g></g><g data-cell-id="IdahOF6jb9ku1gB2LzXh-47"><g><rect x="190" y="555" width="570" height="40" fill="rgb(255, 255, 255)" stroke="#333333" pointer-events="all"/></g><g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe flex-start; justify-content: unsafe flex-start; width: 558px; height: 1px; padding-top: 567px; margin-left: 197px;"><div data-drawio-colors="color: #333333; " style="box-sizing: border-box; font-size: 0px; text-align: left;"><div style="display: inline-block; font-size: 15px; font-family: &quot;Courier New&quot;; color: rgb(51, 51, 51); line-height: 1.2; pointer-events: all; font-weight: bold; white-space: normal; overflow-wrap: normal;">logits = self._model(**batch)</div></div></div></foreignObject><text x="197" y="582" fill="#333333" font-family="&quot;Courier New&quot;" font-size="15px" font-weight="bold">logits = self._model(**batch)</text></switch></g></g></g></g></g></g></svg>
\ No newline at end of file
Binary files marc_original/third_party/torchtune/docs/source/_static/img/torchtune_workspace.png and marc/third_party/torchtune/docs/source/_static/img/torchtune_workspace.png differ
diff -ruN marc_original/third_party/torchtune/docs/source/_templates/autosummary/class.rst marc/third_party/torchtune/docs/source/_templates/autosummary/class.rst
--- marc_original/third_party/torchtune/docs/source/_templates/autosummary/class.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/_templates/autosummary/class.rst	2025-02-20 17:49:28.930023227 -0500
@@ -0,0 +1,9 @@
+.. role:: hidden
+    :class: hidden-section
+.. currentmodule:: {{ module }}
+
+
+{{ name | underline}}
+
+.. autoclass:: {{ name }}
+    :members:
diff -ruN marc_original/third_party/torchtune/docs/source/_templates/autosummary/function.rst marc/third_party/torchtune/docs/source/_templates/autosummary/function.rst
--- marc_original/third_party/torchtune/docs/source/_templates/autosummary/function.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/_templates/autosummary/function.rst	2025-02-20 17:49:28.934023233 -0500
@@ -0,0 +1,8 @@
+.. role:: hidden
+    :class: hidden-section
+.. currentmodule:: {{ module }}
+
+
+{{ name | underline}}
+
+.. autofunction:: {{ name }}
diff -ruN marc_original/third_party/torchtune/docs/source/_templates/layout.html marc/third_party/torchtune/docs/source/_templates/layout.html
--- marc_original/third_party/torchtune/docs/source/_templates/layout.html	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/_templates/layout.html	2025-02-20 17:49:28.938023240 -0500
@@ -0,0 +1,21 @@
+{% extends "!layout.html" %}
+
+{% block sidebartitle %}
+    <div class="version">
+        <a href='https://pytorch.org/torchtune/versions.html'>{{ version }} &#x25BC</a>
+      </div>
+    {% include "searchbox.html" %}
+{% endblock %}
+
+
+{% block footer %}
+<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
+<!-- {{ super() }}
+<script script type="text/javascript">
+    var collapsedSections = ['Introduction', 'Getting Started', 'Tutorials']
+</script> -->
+
+<script script type="text/javascript">
+    var collapsedSections = []
+</script>
+{% endblock %}
diff -ruN marc_original/third_party/torchtune/docs/source/tune_cli.rst marc/third_party/torchtune/docs/source/tune_cli.rst
--- marc_original/third_party/torchtune/docs/source/tune_cli.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tune_cli.rst	2025-02-20 17:49:29.062023444 -0500
@@ -0,0 +1,222 @@
+.. _cli_label:
+
+=============
+torchtune CLI
+=============
+
+This page is the documentation for using the torchtune CLI - a convenient way to
+download models, find and copy relevant recipes/configs, and run recipes. It is automatically
+available when you install torchtune.
+
+Getting started
+---------------
+
+The ``--help`` option will show all the possible commands available through the torchtune CLI,
+with a short description of each.
+
+.. code-block:: bash
+
+    $ tune --help
+    usage: tune [-h] {download,ls,cp,run,validate} ...
+
+    Welcome to the torchtune CLI!
+
+    options:
+    -h, --help            show this help message and exit
+
+    subcommands:
+      {download,ls,cp,run,validate}
+        download            Download a model from the Hugging Face Hub.
+        ls                  List all built-in recipes and configs
+        ...
+
+The ``--help`` option is convenient for getting more details about any command. You can use it anytime to list all
+available options and their details. For example, ``tune download --help`` provides more information on how
+to download files using the CLI.
+
+.. _tune_download_label:
+
+Download a model
+----------------
+
+The ``tune download <path>`` command downloads any model from the Hugging Face Hub.
+
+.. list-table::
+   :widths: 30 60
+
+   * - \--output-dir
+     - Directory in which to save the model.
+   * - \--output-dir-use-symlinks
+     - To be used with `output-dir`. If set to 'auto', the cache directory will be used and the file will be either duplicated or symlinked to the local directory depending on its size. It set to `True`, a symlink will be created, no matter the file size. If set to `False`, the file will either be duplicated from cache (if already exists) or downloaded from the Hub and not cached.
+   * - \--hf-token
+     - Hugging Face API token. Needed for gated models like Llama.
+   * - \--ignore-patterns
+     - If provided, files matching any of the patterns are not downloaded. Defaults to ignoring safetensors files to avoid downloading duplicate weights.
+
+.. code-block:: bash
+
+    $ tune download meta-llama/Meta-Llama-3-8B-Instruct
+    Successfully downloaded model repo and wrote to the following locations:
+    ./model/config.json
+    ./model/README.md
+    ./model/model-00001-of-00002.bin
+    ...
+
+
+**Download a gated model**
+
+A lot of recent large pretrained models released from organizations like Meta or MistralAI require you to agree
+to the usage terms and conditions before you are allowed to download their model. If this is the case, you can specify
+a Hugging Face access token.
+
+You can find the access token `here <https://huggingface.co/docs/hub/en/security-tokens>`_.
+
+.. code-block:: bash
+
+    $ tune download meta-llama/Meta-Llama-3-8B-Instruct --hf-token <TOKEN>
+    Successfully downloaded model repo and wrote to the following locations:
+    ./model/config.json
+    ./model/README.md
+    ./model/model-00001-of-00002.bin
+    ...
+
+.. note::
+    If you'd prefer, you can also use ``huggingface-cli login`` to permanently login to the Hugging Face Hub on your machine.
+    The ``tune download`` command will pull the access token from your environment.
+
+**Specify model files you don't want to download**
+
+Some checkpoint directories can be very large and it can eat up a lot of bandwith and local storage to download the all of the files every time, even if you might
+not need a lot of them. This is especially common when the same checkpoint exists in different formats. You can specify patterns to ignore to prevent downloading files
+with matching names. By default we ignore safetensor files, but if you want to include all files you can pass in an empty string.
+
+.. code-block:: bash
+
+    $ tune download meta-llama/Meta-Llama-3-8B-Instruct --hf-token <TOKEN> --ignore-patterns None
+    Successfully downloaded model repo and wrote to the following locations:
+    ./model/config.json
+    ./model/README.md
+    ./model/model-00001-of-00030.safetensors
+    ...
+
+.. note::
+    Just because a model can be downloaded does not mean that it will work OOTB with torchtune's
+    built-in recipes or configs. For a list of supported model families and architectures, see :ref:`models<models>`.
+
+
+.. _tune_ls_label:
+
+List built-in recipes and configs
+---------------------------------
+
+The ``tune ls`` command lists out all the built-in recipes and configs within torchtune.
+
+
+.. code-block:: bash
+
+    $ tune ls
+    RECIPE                                   CONFIG
+    full_finetune_single_device              llama2/7B_full_low_memory
+                                             code_llama2/7B_full_low_memory
+                                             llama3/8B_full_single_device
+                                             mistral/7B_full_low_memory
+                                             phi3/mini_full_low_memory
+    full_finetune_distributed                llama2/7B_full
+                                             llama2/13B_full
+                                             llama3/8B_full
+                                             llama3/70B_full
+    ...
+
+.. _tune_cp_cli_label:
+
+Copy a built-in recipe or config
+--------------------------------
+
+The ``tune cp <recipe|config> <path>`` command copies built-in recipes and configs to a provided location. This allows you to make a local copy of a library
+recipe or config to edit directly for yourself. See :ref:`here <tune_cp_label>` for an example of how to use this command.
+
+.. list-table::
+   :widths: 30 60
+
+   * - \-n, \--no-clobber
+     - Do not overwrite destination if it already exists
+   * - \--make-parents
+     - Create parent directories for destination if they do not exist. If not set to True, will error if parent directories do not exist
+
+.. code-block:: bash
+
+    $ tune cp lora_finetune_distributed .
+    Copied file to ./lora_finetune_distributed.py
+
+Run a recipe
+------------
+
+The ``tune run <recipe> --config <config>`` is a wrapper around `torchrun <https://pytorch.org/docs/stable/elastic/run.html>`_. ``tune run`` allows you to specify
+a built-in recipe or config by name, or by path to use your local recipes/configs.
+
+To run a tune recipe
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device --config llama3/8B_lora_single_device
+
+**Specifying distributed (torchrun) arguments**
+
+``tune run`` supports launching distributed runs by passing through arguments preceding the recipe directly to torchrun. This follows the pattern used by torchrun
+of specifying distributed and host machine flags before the script (recipe). For a full list of available flags for distributed setup, see the `torchrun docs <https://pytorch.org/docs/stable/elastic/run.html>`_.
+
+Some common flags:
+
+.. list-table::
+   :widths: 30 60
+
+   * - \--nproc-per-node
+     - Number of workers per node; supported values: [auto, cpu, gpu, int].
+   * - \--nnodes
+     - Number of nodes, or the range of nodes in form <minimum_nodes>:<maximum_nodes>.
+   * - \--max-restarts
+     - Maximum number of worker group restarts before failing.
+   * - \--rdzv-backend
+     - Rendezvous backend.
+   * - \--rdzv-endpoint
+     - Rendezvous backend endpoint; usually in form <host>:<port>.
+
+.. code-block:: bash
+
+    tune run --nnodes=1 --nproc-per-node=4 lora_finetune_distributed --config llama3/8B_lora
+
+.. note::
+    If no arguments are provided before the recipe, tune will bypass torchrun and launch directly with ``python``. This can simplify running and debugging recipes
+    when distributed isn't needed. If you want to launch with torchrun, but use only a single device, you can specify ``tune run --nnodes=1 --nproc-per-node=1 <recipe> --config <config>``.
+
+**Running a custom (local) recipe and config**
+
+To use ``tune run`` with your own local recipes and configs, simply pass in a file path instead of a name to the run command. You can mix and match a custom recipe with a
+torchtune config or vice versa or you can use both custom configs and recipes.
+
+.. code-block:: bash
+
+    tune run my/fancy_lora.py --config my/configs/8B_fancy_lora.yaml
+
+**Overriding the config**
+
+You can override existing parameters from the command line using a key=value format. Lets say you want to set the number of training epochs to 1.
+Further information on config overrides can be found :ref:`here  <cli_override>`.
+
+.. code-block:: bash
+
+  tune run <RECIPE> --config <CONFIG> epochs=1
+
+.. _validate_cli_label:
+
+Validate a config
+-----------------
+
+The ``tune validate <config>`` command will validate that your config is formatted properly.
+
+
+.. code-block:: bash
+
+    # If you've copied over a built-in config and want to validate custom changes
+    $ tune validate my_configs/llama3/8B_full.yaml
+    Config is well-formed!
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/chat.rst marc/third_party/torchtune/docs/source/tutorials/chat.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/chat.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/chat.rst	2025-02-20 17:49:29.070023456 -0500
@@ -0,0 +1,331 @@
+.. _chat_tutorial_label:
+
+=================================
+Fine-Tuning Llama3 with Chat Data
+=================================
+
+Llama3 Instruct introduced a new prompt template for fine-tuning with chat data. In this tutorial,
+we'll cover what you need to know to get you quickly started on preparing your own
+custom chat dataset for fine-tuning Llama3 Instruct.
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` You will learn:
+
+      * How the Llama3 Instruct format differs from Llama2
+      * All about prompt templates and special tokens
+      * How to use your own chat dataset to fine-tune Llama3 Instruct
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Be familiar with :ref:`configuring datasets<dataset_tutorial_label>`
+      * Know how to :ref:`download Llama3 Instruct weights <llama3_label>`
+
+
+Template changes from Llama2 to Llama3
+--------------------------------------
+
+The Llama2 chat model requires a specific template when prompting the pre-trained
+model. Since the chat model was pretrained with this prompt template, if you want to run
+inference on the model, you'll need to use the same template for optimal performance
+on chat data. Otherwise, the model will just perform standard text completion, which
+may or may not align with your intended use case.
+
+From the `official Llama2 prompt
+template guide <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-2>`_
+for the Llama2 chat model, we can see that special tags are added:
+
+.. code-block:: text
+
+    <s>[INST] <<SYS>>
+    You are a helpful, respectful, and honest assistant.
+    <</SYS>>
+
+    Hi! I am a human. [/INST] Hello there! Nice to meet you! I'm Meta AI, your friendly AI assistant </s>
+
+Llama3 Instruct `overhauled <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3>`_
+the template from Llama2 to better support multiturn conversations. The same text
+in the Llama3 Instruct format would look like this:
+
+.. code-block:: text
+
+    <|begin_of_text|><|start_header_id|>system<|end_header_id|>
+
+    You are a helpful, respectful, and honest assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>
+
+    Hi! I am a human.<|eot_id|><|start_header_id|>assistant<|end_header_id|>
+
+    Hello there! Nice to meet you! I'm Meta AI, your friendly AI assistant<|eot_id|>
+
+The tags are entirely different, and they are actually encoded differently than in
+Llama2. Let's walk through tokenizing an example with the Llama2 template and the
+Llama3 template to understand how.
+
+.. note::
+    The Llama3 Base model uses a `different prompt template
+    <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3>`_ than Llama3 Instruct
+    because it has not yet been instruct tuned and the extra special tokens are untrained. If you
+    are running inference on the Llama3 Base model without fine-tuning we recommend the base
+    template for optimal performance. Generally, for instruct and chat data, we recommend using
+    Llama3 Instruct with its prompt template. The rest of this tutorial assumes you are using
+    Llama3 Instruct.
+
+.. _prompt_template_vs_special_tokens:
+
+Tokenizing prompt templates & special tokens
+--------------------------------------------
+
+Let's say I have a sample of a single user-assistant turn accompanied with a system
+prompt:
+
+.. code-block:: python
+
+    sample = [
+        {
+            "role": "system",
+            "content": "You are a helpful, respectful, and honest assistant.",
+        },
+        {
+            "role": "user",
+            "content": "Who are the most influential hip-hop artists of all time?",
+        },
+        {
+            "role": "assistant",
+            "content": "Here is a list of some of the most influential hip-hop "
+            "artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.",
+        },
+    ]
+
+Now, let's format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and
+see how it gets tokenized. The Llama2ChatTemplate is an example of a **prompt template**,
+which simply structures a prompt with flavor text to indicate a certain task.
+
+.. code-block:: python
+
+    from torchtune.data import Llama2ChatTemplate, Message
+
+    messages = [Message.from_dict(msg) for msg in sample]
+    formatted_messages = Llama2ChatTemplate.format(messages)
+    print(formatted_messages)
+    # [
+    #     Message(
+    #         role='user',
+    #         content='[INST] <<SYS>>\nYou are a helpful, respectful, and honest assistant.\n<</SYS>>\n\nWho are the most influential hip-hop artists of all time? [/INST] ',
+    #         ...,
+    #     ),
+    #     Message(
+    #         role='assistant',
+    #         content='Here is a list of some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.',
+    #         ...,
+    #     ),
+    # ]
+
+There are also special tokens used by Llama2, which are not in the prompt template.
+If you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you'll notice that
+we don't include the :code:`<s>` and :code:`</s>` tokens. These are the beginning-of-sequence
+(BOS) and end-of-sequence (EOS) tokens that are represented differently in the tokenizer
+than the rest of the prompt template. Let's tokenize this example with the
+:func:`~torchtune.models.llama2.llama2_tokenizer` used by Llama2 to see
+why.
+
+.. code-block:: python
+
+    from torchtune.models.llama2 import llama2_tokenizer
+
+    tokenizer = llama2_tokenizer("/tmp/Llama-2-7b-hf/tokenizer.model")
+    user_message = formatted_messages[0].text_content
+    tokens = tokenizer.encode(user_message, add_bos=True, add_eos=True)
+    print(tokens)
+    # [1, 518, 25580, 29962, 3532, 14816, 29903, 6778, ..., 2]
+
+We've added the BOS and EOS tokens when encoding our example text. This shows up
+as IDs 1 and 2. We can verify that these are our BOS and EOS tokens.
+
+.. code-block:: python
+
+    print(tokenizer._spm_model.spm_model.piece_to_id("<s>"))
+    # 1
+    print(tokenizer._spm_model.spm_model.piece_to_id("</s>"))
+    # 2
+
+The BOS and EOS tokens are what we call special tokens, because they have their own
+reserved token IDs. This means that they will index to their own individual vectors in
+the model's learnt embedding table. The rest of the prompt template tags, :code:`[INST]`
+and :code:`<<SYS>>` are tokenized as normal text and not their own IDs.
+
+.. code-block:: python
+
+    print(tokenizer.decode(518))
+    # '['
+    print(tokenizer.decode(25580))
+    # 'INST'
+    print(tokenizer.decode(29962))
+    # ']'
+    print(tokenizer.decode([3532, 14816, 29903, 6778]))
+    # '<<SYS>>'
+
+It's important to note that you should not place the special reserved tokens in your
+input prompts manually, as it will be treated as normal text and not as a special
+token.
+
+.. code-block:: python
+
+    print(tokenizer.encode("<s>", add_bos=False, add_eos=False))
+    # [529, 29879, 29958]
+
+Now let's take a look at Llama3's formatting to see how it's tokenized differently
+than Llama2.
+
+.. code-block:: python
+
+    from torchtune.models.llama3 import llama3_tokenizer
+
+    tokenizer = llama3_tokenizer("/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model")
+    messages = [Message.from_dict(msg) for msg in sample]
+    tokens, mask = tokenizer.tokenize_messages(messages)
+    print(tokenizer.decode(tokens))
+    # '<|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful,
+    # and honest assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWho
+    # are the most influential hip-hop artists of all time?<|eot_id|><|start_header_id|>
+    # assistant<|end_header_id|>\n\nHere is a list of some of the most influential hip-hop
+    # artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.<|eot_id|>'
+
+.. note::
+    We used the ``tokenize_messages`` API for Llama3, which is different than
+    encode. It simply manages adding all the special tokens in the correct
+    places after encoding the individual messages.
+
+We can see that the tokenizer handled all the formatting without us specifying a prompt
+template. It turns out that all of the additional tags are special tokens, and we don't require
+a separate prompt template. We can verify this by checking if the tags get encoded
+as their own token IDs.
+
+.. code-block:: python
+
+    print(tokenizer.special_tokens["<|begin_of_text|>"])
+    # 128000
+    print(tokenizer.special_tokens["<|eot_id|>"])
+    # 128009
+
+The best part is - all these special tokens are handled purely by the tokenizer.
+That means you won't have to worry about messing up any required prompt templates!
+
+
+When should I use a prompt template?
+------------------------------------
+
+Whether or not to use a prompt template is governed by what your desired inference
+behavior is. You should use a prompt template if you are running inference on the
+base model and it was pre-trained with a prompt template, or you want to prime a
+fine-tuned model to expect a certain prompt structure on inference for a specific task.
+
+It is not strictly necessary to fine-tune with a prompt template, but generally
+specific tasks will require specific templates. For example, the :class:`~torchtune.data.SummarizeTemplate`
+provides a lightweight structure to prime your fine-tuned model for prompts asking to summarize text.
+This would wrap around the user message, with the assistant message untouched.
+
+.. code-block:: python
+
+    f"Summarize this dialogue:\n{dialogue}\n---\nSummary:\n"
+
+You can fine-tune Llama2 with this template even though the model was originally pre-trained
+with the :class:`~torchtune.models.llama2.Llama2ChatTemplate`, as long as this is what the model
+sees during inference. The model should be robust enough to adapt to a new template.
+
+
+Fine-tuning on a custom chat dataset
+------------------------------------
+
+Let's test our understanding by trying to fine-tune the Llama3-8B instruct model with a custom
+chat dataset. We'll walk through how to set up our data so that it can be tokenized
+correctly and fed into our model.
+
+Let's say we have a local dataset saved as a JSON file that contains conversations
+with an AI model. How can we get something like this into a format
+Llama3 understands and tokenizes correctly?
+
+.. code-block:: python
+
+    # data/my_data.json
+    [
+        {
+            "dialogue": [
+                {
+                    "from": "human",
+                    "value": "What is your name?"
+                },
+                {
+                    "from": "gpt",
+                    "value": "I am an AI assistant, I don't have a name."
+                },
+                {
+                    "from": "human",
+                    "value": "Pretend you have a name."
+                },
+                {
+                    "from": "gpt",
+                    "value": "My name is Mark Zuckerberg."
+                }
+            ]
+        },
+    ]
+
+Let's first take a look at the :ref:`dataset_builders` and see which fits our use case. Since we
+have conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. For any
+custom local dataset we always need to specify ``source``, ``data_files``, and ``split`` for any dataset
+builder in torchtune. For :func:`~torchtune.datasets.chat_dataset`, we additionally need to specify
+``conversation_column`` and ``conversation_style``. Our data follows the ``"sharegpt"`` format, so
+we can specify that here. Altogether, our :func:`~torchtune.datasets.chat_dataset` call should
+look like so:
+
+.. code-block:: python
+
+    from torchtune.datasets import chat_dataset
+    from torchtune.models.llama3 import llama3_tokenizer
+
+    tokenizer = llama3_tokenizer("/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model")
+    ds = chat_dataset(
+        tokenizer=tokenizer,
+        source="json",
+        data_files="data/my_data.json",
+        split="train",
+        conversation_column="dialogue",
+        conversation_style="sharegpt",
+    )
+
+.. code-block:: yaml
+
+    # In config
+    tokenizer:
+      _component_: torchtune.models.llama3.llama3_tokenizer
+      path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+
+    dataset:
+      _component_: torchtune.datasets.chat_dataset
+      source: json
+      data_files: data/my_data.json
+      split: train
+      conversation_column: dialogue
+      conversation_style: sharegpt
+
+.. note::
+    You can pass in any keyword argument for `load_dataset <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_ into all our
+    Dataset classes and they will honor them. This is useful for common parameters
+    such as specifying the data split with :code:`split` or configuration with
+    :code:`name`
+
+If you needed to add a prompt template, you would simply pass it into the tokenizer.
+Since we're fine-tuning Llama3, the tokenizer will handle all formatting for
+us and prompt templates are optional. Other models such as Mistral's :class:`~torchtune.models.mistral._tokenizer.MistralTokenizer`,
+use a chat template by default (:class:`~torchtune.models.mistral.MistralChatTemplate`) to format
+all messages according to their `recommendations <https://docs.mistral.ai/getting-started/open_weight_models/#chat-template>`_.
+
+Now we're ready to start fine-tuning! We'll use the built-in LoRA single device recipe.
+Use the :ref:`tune cp <tune_cp_cli_label>` command to get a copy of the :code:`8B_lora_single_device.yaml`
+config and update it with your dataset configuration.
+
+Launch the fine-tune!
+
+.. code-block:: bash
+
+    $ tune run lora_finetune_single_device --config custom_8B_lora_single_device.yaml epochs=15
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/datasets.rst marc/third_party/torchtune/docs/source/tutorials/datasets.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/datasets.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/datasets.rst	2025-02-20 17:49:29.074023463 -0500
@@ -0,0 +1,562 @@
+.. _dataset_tutorial_label:
+
+====================================
+Configuring Datasets for Fine-Tuning
+====================================
+
+This tutorial will guide you through how to set up a dataset to fine-tune on.
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn
+
+      * How to quickly get started with built-in datasets
+      * How to use any dataset from Hugging Face Hub
+      * How to use instruct, chat, or text completion datasets
+      * How to configure datasets from code, config, or command-line
+      * How to fully customize your own dataset
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Know how to :ref:`configure components from the config<config_tutorial_label>`
+
+Datasets are a core component of fine-tuning workflows that serve as a "steering
+wheel" to guide LLM generation for a particular use case. Many publicly shared
+open-source datasets have become popular for fine-tuning LLMs and serve as a great
+starting point to train your model. torchtune gives you the tools to download external
+community datasets, load in custom local datasets, or create your own datasets.
+
+Built-in datasets
+-----------------
+
+To use one of the built-in datasets in the library, simply import and call the dataset builder
+function. You can see a list of all supported datasets :ref:`here<datasets>`.
+
+.. code-block:: python
+
+    from torchtune.datasets import alpaca_dataset
+
+    # Load in tokenizer
+    tokenizer = ...
+    dataset = alpaca_dataset(tokenizer)
+
+.. code-block:: yaml
+
+    # YAML config
+    dataset:
+      _component_: torchtune.datasets.alpaca_dataset
+
+.. code-block:: bash
+
+    # Command line
+    tune run full_finetune_single_device --config llama3/8B_full_single_device \
+    dataset=torchtune.datasets.alpaca_dataset
+
+Hugging Face datasets
+---------------------
+
+We provide first class support for datasets on the Hugging Face hub. Under the hood,
+all of our built-in datasets and dataset builders are using Hugging Face's `load_dataset() <https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset>`_
+to load in your data, whether local or on the hub.
+
+You can pass in a Hugging Face dataset path to the ``source`` parameter in any of our builders
+to specify which dataset on the hub to download or use from a local directory path (see `Local and remote datasets`_). Additionally, all builders accept
+any keyword-arguments that ``load_dataset()`` supports. You can see a full list
+on Hugging Face's `documentation. <https://huggingface.co/docs/datasets/en/loading>`_
+
+.. code-block:: python
+
+    from torchtune.datasets import text_completion_dataset
+
+    # Load in tokenizer
+    tokenizer = ...
+    dataset = text_completion_dataset(
+        tokenizer,
+        source="allenai/c4",
+        # Keyword-arguments that are passed into load_dataset
+        split="train",
+        data_dir="realnewslike",
+    )
+
+.. code-block:: yaml
+
+    # YAML config
+    dataset:
+      _component_: torchtune.datasets.text_completion_dataset
+      source: allenai/c4
+      split: train
+      data_dir: realnewslike
+
+.. code-block:: bash
+
+    # Command line
+    tune run full_finetune_single_device --config llama3/8B_full_single_device \
+    dataset=torchtune.datasets.text_completion_dataset dataset.source=allenai/c4 \
+    dataset.split=train dataset.data_dir=realnewslike
+
+Setting max sequence length
+---------------------------
+
+The default collator, :func:`~torchtune.data.padded_collate`, used in all
+our training recipes will pad samples to the max sequence length within the batch,
+not globally. If you wish to set an upper limit on the max sequence length globally,
+you can specify it in the dataset builder with ``max_seq_len``. Any sample in the dataset
+that is longer than ``max_seq_len`` will be truncated in :func:`~torchtune.data.truncate`.
+The tokenizer's EOS ids are ensured to be the last token, except in :class:`~torchtune.datasets.TextCompletionDataset`.
+
+Generally, you want the max sequence length returned in each data sample to match the context window
+size of your model. You can also decrease this value to reduce memory usage
+depending on your hardware constraints.
+
+.. code-block:: python
+
+    from torchtune.datasets import alpaca_dataset
+
+    # Load in tokenizer
+    tokenizer = ...
+    dataset = alpaca_dataset(
+        tokenizer=tokenizer,
+        max_seq_len=4096,
+    )
+
+.. code-block:: yaml
+
+    # YAML config
+    dataset:
+      _component_: torchtune.datasets.alpaca_dataset
+      max_seq_len: 4096
+
+.. code-block:: bash
+
+    # Command line
+    tune run full_finetune_single_device --config llama3/8B_full_single_device \
+    dataset.max_seq_len=4096
+
+Sample packing
+--------------
+
+You can use sample packing with any of the single dataset builders by passing in
+:code:`packed=True`. This requires some pre-processing of the dataset which may
+slow down time-to-first-batch, but can introduce significant training speedups
+depending on the dataset.
+
+.. code-block:: python
+
+    from torchtune.datasets import alpaca_dataset, PackedDataset
+
+    # Load in tokenizer
+    tokenizer = ...
+    dataset = alpaca_dataset(
+        tokenizer=tokenizer,
+        packed=True,
+    )
+    print(isinstance(dataset, PackedDataset))  # True
+
+.. code-block:: yaml
+
+    # YAML config
+    dataset:
+      _component_: torchtune.datasets.alpaca_dataset
+      packed: True
+
+.. code-block:: bash
+
+    # Command line
+    tune run full_finetune_single_device --config llama3/8B_full_single_device \
+    dataset.packed=True
+
+
+Custom unstructured text corpus
+-------------------------------
+
+For continued pre-training, typically a similar data setup to pre-training is used
+for a simple text completion task. This means no instruct templates, chat formats,
+and minimal special tokens (only BOS and, optionally,  EOS). To specify an unstructured text corpus,
+you can use the :func:`~torchtune.datasets.text_completion_dataset` builder with
+a Hugging Face dataset or a custom local corpus. Here is how to specify it for local
+files:
+
+.. code-block:: python
+
+    from torchtune.datasets import text_completion_dataset
+
+    # Load in tokenizer
+    tokenizer = ...
+    dataset = text_completion_dataset(
+        tokenizer,
+        source="text",
+        data_files="path/to/my_data.txt",
+        split="train",
+    )
+
+.. code-block:: yaml
+
+    # YAML config
+    dataset:
+      _component_: torchtune.datasets.text_completion_dataset
+      source: text
+      data_files: path/to/my_data.txt
+      split: train
+
+.. code-block:: bash
+
+    # Command line
+    tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full \
+    dataset=torchtune.datasets.text_completion_dataset dataset.source=text \
+    dataset.data_files=path/to/my_data.txt dataset.split=train
+
+Custom instruct dataset and instruct templates
+----------------------------------------------
+
+If you have a custom instruct dataset that's not already provided in the library,
+you can use the :func:`~torchtune.datasets.instruct_dataset` builder and specify
+the source path. Instruct datasets typically have multiple columns with text that
+are formatted into a prompt template.
+
+To fine-tune an LLM on a particular task, a common approach is to create a fixed instruct
+template that guides the model to generate output with a specific goal. Instruct templates
+are simply flavor text that structures your inputs for the model. It is model agnostic
+and is tokenized normally just like any other text, but it can help condition the model
+to respond better to an expected format. For example, the :class:`~torchtune.data.AlpacaInstructTemplate`
+structures the data in the following way:
+
+.. code-block:: python
+
+    "Below is an instruction that describes a task, paired with an input that provides further context. "
+    "Write a response that appropriately completes the request.\n\n"
+    "### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n"
+
+Here is an example of a sample that is formatted with :class:`~torchtune.data.AlpacaInstructTemplate`:
+
+.. code-block:: python
+
+    from torchtune.data import AlpacaInstructTemplate
+
+    sample = {
+        "instruction": "Classify the following into animals, plants, and minerals",
+        "input": "Oak tree, copper ore, elephant",
+    }
+    prompt = AlpacaInstructTemplate.format(sample)
+    print(prompt)
+    # Below is an instruction that describes a task, paired with an input that provides further context.
+    # Write a response that appropriately completes the request.
+    #
+    # ### Instruction:
+    # Classify the following into animals, plants, and minerals
+    #
+    # ### Input:
+    # Oak tree, copper ore, elephant
+    #
+    # ### Response:
+    #
+
+We provide :ref:`other instruct templates <data>`
+for common tasks such summarization and grammar correction. If you need to create your own
+instruct template for a custom task, you can inherit from :class:`~torchtune.data.InstructTemplate`
+and create your own class.
+
+.. code-block:: python
+
+    from torchtune.datasets import instruct_dataset
+    from torchtune.data import InstructTemplate
+
+    class CustomTemplate(InstructTemplate):
+        # Define the template as string with {} as placeholders for data columns
+        template = ...
+
+        # Implement this method
+        @classmethod
+        def format(
+            cls, sample: Mapping[str, Any], column_map: Optional[Dict[str, str]] = None
+        ) -> str:
+            ...
+
+    # Load in tokenizer
+    tokenizer = ...
+    dataset = instruct_dataset(
+        tokenizer=tokenizer,
+        source="my/dataset/path",
+        template="import.path.to.CustomTemplate",
+    )
+
+.. code-block:: yaml
+
+    # YAML config
+    dataset:
+      _component_: torchtune.datasets.instruct_dataset
+      source: my/dataset/path
+      template: import.path.to.CustomTemplate
+
+.. code-block:: bash
+
+    # Command line
+    tune run full_finetune_single_device --config llama3/8B_full_single_device \
+    dataset=torchtune.datasets.instruct_dataset dataset.source=my/dataset/path \
+    dataset.template=import.path.to.CustomTemplate
+
+
+torchtune uses :code:`importlib.import_module` (see ``importlib`` `docs <https://docs.python.org/3/library/importlib.html>`_ for more details)
+to locate components from their dotpaths. You can place your custom template class
+in any Python file as long as the file is accessible by Python's import mechanism.
+This means the module should be in a directory that is included in Python's search
+paths (:code:`sys.path`). This often includes:
+
+- The current directory from which your Python interpreter or script is run.
+- Directories where Python packages are installed (like :code:`site-packages`).
+- Any directories added to :code:`sys.path` at runtime using :code:`sys.path.append` or through the :code:`PYTHONPATH` environment variable.
+
+
+Custom chat dataset and chat formats
+------------------------------------
+
+If you have a custom chat/conversational dataset that's not already provided in the library,
+you can use the :func:`~torchtune.datasets.chat_dataset` builder and specify
+the source path. Chat datasets typically have a single column with multiple back
+and forth messages between the user and assistant.
+
+Chat formats are similar to instruct templates, except that they format system,
+user, and assistant messages into a list of messages (see :class:`~torchtune.data.ChatFormat`)
+for a conversational dataset. These can be configured quite similarly to instruct
+datasets.
+
+Here is how messages would be formatted using the :class:`~torchtune.data.Llama2ChatFormat`:
+
+.. code-block:: python
+
+    from torchtune.data import Llama2ChatFormat, Message
+
+    messages = [
+        Message(
+            role="system",
+            content="You are a helpful, respectful, and honest assistant.",
+        ),
+        Message(
+            role="user",
+            content="I am going to Paris, what should I see?",
+        ),
+        Message(
+            role="assistant",
+            content="Paris, the capital of France, is known for its stunning architecture..."
+        ),
+    ]
+    formatted_messages = Llama2ChatFormat.format(messages)
+    print(formatted_messages)
+    # [
+    #     Message(
+    #         role="user",
+    #         content="[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant.\n<</SYS>>\n\n"
+    #         "I am going to Paris, what should I see? [/INST] ",
+    #     ),
+    #     Message(
+    #         role="assistant",
+    #         content="Paris, the capital of France, is known for its stunning architecture..."
+    #     ),
+    # ]
+
+Note that the system message is now incorporated in the user message. If you create custom ChatFormats
+you can also add more advanced behavior.
+
+.. code-block:: python
+
+    from torchtune.datasets import chat_dataset
+    from torchtune.data import ChatFormat
+
+    class CustomChatFormat(ChatFormat):
+        # Define templates for system, user, assistant messages
+        # as strings with {} as placeholders for message content
+        system = ...
+        user = ...
+        assistant = ...
+
+        # Implement this method
+        @classmethod
+        def format(
+            cls,
+            sample: List[Message],
+        ) -> List[Message]:
+            ...
+
+    # Load in tokenizer
+    tokenizer = ...
+    dataset = chat_dataset(
+        tokenizer=tokenizer,
+        source="my/dataset/path",
+        split="train",
+        conversation_style="openai",
+        chat_format="import.path.to.CustomChatFormat",
+    )
+
+.. code-block:: yaml
+
+    # YAML config
+    dataset:
+      _component_: torchtune.datasets.chat_dataset
+      source: my/dataset/path
+      conversation_style: openai
+      chat_format: import.path.to.CustomChatFormat
+
+.. code-block:: bash
+
+    # Command line
+    tune run full_finetune_single_device --config llama3/8B_full_single_device \
+    dataset=torchtune.datasets.chat_dataset dataset.source=my/dataset/path \
+    dataset.conversation_style=openai dataset.chat_format=import.path.to.CustomChatFormat
+
+
+Multiple in-memory datasets
+---------------------------
+
+It is also possible to train on multiple datasets and configure them individually using
+our :class:`~torchtune.datasets.ConcatDataset` interface. You can even mix instruct and chat datasets
+or other custom datasets.
+
+.. code-block:: yaml
+
+  # YAML config
+  dataset:
+    - _component_: torchtune.datasets.instruct_dataset
+      source: vicgalle/alpaca-gpt4
+      template: torchtune.data.AlpacaInstructTemplate
+      split: train
+      train_on_input: True
+    - _component_: torchtune.datasets.instruct_dataset
+      source: samsum
+      template: torchtune.data.SummarizeTemplate
+      column_map:
+        output: summary
+      split: train
+      train_on_input: False
+    - _component_: torchtune.datasets.chat_dataset
+      ...
+
+
+Local and remote datasets
+-------------------------
+
+To use a dataset saved on your local hard drive, simply specify the file type for
+``source`` and pass in the ``data_files`` argument using any of the dataset
+builder functions. We support all `file types <https://huggingface.co/docs/datasets/en/loading#local-and-remote-files>`_
+supported by Hugging Face's ``load_dataset``, including csv, json, txt, and more.
+
+.. code-block:: python
+
+    from torchtune.datasets import instruct_dataset
+
+    # Load in tokenizer
+    tokenizer = ...
+    # Local files
+    dataset = instruct_dataset(
+        tokenizer=tokenizer,
+        source="csv",
+        split="train",
+        template="import.path.to.CustomTemplate"
+        data_files="path/to/my/data.csv",
+    )
+    # Remote files
+    dataset = instruct_dataset(
+        tokenizer=tokenizer,
+        source="json",
+        split="train",
+        template="import.path.to.CustomTemplate"
+        data_files="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json",
+        # You can also pass in any kwarg that load_dataset accepts
+        field="data",
+    )
+
+.. code-block:: yaml
+
+    # YAML config - local files
+    dataset:
+      _component_: torchtune.datasets.instruct_dataset
+      source: csv
+      template: import.path.to.CustomTemplate
+      data_files: path/to/my/data.csv
+
+    # YAML config - remote files
+    dataset:
+      _component_: torchtune.datasets.instruct_dataset
+      source: json
+      template: import.path.to.CustomTemplate
+      data_files: https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json
+      field: data
+
+.. code-block:: bash
+
+    # Command line - local files
+    tune run full_finetune_single_device --config llama3/8B_full_single_device \
+    dataset=torchtune.datasets.chat_dataset dataset.source=csv \
+    dataset.template=import.path.to.CustomTemplate dataset.data_files=path/to/my/data.csv
+
+Fully customized datasets
+-------------------------
+
+More advanced tasks and dataset formats that don't fit into the templating and processing
+that :class:`~torchtune.datasets.SFTDataset` and :class:`~torchtune.datasets.TextCompletionDataset` provide may require
+you to create your own dataset class for more flexibility. Let's walk through the :class:`~torchtune.datasets.PreferenceDataset`,
+which has custom functionality for RLHF preference data, as an example to understand what you'll need to do.
+
+.. code-block:: python
+
+    chosen_message = [
+        Message(role="user", content=prompt, masked=True),
+        Message(role="assistant", content=transformed_sample[key_chosen]),
+    ]
+    rejected_message = [
+        Message(role="user", content=prompt, masked=True),
+        Message(role="assistant", content=transformed_sample[key_rejected]),
+    ]
+
+    chosen_input_ids, c_masks = self._tokenizer.tokenize_messages(
+        chosen_message, self.max_seq_len
+    )
+    chosen_labels = list(
+        np.where(c_masks, CROSS_ENTROPY_IGNORE_IDX, chosen_input_ids)
+    )
+
+    rejected_input_ids, r_masks = self._tokenizer.tokenize_messages(
+        rejected_message, self.max_seq_len
+    )
+    rejected_labels = list(
+        np.where(r_masks, CROSS_ENTROPY_IGNORE_IDX, rejected_input_ids)
+    )
+
+For a specific dataset that's easy to customize from the config, you can create
+a builder function. This is the builder function for the :func:`~torchtune.datasets.stack_exchanged_paired_dataset`,
+which creates a :class:`~torchtune.datasets.PreferenceDataset` configured to use
+a paired dataset from Hugging Face. Notice that we've also had
+to add a custom instruct template as well.
+
+.. code-block:: python
+
+    def stack_exchanged_paired_dataset(
+        tokenizer: ModelTokenizer,
+        max_seq_len: int = 1024,
+    ) -> PreferenceDataset:
+        return PreferenceDataset(
+            tokenizer=tokenizer,
+            source="lvwerra/stack-exchange-paired",
+            template=StackExchangedPairedTemplate(),
+            column_map={
+                "prompt": "question",
+                "chosen": "response_j",
+                "rejected": "response_k",
+            },
+            max_seq_len=max_seq_len,
+            split="train",
+            data_dir="data/rl",
+        )
+
+Now we can easily specify our custom dataset from the config, or from command-line.
+
+.. code-block:: yaml
+
+    # This is how you would configure the Alpaca dataset using the builder
+    dataset:
+      _component_: torchtune.datasets.stack_exchanged_paired_dataset
+      max_seq_len: 512
+
+.. code-block:: bash
+
+    # Command line - local files
+    tune run full_finetune_single_device --config llama3/8B_full_single_device \
+    dataset=torchtune.datasets.stack_exchanged_paired_dataset dataset.max_seq_len=512
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/e2e_flow.rst marc/third_party/torchtune/docs/source/tutorials/e2e_flow.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/e2e_flow.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/e2e_flow.rst	2025-02-20 17:49:29.078023470 -0500
@@ -0,0 +1,450 @@
+.. _e2e_flow:
+
+==================================
+End-to-End Workflow with torchtune
+==================================
+
+In this tutorial, we'll walk through an end-to-end example of how you can fine-tune,
+evaluate, optionally quantize and then run generation with your favorite LLM using
+torchtune. We'll also go over how you can use some popular tools and libraries
+from the community seemlessly with torchtune.
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What this tutorial will cover:
+
+      * Different type of recipes available in torchtune beyond fine-tuning
+      * End-to-end example connecting all of these recipes
+      * Different tools and libraries you can use with torchtune
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Be familiar with the :ref:`overview of torchtune<overview_label>`
+      * Make sure to :ref:`install torchtune<install_label>`
+      * Concepts such as :ref:`configs <config_tutorial_label>` and
+        :ref:`checkpoints <understand_checkpointer>`
+
+
+Overview
+--------
+
+Fine-tuning an LLM is usually only one step in a larger workflow. An example workflow that you
+might have can look something like this:
+
+- Download a popular model from `HF Hub <https://huggingface.co/docs/hub/en/index>`_
+- Fine-tune the model using a relevant fine-tuning technique. The exact technique used
+  will depend on factors such as the model, amount and nature of training data, your hardware
+  setup and the end task for which the model will be used
+- Evaluate the model on some benchmarks to validate model quality
+- Run some generations to make sure the model output looks reasonable
+- Quantize the model for efficient inference
+- [Optional] Export the model for specific environments such as inference on a mobile phone
+
+In this tutorial, we'll cover how you can use torchtune for all of the above, leveraging
+integrations with popular tools and libraries from the ecosystem.
+
+We'll use the Llama2 7B model for this tutorial. You can find a complete set of models supported
+by torchtune `here <https://github.com/pytorch/torchtune/blob/main/README.md#introduction>`_.
+
+|
+
+Download Llama2 7B
+------------------
+
+In this tutorial, we'll use the Hugging Face model weights for the Llama2 7B mode.
+For more information on checkpoint formats and how these are handled in torchtune, take a look at
+this tutorial on :ref:`checkpoints <understand_checkpointer>`.
+
+To download the HF format Llama2 7B model, we'll use the tune CLI.
+
+.. code-block:: bash
+
+  tune download \
+  meta-llama/Llama-2-7b-hf \
+  --output-dir <checkpoint_dir> \
+  --hf-token <ACCESS TOKEN>
+
+Make a note of ``<checkpoint_dir>``, we'll use this many times in this tutorial.
+
+|
+
+Finetune the model using LoRA
+-----------------------------
+
+For this tutorial, we'll fine-tune the model using LoRA. LoRA is a parameter efficient fine-tuning
+technique which is especially helpful when you don't have a lot of GPU memory to play with. LoRA
+freezes the base LLM and adds a very small percentage of learnable parameters. This helps keep
+memory associated with gradients and optimizer state low. Using torchtune, you should be able to
+fine-tune a Llama2 7B model with LoRA in less than 16GB of GPU memory using bfloat16 on a
+RTX 3090/4090. For more information on how to use LoRA, take a look at our
+:ref:`LoRA Tutorial <lora_finetune_label>`.
+
+We'll fine-tune using our
+`single device LoRA recipe <https://github.com/pytorch/torchtune/blob/main/recipes/lora_finetune_single_device.py>`_
+and use the standard settings from the
+`default config <https://github.com/pytorch/torchtune/blob/main/recipes/configs/llama2/7B_lora_single_device.yaml>`_.
+
+This will fine-tune our model using a ``batch_size=2`` and ``dtype=bfloat16``. With these settings the model
+should have a peak memory usage of ~16GB and total training time of around two hours for each epoch.
+We'll need to make some changes to the config to make sure our recipe can access the
+right checkpoints.
+
+Let's look for the right config for this use case by using the tune CLI.
+
+.. code-block:: bash
+
+    tune ls
+
+    RECIPE                                   CONFIG
+    full_finetune_single_device              llama2/7B_full_low_memory
+                                             mistral/7B_full_low_memory
+    full_finetune_distributed                llama2/7B_full
+                                             llama2/13B_full
+                                             mistral/7B_full
+    lora_finetune_single_device              llama2/7B_lora_single_device
+                                             llama2/7B_qlora_single_device
+                                             mistral/7B_lora_single_device
+    ...
+
+
+For this tutorial we'll use the ``llama2/7B_lora_single_device`` config.
+
+The config already points to the HF Checkpointer and the right checkpoint files.
+All we need to do is update the checkpoint directory for both the model and the
+tokenizer. Let's do this using the overrides in the tune CLI while starting training!
+
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device \
+    --config llama2/7B_lora_single_device \
+    checkpointer.checkpoint_dir=<checkpoint_dir> \
+    tokenizer.path=<checkpoint_dir>/tokenizer.model \
+    checkpointer.output_dir=<checkpoint_dir>
+
+
+Once training is complete, you'll see the following in the logs.
+
+.. code-block:: bash
+
+    [_checkpointer.py:473] Model checkpoint of size 9.98 GB saved to <checkpoint_dir>/hf_model_0001_0.pt
+
+    [_checkpointer.py:473] Model checkpoint of size 3.50 GB saved to <checkpoint_dir>/hf_model_0002_0.pt
+
+    [_checkpointer.py:484] Adapter checkpoint of size 0.01 GB saved to <checkpoint_dir>/adapter_0.pt
+
+
+The final trained weights are merged with the original model and split across two checkpoint files
+similar to the source checkpoints from the HF Hub
+(see the :ref:`LoRA Tutorial <lora_finetune_label>` for more details).
+In fact the keys will be identical between these checkpoints.
+We also have a third checkpoint file which is much smaller in size
+and contains the learnt LoRA adapter weights. For this tutorial, we'll only use the model
+checkpoints and not the adapter weights.
+
+|
+
+.. _eval_harness_label:
+
+Run Evaluation using EleutherAI's Eval Harness
+----------------------------------------------
+
+We've fine-tuned a model. But how well does this model really do? Let's run some Evaluations!
+
+.. TODO (SalmanMohammadi) ref eval recipe docs
+
+torchtune integrates with
+`EleutherAI's evaluation harness <https://github.com/EleutherAI/lm-evaluation-harness>`_.
+An example of this is available through the
+``eleuther_eval`` recipe. In this tutorial, we're going to directly use this recipe by
+modifying its associated config ``eleuther_evaluation.yaml``.
+
+.. note::
+    For this section of the tutorial, you should first run :code:`pip install lm_eval==0.4.*`
+    to install the EleutherAI evaluation harness.
+
+Since we plan to update all of the checkpoint files to point to our fine-tuned checkpoints,
+let's first copy over the config to our local working directory so we can make changes. This
+will be easier than overriding all of these elements through the CLI.
+
+.. code-block:: bash
+
+    tune cp eleuther_evaluation ./custom_eval_config.yaml \
+
+For this tutorial we'll use the `truthfulqa_mc2 <https://github.com/sylinrl/TruthfulQA>`_ task from the harness.
+This task measures a model's propensity to be truthful when answering questions and
+measures the model's zero-shot accuracy on a question followed by one or more true
+responses and one or more false responses. Let's first run a baseline without fine-tuning.
+
+
+.. code-block:: bash
+
+    tune run eleuther_eval --config ./custom_eval_config.yaml
+    checkpointer.checkpoint_dir=<checkpoint_dir> \
+    tokenizer.path=<checkpoint_dir>/tokenizer.model
+
+    [evaluator.py:324] Running loglikelihood requests
+    [eleuther_eval.py:195] Eval completed in 121.27 seconds.
+    [eleuther_eval.py:197] truthfulqa_mc2: {'acc,none': 0.388...
+
+The model has an accuracy around 38.8%. Let's compare this with the fine-tuned model.
+
+
+First, we modify ``custom_eval_config.yaml`` to include the fine-tuned checkpoints.
+
+.. code-block:: yaml
+
+    checkpointer:
+        _component_: torchtune.training.FullModelHFCheckpointer
+
+        # directory with the checkpoint files
+        # this should match the output_dir specified during
+        # finetuning
+        checkpoint_dir: <checkpoint_dir>
+
+        # checkpoint files for the fine-tuned model. This should
+        # match what's shown in the logs above
+        checkpoint_files: [
+            hf_model_0001_0.pt,
+            hf_model_0002_0.pt,
+        ]
+
+        output_dir: <checkpoint_dir>
+        model_type: LLAMA2
+
+    # Make sure to update the tokenizer path to the right
+    # checkpoint directory as well
+    tokenizer:
+        _component_: torchtune.models.llama2.llama2_tokenizer
+        path: <checkpoint_dir>/tokenizer.model
+
+
+Now, let's run the recipe.
+
+.. code-block:: bash
+
+    tune run eleuther_eval --config ./custom_eval_config.yaml
+
+
+The results should look something like this.
+
+.. code-block:: bash
+
+    [evaluator.py:324] Running loglikelihood requests
+    [eleuther_eval.py:195] Eval completed in 121.27 seconds.
+    [eleuther_eval.py:197] truthfulqa_mc2: {'acc,none': 0.489 ...
+
+Our fine-tuned model gets ~48% on this task, which is ~10 points
+better than the baseline. Great! Seems like our fine-tuning helped.
+
+|
+
+Generation
+-----------
+
+We've run some evaluations and the model seems to be doing well. But does it really
+generate meaningful text for the prompts you care about? Let's find out!
+
+For this, we'll use the
+`generate recipe <https://github.com/pytorch/torchtune/blob/main/recipes/generate.py>`_
+and the associated
+`config <https://github.com/pytorch/torchtune/blob/main/recipes/configs/generation.yaml>`_.
+
+
+Let's first copy over the config to our local working directory so we can make changes.
+
+.. code-block:: bash
+
+    tune cp generation ./custom_generation_config.yaml
+
+Let's modify ``custom_generation_config.yaml`` to include the following changes.
+
+.. code-block:: yaml
+
+    checkpointer:
+        _component_: torchtune.training.FullModelHFCheckpointer
+
+        # directory with the checkpoint files
+        # this should match the output_dir specified during
+        # finetuning
+        checkpoint_dir: <checkpoint_dir>
+
+        # checkpoint files for the fine-tuned model. This should
+        # match what's shown in the logs above
+        checkpoint_files: [
+            hf_model_0001_0.pt,
+            hf_model_0002_0.pt,
+        ]
+
+        output_dir: <checkpoint_dir>
+        model_type: LLAMA2
+
+    # Make sure to update the tokenizer path to the right
+    # checkpoint directory as well
+    tokenizer:
+        _component_: torchtune.models.llama2.llama2_tokenizer
+        path: <checkpoint_dir>/tokenizer.model
+
+
+Once the config is updated, let's kick off generation! We'll use the
+default settings for sampling with ``top_k=300`` and a
+``temperature=0.8``. These parameters control how the probabilities for
+sampling are computed. These are standard settings for Llama2 7B and
+we recommend inspecting the model with these before playing around with
+these parameters.
+
+We'll use a different prompt from the one in the config
+
+.. code-block:: bash
+
+    tune run generate --config ./custom_generation_config.yaml \
+    prompt="What are some interesting sites to visit in the Bay Area?"
+
+
+Once generation is complete, you'll see the following in the logs.
+
+
+.. code-block:: bash
+
+    [generate.py:92] Exploratorium in San Francisco has made the cover of Time Magazine,
+                     and its awesome. And the bridge is pretty cool...
+
+    [generate.py:96] Time for inference: 11.61 sec total, 25.83 tokens/sec
+    [generate.py:99] Memory used: 15.72 GB
+
+
+Indeed, the bridge is pretty cool! Seems like our LLM knows a little something about the
+Bay Area!
+
+|
+
+Speeding up Generation using Quantization
+-----------------------------------------
+
+We rely on `torchao <https://github.com/pytorch-labs/ao>`_ for `post-training quantization <https://github.com/pytorch/ao/tree/main/torchao/quantization#quantization>`_.
+To quantize the fine-tuned model after installing torchao we can run the following command::
+
+  # we also support `int8_weight_only()` and `int8_dynamic_activation_int8_weight()`, see
+  # https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques
+  # for a full list of techniques that we support
+  from torchao.quantization.quant_api import quantize_, int4_weight_only
+  quantize_(model, int4_weight_only())
+
+After quantization, we rely on torch.compile for speedups. For more details, please see `this example usage <https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md#quantization-flow-example>`_.
+
+torchao also provides `this table <https://github.com/pytorch/ao#inference>`_ listing performance and accuracy results for ``llama2`` and ``llama3``.
+
+For Llama models, you can run generation directly in torchao on the quantized model using their ``generate.py`` script as
+discussed in `this readme <https://github.com/pytorch/ao/tree/main/torchao/_models/llama>`_. This way you can compare your own results
+to those in the previously-linked table.
+
+|
+
+Using torchtune checkpoints with other libraries
+------------------------------------------------
+
+As we mentioned above, one of the benefits of handling of the checkpoint
+conversion is that you can directly work with standard formats. This helps
+with interoperability with other libraries since torchtune doesn't add yet
+another format to the mix.
+
+Let's take a look at an example of how this would work with a popular codebase
+used for running performant inference with LLMs -
+`gpt-fast <https://github.com/pytorch-labs/gpt-fast/tree/main>`_. This section
+assumes that you've cloned that repository on your machine.
+
+``gpt-fast`` makes some assumptions about the checkpoint and the availability of
+the key-to-file mapping i.e. a file mapping parameter names to the files containing them.
+Let's satisfy these assumptions, by creating this mapping
+file. Let's assume we'll be using ``<new_dir>/Llama-2-7B-hf`` as the directory
+for this. ``gpt-fast`` assumes that the directory with checkpoints has the
+same format at the HF repo-id.
+
+.. code-block:: python
+
+    import json
+    import torch
+
+    # create the output dictionary
+    output_dict = {"weight_map": {}}
+
+    # Load the checkpoints
+    sd_1 = torch.load('<checkpoint_dir>/hf_model_0001_0.pt', mmap=True, map_location='cpu')
+    sd_2 = torch.load('<checkpoint_dir>/hf_model_0002_0.pt', mmap=True, map_location='cpu')
+
+    # create the weight map
+    for key in sd_1.keys():
+        output_dict['weight_map'][key] =  "hf_model_0001_0.pt"
+    for key in sd_2.keys():
+        output_dict['weight_map'][key] =  "hf_model_0002_0.pt"
+
+    with open('<new_dir>/Llama-2-7B-hf/pytorch_model.bin.index.json', 'w') as f:
+        json.dump(output_dict, f)
+
+
+Now that we've created the weight_map, let's copy over our checkpoints.
+
+.. code-block:: bash
+
+    cp  <checkpoint_dir>/hf_model_0001_0.pt  <new_dir>/Llama-2-7B-hf/
+    cp  <checkpoint_dir>/hf_model_0002_0.pt  <new_dir>/Llama-2-7B-hf/
+    cp  <checkpoint_dir>/tokenizer.model     <new_dir>/Llama-2-7B-hf/
+
+Once the directory structure is setup, let's convert the checkpoints and run inference!
+
+.. code-block:: bash
+
+    cd gpt-fast/
+
+    # convert the checkpoints into a format readable by gpt-fast
+    python scripts/convert_hf_checkpoint.py \
+    --checkpoint_dir <new_dir>/Llama-2-7B-hf/ \
+    --model 7B
+
+    # run inference using the converted model
+    python generate.py \
+    --compile \
+    --checkpoint_path <new_dir>/Llama-2-7B-hf/model.pth \
+    --device cuda
+
+The output should look something like this:
+
+.. code-block:: bash
+
+    Hello, my name is Justin. I am a middle school math teacher
+    at WS Middle School ...
+
+    Time for inference 5: 1.94 sec total, 103.28 tokens/sec
+    Bandwidth achieved: 1391.84 GB/sec
+
+
+And thats it! Try your own prompt!
+
+Uploading your model to the Hugging Face Hub
+--------------------------------------------
+
+Your new model is working great and you want to share it with the world. The easiest way to do this
+is utilizing the `huggingface-cli <https://huggingface.co/docs/huggingface_hub/en/guides/cli>`_ command, which works seamlessly with torchtune. Simply point the CLI
+to your finetuned model directory like so:
+
+.. code-block:: bash
+
+    huggingface-cli upload <hf-repo-id> <checkpoint-dir>
+
+The command should output a link to your repository on the Hub. If the repository doesn't exist yet, it will
+be created automatically:
+
+.. code-block:: text
+
+    https://huggingface.co/<hf-repo-id>/tree/main/.
+
+.. note::
+
+    Before uploading, make sure you are `authenticated with Hugging Face <https://huggingface.co/docs/huggingface_hub/quick-start#authentication>`_ by running ``huggingface-cli login``.
+
+For more details on the ``huggingface-cli upload`` feature check out the `Hugging Face docs <https://huggingface.co/docs/huggingface_hub/en/guides/cli#huggingface-cli-upload>`_.
+
+|
+
+Hopefully this tutorial gave you some insights into how you can use torchtune for
+your own workflows. Happy Tuning!
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/first_finetune_tutorial.rst marc/third_party/torchtune/docs/source/tutorials/first_finetune_tutorial.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/first_finetune_tutorial.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/first_finetune_tutorial.rst	2025-02-20 17:49:29.082023476 -0500
@@ -0,0 +1,167 @@
+.. _finetune_llama_label:
+
+========================
+Fine-Tune Your First LLM
+========================
+
+This guide will walk you through the process of launching your first finetuning
+job using torchtune.
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn
+
+      * How to download a model from the `Hugging Face Hub <https://huggingface.co/docs/hub/en/index>`_
+      * How to modify a recipe's parameters to suit your needs
+      * How to run a finetune
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Be familiar with the :ref:`overview of torchtune<overview_label>`
+      * Make sure to :ref:`install torchtune<install_label>`
+
+.. _download_llama_label:
+
+Downloading a model
+-------------------
+The first step in any finetuning job is to download a pretrained base model. torchtune supports an integration
+with the `Hugging Face Hub <https://huggingface.co/docs/hub/en/index>`_ - a collection of the latest and greatest model weights.
+
+For this tutorial, you're going to use the `Llama2 7B model from Meta <https://llama.meta.com/>`_. Llama2 is a "gated model",
+meaning that you need to be granted access in order to download the weights. Follow `these instructions <https://huggingface.co/meta-llama>`_ on the official Meta page
+hosted on Hugging Face to complete this process. This should take less than 5 minutes. To verify that you have the access, go to the `model page <https://huggingface.co/meta-llama/Llama-2-7b-hf/tree/main>`_.
+You should be able to see the model files. If not, you may need to accept the agreement to complete the process.
+
+.. note::
+
+  Alternatively, you can opt to download the model directly through the Llama2 repository.
+  See `this page <https://llama.meta.com/get-started#getting-the-models>`_ for more details.
+
+Once you have authorization, you will need to authenticate with Hugging Face Hub. The easiest way to do so is to provide an
+access token to the download script. You can find your token `here <https://huggingface.co/settings/tokens>`_.
+
+Then, it's as simple as:
+
+.. code-block:: bash
+
+  tune download meta-llama/Llama-2-7b-hf \
+    --output-dir /tmp/Llama-2-7b-hf \
+    --hf-token <ACCESS TOKEN>
+
+This command will also download the model tokenizer and some other helpful files such as a Responsible Use guide.
+
+|
+
+Selecting a recipe
+------------------
+Recipes are the primary entry points for torchtune users.
+These can be thought of as **hackable, singularly-focused scripts for interacting with LLMs** including training,
+inference, evaluation, and quantization.
+
+Each recipe consists of three components:
+
+* **Configurable parameters**, specified through yaml configs and command-line overrides
+* **Recipe script**, entry-point which puts everything together including parsing and validating configs, setting up the environment, and correctly using the recipe class
+* **Recipe class**, core logic needed for training, exposed through a set of APIs
+
+.. note::
+
+  To learn more about the concept of "recipes", check out our technical deep-dive: :ref:`recipe_deepdive`.
+
+torchtune provides built-in recipes for finetuning on single device, on multiple devices with `FSDP <https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/>`_,
+using memory efficient techniques like `LoRA <https://arxiv.org/abs/2106.09685>`_, and more! Check out all our built-in recipes in our :ref:`recipes overview<recipes_overview_label>`. You can also utilize the
+:code:`tune ls` command to print out all recipes and corresponding configs.
+
+.. code-block:: bash
+
+  $ tune ls
+  RECIPE                                   CONFIG
+  full_finetune_single_device              llama2/7B_full_low_memory
+                                           mistral/7B_full_low_memory
+  full_finetune_distributed                llama2/7B_full
+                                           llama2/13B_full
+                                           mistral/7B_full
+  lora_finetune_single_device              llama2/7B_lora_single_device
+                                           llama2/7B_qlora_single_device
+                                           mistral/7B_lora_single_device
+  ...
+
+For the purposes of this tutorial, you'll will be using the recipe for finetuning a Llama2 model using `LoRA <https://arxiv.org/abs/2106.09685>`_ on
+a single device. For a more in-depth discussion on LoRA in torchtune, you can see the complete ":ref:`lora_finetune_label`" tutorial.
+
+.. note::
+
+  **Why have a separate recipe for single device vs. distributed?** This is discussed in
+  ":ref:`recipe_deepdive`" but one of our :ref:`core principles <design_principles_label>` in torchtune is minimal abstraction and boilerplate code.
+  If you only want to train on a single GPU, our single-device recipe ensures you don't have to worry about additional
+  features like FSDP that are only required for distributed training.
+
+|
+
+.. _tune_cp_label:
+
+Modifying a config
+------------------
+YAML configs hold most of the important information needed for running your recipe.
+You can set hyperparameters, specify metric loggers like `WandB <wandb.ai>`_, select a new dataset, and more.
+For a list of all currently supported datasets, see :ref:`datasets`.
+
+There are two ways to modify an existing config:
+
+**Override existing parameters from the command line**
+
+You can override existing parameters from the command line using a :code:`key=value` format. Let's say
+you want to set the number of training epochs to 1.
+
+.. code-block:: bash
+
+  tune run <RECIPE> --config <CONFIG> epochs=1
+
+**Copy the config through `tune cp` and modify directly**
+
+If you want to make more substantial changes to the config, you can use the :ref:`tune <cli_label>` CLI to copy it to your local directory.
+
+.. code-block:: bash
+
+  $ tune cp llama2/7B_lora_single_device custom_config.yaml
+  Copied file to custom_config.yaml
+
+Now you can update the custom YAML config any way you like. Try setting the random seed in order to make replication easier,
+changing the LoRA rank, update batch size, etc.
+
+.. note::
+
+  Check out ":ref:`config_tutorial_label`" for a deeper dive on configs in torchtune.
+
+|
+
+Training a model
+----------------
+Now that you have a model in the proper format and a config that suits your needs, let's get training!
+
+Just like all the other steps, you will be using the tune CLI tool to launch your finetuning run.
+
+.. code-block:: bash
+
+  $ tune run lora_finetune_single_device --config llama2/7B_lora_single_device epochs=1
+  INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:
+  Writing logs to /tmp/lora_finetune_output/log_1713194212.txt
+  INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.
+  INFO:torchtune.utils.logging:Tokenizer is initialized from file.
+  INFO:torchtune.utils.logging:Optimizer and loss are initialized.
+  INFO:torchtune.utils.logging:Loss is initialized.
+  INFO:torchtune.utils.logging:Dataset and Sampler are initialized.
+  INFO:torchtune.utils.logging:Learning rate scheduler is initialized.
+  1|52|Loss: 2.3697006702423096:   0%|                     | 52/25880 [00:24<3:55:01,  1.83it/s]
+
+You can see that all the modules were successfully initialized and the model has started training.
+You can monitor the loss and progress through the `tqdm <https://tqdm.github.io/>`_ bar but torchtune
+will also log some more metrics, such as GPU memory usage, at an interval defined in the config.
+
+|
+
+Next steps
+----------
+
+Now that you have trained your model and set up your environment, let's take a look at what we can do with our
+new model by checking out the ":ref:`E2E Workflow Tutorial<e2e_flow>`".
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/llama3.rst marc/third_party/torchtune/docs/source/tutorials/llama3.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/llama3.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/llama3.rst	2025-02-20 17:49:29.086023483 -0500
@@ -0,0 +1,263 @@
+.. _llama3_label:
+
+========================
+Meta Llama3 in torchtune
+========================
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` You will learn how to:
+
+      * Download the Llama3-8B-Instruct weights and tokenizer
+      * Fine-tune Llama3-8B-Instruct with LoRA and QLoRA
+      * Evaluate your fine-tuned Llama3-8B-Instruct model
+      * Generate text with your fine-tuned model
+      * Quantize your model to speed up generation
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Be familiar with :ref:`torchtune<overview_label>`
+      * Make sure to :ref:`install torchtune<install_label>`
+
+
+Llama3-8B
+---------
+
+`Meta Llama 3 <https://llama.meta.com/llama3>`_ is a new family of models released by Meta AI that improves upon the performance of the Llama2 family
+of models across a `range of different benchmarks <https://huggingface.co/meta-llama/Meta-Llama-3-8B#base-pretrained-models>`_.
+Currently there are two different sizes of Meta Llama 3: 8B and 70B. In this tutorial we will focus on the 8B size model.
+There are a few main changes between Llama2-7B and Llama3-8B models:
+
+- Llama3-8B uses `grouped-query attention <https://arxiv.org/abs/2305.13245>`_ instead of the standard multi-head attention from Llama2-7B
+- Llama3-8B has a larger vocab size (128,256 instead of 32,000 from Llama2 models)
+- Llama3-8B uses a different tokenizer than Llama2 models (`tiktoken <https://github.com/openai/tiktoken>`_ instead of `sentencepiece <https://github.com/google/sentencepiece>`_)
+- Llama3-8B uses a larger intermediate dimension in its MLP layers than Llama2-7B
+- Llama3-8B uses a higher base value to calculate theta in its `rotary positional embeddings <https://arxiv.org/abs/2104.09864>`_
+
+|
+
+Getting access to Llama3-8B-Instruct
+------------------------------------
+
+For this tutorial, we will be using the instruction-tuned version of Llama3-8B. First, let's download the model from Hugging Face. You will need to follow the instructions
+on the `official Meta page <https://github.com/meta-llama/llama3/blob/main/README.md>`_ to gain access to the model.
+Next, make sure you grab your Hugging Face token from `here <https://huggingface.co/settings/tokens>`_.
+
+
+.. code-block:: bash
+
+    tune download meta-llama/Meta-Llama-3-8B-Instruct \
+        --output-dir <checkpoint_dir> \
+        --hf-token <ACCESS TOKEN>
+
+|
+
+Fine-tuning Llama3-8B-Instruct in torchtune
+-------------------------------------------
+
+torchtune provides `LoRA <https://arxiv.org/abs/2106.09685>`_, `QLoRA <https://arxiv.org/abs/2305.14314>`_, and full fine-tuning
+recipes for fine-tuning Llama3-8B on one or more GPUs. For more on LoRA in torchtune, see our :ref:`LoRA Tutorial <lora_finetune_label>`.
+For more on QLoRA in torchtune, see our :ref:`QLoRA Tutorial <qlora_finetune_label>`.
+
+Let's take a look at how we can fine-tune Llama3-8B-Instruct with LoRA on a single device using torchtune. In this example, we will fine-tune
+for one epoch on a common instruct dataset for illustrative purposes. The basic command for a single-device LoRA fine-tune is
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device --config llama3/8B_lora_single_device
+
+.. note::
+    To see a full list of recipes and their corresponding configs, simply run ``tune ls`` from the command line.
+
+We can also add :ref:`command-line overrides <cli_override>` as needed, e.g.
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device --config llama3/8B_lora_single_device \
+        checkpointer.checkpoint_dir=<checkpoint_dir> \
+        tokenizer.path=<checkpoint_dir>/tokenizer.model \
+        checkpointer.output_dir=<checkpoint_dir>
+
+This will load the Llama3-8B-Instruct checkpoint and tokenizer from ``<checkpoint_dir>`` used in the :ref:`tune download <tune_download_label>` command above,
+then save a final checkpoint in the same directory following the original format. For more details on the
+checkpoint formats supported in torchtune, see our :ref:`checkpointing deep-dive <understand_checkpointer>`.
+
+.. note::
+    To see the full set of configurable parameters for this (and other) configs we can use :ref:`tune cp <tune_cp_cli_label>` to copy (and modify)
+    the default config. :ref:`tune cp <tune_cp_cli_label>` can be used with recipe scripts too, in case you want to make more custom changes
+    that cannot be achieved by directly modifying existing configurable parameters. For more on :ref:`tune cp <tune_cp_cli_label>` see the section on
+    :ref:`modifying configs <tune_cp_label>` in our ":ref:`finetune_llama_label`" tutorial.
+
+Once training is complete, the model checkpoints will be saved and their locations will be logged. For
+LoRA fine-tuning, the final checkpoint will contain the merged weights, and a copy of just the (much smaller) LoRA weights
+will be saved separately.
+
+In our experiments, we observed a peak memory usage of 18.5 GB. The default config can be trained on a consumer GPU with 24 GB VRAM.
+
+If you have multiple GPUs available, you can run the distributed version of the recipe.
+torchtune makes use of the `FSDP <https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`_ APIs from PyTorch Distributed
+to shard the model, optimizer states, and gradients. This should enable you to increase your batch size, resulting in faster overall training.
+For example, on two devices:
+
+.. code-block:: bash
+
+    tune run --nproc_per_node 2 lora_finetune_distributed --config llama3/8B_lora
+
+Finally, if we want to use even less memory, we can leverage torchtune's QLoRA recipe via:
+
+.. TODO (SalmanMohammadi) ref qlora recipe page
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device --config llama3/8B_qlora_single_device
+
+Since our default configs enable full bfloat16 training, all of the above commands can be run with
+devices having at least 24 GB of VRAM, and in fact the QLoRA recipe should have peak allocated memory
+below 10 GB. You can also experiment with different configurations of LoRA and QLoRA, or even run a full fine-tune.
+Try it out!
+
+|
+
+Evaluating fine-tuned Llama3-8B models with EleutherAI's Eval Harness
+---------------------------------------------------------------------
+
+Now that we've fine-tuned our model, what's next? Let's take our LoRA-finetuned model from the
+preceding section and look at a couple different ways we can evaluate its performance on the tasks we care about.
+
+First, torchtune provides an integration with
+`EleutherAI's evaluation harness <https://github.com/EleutherAI/lm-evaluation-harness>`_
+for model evaluation on common benchmark tasks.
+
+.. note::
+    Make sure you've first installed the evaluation harness via :code:`pip install "lm_eval==0.4.*"`.
+
+For this tutorial we'll use the `truthfulqa_mc2 <https://github.com/sylinrl/TruthfulQA>`_ task from the harness.
+This task measures a model's propensity to be truthful when answering questions and
+measures the model's zero-shot accuracy on a question followed by one or more true
+responses and one or more false responses. First, let's copy the config so we can point the YAML
+file to our fine-tuned checkpoint files.
+
+.. code-block:: bash
+
+    tune cp eleuther_evaluation ./custom_eval_config.yaml
+
+Next, we modify ``custom_eval_config.yaml`` to include the fine-tuned checkpoints.
+
+.. code-block:: yaml
+
+    model:
+      _component_: torchtune.models.llama3.llama3_8b
+
+    checkpointer:
+      _component_: torchtune.training.FullModelMetaCheckpointer
+
+      # directory with the checkpoint files
+      # this should match the output_dir specified during
+      # fine-tuning
+      checkpoint_dir: <checkpoint_dir>
+
+      # checkpoint files for the fine-tuned model. These will be logged
+      # at the end of your fine-tune
+      checkpoint_files: [
+        meta_model_0.pt
+      ]
+
+      output_dir: <checkpoint_dir>
+      model_type: LLAMA3
+
+    # Make sure to update the tokenizer path to the right
+    # checkpoint directory as well
+    tokenizer:
+      _component_: torchtune.models.llama3.llama3_tokenizer
+      path: <checkpoint_dir>/tokenizer.model
+
+Finally, we can run evaluation using our modified config.
+
+.. code-block:: bash
+
+    tune run eleuther_eval --config ./custom_eval_config.yaml
+
+Try it for yourself and see what accuracy your model gets!
+
+|
+
+Generating text with our fine-tuned Llama3 model
+------------------------------------------------
+
+.. TODO (SalmanMohammadi) ref generate recipe page
+
+Next, let's look at one other way we can evaluate our model: generating text! torchtune provides a
+`recipe for generation <https://github.com/pytorch/torchtune/blob/main/recipes/generate.py>`_ as well.
+
+Similar to what we did, let's copy and modify the default generation config.
+
+.. code-block:: bash
+
+    tune cp generation ./custom_generation_config.yaml
+
+Now we modify ``custom_generation_config.yaml`` to point to our checkpoint and tokenizer.
+
+.. code-block:: yaml
+
+    model:
+      _component_: torchtune.models.llama3.llama3_8b
+
+    checkpointer:
+      _component_: torchtune.training.FullModelMetaCheckpointer
+
+      # directory with the checkpoint files
+      # this should match the output_dir specified during
+      # fine-tuning
+      checkpoint_dir: <checkpoint_dir>
+
+      # checkpoint files for the fine-tuned model. These will be logged
+      # at the end of your fine-tune
+      checkpoint_files: [
+        meta_model_0.pt
+      ]
+
+      output_dir: <checkpoint_dir>
+      model_type: LLAMA3
+
+    # Make sure to update the tokenizer path to the right
+    # checkpoint directory as well
+    tokenizer:
+      _component_: torchtune.models.llama3.llama3_tokenizer
+      path: <checkpoint_dir>/tokenizer.model
+
+Running generation with our LoRA-finetuned model, we see the following output:
+
+.. code-block:: bash
+
+    tune run generate --config ./custom_generation_config.yaml \
+    prompt="Hello, my name is"
+
+    [generate.py:122] Hello, my name is Sarah and I am a busy working mum of two young children, living in the North East of England.
+    ...
+    [generate.py:135] Time for inference: 10.88 sec total, 18.94 tokens/sec
+    [generate.py:138] Bandwidth achieved: 346.09 GB/s
+    [generate.py:139] Memory used: 18.31 GB
+
+Faster generation via quantization
+----------------------------------
+
+We rely on `torchao <https://github.com/pytorch-labs/ao>`_ for `post-training quantization <https://github.com/pytorch/ao/tree/main/torchao/quantization#quantization>`_.
+To quantize the fine-tuned model after installing torchao we can run the following command::
+
+  # we also support `int8_weight_only()` and `int8_dynamic_activation_int8_weight()`, see
+  # https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques
+  # for a full list of techniques that we support
+  from torchao.quantization.quant_api import quantize_, int4_weight_only
+  quantize_(model, int4_weight_only())
+
+After quantization, we rely on torch.compile for speedups. For more details, please see `this example usage <https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md#quantization-flow-example>`_.
+
+torchao also provides `this table <https://github.com/pytorch/ao#inference>`_ listing performance and accuracy results for ``llama2`` and ``llama3``.
+
+For Llama models, you can run generation directly in torchao on the quantized model using their ``generate.py`` script as
+discussed in `this readme <https://github.com/pytorch/ao/tree/main/torchao/_models/llama>`_. This way you can compare your own results
+to those in the previously-linked table.
+
+
+This is just the beginning of what you can do with Meta Llama3 using torchtune and the broader ecosystem.
+We look forward to seeing what you build!
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/llama_kd_tutorial.rst marc/third_party/torchtune/docs/source/tutorials/llama_kd_tutorial.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/llama_kd_tutorial.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/llama_kd_tutorial.rst	2025-02-20 17:49:29.090023490 -0500
@@ -0,0 +1,229 @@
+.. _llama_kd_label:
+
+====================================================================
+Distilling Llama3.1 8B into Llama3.2 1B using Knowledge Distillation
+====================================================================
+
+This guide will teach you about knowledge distillation (KD) and show you how you can use torchtune to distill a Llama3.1 8B model into Llama3.2 1B.
+If you already know what knowledge distillation is and want to get straight to running your own distillation in torchtune,
+you can jump to the `KD recipe in torchtune`_ tutorial.
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn
+
+      * What KD is and how it can help improve model performance
+      * An overview of KD components in torchtune
+      * How to distill from a teacher to student model using torchtune
+      * How to experiment with different KD configurations
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Be familiar with :ref:`torchtune<overview_label>`
+      * Make sure to :ref:`install torchtune<install_label>`
+      * Make sure you have downloaded the :ref:`Llama3 model weights<download_llama_label>`
+      * Be familiar with :ref:`LoRA<lora_finetune_label>`
+
+What is Knowledge Distillation?
+-------------------------------
+
+`Knowledge Distillation <https://arxiv.org/pdf/1503.02531>`_ is a widely used compression technique
+that transfers knowledge from a larger (teacher) model to a smaller (student) model. Larger models have
+more parameters and capacity for knowledge, however, this larger capacity is also more computationally
+expensive to deploy. Knowledge distillation can be used to compress the knowledge of a larger model into
+a smaller model. The idea is that performance of smaller models can be improved by learning from larger
+model's outputs.
+
+How does Knowledge Distillation work?
+-------------------------------------
+
+Knowledge is transferred from the teacher to student model by training it on a transfer set where the
+student is trained to imitate the token-level probability distributions of the teacher. The diagram below
+is a simplified representation of how KD works.
+
+.. image:: /_static/img/kd-simplified.png
+
+The total loss can be configured in many ways. The default KD config in torchtune combines the cross-entropy (CE) loss with the
+forward `Kullback-Leibler (KL) divergence <https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>`_ loss,
+which is used in standard KD approaches. Forward KL divergence aims to minimize the difference by forcing the student's
+distribution to align with all of the teacher's distributions. However, aligning the student distribution to the whole
+teacher distribution may not be effective and there are multiple papers, such as `MiniLLM <https://arxiv.org/pdf/2306.08543>`_,
+`DistiLLM <https://arxiv.org/pdf/2402.03898>`_, and `Generalized KD <https://arxiv.org/pdf/2306.13649>`_,
+that introduce new KD losses to address the limitations. For this tutorial, let's take a look at the implementation of
+the forward KL divergence loss.
+
+.. code-block:: python
+
+  import torch
+  import torch.nn.functional as F
+
+  class ForwardKLLoss(torch.nn.Module):
+    def __init__(self, ignore_index: int = -100)
+      super().__init__()
+      self.ignore_index = ignore_index
+
+    def forward(self, student_logits, teacher_logits, labels) -> torch.Tensor:
+      # Implementation from https://github.com/jongwooko/distillm
+      # Computes the softmax of the teacher logits
+      teacher_prob = F.softmax(teacher_logits, dim=-1, dtype=torch.float32)
+      # Computes the student log softmax probabilities
+      student_logprob = F.log_softmax(student_logits, dim=-1, dtype=torch.float32)
+      # Computes the forward KL divergence
+      prod_probs = teacher_prob * student_logprob
+      # Compute the sum
+      x = torch.sum(prod_probs, dim=-1).view(-1)
+      # We don't want to include the ignore labels in the average
+      mask = (labels != self.ignore_index).int()
+      # Loss is averaged over non-ignored targets
+      return -torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)
+
+There are some details omitted to simplify the computation, but if you'd like to know more,
+you can see the implementation in :class:`~torchtune.modules.loss.ForwardKLLoss`.
+By default, the KD configs use :class:`~torchtune.modules.loss.ForwardKLWithChunkedOutputLoss` to reduce memory.
+The current implementation only supports student and teacher models that have the same output
+logit shape and same tokenizer.
+
+KD recipe in torchtune
+----------------------
+
+With torchtune, we can easily apply knowledge distillation to Llama3, as well as other LLM model families.
+Let's take a look at how you could distill a model using torchtune's `KD recipe <https://github.com/pytorch/torchtune/blob/4234b78b914af23384ce0348f564e2119d107a96/recipes/knowledge_distillation_single_device.py>`_.
+
+First, make sure that you have downloaded all the model weights. For this example, we'll use the Llama3.1-8B as teacher and Llama3.2-1B as student.
+
+.. code-block:: bash
+
+    tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth" --hf_token <HF_TOKEN>
+
+    tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns "original/consolidated.00.pth" --hf_token <HF_TOKEN>
+
+Then, we will fine-tune the teacher model using LoRA. Based on our experiments and previous work,
+we've found that KD performs better when the teacher model is already fine-tuned on the target dataset.
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device
+
+Finally, we can run the following command to distill the fine-tuned 8B model into the 1B model on a single GPU.
+
+.. code-block:: bash
+
+    tune run knowledge_distillation_single_device --config llama3_2/knowledge_distillation_single_device
+
+Ablation studies
+----------------
+
+In the previous example, we used the LoRA fine-tuned 8B teacher model and baseline 1B student model,
+but we may want to experiment a bit with different configurations and hyperparameters.
+For this tutorial, we are going to fine-tune on the :class:`~torchtune.datasets.alpaca_cleaned_dataset`
+and evaluate the models on `truthfulqa_mc2 <https://github.com/EleutherAI/lm-evaluation-harness/tree/feff1b55c57993c4d42c8f913a22eeec395cd690/lm_eval/tasks/truthfulqa>`_,
+`hellaswag <https://github.com/EleutherAI/lm-evaluation-harness/tree/517aadc/lm_eval/tasks/hellaswagd>`_
+and `commonsense_qa <https://github.com/EleutherAI/lm-evaluation-harness/tree/b62b9bd/lm_eval/tasks/commonsense_qa>`_ tasks
+through the EleutherAI `LM evaluation harness <https://github.com/EleutherAI/lm-evaluation-harness/tree/main>`_.
+Let's take a look at the effects of:
+
+#. Using a fine-tuned teacher model
+#. Using a fine-tuned student model
+#. Hyperparameter tuning of kd_ratio and learning rate
+#. Teacher and student models with closer number of parameters
+
+Using a fine-tuned teacher model
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+The default settings in the config uses the fine-tuned teacher model. Now, let's take a look at the
+effects of not fine-tuning the teacher model first. To change the teacher model, you can modify the
+``teacher_checkpointer`` in the config:
+
+.. code-block:: yaml
+
+  teacher_checkpointer:
+    _component_: torchtune.training.FullModelHFCheckpointer
+    checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+    checkpoint_files: [
+        model-00001-of-00004.safetensors,
+        model-00002-of-00004.safetensors,
+        model-00003-of-00004.safetensors,
+        model-00004-of-00004.safetensors
+    ]
+
+In the table below, we can see that standard fine-tuning of the 1B model achieves better accuracy
+than the baseline 1B model. By using the fine-tuned 8B teacher model, we see comparable results
+for truthfulqa and improvement for hellaswag and commonsense. When using the baseline 8B as a
+teacher, we see improvement across all metrics, but lower than the other configurations.
+
+.. image:: /_static/img/kd-finetune-teacher.png
+
+Taking a look at the losses, using the baseline 8B as teacher results in a higher loss than
+using the fine-tuned teacher model. The KD loss also remains relatively constant, suggesting
+that the teacher model should have the same distributions as the transfer dataset.
+
+Using a fine-tuned student model
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+For these experiments, let's take a look at the effects of KD when the student model is already
+fine-tuned. In these experiments, we look at different combinations of baseline and fine-tuned 8B
+and 1B models. To change the student model, you can first fine-tune the 1B model then modify the
+student model checkpointer in the config:
+
+.. code-block:: yaml
+
+ checkpointer:
+    _component_: torchtune.training.FullModelHFCheckpointer
+    checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
+    checkpoint_files: [
+      hf_model_0001_0.pt
+    ]
+
+Using the fine-tuned student model boosts accuracy even further for truthfulqa, but the accuracy
+drops for hellaswag and commonsense. Using a fine-tuned teacher model and baseline student
+model achieved the best results on hellaswag and commonsense dataset. Based on these findings,
+the best configuration will change depending on which evaluation dataset and metric you are optimizing for.
+
+.. image:: /_static/img/kd-finetune-student.png
+
+Based on the loss graphs, using a fine-tuned teacher model results in a lower loss irrespective of
+whether the student model is fine-tuned or not. It's also interesting to note that the class loss
+starts to increase when using a fine-tuned student model.
+
+Hyperparameter tuning: learning rate
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+By default, the config has the learning rate as :math:`3e^{-4}`, which is the same as the LoRA configs. For these experiments,
+we changed the learning rate from as high as :math:`1e^{-3}` to as low as :math:`1e^{-5}`. To change the learning rate,
+you can simply override the learning rate parameter using:
+
+.. code-block:: bash
+
+    tune run knowledge_distillation_single_device --config llama3_2/knowledge_distillation_single_device optimizer.lr=1e-3
+
+Based on the results, the optimal learning rate changes depending on which metric you are optimizing for.
+
+.. image:: /_static/img/kd-hyperparam-lr.png
+
+Based on the loss graphs, all learning rates result in similar losses except for :math:`1e^{-5}`, which has a higher KD and class loss.
+
+Hyperparameter tuning: KD ratio
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+In the config, we have the ``kd_ratio`` as 0.5, which gives even weightings to both the class and KD loss. In these experiments,
+we look at the effects of different KD ratios, where 0 only uses the class loss and 1 only uses the KD loss.
+Similar to changing the learning rate, the KD ratio can be adjusted using:
+
+.. code-block:: bash
+
+    tune run knowledge_distillation_single_device --config llama3_2/knowledge_distillation_single_device kd_ratio=0.25
+
+
+Overall, the evaluation results are slightly better for higher KD ratios.
+
+.. image:: /_static/img/kd-hyperparam-kd-ratio.png
+
+Qwen2 1.5B to 0.5B
+^^^^^^^^^^^^^^^^^^
+
+The KD recipe can also be applied to different model families. Here we look at the effect of KD when the number of
+parameters between the teacher and student models are closer. For this experiment, we used Qwen2 1.5B and Qwen2 0.5B, the configs for which can be found in
+`qwen2/knowledge_distillation_single_device <https://github.com/pytorch/torchtune/blob/4234b78b914af23384ce0348f564e2119d107a96/recipes/configs/qwen2/knowledge_distillation_single_device.yaml>`_
+config. Here we see that training on the alpaca cleaned dataset only improves truthful_qa performance and drops the metrics for the other evaluation tasks.
+For truthful_qa, KD improves the student model performance by 5.8% whereas fine-tuning improves performance by 1.3%.
+
+.. image:: /_static/img/kd-qwen2-res.png
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/lora_finetune.rst marc/third_party/torchtune/docs/source/tutorials/lora_finetune.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/lora_finetune.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/lora_finetune.rst	2025-02-20 17:49:29.090023490 -0500
@@ -0,0 +1,383 @@
+.. _lora_finetune_label:
+
+============================
+Fine-Tuning Llama2 with LoRA
+============================
+
+This guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,
+and show you how you can use torchtune to finetune a Llama2 model with LoRA.
+If you already know what LoRA is and want to get straight to running
+your own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn
+
+      * What LoRA is and how it saves memory during finetuning
+      * An overview of LoRA components in torchtune
+      * How to run a LoRA finetune using torchtune
+      * How to experiment with different LoRA configurations
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Be familiar with :ref:`torchtune<overview_label>`
+      * Make sure to :ref:`install torchtune<install_label>`
+      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`
+
+What is LoRA?
+-------------
+
+`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for
+parameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,
+then freezes the network's remaining parameters. LoRA is most commonly applied to
+transformer models, in which case it is common to add the low-rank matrices
+to some of the linear projections in each transformer layer's self-attention.
+
+.. note::
+
+    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_
+    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.
+
+By finetuning with LoRA (as opposed to finetuning all model parameters),
+you can expect to see memory savings due to a substantial reduction in the
+number of parameters with gradients. When using an optimizer with momentum,
+like `AdamW <https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html>`_,
+you can expect to see further memory savings from the optimizer state.
+
+.. note::
+
+    LoRA memory savings come primarily from gradient and optimizer states,
+    so if your model's peak memory comes in its :code:`forward()` method, then LoRA
+    may not reduce peak memory.
+
+How does LoRA work?
+-------------------
+
+LoRA replaces weight update matrices with a low-rank approximation. In general, weight updates
+for an arbitrary :code:`nn.Linear(in_dim,out_dim)` layer could have rank as high as
+:code:`min(in_dim,out_dim)`. LoRA (and other related papers such as `Aghajanyan et al. <https://arxiv.org/abs/2012.13255>`_)
+hypothesize that the `intrinsic dimension <https://en.wikipedia.org/wiki/Intrinsic_dimension>`_
+of these updates during LLM fine-tuning can in fact be much lower.
+To take advantage of this property, LoRA finetuning will freeze the original model,
+then add a trainable weight update from a low-rank projection. More explicitly, LoRA trains two
+matrices :code:`A` and :code:`B`. :code:`A` projects the inputs down to a much smaller rank (often four or eight in practice), and
+:code:`B` projects back up to the dimension output by the original linear layer.
+
+The image below gives a simplified representation of a single weight update step from a full finetune
+(on the left) compared to a weight update step with LoRA (on the right). The LoRA matrices :code:`A` and :code:`B`
+serve as an approximation to the full rank weight update in blue.
+
+.. image:: /_static/img/lora_diagram.png
+
+Although LoRA introduces a few extra parameters in the model :code:`forward()`, only the :code:`A` and :code:`B` matrices are trainable.
+This means that with a rank :code:`r` LoRA decomposition, the number of gradients we need to store reduces
+from :code:`in_dim*out_dim` to :code:`r*(in_dim+out_dim)`. (Remember that in general :code:`r`
+is much smaller than :code:`in_dim` and :code:`out_dim`.)
+
+For example, in the 7B Llama2's self-attention, :code:`in_dim=out_dim=4096` for the Q, K,
+and V projections. This means a LoRA decomposition of rank :code:`r=8` will reduce the number of trainable
+parameters for a given projection from :math:`4096 * 4096 \approx 15M` to :math:`8 * 8192 \approx 65K`, a
+reduction of over 99%.
+
+Let's take a look at a minimal implementation of LoRA in native PyTorch.
+
+
+.. code-block:: python
+
+  import torch
+  from torch import nn
+
+  class LoRALinear(nn.Module):
+    def __init__(
+      self,
+      in_dim: int,
+      out_dim: int,
+      rank: int,
+      alpha: float,
+      dropout: float
+    ):
+      # These are the weights from the original pretrained model
+      self.linear = nn.Linear(in_dim, out_dim, bias=False)
+
+      # These are the new LoRA params. In general rank << in_dim, out_dim
+      self.lora_a = nn.Linear(in_dim, rank, bias=False)
+      self.lora_b = nn.Linear(rank, out_dim, bias=False)
+
+      # Rank and alpha are commonly-tuned hyperparameters
+      self.rank = rank
+      self.alpha = alpha
+
+      # Most implementations also include some dropout
+      self.dropout = nn.Dropout(p=dropout)
+
+      # The original params are frozen, and only LoRA params are trainable.
+      self.linear.weight.requires_grad = False
+      self.lora_a.weight.requires_grad = True
+      self.lora_b.weight.requires_grad = True
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+      # This would be the output of the original model
+      frozen_out = self.linear(x)
+
+      # lora_a projects inputs down to the much smaller self.rank,
+      # then lora_b projects back up to the output dimension
+      lora_out = self.lora_b(self.lora_a(self.dropout(x)))
+
+      # Finally, scale by the alpha parameter (normalized by rank)
+      # and add to the original model's outputs
+      return frozen_out + (self.alpha / self.rank) * lora_out
+
+There are some other details around initialization which we omit here, but if you'd like to know more
+you can see our implementation in :class:`~torchtune.modules.peft.LoRALinear`.
+Now that we understand what LoRA is doing, let's look at how we can apply it to our favorite models.
+
+Applying LoRA to Llama2 models
+------------------------------
+
+With torchtune, we can easily apply LoRA to Llama2 with a variety of different configurations.
+Let's take a look at how to construct Llama2 models in torchtune with and without LoRA.
+
+.. code-block:: python
+
+  from torchtune.models.llama2 import llama2_7b, lora_llama2_7b
+
+  # Build Llama2 without any LoRA layers
+  base_model = llama2_7b()
+
+  # The default settings for lora_llama2_7b will match those for llama2_7b
+  # We just need to define which layers we want LoRA applied to.
+  # Within each self-attention, we can choose from ["q_proj", "k_proj", "v_proj", and "output_proj"].
+  # We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear
+  # layers outside of the self-attention.
+  lora_model = lora_llama2_7b(lora_attn_modules=["q_proj", "v_proj"])
+
+.. note::
+
+    Calling :func:`lora_llama_2_7b <torchtune.models.llama2.lora_llama2_7b>` alone will not handle the definition of which parameters are trainable.
+    See :ref:`below<setting_trainable_params>` for how to do this.
+
+Let's inspect each of these models a bit more closely.
+
+.. code-block:: bash
+
+  # Print the first layer's self-attention in the usual Llama2 model
+  >>> print(base_model.layers[0].attn)
+  MultiHeadAttention(
+    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
+    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
+    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
+    (output_proj): Linear(in_features=4096, out_features=4096, bias=False)
+    (pos_embeddings): RotaryPositionalEmbeddings()
+  )
+
+  # Print the same for Llama2 with LoRA weights
+  >>> print(lora_model.layers[0].attn)
+  MultiHeadAttention(
+    (q_proj): LoRALinear(
+      (dropout): Dropout(p=0.0, inplace=False)
+      (lora_a): Linear(in_features=4096, out_features=8, bias=False)
+      (lora_b): Linear(in_features=8, out_features=4096, bias=False)
+    )
+    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
+    (v_proj): LoRALinear(
+      (dropout): Dropout(p=0.0, inplace=False)
+      (lora_a): Linear(in_features=4096, out_features=8, bias=False)
+      (lora_b): Linear(in_features=8, out_features=4096, bias=False)
+    )
+    (output_proj): Linear(in_features=4096, out_features=4096, bias=False)
+    (pos_embeddings): RotaryPositionalEmbeddings()
+  )
+
+
+Notice that our LoRA model's layer contains additional weights in the Q and V projections,
+as expected. Additionally, inspecting the type of :code:`lora_model` and
+:code:`base_model`, would show that they are both instances of the same :class:`~torchtune.modules.TransformerDecoder`.
+(Feel free to verify this for yourself.)
+
+Why does this matter? torchtune makes it easy to load checkpoints for LoRA directly from our Llama2
+model without any wrappers or custom checkpoint conversion logic.
+
+.. code-block:: python
+
+  # Assuming that base_model already has the pretrained Llama2 weights,
+  # this will directly load them into your LoRA model without any conversion necessary.
+  lora_model.load_state_dict(base_model.state_dict(), strict=False)
+
+.. note::
+    Whenever loading weights with :code:`strict=False`, you should verify that any missing or extra keys in
+    the loaded :code:`state_dict` are as expected. torchtune's LoRA recipes do this by default via e.g.
+    :func:`validate_state_dict_for_lora() <torchtune.modules.peft.validate_state_dict_for_lora>` or
+    :func:`validate_missing_and_unexpected_for_lora() <torchtune.modules.peft.validate_missing_and_unexpected_for_lora>`.
+
+Once we've loaded the base model weights, we also want to set only LoRA parameters to trainable.
+
+.. _setting_trainable_params:
+
+.. code-block:: python
+
+  from torchtune.modules.peft.peft_utils import get_adapter_params, set_trainable_params
+
+  # Fetch all params from the model that are associated with LoRA.
+  lora_params = get_adapter_params(lora_model)
+
+  # Set requires_grad=True on lora_params, and requires_grad=False on all others.
+  set_trainable_params(lora_model, lora_params)
+
+  # Print the total number of parameters
+  total_params = sum([p.numel() for p in lora_model.parameters()])
+  trainable_params = sum([p.numel() for p in lora_model.parameters() if p.requires_grad])
+  print(
+    f"""
+    {total_params} total params,
+    {trainable_params}" trainable params,
+    {(100.0 * trainable_params / total_params):.2f}% of all params are trainable.
+    """
+  )
+
+  6742609920 total params,
+  4194304 trainable params,
+  0.06% of all params are trainable.
+
+.. note::
+    If you are directly using the LoRA recipe (as detailed :ref:`here<lora_recipe_label>`), you need only pass the
+    relevant checkpoint path. Loading model weights and setting trainable parameters will be taken care
+    of in the recipe.
+
+
+.. _lora_recipe_label:
+
+LoRA finetuning recipe in torchtune
+-----------------------------------
+
+Finally, we can put it all together and finetune a model using torchtune's `LoRA recipe <https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py>`_.
+Make sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.
+You can then run the following command to perform a LoRA finetune of Llama2-7B with two GPUs (each having VRAM of at least 16GB):
+
+.. code-block:: bash
+
+    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora
+
+.. note::
+    Make sure to point to the location of your Llama2 weights and tokenizer. This can be done
+    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`
+    or by directly modifying the :code:`7B_lora.yaml` file. See our "":ref:`config_tutorial_label`" recipe
+    for more details on how you can easily clone and modify torchtune configs.
+
+.. note::
+    You can modify the value of :code:`nproc_per_node` depending on (a) the number of GPUs you have available,
+    and (b) the memory constraints of your hardware.
+
+The preceding command will run a LoRA finetune with torchtune's factory settings, but we may want to experiment a bit.
+Let's take a closer look at some of the :code:`lora_finetune_distributed` config.
+
+.. code-block:: yaml
+
+  # Model Arguments
+  model:
+    _component_: lora_llama2_7b
+    lora_attn_modules: ['q_proj', 'v_proj']
+    lora_rank: 8
+    lora_alpha: 16
+  ...
+
+We see that the default is to apply LoRA to Q and V projections with a rank of 8.
+Some experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers in
+the self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max memory,
+but as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.
+
+Let's run this experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).
+
+.. code-block:: bash
+
+    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \
+    lora_attn_modules=['q_proj','k_proj','v_proj','output_proj'] \
+    lora_rank=32 lora_alpha=64 output_dir=./lora_experiment_1
+
+A comparison of the (smoothed) loss curves between this run and our baseline over the first 500 steps can be seen below.
+
+.. image:: /_static/img/lora_experiment_loss_curves.png
+
+.. note::
+    The above figure was generated with W&B. You can use torchtune's :class:`~torchtune.training.metric_logging.WandBLogger`
+    to generate similar loss curves, but you will need to install W&B and setup an account separately. For more details on
+    using W&B in torchtune, see our ":ref:`wandb_logging`" recipe.
+
+.. _lora_tutorial_memory_tradeoff_label:
+
+Trading off memory and model performance with LoRA
+--------------------------------------------------
+
+In the preceding example, we ran LoRA on two devices. But given LoRA's low memory footprint, we can run fine-tuning
+on a single device using most commodity GPUs which support `bfloat16 <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format#bfloat16_floating-point_format>`_
+floating-point format. This can be done via the command:
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device --config llama2/7B_lora_single_device
+
+On a single device, we may need to be more cognizant of our peak memory. Let's run a few experiments
+to see our peak memory during a finetune. We will experiment along two axes:
+first, which model layers have LoRA applied, and second, the rank of each LoRA layer. (We will scale
+alpha in parallel to LoRA rank, as discussed above.)
+
+To compare the results of our experiments, we can evaluate our models on `truthfulqa_mc2 <https://github.com/sylinrl/TruthfulQA>`_, a task from
+the `TruthfulQA <https://arxiv.org/abs/2109.07958>`_ benchmark for language models. For more details on how to run this and other evaluation tasks
+with torchtune's EleutherAI evaluation harness integration, see our :ref:`End-to-End Workflow Tutorial <eval_harness_label>`.
+
+Previously, we only enabled LoRA for the linear layers in each self-attention module, but in fact there are other linear
+layers we can apply LoRA to: MLP layers and our model's final output projection. Note that for Llama-2-7B the final output
+projection maps to the vocabulary dimension (32000 instead of 4096 as in the other linear layers), so enabling LoRA for this layer will increase
+our peak memory a bit more than the other layers. We can make the following changes to our config:
+
+.. code-block:: yaml
+
+  # Model Arguments
+  model:
+    _component_: lora_llama2_7b
+    lora_attn_modules: ['q_proj', 'k_proj', 'v_proj', 'output_proj']
+    apply_lora_to_mlp: True
+    apply_lora_to_output: True
+  ...
+
+.. note::
+    All the finetuning runs below use the `llama2/7B_lora_single_device <https://github.com/pytorch/torchtune/blob/main/recipes/configs/llama2/7B_lora_single_device.yaml>`_
+    config, which has a default batch size of 2. Modifying the batch size (or other hyperparameters, e.g. the optimizer) will impact both peak memory
+    and final evaluation results.
+
+.. list-table::
+   :widths: 25 25 25 25 25
+   :header-rows: 1
+
+   * - LoRA Layers
+     - Rank
+     - Alpha
+     - Peak Memory
+     - Accuracy (truthfulqa_mc2)
+   * - Q and V only
+     - 8
+     - 16
+     - **15.57 GB**
+     - 0.475
+   * - all layers
+     - 8
+     - 16
+     - 15.87 GB
+     - 0.508
+   * - Q and V only
+     - 64
+     - 128
+     - 15.86 GB
+     - 0.504
+   * - all layers
+     - 64
+     - 128
+     - 17.04 GB
+     - **0.514**
+
+We can see that our baseline settings give the lowest peak memory, but our evaluation performance is relatively lower.
+By enabling LoRA for all linear layers and increasing the rank to 64, we see almost a 4% absolute improvement
+in our accuracy on this task, but our peak memory also increases by about 1.4GB. These are just a couple simple
+experiments; we encourage you to run your own finetunes to find the right tradeoff for your particular setup.
+
+Additionally, if you want to decrease your model's peak memory even further (and still potentially achieve similar
+model quality results), you can check out our :ref:`QLoRA tutorial<qlora_finetune_label>`.
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/memory_optimizations.rst marc/third_party/torchtune/docs/source/tutorials/memory_optimizations.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/memory_optimizations.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/memory_optimizations.rst	2025-02-20 17:49:29.094023497 -0500
@@ -0,0 +1,349 @@
+.. _memory_optimization_overview_label:
+
+============================
+Memory Optimization Overview
+============================
+
+**Author**: `Salman Mohammadi <https://github.com/SalmanMohammadi>`_
+
+torchtune comes with a host of plug-and-play memory optimization components which give you lots of flexibility
+to ``tune`` our recipes to your hardware. This page provides a brief glossary of these components and how you might use them.
+To make things easy, we've summarized these components in the following table:
+
+.. csv-table:: Memory optimization components
+   :header: "Component", "When to use?"
+   :widths: auto
+
+   ":ref:`glossary_precision`", "You'll usually want to leave this as its default ``bfloat16``. If you're struggling with training stability or accuracy due to precision, fp32 may help, but will significantly increase memory usage and decrease training speed."
+   ":ref:`glossary_act_ckpt`", "Use when you're memory constrained and need to handle larger batch sizes or longer context lengths. Be aware that it may slow down training speed."
+   ":ref:`glossary_grad_accm`", "Helpful when memory-constrained to simulate larger batch sizes. Often preferable to activation checkpointing for better training speed."
+   ":ref:`glossary_low_precision_opt`", "When you need to further reduce memory usage beyond using ``bf16`` by reducing the precision in the optimizer states. Note that lower precision optimizers may reduce training stability/accuracy."
+   ":ref:`glossary_opt_in_bwd`", "Helps reduce memory usage when using stateful optimizers, particularly when full-finetuning large models with high gradient memory usage. This is not compatible with ``gradient_accumulation_steps``, so training may slow down due to reduced model throughput."
+   ":ref:`glossary_lora`", "When you want to significantly reduce the number of trainable parameters, saving gradient and optimizer memory during training, and significantly speeding up training."
+   ":ref:`glossary_qlora`", "When you need even more memory savings than LoRA, at the potential cost of some training speed. Useful for very large models or limited hardware."
+
+
+.. note::
+
+  In its current state, this tutorial is focused on single-device optimizations. Check in soon as we update this page
+  for the latest memory optimization features for distributed fine-tuning.
+
+.. _glossary_precision:
+
+
+Model Precision
+---------------
+
+*What's going on here?*
+
+We use the term "precision" to refer to the underlying data type used to represent the model and optimizer parameters.
+We support two data types in torchtune:
+
+.. note::
+
+  We recommend diving into Sebastian Raschka's `blogpost on mixed-precision techniques <https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html>`_
+  for a deeper understanding of concepts around precision and data formats.
+
+* ``fp32``, commonly referred to as "full-precision", uses 4 bytes per model and optimizer parameter.
+* ``bfloat16``, referred to as "half-precision", uses 2 bytes per model and optimizer parameter - effectively half
+  the memory of ``fp32``, and also improves training speed. Generally, if your hardware supports training with ``bfloat16``,
+  we recommend using it - this is the default setting for our recipes.
+
+.. note::
+
+  Another common paradigm is "mixed-precision" training: where model weights are in ``bfloat16`` (or ``fp16``), and optimizer
+  states are in ``fp32``. Currently, we don't support mixed-precision training in torchtune.
+
+*Sounds great! How do I use it?*
+
+Simply use the ``dtype`` flag or config entry in all our recipes! For example, to use half-precision training in ``bf16``,
+set ``dtype=bf16``.
+
+.. _glossary_act_ckpt:
+
+Activation Checkpointing
+------------------------
+
+*What's going on here?*
+
+The relevant section in the `PyTorch documentation <https://pytorch.org/docs/stable/checkpoint.html>`_ explains this concept well.
+To quote:
+
+  Activation checkpointing is a technique that trades compute for memory.
+  Instead of keeping tensors needed for backward alive until they are used in
+  gradient computation during backward, forward computation in checkpointed
+  regions omits saving tensors for backward and recomputes them during the backward pass.
+
+This setting is helpful for when you're memory-constrained, especially due to larger batch sizes or longer context lengths.
+However, these savings in memory come at the cost of training speed (i.e. tokens-per-second),
+and in most cases training can slow down quite a bit as a result of this activation recomputation.
+
+*Sounds great! How do I use it?*
+
+To enable activation checkpointing, use the ``enable_activation_checkpointing`` config entry or flag
+in any of our recipes, e.g. ``enable_activation_checkpointing=True``.
+
+.. _glossary_act_off:
+
+Activation Offloading
+---------------------
+
+*What's going on here?*
+
+You may have just read about activation checkpointing! Similar to checkpointing, offloading is a memory
+efficiency technique that allows saving GPU VRAM by temporarily moving activations to CPU and bringing
+them back when needed in the backward pass.
+
+See `PyTorch autograd hook tutorial <https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html#saving-tensors-to-cpu>`_
+for more details about how this is implemented through saved_tensors_hooks.
+
+This setting is especially helpful for larger batch sizes, or longer context lengths when you're memory constrained.
+While of course it takes runtime and resources to move Tensors from GPU to CPU and back, the implementation in
+torchtune uses multiple CUDA streams (when available) in order to overlap the extra communication with the computation
+to hide the extra runtime. As the communication workload is variable depending on the number and size of tensors being
+offloaded, it is common to not offload every single activation. In fact, one can use offloading in conjunction with activations
+checkpointing, where all activations will either be recomputed later in the backward or brought back from the CPU.
+
+*Sounds great! How do I use it?*
+
+To enable activation offloading, use the ``enable_activation_offloading`` config entry or flag
+in our lora finetuning single device recipe, e.g. ``enable_activation_offloading=True``. To allow
+usage of streams, make sure you are on a torch version later than PyTorch 2.5.0.dev20240907.
+
+.. _glossary_grad_accm:
+
+Gradient Accumulation
+---------------------
+
+*What's going on here?*
+
+Gradient accumulation allows you to simulate large batch sizes by *accumulating* gradients over several
+batches before updating model parameters using the optimizer. Concretely, the total number of samples used
+for a gradient update is when using gradient accumulation is:
+
+  ``total_batch_size = batch_size * gradient_accumulation_steps``
+
+For example: with ``batch_size=1`` and ``gradient_accumulation_steps=32`` we get a total batch size of 32.
+
+.. note::
+
+  For other components in torchtune which use "steps", such as :ref:`metric logging <metric_logging_label>`, or
+  :func:`learning rate schedulers <torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup>`, a "step" is counted as a
+  single update to model parameters, rather than a single model forward pass with the data.
+  Suppose ``gradient_accumulation_steps = 4`` and ``log_every_n_steps = 10``.
+  Metrics would be logged every 10 global steps, which translates to every 40 model forward passes.
+  For this reason, metric logging will appear less frequently when training with gradient accumulation,
+  and progress bars may update more slowly.
+
+
+If you're using one of our distributed recipes, simply multiply by the number of devices:
+
+  ``total_batch_size = batch_size * gradient_accumulation_steps * num_devices``
+
+Gradient accumulation is especially useful when you are memory constrained. In this case,
+accumulating gradients might give you better training speed than enabling :ref:`activation
+checkpointing <glossary_act_ckpt>`, since activation checkpointing reduces memory consumption at the cost of repeated
+computations.
+
+*Sounds great! How do I use it?*
+
+All of our finetuning recipes support simulating larger batch sizes by accumulating gradients. Just set the
+``gradient_accumulation_steps`` flag or config entry.
+
+.. note::
+
+  Gradient accumulation should always be set to 1 when :ref:`fusing the optimizer step into the backward pass <glossary_opt_in_bwd>`.
+
+.. _glossary_low_precision_opt:
+
+Lower Precision Optimizers
+--------------------------
+
+*What's going on here?*
+
+In addition to :ref:`reducing model and optimizer precision <glossary_precision>` during training, we can further reduce precision in our optimizer states.
+All of our single-device fine-tuning recipes support lower-precision optimizers from the `bitsandbytes <https://huggingface.co/docs/bitsandbytes/main/en/index>`_ library -
+a good place to start might be the ``AdamW8bit`` and ``PagedAdamW8bit`` optimizers, which we've tested our recipes with.
+
+*Sounds great! How do I use it?*
+
+To use this in your recipes, make sure you have installed bitsandbytes (``pip install bitsandbytes``). Then, enable
+a low precision optimizer using the :ref:`cli_label`:
+
+.. code-block:: bash
+
+  tune run <RECIPE> --config <CONFIG> \
+  optimizer=bitsandbytes.optim.PagedAdamW
+
+or by directly :ref:`modifying a config file<config_tutorial_label>`:
+
+.. code-block:: yaml
+
+  optimizer:
+    _component_: bitsandbytes.optim.PagedAdamW
+    lr: 2e-5
+
+.. _glossary_opt_in_bwd:
+
+Fusing Optimizer Step into Backward Pass
+----------------------------------------
+
+*What's going on here?*
+
+Stateful optimizers (e.g. optimizers which use momentum) are the default in modern deep learning due to their stable convergence properties.
+However, maintaining a state of gradient statistics comes at the cost of additional memory usage. An immediate alternative might be to
+turn to stateless optimizers such as `stochastic gradient descent <https://pytorch.org/docs/stable/generated/torch.optim.SGD.html>`_
+without momentum, which don't require any additional memory usage, but will likely result in worse convergence during training.
+
+Can we find a middle ground here? Let's consider a technique which enables the use of "stateful" optimizers such as `AdamW <https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html>`_
+without the memory overhead of gradient statistics, and without sacrificing their desirable convergence properties.
+How is this possible, you might ask? By *completely removing the buffer of gradients* which are stored by the optimizer during its ``step()``.
+
+To understand how this works, we encourage you to read through the relevant PyTorch tutorial on this concept:
+`How to save memory by fusing the optimizer step into the backward pass <https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html>`_.
+
+
+*Sounds great! How do I use it?*
+
+.. todo ref full finetune recipe doc
+
+In torchtune, you can enable this feature using the ``optimizer_in_bwd`` flag, which is currently only supported in our
+single-device full finetune recipe. This feature works best when gradient memory is particularly large;
+e.g. when using a stateful optimizer with a model with a lot of parameters, and when you don't need to use
+:ref:`gradient accumulation <glossary_grad_accm>`.
+
+.. _glossary_peft:
+
+Parameter Efficient Fine-Tuning (PEFT)
+--------------------------------------
+
+.. _glossary_lora:
+
+Low Rank Adaptation (LoRA)
+^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+
+*What's going on here?*
+
+You can read our tutorial on :ref:`finetuning Llama2 with LoRA<lora_finetune_label>` to understand how LoRA works, and how to use it.
+Simply stated, LoRA greatly reduces the number of trainable parameters, thus saving significant gradient and optimizer
+memory during training.
+
+*Sounds great! How do I use it?*
+
+You can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize
+LoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.
+the :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.
+We aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,
+just specify any config with ``_lora`` in its name, e.g:
+
+.. code-block:: bash
+
+  tune run lora_finetune_single_device --config llama3/8B_lora_single_device
+
+
+There are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control
+which linear layers LoRA should be applied to in the model:
+
+* ``lora_attn_modules: List[str]`` accepts a list of strings specifying which layers of the model to apply
+  LoRA to:
+
+  * ``q_proj`` applies LoRA to the query projection layer.
+  * ``k_proj`` applies LoRA to the key projection layer.
+  * ``v_proj`` applies LoRA to the value projection layer.
+  * ``output_proj`` applies LoRA to the attention output projection layer.
+
+  Whilst adding more layers to be fine-tuned may improve model accuracy,
+  this will come at the cost of increased memory usage and reduced training speed.
+
+* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.
+* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.
+  This is usually a projection to vocabulary space (e.g. in language models), but
+  other modelling tasks may have different projections - classifier models will project
+  to the number of classes, for example
+
+.. note::
+
+  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the
+  final output projection do not support ``apply_lora_to_output``.
+
+These are all specified under the ``model`` flag or config entry, i.e:
+
+.. code-block:: bash
+
+  tune run lora_finetune_single_device --config llama3/8B_lora_single_device  \
+  model.apply_lora_to_mlp=True \
+  model.lora_attn_modules=["q_proj","k_proj","v_proj"]
+
+.. code-block:: yaml
+
+  model:
+    apply_lora_to_mlp: True
+    model.lora_attn_modules: ["q_proj", "k_proj", "v_proj"]
+
+Secondly, parameters which control the scale of the impact of LoRA on the model:
+
+* ``lora_rank: int`` affects the scale of the LoRA decomposition, where ``lora_rank << in_dim`` and ``lora_rank << out_dim``
+  \- the dimensions of an arbitrary linear layer in the model. Concretely, ``lora_rank`` reduces the number of gradients stored
+  in a linear fashion from ``in_dim * out_dim`` to ``lora_rank * (in_dim + out_dim)``. Typically, we have ``lora_rank in [8, 128]``.
+* ``lora_alpha: float`` affects the magnitude of the LoRA updates. A larger alpha results in larger updates to the base model weights
+  , potentially at the cost of training stability, conversely, smaller alpha can stabilize training at the cost of slower learning.
+  We provide default settings for these parameters which we've tested with all of our models, but we encourage you to adjust them
+  to your specific use case. Typically, one jointly changes ``lora_rank`` and ``lora_alpha`` together, where ``lora_alpha ~= 2*lora_rank``.
+* ``lora_dropout`` introduces dropout in the LoRA layers to help regularize training. We default to 0.0 for all of our models.
+
+As above, these parameters are also specified under the ``model`` flag or config entry.
+
+.. note::
+
+  To get a deeper sense of how LoRA parameters affect memory usage during training,
+  see the :ref:`relevant section in our Llama2 LoRA tutorial<lora_tutorial_memory_tradeoff_label>`.
+
+.. _glossary_qlora:
+
+Quantized Low Rank Adaptation (QLoRA)
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+*What's going on here?*
+
+`QLoRA <https://arxiv.org/abs/2305.14314>`_ is an enhancement on top of `LoRA <https://arxiv.org/abs/2106.09685>`_
+that maintains the frozen model parameters from LoRA in 4-bit quantized precision, thereby reducing memory usage.
+This is enabled through a novel  4-bit NormalFloat (NF4) data type proposed by the authors, which allows for 4-8x less
+parameter memory usage whilst retaining model accuracy. You can read our tutorial on :ref:`finetuning Llama2 with QLoRA<qlora_finetune_label>`
+for a deeper understanding of how it works.
+
+When considering using QLoRA to reduce memory usage, it's worth noting that QLoRA prevents accuracy degradation during quantization
+by up-casting quantized parameters to the original higher precision datatype during model forward passes - this up-casting may
+incur penalties to training speed. The :ref:`relevant section <qlora_compile_label>` in our QLoRA tutorial demonstrates the usage of ``torch.compile``
+to address this by speeding up training.
+
+*Sounds great! How do I use it?*
+
+You can finetune using QLoRA with any of our LoRA recipes, i.e. recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize
+QLoRA-enabled model builders, which we support for all our models, and also use the ``qlora_`` prefix, e.g.
+the :func:`torchtune.models.llama3.llama3_8b` model has a corresponding :func:`torchtune.models.llama3.qlora_llama3_8b`.
+We aim to provide a comprehensive set of configurations to allow you to get started with training with QLoRA quickly,
+just specify any config with ``_qlora`` in its name, e.g:
+
+
+.. code-block:: bash
+
+  tune run lora_finetune_single_device --config llama3/8B_qlora_single_device
+
+All the rest of the LoRA parameters remain the same for QLoRA - check out the section above on :ref:`LoRA <glossary_lora>`
+to see how to configure.
+
+.. _glossary_distrib:
+
+.. TODO
+
+.. Distributed
+.. -----------
+
+.. .. _glossary_fsdp:
+
+.. Fully Sharded Data Parallel (FSDP)
+.. ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+.. All our ``_distributed`` recipes use `FSDP <https://pytorch.org/docs/stable/fsdp.html>`.
+.. .. _glossary_fsdp2:
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/qat_finetune.rst marc/third_party/torchtune/docs/source/tutorials/qat_finetune.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/qat_finetune.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/qat_finetune.rst	2025-02-20 17:49:29.098023502 -0500
@@ -0,0 +1,395 @@
+.. _qat_finetune_label:
+
+===========================
+Fine-Tuning Llama3 with QAT
+===========================
+
+Quantization-Aware Training (QAT) is a common technique for users to quantize their
+models without incurring significant degradations in accuracy or perplexity. In this
+tutorial, well walk through how to apply QAT during fine-tuning, quantize the
+resulting model, and evaluate your quantized model using torchtune.
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn
+
+      * What QAT is and how it helps reduce quantization degradation
+      * How to run QAT during fine-tuning in torchtune
+      * End-to-end example of connecting QAT, quantization, and evaluation recipes
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Be familiar with :ref:`torchtune<overview_label>`
+      * Make sure to :ref:`install torchtune<install_label>`
+      * Make sure you have downloaded the :ref:`Llama3-8B model weights<download_llama_label>`
+
+.. _what_is_qat_label:
+
+What is QAT?
+------------
+
+`Quantization-Aware Training <https://pytorch.org/blog/introduction-to-quantization-on-pytorch/#quantization-aware-training>`_ (QAT) refers to simulating quantization numerics during
+training or fine-tuning, with the end goal of ultimately producing a higher quality
+quantized model compared to simple post-training quantization (PTQ). During QAT,
+the weights and/or activations are fake quantized, meaning they are transformed
+as if they were being quantized, but kept in the original data type (e.g. bfloat16)
+without being actually cast to lower bit-widths. Thus, fake quantization allows the
+model to adjust for quantization noise when updating the weights, hence the training
+process is aware that the model will ultimately be quantized after training.
+
+.. code-block:: python
+
+  # PTQ: x_q is quantized and cast to int8
+  # scale and zero point (zp) refer to parameters used to quantize x_float
+  # qmin and qmax refer to the range of quantized values
+  x_q = (x_float / scale + zp).round().clamp(qmin, qmax).cast(int8)
+
+  # QAT: x_fq is still in float
+  # Fake quantize simulates the numerics of quantize + dequantize
+  x_fq = (x_float / scale + zp).round().clamp(qmin, qmax)
+  x_fq = (x_fq - zp) * scale
+
+QAT typically involves applying a transformation to your model before and after training.
+For example, in the `torchao QAT implementation <https://github.com/pytorch/ao/blob/v0.2.0/torchao/quantization/prototype/qat.py>`_,
+these are represented as the ``prepare()`` and ``convert()`` steps: (1) ``prepare()`` inserts fake quantize
+operations into linear layers, and (2) ``convert()`` transforms the fake quantize operations
+to actual quantize and dequantize operations after training, thereby producing a quantized
+model (dequantize operations are typically fused with linear after lowering).
+Between these two steps, training can proceed exactly as before.
+
+.. image:: /_static/img/qat_diagram.png
+
+.. _apply_qat_label:
+
+Applying QAT to Llama3 models
+-----------------------------
+
+We can easily apply the above QAT transformations to Llama3 in torchtune for fine-tuning:
+
+.. code-block:: python
+
+  from torchtune.training.quantization import Int8DynActInt4WeightQATQuantizer
+  from torchtune.models.llama3 import llama3_8b
+
+  model = llama3_8b()
+
+  # Quantizer for int8 dynamic per token activations +
+  # int4 grouped per channel weights, only for linear layers
+  quantizer = Int8DynActInt4WeightQATQuantizer()
+
+  # Insert "fake quantize" operations into linear layers.
+  # These operations simulate quantization numerics during
+  # fine-tuning without performing any dtype casting
+  prepared_model = quantizer.prepare(model)
+
+If we print the model well see that all linear layers have been swapped with
+:code:`Int8DynActInt4WeightQATLinear`, which simulates the numerics of int8
+dynamic per token activations + int4 grouped per channel weights. Now the model
+is ready for fine-tuning.
+
+.. code-block:: bash
+
+  >>> print(model.layers[0].attn)
+  MultiHeadAttention(
+    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
+    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
+    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
+    (output_proj): Linear(in_features=4096, out_features=4096, bias=False)
+    (pos_embeddings): RotaryPositionalEmbeddings()
+  )
+
+  >>> print(prepared_model.layers[0].attn)
+  MultiHeadAttention(
+    (q_proj): Int8DynActInt4WeightQATLinear(in_features=4096, out_features=4096, bias=False)
+    (k_proj): Int8DynActInt4WeightQATLinear(in_features=4096, out_features=1024, bias=False)
+    (v_proj): Int8DynActInt4WeightQATLinear(in_features=4096, out_features=1024, bias=False)
+    (output_proj): Int8DynActInt4WeightQATLinear(in_features=4096, out_features=4096, bias=False)
+    (pos_embeddings): RotaryPositionalEmbeddings()
+  )
+
+After fine-tuning, we can convert the model to get an actual quantized model.
+If we print the converted model, well see that the QAT linears have been
+swapped with `Int8DynActInt4WeightLinear <https://github.com/pytorch/ao/blob/428084356ace4ea94c22a3a9b3d74cff8ee41db3/torchao/quantization/prototype/qat.py#L38>`_, which are the quantized versions
+of the linear layers. This quantized model can then be saved to checkpoint and
+used for inference or generation.
+
+.. code-block:: python
+
+  # Fine-tune as before
+  train_loop(prepared_model)
+
+  # Convert fake quantize to actual quantize operations
+  converted_model = quantizer.convert(prepared_model)
+
+.. code-block:: bash
+
+  >>> print(converted_model.layers[0].attn)
+  MultiHeadAttention(
+    (q_proj): Int8DynActInt4WeightLinear()
+    (k_proj): Int8DynActInt4WeightLinear()
+    (v_proj): Int8DynActInt4WeightLinear()
+    (output_proj): Int8DynActInt4WeightLinear()
+    (pos_embeddings): RotaryPositionalEmbeddings()
+  )
+
+
+QAT finetuning recipe in torchtune
+----------------------------------
+
+Putting it all together, we can now fine-tune a model using torchtunes `QAT recipe <qat_distributed_recipe_label>`.
+Make sure that you have first downloaded the Llama3 weights and tokenizer by
+following :ref:`these instructions<download_llama_label>`. In this tutorial,
+we use the following settings to demonstrate QATs effectiveness in recovering
+quantization degradation compared to directly quantizing a model fine-tuned
+without QAT. You can copy the default QAT config and make the following
+modifications accordingly:
+
+.. code-block:: bash
+
+  tune cp llama3/8B_qat_full custom_8B_qat_full.yaml
+
+.. code-block:: yaml
+
+  # Dataset
+  dataset:
+    _component_: torchtune.datasets.text_completion_dataset
+    source: allenai/c4
+    max_seq_len: 8192
+    column: text
+    name: en
+    split: train
+  seed: null
+  shuffle: True
+
+  ...
+
+  epochs: 1
+  max_steps_per_epoch: 2000
+  fake_quant_after_n_steps: 1000
+  memory_efficient_fsdp_wrap: False
+
+.. note::
+
+  QAT in torchtune is currently not compatible with `memory_efficient_fsdp_wrap <https://pytorch.org/torchtune/stable/generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.html#torchtune.utils.get_full_finetune_fsdp_wrap_policy>`_.
+  This is a known issue and will be fixed in a future torchtune version.
+
+Empirically, we observed that disabling fake quantization for the first N steps
+led to better results, presumably because doing so allows the weights to stabilize
+before we start introducing quantization noise to the fine-tuning process.
+For this reason, here we disable fake quantization for the first 1000 steps.
+
+You can then use the following command to run fine-tuning with QAT using the above
+config. This workload requires at least 6 GPUs, each with VRAM of at least 80GB.
+By default, this uses the int8 dynamic per token activations + int4 grouped per
+channel weights quantization configuration as shown above:
+
+.. code-block:: bash
+
+  tune run --nnodes 1 --nproc_per_node 6 qat_distributed --config custom_8B_qat_full.yaml
+
+.. note::
+
+  Make sure to point to the location of your Llama3 weights and tokenizer. This can be done
+  either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`
+  or by directly modifying the :code:`8B_qat_full.yaml` file. See our :ref:`config_tutorial_label`
+  for more details on how you can easily clone and modify torchtune configs.
+
+.. note::
+
+  QAT introduces memory and computation overheads compared to regular fine-tuning,
+  since fake quantization fundamentally involves extra ops and requires cloning
+  the weights to avoid mutating them when computing the fake quantized values.
+  In general, we expect around 30% decrease in fine-tuning speed for models like
+  Llama3-8B. With activation checkpointing, the increase in memory footprint per
+  GPU is minimal (< 5GB per GPU).
+
+
+Quantizing the QAT model
+------------------------
+
+Note that the QAT recipe above produces an unquantized bfloat16 model. The model
+structure is exactly the same as the model produced with regular full fine-tuning
+without QAT, just with different weights. To actually get a quantized model,
+copy and make the following modifications to the quantization config:
+
+.. code-block:: bash
+
+  tune cp quantization custom_quantization.yaml
+
+.. code-block:: yaml
+
+  # Model arguments
+  model:
+    _component_: torchtune.models.llama3.llama3_8b
+
+  checkpointer:
+    _component_: torchtune.training.FullModelMetaCheckpointer
+    checkpoint_dir: <your QAT checkpoint dir>
+    checkpoint_files: [meta_model_0.pt]
+    recipe_checkpoint: null
+    output_dir: <your QAT checkpoint dir>
+    model_type: LLAMA3
+
+  ...
+
+  quantizer:
+    _component_: torchtune.training.quantization.Int8DynActInt4WeightQATQuantizer
+    groupsize: 256
+
+The following command performs the convert step in the QAT flow, which actually
+quantizes the float model to a model with quantized weights:
+
+.. code-block:: bash
+
+  tune run quantize --config custom_quantization.yaml
+
+.. note::
+
+  Make sure to use the same QAT quantizer you used to fine-tune your model,
+  otherwise the numerics will be off and the quantized model will perform poorly.
+
+.. _qat_eval_label:
+
+Evaluating the quantized model
+------------------------------
+
+Now that we have a quantized model, we can run some evaluations on it and compare the
+results against regular fine-tuning without QAT (i.e. post-training quantization).
+To achieve this, we use `EleutherAIs evaluation harness <https://github.com/EleutherAI/lm-evaluation-harness>`_
+integrated in torchtune. First, copy the evaluation config and make the following changes:
+
+.. code-block:: bash
+
+  tune cp eleuther_evaluation custom_eleuther_evaluation.yaml
+
+.. code-block:: yaml
+
+  # Model arguments
+  model:
+    _component_: torchtune.models.llama3.llama3_8b
+
+  checkpointer:
+    _component_: torchtune.training.FullModelTorchTuneCheckpointer
+    checkpoint_dir: <your quantized model checkpoint dir>
+    checkpoint_files: [meta_model_0-8da4w.pt]
+    recipe_checkpoint: null
+    output_dir: <your quantized model checkpoint dir>
+    model_type: LLAMA3
+
+  ...
+
+  # EleutherAI specific eval args
+  tasks: ["hellaswag", "wikitext"]
+  limit: null
+  max_seq_length: 8192
+  batch_size: 8
+
+  quantizer:
+    _component_: torchtune.training.quantization.Int8DynActInt4WeightQuantizer
+    groupsize: 256
+
+.. note::
+
+  Since we are passing in a quantized model, be sure to use the corresponding
+  post-training quantizer instead of the QAT quantizer. For example, if you
+  used the :code:`Int8DynActInt4WeightQATQuantizer` during fine-tuning, you
+  should specify :code:`Int8DynActInt4WeightQuantizer` in this step. See the
+  `quantization recipe <https://github.com/pytorch/torchtune/blob/main/recipes/quantize.py>`_
+  for a full list of supported quantizers.
+
+Now run the evaluation recipe:
+
+.. code-block:: bash
+
+  tune run eleuther_eval --config my_eleuther_evaluation.yaml
+
+The results should look something like this:
+
+.. code-block:: bash
+
+  # QAT quantized model evaluation results (int8 activations + int4 weights)
+
+  |  Tasks  |Version|Filter|n-shot|    Metric     |Value |   |Stderr|
+  |---------|------:|------|-----:|---------------|-----:|---|------|
+  |wikitext |      2|none  |     0|word_perplexity|9.9148|  |N/A   |
+  |         |       |none  |     0|byte_perplexity|1.5357|  |N/A   |
+  |         |       |none  |     0|bits_per_byte  |0.6189|  |N/A   |
+  |hellaswag|      1|none  |     0|acc            |0.5687|  |0.0049|
+  |         |       |none  |     0|acc_norm       |0.7536|  |0.0043|
+
+Comparing these results to the model fine-tuned without QAT, we can see that
+QAT was able to recover a significant portion of the quantization degradations
+from the original unquantized model compared to PTQ. For example, normalized
+accuracy in the hellaswag task dropped by 2.20% with PTQ but only 0.74% with
+QAT when compared to the original unquantized model. Similarly, word perplexity
+in the wikitext task increased by 2.048 with PTQ but only 1.190 with QAT (lower
+is better).
+
+.. code-block:: bash
+
+  # PTQ quantized model evaluation results (int8 activations + int4 weights)
+
+  |  Tasks  |Version|Filter|n-shot|    Metric     | Value |   |Stderr|
+  |---------|------:|------|-----:|---------------|------:|---|------|
+  |wikitext |      2|none  |     0|word_perplexity|10.7735|  |N/A   |
+  |         |       |none  |     0|byte_perplexity| 1.5598|  |N/A   |
+  |         |       |none  |     0|bits_per_byte  | 0.6413|  |N/A   |
+  |hellaswag|      1|none  |     0|acc            | 0.5481|  |0.0050|
+  |         |       |none  |     0|acc_norm       | 0.7390|  |0.0044|
+
+.. code-block:: bash
+
+  # Float model evaluation results (bfloat16)
+
+  |  Tasks  |Version|Filter|n-shot|    Metric     |Value |   |Stderr|
+  |---------|------:|------|-----:|---------------|-----:|---|------|
+  |wikitext |      2|none  |     0|word_perplexity|8.7251|  |N/A   |
+  |         |       |none  |     0|byte_perplexity|1.4994|  |N/A   |
+  |         |       |none  |     0|bits_per_byte  |0.5844|  |N/A   |
+  |hellaswag|      1|none  |     0|acc            |0.5740|  |0.0049|
+  |         |       |none  |     0|acc_norm       |0.7610|  |0.0043|
+
+Thus, the QAT flow produced a quantized model that outperforms the post-training
+quantized model. Importantly, the quantized model structure is identical in both
+flows, and so the model size, memory usage, and all other performance
+characteristics are also the same.
+
+Note that although the weights are quantized to int4, the quantized model size
+for both the QAT and the PTQ flows are 8.187 GB, while the original float model
+is 14.958 GB. This is because this quantizer uses int8 to represent the weights
+as PyTorch does not have native int4 dtype support. A more efficient representation
+is to pack the int4 weights, which will halve the quantized model size. This is
+what the Int4WeightOnlyQuantizer does, and the corresponding QAT quantizer will
+be added in the future.
+
+Lowering QAT model to device (optional)
+---------------------------------------
+
+One important motivation for quantizing a model is to be able to run it in resource
+constrained environments. You can further lower your QAT Llama3 model to edge devices
+such as smartphones using `executorch <https://github.com/pytorch/executorch/>`_ by
+following `these instructions <https://github.com/pytorch/executorch/tree/main/examples/models/llama2>`_.
+For example, the following command lowers the model to the XNNPACK backend:
+
+.. code-block:: bash
+
+  python -m examples.models.llama2.export_llama --checkpoint <your QAT checkpoint> -p <params.json> -kv --use_sdpa_with_kv_cache -X -qmode 8da4w --group_size 256 -d fp32 --metadata '{"get_bos_id":128000, "get_eos_id":128001}' --embedding-quantize 4,32 --output_name="llama3_8da4w.pte"
+
+This results in a much smaller quantized model of size 3.881 GB. When benchmarked on a OnePlus 12 smartphone, this model also achieved the same inference and generation speeds as the post-training quantized model. This is because the model structures are the same across the two flows:
+
+.. list-table::
+   :widths: 25 25 25
+   :header-rows: 1
+
+   * -
+     - QAT
+     - PTQ
+   * - Quantized model size
+     - 3.881 GB
+     - 3.881 GB
+   * - Inference speed
+     - 9.709 tok/s
+     - 9.815 tok/s
+   * - Generation speed
+     - 11.316 tok/s
+     - 11.364 tok/s
diff -ruN marc_original/third_party/torchtune/docs/source/tutorials/qlora_finetune.rst marc/third_party/torchtune/docs/source/tutorials/qlora_finetune.rst
--- marc_original/third_party/torchtune/docs/source/tutorials/qlora_finetune.rst	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/docs/source/tutorials/qlora_finetune.rst	2025-02-20 17:49:29.102023509 -0500
@@ -0,0 +1,281 @@
+.. _qlora_finetune_label:
+
+=============================
+Fine-Tuning Llama2 with QLoRA
+=============================
+
+In this tutorial, we'll learn about `QLoRA <https://arxiv.org/abs/2305.14314>`_, an enhancement on top of
+`LoRA <https://arxiv.org/abs/2106.09685>`_ that maintains frozen model parameters in 4-bit quantized precision, thereby reducing memory usage. We'll
+walk through how QLoRA can be utilized within torchtune to finetune a Llama2-7b model in <10 GB of memory.
+It is highly recommended to first develop an understanding of :ref:`LoRA finetuning in torchtune<lora_finetune_label>`.
+
+
+.. grid:: 2
+
+    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn
+
+      * How QLoRA saves memory over LoRA finetuning
+      * An overview of QLoRA in torchtune
+      * How to run a QLoRA finetune in torchtune
+
+    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
+
+      * Be familiar with :ref:`torchtune<overview_label>`
+      * Make sure to :ref:`install torchtune<install_label>`
+      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`
+      * Be familiar with :ref:`LoRA in torchtune<lora_finetune_label>`
+
+What is QLoRA?
+---------------
+
+QLoRA builds on top of LoRA to enable further
+memory savings. In LoRA, model parameters can be thought of as existing in two partitions: adapters, which are
+low-rank matrices added to different layers of a neural network, and base model parameters, which are parameters that are part of
+the original model. In vanilla LoRA-style training, both these parameters are held in the same precision (typically fp32 or `bf16 <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format#bfloat16_floating-point_format>`_.), and
+therefore activations and intermediate gradients computed are in fp32/bf16.
+
+QLoRA further quantizes the base model parameters into a bespoke 4-bit NormalFloat (`NF4 <https://www.youtube.com/watch?v=TPcXVJ1VSRI&t=563s>`_) data type, resulting in 4-8x less parameter memory usage while
+largely retaining model accuracy. As a result, the vast majority of parameters only take up 4 bits (as opposed to 16 or 32 bits by bf16/fp32 dtypes). This
+quantization is done through the method highlighted in the original `QLoRA paper <https://arxiv.org/abs/2305.14314>`_. Adapter
+parameters are still held in the original precision, and activations, gradients, and optimizer states still exist in the higher precision to preserve
+accuracy.
+
+The QLoRA authors introduce two key abstractions to decrease memory usage and avoid accuracy degradation: the bespoke 4-bit NormatFloat
+type, and a double quantization method that quantizes the quantization parameters themselves to save even more memory. torchtune uses
+the `NF4Tensor <https://github.com/pytorch-labs/ao/blob/b9beaf351e27133d189b57d6fa725b1a7824a457/torchao/dtypes/nf4tensor.py#L153>`_ abstraction from the `torchao library <https://github.com/pytorch-labs/ao>`_ to build QLoRA components as specified in the paper.
+torchao is a PyTorch-native library that allows you to quantize and prune your models.
+
+
+.. _qlora_core_highlevel:
+
+Using QLoRA to save memory
+----------------------------------------
+
+In this section, we'll overview how to apply QLoRA to a :class:`~torchtune.modules.peft.LoRALinear` layer in torchtune. For a deep dive into details on QLoRA in torchtune and underlying abstractions,
+please see the :ref:`QLoRA in torchtune deepdive <qlora_deepdive_label>` section of this tutorial.
+
+A core idea of QLoRA is the distinction between compute and storage datatypes (dtypes). Specifically, QLoRA stores base model parameters in 4-bit precision (i.e. the storage dtype), and runs
+computation in an original higher precision (the compute dtype), generally either fp32 or bf16. As a first step, QLoRA needs to quantize these base model parameters to 4-bit precision
+and store them.
+
+To quantize a :class:`~torchtune.modules.peft.LoRALinear` layer in the QLoRA style, simply pass in the ``quantize_base`` flag as ``True`` into :class:`~torchtune.modules.peft.LoRALinear`. This flag
+will result in base model weights being quantized and backed by the ``NF4Tensor`` dtype. Forward passes will also be automatically handled to work with the ``NF4Tensor`` dtype,
+specifically, the ``NF4`` base weight will be de-quantized to the compute precision, activation will be computed, and only the 4-bit parameter will be stored for gradient computation
+in the backward pass, avoiding extra memory usage that would be incurred by storing the higher precision compute dtype.
+
+Here's an example of creating a quantized ``LoRALinear`` layer in comparison to an unquantized ``LoRALinear`` layer. As we can see, the quantized layer consumes
+~8x less memory than the unquantized counterpart.
+
+.. code-block:: python
+
+  import torch
+  from torchtune.modules.peft import LoRALinear
+
+  torch.set_default_device("cuda")
+  qlora_linear = LoRALinear(512, 512, rank=8, alpha=0.1, quantize_base=True)
+  print(torch.cuda.memory_allocated())  # 177,152 bytes
+  del qlora_linear
+  torch.cuda.empty_cache()
+  lora_linear = LoRALinear(512, 512, rank=8, alpha=0.1, quantize_base=False)
+  print(torch.cuda.memory_allocated()) # 1,081,344 bytes
+
+
+Using QLoRA in torchtune
+----------------------------
+
+We'll now cover how you can initialize a QLoRA-enabled Llama2-7b model as well as some details around
+checkpointing with QLoRA.
+
+With torchtune, you can use a simple builder similar to the LoRA builder (:func:`lora_llama_2_7b <torchtune.models.llama2.lora_llama2_7b>`) to apply QLoRA to Llama2 models. Here's a simple example of
+initializing a Llama2-7b model with QLoRA enabled:
+
+.. code-block:: python
+
+  from torchtune.models.llama2 import qlora_llama2_7b
+
+  qlora_model = qlora_llama2_7b(lora_attn_modules=["q_proj", "v_proj"])
+
+Under the hood, this will apply LoRA to the ``q_proj`` and ``v_proj`` matrices in all attention layers, and further quantize the base parameters
+in these matrices to the ``NF4`` dtype. Note that quantization of base model parameters is only applied to layers that are configured to have
+LoRA adapters added. For example, in this case, ``k_proj`` and ``output_proj`` in the attention layers don't have LoRA applied to them, so their
+base model parameters are not quantized. We can see this by printing the base model parameter dtypes for a particular attention layer:
+
+.. code-block:: python
+
+  attn = qlora_model.layers[0].attn
+  print(type(attn.q_proj.weight))  # <class 'torchao.dtypes.nf4tensor.NF4Tensor'>
+  print(type(attn.k_proj.weight))  # <class 'torch.nn.parameter.Parameter'>
+
+
+Next, there are a couple of details essential to checkpointing (i.e. ``state_dict``) of QLoRA-enabled models.
+To integrate well with torchtune's :ref:`checkpointing <checkpointing_label>`, we need to convert ``NF4Tensors`` back to their
+original precision (generally fp32/bf16). This allows QLoRA-trained checkpoints to interoperate well with the rest of the ecosystem, within
+torchtune and beyond (e.g. post-training quantization, evaluation, inference). This conversion process also allows LoRA adapter weights to be merged back into the base model as done
+in a typical LoRA training flow.
+
+To achieve this, when using torchtune's :func:`lora_llama_2_7b <torchtune.models.llama2.lora_llama2_7b>` builder, we automatically register a hook,
+:func:`reparametrize_as_dtype_state_dict_post_hook <torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook>`,
+that runs after calling ``.state_dict()`` on the top level model. This hook converts ``NF4Tensors`` back to their original precision, while also offloading these
+converted tensors to the CPU. This offloading is to avoid peaking memory; if we did not, we would have to maintain an entire bf16/fp32 copy of the ``state_dict``
+on GPU.
+
+
+.. _qlora_compile_label:
+
+Putting it all together: QLoRA finetune
+-----------------------------------------
+
+Putting it all together, we can now finetune a model using torchtune's :ref:`LoRA single-device finetuning <lora_finetune_recipe_label>` recipe,
+with a `QLoRA configuration <https://github.com/pytorch/torchtune/blob/main/recipes/configs/llama2/7B_qlora_single_device.yaml>`_.
+
+Make sure that you have first downloaded the Llama2 weights and tokenizer by following :ref:`these instructions<download_llama_label>`.
+You can then run the following command to perform a QLoRA finetune of Llama2-7B on a single GPU.
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device --config llama2/7B_qlora_single_device
+
+.. note::
+    Make sure to correctly point to the location of your Llama2 weights and tokenizer. This can be done
+    either by adding :code:`checkpointer.checkpoint_files=[my_model_checkpoint_path] tokenizer_checkpoint=my_tokenizer_checkpoint_path`
+    or by directly modifying the :code:`7B_qlora_single_device.yaml` file. See our ":ref:`config_tutorial_label`" recipe
+    for more details on how you can easily clone and modify torchtune configs.
+
+By default, this run should log peak memory stats at model initialization time and every 100
+iterations during training. Let's understand the memory savings enabled by QLoRA on top of LoRA training. LoRA training
+can be run as follows:
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device --config llama2/7B_lora_single_device
+
+You should see the memory usage printed out during model initialization and training. An example log for LoRA model initialization is as follows:
+
+.. code-block:: python
+
+  Memory Stats after model init::
+  GPU peak memory allocation: 13.96 GB
+  GPU peak memory reserved: 13.98 GB
+  GPU peak memory active: 13.96 GB
+
+The following table compares the QLoRA's memory reserved during model initialization and training against vanilla LoRA's.
+We can see that QLoRA reduces peak memory by about 35% during model initialization, and about 40% during model training:
+
+==================  ==================================  ================================
+Finetuning method    Peak memory reserved, model init    Peak memory reserved, training
+==================  ==================================  ================================
+LoRA                   13.98 GB                            15.57 GB
+QLoRA                  9.13 GB                             9.29 GB
+==================  ==================================  ================================
+
+From the logs, one can see that the out-of-the-box training performance is quite slow, slower than 1 iteration per
+second:
+
+.. code-block:: python
+
+  1|149|Loss: 0.9157477021217346:   1%|          | 149/25880 [02:08<6:14:19,  1.15it/s
+
+To speed things up, we can leverage `torch.compile <https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_ to compile our model and run the compiled result. To work with
+QLoRA training, a nightly build of PyTorch must be used. To update PyTorch to the latest nightly,
+please see `the installation instructions <https://pytorch.org/get-started/locally/>`_. Once updated,
+you can specify the compile flag as ``True`` via a config override:
+
+.. code-block:: bash
+
+    tune run lora_finetune_single_device --config llama2/7B_qlora_single_device compile=True
+
+From the logs, we can see about a 200% speed up (after a few hundred iterations once the training has stabilized):
+
+.. code-block:: python
+
+  1|228|Loss: 0.8158286809921265:   1%|          | 228/25880 [11:59<1:48:16,  3.95it/s
+
+A comparison of the smoothed loss curves between QLoRA and LoRA can be seen below.
+
+.. image:: /_static/img/qlora_exp.png
+
+.. note::
+    The above figure was generated with W&B. You can use torchtune's :class:`~torchtune.training.metric_logging.WandBLogger`
+    to generate similar loss curves, but you will need to install W&B and setup an account separately. For more details on
+    using W&B in torchtune, see our ":ref:`wandb_logging`" recipe.
+
+As an exercise, you can also try running some evaluation tasks or manually inspecting generations
+output by your saved checkpoints (which can be found in :code:`output_dir`).
+
+In the final section, we'll go over a deep dive on how a QLoRA component can be built from a LoRA component.
+
+.. _qlora_deepdive_label:
+
+Deep-dive: Building QLoRA from LoRA
+-----------------------------------------
+
+This deep-dive section resumes from the :ref:`Using QLoRA to save memory<qlora_core_highlevel>` portion of this tutorial and dives into how quantization is done with ``NF4Tensor`` and handled appropriately in the forward pass.
+
+First, we'll begin with
+a vanilla minimal LoRA layer, taken from :ref:`the LoRA tutorial <lora_finetune_label>` and augmented to support quantization:
+
+.. code-block:: python
+  :emphasize-lines: 3, 13, 19, 20, 39, 40, 41
+
+  import torch
+  from torch import nn
+  import torch.nn.functional as F
+  from torchao.dtypes.nf4tensor import linear_nf4, to_nf4
+
+  class LoRALinear(nn.Module):
+    def __init__(
+      self,
+      in_dim: int,
+      out_dim: int,
+      rank: int,
+      alpha: float,
+      dropout: float,
+      quantize_base: bool
+    ):
+      # These are the weights from the original pretrained model
+      self.linear = nn.Linear(in_dim, out_dim, bias=False)
+      self.linear_weight = self.linear.weight
+      # Use torchao's to_nf4 API to quantize the base weight if needed.
+      if quantize_base:
+        self.linear_weight = to_nf4(self.linear_weight)
+      # These are the new LoRA params. In general rank << in_dim, out_dim
+      self.lora_a = nn.Linear(in_dim, rank, bias=False)
+      self.lora_b = nn.Linear(rank, out_dim, bias=False)
+
+      # Rank and alpha are commonly-tuned hyperparameters
+      self.rank = rank
+      self.alpha = alpha
+
+      # Most implementations also include some dropout
+      self.dropout = nn.Dropout(p=dropout)
+
+      # The original params are frozen, and only LoRA params are trainable.
+      self.linear.weight.requires_grad = False
+      self.lora_a.weight.requires_grad = True
+      self.lora_b.weight.requires_grad = True
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+      # frozen_out would be the output of the original model
+      if quantize_base:
+        # Call into torchao's linear_nf4 to run linear forward pass w/quantized weight.
+        frozen_out  = linear_nf4(x, self.weight)
+      else:
+        frozen_out = F.linear(x, self.weight)
+
+      # lora_a projects inputs down to the much smaller self.rank,
+      # then lora_b projects back up to the output dimension
+      lora_out = self.lora_b(self.lora_a(self.dropout(x)))
+
+      # Finally, scale by the alpha parameter (normalized by rank)
+      # and add to the original model's outputs
+      return frozen_out + (self.alpha / self.rank) * lora_out
+
+As mentioned above, torchtune takes a dependency on torchao for some of the core components required for QLoRA. This includes the
+``NF4Tensor``, as well as helpful utilities including ``to_nf4`` and ``linear_nf4``.
+
+The key changes on top of the LoRA layer are the usage of the ``to_nf4`` and ``linear_nf4`` APIs.
+
+``to_nf4`` accepts an unquantized (bf16 or fp32) tensor and produces an ``NF4`` representation of the weight. See the `implementation <https://github.com/pytorch-labs/ao/blob/c40358072f99b50cd7e58ec11e0e8d90440e3e25/torchao/dtypes/nf4tensor.py#L587>`_ of ``to_nf4`` for more details.
+``linear_nf4`` handles the forward pass and autograd when running with quantized base model weights. It computes the forward pass as a regular
+``F.linear`` with the incoming activation and unquantized weight. The quantized weight is saved for backward, as opposed to the unquantized version of the weight, to avoid extra
+memory usage due to storing higher precision variables to compute gradients in the backward pass. See `linear_nf4 <https://github.com/pytorch-labs/ao/blob/main/torchao/dtypes/nf4tensor.py#L577>`_ for more details.
diff -ruN marc_original/third_party/torchtune/.flake8 marc/third_party/torchtune/.flake8
--- marc_original/third_party/torchtune/.flake8	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.flake8	2025-02-20 17:49:28.622022720 -0500
@@ -0,0 +1,28 @@
+[flake8]
+# Suggested config from pytorch that we can adapt
+select = B,C,E,F,N,P,T4,W,B9,TOR0,TOR1,TOR2
+max-line-length = 120
+# C408 ignored because we like the dict keyword argument syntax
+# E501 is not flexible enough, we're using B950 instead
+# N812 ignored because import torch.nn.functional as F is PyTorch convention
+# N817 ignored because importing using acronyms is convention (DistributedDataParallel as DDP)
+# E731 allow usage of assigning lambda expressions
+ignore =
+    E203,E305,E402,E501,E721,E741,F405,F821,F841,F999,W503,W504,C408,E302,W291,E303,N812,N817,E731
+    # shebang has extra meaning in fbcode lints, so I think it's not worth trying
+    # to line this up with executable bit
+    EXE001,
+    # these ignores are from flake8-bugbear; please fix!
+    B007,B008,
+optional-ascii-coding = True
+exclude =
+    ./.git,
+    ./docs
+    ./build
+    ./scripts,
+    ./venv,
+    *.pyi
+    .pre-commit-config.yaml
+    *.md
+    .flake8
+    tests/torchtune/models/llama2/scripts/*.py
diff -ruN marc_original/third_party/torchtune/.git marc/third_party/torchtune/.git
--- marc_original/third_party/torchtune/.git	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.git	2025-02-20 17:49:28.626022727 -0500
@@ -0,0 +1 @@
+gitdir: ../../.git/modules/third_party/torchtune
diff -ruN marc_original/third_party/torchtune/.github/PULL_REQUEST_TEMPLATE.md marc/third_party/torchtune/.github/PULL_REQUEST_TEMPLATE.md
--- marc_original/third_party/torchtune/.github/PULL_REQUEST_TEMPLATE.md	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.github/PULL_REQUEST_TEMPLATE.md	2025-02-20 17:49:28.630022733 -0500
@@ -0,0 +1,31 @@
+#### Context
+What is the purpose of this PR? Is it to
+- [ ] add a new feature
+- [ ] fix a bug
+- [ ] update tests and/or documentation
+- [ ] other (please add here)
+
+Please link to any issues this PR addresses.
+
+#### Changelog
+What are the changes made in this PR?
+*
+
+#### Test plan
+Please make sure to do each of the following if applicable to your PR. If you're unsure about any one of these just ask and we will happily help. We also have a [contributing page](https://github.com/pytorch/torchtune/blob/main/CONTRIBUTING.md) for some guidance on contributing.
+
+- [ ] run pre-commit hooks and linters (make sure you've first installed via `pre-commit install`)
+- [ ] add [unit tests](https://github.com/pytorch/torchtune/tree/main/tests/torchtune) for any new functionality
+- [ ] update [docstrings](https://github.com/pytorch/torchtune/tree/main/docs/source) for any new or updated methods or classes
+- [ ] run unit tests via `pytest tests`
+- [ ] run recipe tests via `pytest tests -m integration_test`
+- [ ] manually run any new or modified recipes with sufficient proof of correctness
+- [ ] include relevant commands and any other artifacts in this summary (pastes of loss curves, eval results, etc.)
+
+#### UX
+If your function changed a public API, please add a dummy example of what the user experience will look like when calling it.
+Here is a [docstring example](https://github.com/pytorch/torchtune/blob/6a7951f1cdd0b56a9746ef5935106989415f50e3/torchtune/modules/vision_transformer.py#L285)
+and a [tutorial example](https://pytorch.org/torchtune/main/tutorials/qat_finetune.html#applying-qat-to-llama3-models)
+
+- [ ] I did not change any public API
+- [ ] I have added an example to docs or docstrings
diff -ruN marc_original/third_party/torchtune/.github/scripts/pre_build_script.sh marc/third_party/torchtune/.github/scripts/pre_build_script.sh
--- marc_original/third_party/torchtune/.github/scripts/pre_build_script.sh	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.github/scripts/pre_build_script.sh	2025-02-20 17:49:28.638022746 -0500
@@ -0,0 +1,10 @@
+#!/bin/bash
+
+# Update __version__ in __init__.py using BUILD_VERSION environment variable
+python_file="torchtune/__init__.py"
+if [[ -n "$BUILD_VERSION" ]]; then
+    sed "s/^__version__ = .*/__version__ = '$BUILD_VERSION'/" "$python_file" > tmp && mv tmp "$python_file"
+else
+    echo "Error: BUILD_VERSION environment variable is not set or empty."
+    exit 1
+fi
diff -ruN marc_original/third_party/torchtune/.github/workflows/build_docs.yaml marc/third_party/torchtune/.github/workflows/build_docs.yaml
--- marc_original/third_party/torchtune/.github/workflows/build_docs.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.github/workflows/build_docs.yaml	2025-02-20 17:49:28.646022760 -0500
@@ -0,0 +1,131 @@
+name: Build Docs
+
+on:
+  push:
+    branches:
+      - main
+      - release/*
+    tags:
+      - v[0-9]+.[0-9]+.[0-9]
+      - v[0-9]+.[0-9]+.[0-9]+-rc[0-9]+
+  pull_request:
+  workflow_dispatch:
+
+concurrency:
+  group: build-docs-${{ github.workflow }}-${{ github.ref == 'refs/heads/main' && github.run_number || github.ref }}
+  cancel-in-progress: true
+
+defaults:
+  run:
+    shell: bash -l -eo pipefail {0}
+
+jobs:
+  build_docs:
+    if: github.repository_owner == 'pytorch'
+    runs-on: ubuntu-latest
+    strategy:
+      matrix:
+        python-version: ['3.11']
+    steps:
+      - name: Check out repo
+        uses: actions/checkout@v3
+      - name: Setup conda env
+        uses: conda-incubator/setup-miniconda@v2
+        with:
+          auto-update-conda: true
+          miniconda-version: "latest"
+          activate-environment: test
+          python-version: ${{ matrix.python-version }}
+      - name: Update pip
+        run: python -m pip install --upgrade pip
+      - name: Install dependencies
+        run: |
+          python -m pip install torch torchvision torchao
+          python -m pip install -e .
+          cd docs
+          python -m pip install -r requirements.txt
+      - name: Build docs
+        env:
+            TORCHTUNE_VERSION_DOCS: ${{ github.ref }}
+        run: |
+          cd docs
+          make html
+      - uses: actions/upload-artifact@v3
+        with:
+          name: Built-Docs
+          path: docs/build/html/
+
+  doc-preview:
+    runs-on: [self-hosted, linux.2xlarge]
+    needs: build_docs
+    if: ${{ github.repository_owner == 'pytorch' && github.event_name == 'pull_request' }}
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v3
+      - name: Download artifact
+        uses: actions/download-artifact@v3
+        with:
+          name: Built-Docs
+          path: docs
+      - name: Add no-index tag
+        run: |
+          find docs -name "*.html" -print0 | xargs -0 sed -i '/<head>/a \ \ <meta name="robots" content="noindex">';
+      - name: Upload docs preview
+        uses: seemethere/upload-artifact-s3@v5
+        if: ${{ github.event_name == 'pull_request' }}
+        with:
+          retention-days: 14
+          s3-bucket: doc-previews
+          if-no-files-found: error
+          path: docs
+          s3-prefix: pytorch/torchtune/${{ github.event.pull_request.number }}
+
+  upload:
+    runs-on: ubuntu-latest
+    needs: build_docs
+    if: github.repository_owner == 'pytorch' && github.event_name == 'push' && (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v'))
+    environment: ${{ (github.ref == 'refs/heads/main' || startsWith(github.event.ref, 'refs/tags/v')) && 'docs-push' || '' }}
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v3
+        with:
+          ref: gh-pages
+          persist-credentials: false
+      - name: Download artifact
+        uses: actions/download-artifact@v3
+        with:
+          name: Built-Docs
+          path: docs
+      - name: Add no-index tag
+        run: |
+          REF_NAME=$(echo "${{ github.ref }}")
+          echo "Ref name: ${REF_NAME}"
+          if [[ "${{ github.ref }}" == 'refs/heads/main' ]]; then
+            find docs -name "*.html" -print0 | xargs -0 sed -i '/<head>/a \ \ <meta name="robots" content="noindex">';
+          fi
+      - name: Move and commit changes
+        env:
+          GITHUB_PYTORCHBOT_TOKEN: ${{ secrets.GH_PYTORCHBOT_TOKEN }}
+        run: |
+          git remote set-url origin https://pytorchbot:${GITHUB_PYTORCHBOT_TOKEN}@github.com/pytorch/torchtune.git
+          set -euo pipefail
+
+          # Convert refs/tags/v1.12.0rc3 into 1.12.
+          # Adopted from https://github.com/pytorch/pytorch/blob/main/.github/workflows/_docs.yml#L150C11-L155C13
+          GITHUB_REF=${{ github.ref }}
+          if [[ "${GITHUB_REF}" =~ ^refs/tags/v([0-9]+\.[0-9]+)\.* ]]; then
+            TARGET_FOLDER="${BASH_REMATCH[1]}"
+          else
+            TARGET_FOLDER="main"
+          fi
+
+          echo "Target Folder: ${TARGET_FOLDER}"
+          mkdir -p "${TARGET_FOLDER}"
+          rm -rf "${TARGET_FOLDER}"/*
+          mv docs/* "${TARGET_FOLDER}"
+
+          git config user.name 'pytorchbot'
+          git config user.email 'soumith+bot@pytorch.org'
+          git add "${TARGET_FOLDER}" || true
+          git commit -m "auto-generating sphinx docs" || true
+          git push -f
diff -ruN marc_original/third_party/torchtune/.github/workflows/build_linux_wheels.yaml marc/third_party/torchtune/.github/workflows/build_linux_wheels.yaml
--- marc_original/third_party/torchtune/.github/workflows/build_linux_wheels.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.github/workflows/build_linux_wheels.yaml	2025-02-20 17:49:28.658022779 -0500
@@ -0,0 +1,46 @@
+name: Build Linux Wheels
+
+on:
+  push:
+    branches:
+      - nightly
+      - release/*
+    tags:
+      # NOTE: Binary build pipelines should only get triggered on release candidate builds
+      # Release candidate tags look like: v1.11.0-rc1
+      - v[0-9]+.[0-9]+.[0-9]+-rc[0-9]+
+  workflow_dispatch:
+
+permissions:
+  id-token: write
+  contents: read
+
+jobs:
+  generate-matrix:
+    if: github.repository_owner == 'pytorch'
+    uses: pytorch/test-infra/.github/workflows/generate_binary_build_matrix.yml@main
+    with:
+      package-type: wheel
+      os: linux
+      test-infra-repository: pytorch/test-infra
+      test-infra-ref: main
+      with-cuda: enable
+      with-rocm: enable
+      build-python-only: enable
+  build:
+    needs: generate-matrix
+    name: ${{ matrix.repository }}
+    uses: pytorch/test-infra/.github/workflows/build_wheels_linux.yml@main
+    strategy:
+      fail-fast: false
+    with:
+      repository: pytorch/torchtune
+      ref: ""
+      package-name: torchtune
+      build-matrix: ${{ needs.generate-matrix.outputs.matrix }}
+      pre-script: .github/scripts/pre_build_script.sh
+      trigger-event: ${{ github.event_name }}
+      build-platform: 'python-build-package'
+      pip-install-torch-extra-args:
+        torchvision
+        torchao
diff -ruN marc_original/third_party/torchtune/.github/workflows/gpu_test.yaml marc/third_party/torchtune/.github/workflows/gpu_test.yaml
--- marc_original/third_party/torchtune/.github/workflows/gpu_test.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.github/workflows/gpu_test.yaml	2025-02-20 17:49:28.682022819 -0500
@@ -0,0 +1,60 @@
+name: GPU tests
+
+on:
+  schedule:
+    # Runs at midnight every day
+    - cron:  '0 0 * * *'
+  push:
+    branches: [ main ]
+  pull_request:
+  workflow_dispatch:
+
+concurrency:
+  group: gpu-test-${{ github.workflow }}-${{ github.ref == 'refs/heads/main' && github.run_number || github.ref }}
+  cancel-in-progress: true
+
+permissions:
+  id-token: write
+  contents: read
+
+defaults:
+  run:
+    shell: bash -l -eo pipefail {0}
+
+jobs:
+  gpu_test:
+    if: github.repository_owner == 'pytorch'
+    runs-on: linux.8xlarge.nvidia.gpu
+    strategy:
+      matrix:
+        python-version: ['3.9', '3.10', '3.11']
+        torch-version: ["stable", "nightly"]
+        # Do not run against nightlies on PR
+        exclude:
+          - torch-version: ${{ github.event_name == 'pull_request' && 'nightly' }}
+    steps:
+      - name: Check out repo
+        uses: actions/checkout@v3
+      - name: Setup conda env
+        uses: conda-incubator/setup-miniconda@v2
+        with:
+          auto-update-conda: true
+          miniconda-version: "latest"
+          activate-environment: test
+          python-version: ${{ matrix.python-version }}
+      - name: Update pip
+        run: python -m pip install --upgrade pip
+      - name: Install torch nightly
+        if: ${{ matrix.torch-version == 'nightly' }}
+        run: python -m pip install --pre torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/cu121
+      - name: Install torch stable
+        if: ${{ matrix.torch-version == 'stable' }}
+        run: python -m pip install torch torchvision torchao
+      - name: Install remaining dependencies
+        run: |
+          python -m pip install -e ".[dev]"
+          python -m pip install lm-eval==0.4.5
+      - name: Run recipe and unit tests with coverage
+        run: pytest tests --with-integration --cov=. --cov-report=xml --durations=20 -vv
+      - name: Upload Coverage to Codecov
+        uses: codecov/codecov-action@v3
diff -ruN marc_original/third_party/torchtune/.github/workflows/lint.yaml marc/third_party/torchtune/.github/workflows/lint.yaml
--- marc_original/third_party/torchtune/.github/workflows/lint.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.github/workflows/lint.yaml	2025-02-20 17:49:28.698022845 -0500
@@ -0,0 +1,40 @@
+name: Lint
+
+on:
+  pull_request:
+  workflow_dispatch:
+
+
+concurrency:
+  group: lint-${{ github.workflow }}-${{ github.ref == 'refs/heads/main' && github.run_number || github.ref }}
+  cancel-in-progress: true
+
+defaults:
+  run:
+    shell: bash -l -eo pipefail {0}
+
+jobs:
+  lint:
+    if: github.repository_owner == 'pytorch'
+    runs-on: ubuntu-latest
+    strategy:
+      matrix:
+        python-version: ['3.10']
+    steps:
+      - name: Check out repo
+        uses: actions/checkout@v3
+      - name: Setup python
+        uses: actions/setup-python@v4
+        with:
+          python-version: ${{ matrix.python-version }}
+      - name: Update pip
+        run: python -m pip install --upgrade pip
+      - name: Install lint utilities
+        run: |
+          python -m pip install pre-commit
+          pre-commit install-hooks
+      - name: Get changed files
+        id: changed-files
+        uses: tj-actions/changed-files@v41.0.0
+      - name: Lint modified files
+        run: pre-commit run --files ${{ steps.changed-files.outputs.all_changed_files }}
diff -ruN marc_original/third_party/torchtune/.github/workflows/recipe_test.yaml marc/third_party/torchtune/.github/workflows/recipe_test.yaml
--- marc_original/third_party/torchtune/.github/workflows/recipe_test.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.github/workflows/recipe_test.yaml	2025-02-20 17:49:28.702022852 -0500
@@ -0,0 +1,49 @@
+name: Recipe Tests
+
+on:
+  push:
+    branches: [ main ]
+  pull_request:
+  workflow_dispatch:
+
+
+concurrency:
+  group: recipe-test-${{ github.workflow }}-${{ github.ref == 'refs/heads/main' && github.run_number || github.ref }}
+  cancel-in-progress: true
+
+permissions:
+  id-token: write
+  contents: read
+
+defaults:
+  run:
+    shell: bash -l -eo pipefail {0}
+
+jobs:
+  recipe_test:
+    if: github.repository_owner == 'pytorch'
+    runs-on: ubuntu-latest
+    strategy:
+      matrix:
+        python-version: ['3.9', '3.10', '3.11']
+    steps:
+      - name: Check out repo
+        uses: actions/checkout@v3
+      - name: Setup conda env
+        uses: conda-incubator/setup-miniconda@v2
+        with:
+          auto-update-conda: true
+          miniconda-version: "latest"
+          activate-environment: test
+          python-version: ${{ matrix.python-version }}
+      - name: Update pip
+        run: python -m pip install --upgrade pip
+      - name: Install dependencies
+        run: |
+          python -m pip install torch torchvision torchao
+          python -m pip install -e ".[dev]"
+          python -m pip install lm-eval==0.4.5
+      - name: Run recipe tests with coverage
+        run: pytest tests -m integration_test --cov=. --cov-report=xml --durations=20 -vv
+      - name: Upload Coverage to Codecov
+        uses: codecov/codecov-action@v3
diff -ruN marc_original/third_party/torchtune/.github/workflows/regression_test.yaml marc/third_party/torchtune/.github/workflows/regression_test.yaml
--- marc_original/third_party/torchtune/.github/workflows/regression_test.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.github/workflows/regression_test.yaml	2025-02-20 17:49:28.702022852 -0500
@@ -0,0 +1,63 @@
+name: Regression Tests
+
+on:
+  schedule:
+    # Runs at midnight every day
+    - cron:  '0 0 * * *'
+
+concurrency:
+  group: regression-test-${{ github.workflow }}-${{ github.ref == 'refs/heads/main' && github.run_number || github.ref }}
+  cancel-in-progress: true
+
+permissions:
+  id-token: write
+  contents: read
+
+defaults:
+  run:
+    shell: bash -l -eo pipefail {0}
+
+jobs:
+  regression_test:
+    if: github.repository_owner == 'pytorch'
+    runs-on: linux.g5.12xlarge.nvidia.gpu
+    strategy:
+      matrix:
+        python-version: ['3.11']
+        torch-version: ["stable", "nightly"]
+      fail-fast: false
+    steps:
+      - name: Check out repo
+        uses: actions/checkout@v3
+      - name: Setup conda env
+        uses: conda-incubator/setup-miniconda@v2
+        with:
+          auto-update-conda: true
+          miniconda-version: "latest"
+          activate-environment: test
+          python-version: ${{ matrix.python-version }}
+      - name: Update pip
+        run: python -m pip install --upgrade pip
+      - name: configure aws credentials
+        id: aws_creds
+        uses: aws-actions/configure-aws-credentials@v1.7.0
+        with:
+          role-to-assume: arn:aws:iam::308535385114:role/gha_workflow_torchtune_pytorch-multimodal
+          aws-region: us-east-1
+      - name: Install S3 CLI
+        run: |
+          python3 -m pip install awscli==1.32.6
+      - name: Install torch nightly
+        if: ${{ matrix.torch-version == 'nightly' }}
+        run: python -m pip install --pre torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/cu118
+      - name: Install torch stable
+        if: ${{ matrix.torch-version == 'stable' }}
+        run: python -m pip install torch torchvision torchao
+      - name: Install remaining dependencies
+        run: |
+          python -m pip install -e ".[dev]"
+          python -m pip install lm-eval==0.4.5
+      - name: Run regression tests with coverage
+        run: pytest tests -m slow_integration_test --silence-s3-logs --cov=. --cov-report=xml --durations=20 -vv
+      - name: Upload Coverage to Codecov
+        uses: codecov/codecov-action@v3
diff -ruN marc_original/third_party/torchtune/.github/workflows/unit_test.yaml marc/third_party/torchtune/.github/workflows/unit_test.yaml
--- marc_original/third_party/torchtune/.github/workflows/unit_test.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.github/workflows/unit_test.yaml	2025-02-20 17:49:28.706022858 -0500
@@ -0,0 +1,42 @@
+name: Unit Test
+
+on:
+  push:
+    branches: [ main ]
+  pull_request:
+
+concurrency:
+  group: unit-test${{ github.workflow }}-${{ github.ref == 'refs/heads/main' && github.run_number || github.ref }}
+  cancel-in-progress: true
+
+defaults:
+  run:
+    shell: bash -l -eo pipefail {0}
+
+jobs:
+  unit_tests:
+    if: github.repository_owner == 'pytorch'
+    runs-on: ubuntu-latest
+    strategy:
+      matrix:
+        python-version: ['3.9', '3.10', '3.11']
+    steps:
+      - name: Check out repo
+        uses: actions/checkout@v3
+      - name: Setup conda env
+        uses: conda-incubator/setup-miniconda@v2
+        with:
+          auto-update-conda: true
+          miniconda-version: "latest"
+          activate-environment: test
+          python-version: ${{ matrix.python-version }}
+      - name: Update pip
+        run: python -m pip install --upgrade pip
+      - name: Install dependencies
+        run: |
+          python -m pip install torch torchvision torchao
+          python -m pip install -e ".[dev]"
+      - name: Run unit tests with coverage
+        run: pytest tests --cov=. --cov-report=xml --durations=20 -vv
+      - name: Upload Coverage to Codecov
+        uses: codecov/codecov-action@v3
diff -ruN marc_original/third_party/torchtune/.gitignore marc/third_party/torchtune/.gitignore
--- marc_original/third_party/torchtune/.gitignore	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.gitignore	2025-02-20 17:49:28.710022865 -0500
@@ -0,0 +1,189 @@
+# Derived from basic .gitignore template for python projects:
+#   https://github.com/github/gitignore/blob/main/Python.gitignore
+# Please maintain the alphabetic order of the section titles
+# To debug why a file is being ignored, use the command:
+#    git check-ignore -v $my_ignored_file
+
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# Cython debug symbols
+cython_debug/
+
+# Celery stuff
+celerybeat-schedule
+celerybeat.pid
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+share/python-wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+
+# Django stuff
+*.log
+local_settings.py
+db.sqlite3
+db.sqlite3-journal
+
+# Environments
+.env
+.venv
+env/
+venv/
+ENV/
+env.bak/
+venv.bak/
+
+# Flask stuff
+instance/
+.webassets-cache
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# IPython
+profile_default/
+ipython_config.py
+
+# Jupyter Notebook
+*.ipynb_checkpoints
+
+# mkdocs documentation
+/site
+
+# Model saving / checkpointing
+*.pt
+*.pth
+*.ckpt
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# PyBuilder
+.pybuilder/
+target/
+
+# PyCharm
+#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
+#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
+#  and can be added to the global gitignore or merged into this file.  For a more nuclear
+#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
+#.idea/
+
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# pyenv
+#   For a library or package, you might want to ignore these files since the code is
+#   intended to run in multiple environments; otherwise, check them in:
+# .python-version
+
+# pipenv
+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
+#   However, in case of collaboration, if having platform-specific dependencies or dependencies
+#   having no cross-platform support, pipenv may install dependencies that don't work, or not
+#   install all needed dependencies.
+# Pipfile.lock
+
+# poetry
+#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
+#   This is especially recommended for binary packages to ensure reproducibility, and is more
+#   commonly ignored for libraries.
+#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
+# poetry.lock
+
+# PEP 582: https://peps.python.org/pep-0582/
+#   This PEP proposes to add to Python a mechanism to automatically recognize a __pypackages__
+#   directory and prefer importing packages installed in this location over user or global site-packages.
+#   This will avoid the steps to create, activate or deactivate virtual environments. Python will use
+#   the __pypackages__ from the base directory of the script when present.
+__pypackages__/
+
+# Pyre type checker
+.pyre/
+
+# pytype static type analyzer
+.pytype/
+
+# Rope project settings
+.ropeproject
+
+# SageMath parsed files
+*.sage.py
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/build
+# sphinx-gallery
+docs/source/generated_examples/
+docs/source/gen_modules/
+docs/source/generated/
+docs/source/sg_execution_times.rst
+# pytorch-sphinx-theme gets installed here
+docs/src
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# System / program generated files
+*.err
+*.log
+*.swp
+.DS_Store
+
+# Translations
+*.mo
+*.pot
+
+# TorchX
+*.torchxconfig
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.nox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+*.py,cover
+.hypothesis/
+.pytest_cache/
+cover/
+
+# VSCode
+.vscode/
+
+# wandb
+wandb/
diff -ruN marc_original/third_party/torchtune/LICENSE marc/third_party/torchtune/LICENSE
--- marc_original/third_party/torchtune/LICENSE	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/LICENSE	2025-02-20 17:49:28.754022938 -0500
@@ -0,0 +1,28 @@
+BSD 3-Clause License
+
+Copyright 2024 Meta
+
+Redistribution and use in source and binary forms, with or without modification,
+are permitted provided that the following conditions are met:
+
+1. Redistributions of source code must retain the above copyright notice,this list
+of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright notice, this
+list of conditions and the following disclaimer in the documentation
+and/or other materials provided with the distribution.
+
+3. Neither the name of the copyright holder nor the names of its contributors may
+be used to endorse or promote products derived from this software without specific
+prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY
+EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT
+SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+DAMAGE.
diff -ruN marc_original/third_party/torchtune/MANIFEST.in marc/third_party/torchtune/MANIFEST.in
--- marc_original/third_party/torchtune/MANIFEST.in	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/MANIFEST.in	2025-02-20 17:49:28.762022950 -0500
@@ -0,0 +1 @@
+prune tests  # Remove all testing files from final dist/
diff -ruN marc_original/third_party/torchtune/.pre-commit-config.yaml marc/third_party/torchtune/.pre-commit-config.yaml
--- marc_original/third_party/torchtune/.pre-commit-config.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/.pre-commit-config.yaml	2025-02-20 17:49:28.734022904 -0500
@@ -0,0 +1,51 @@
+exclude: 'build'
+
+default_language_version:
+    python: python3
+
+repos:
+-   repo: https://github.com/pre-commit/pre-commit-hooks
+    rev: 6306a48f7dae5861702d573c9c247e4e9498e867
+    hooks:
+    -   id: trailing-whitespace
+    -   id: check-ast
+    -   id: check-merge-conflict
+    -   id: no-commit-to-branch
+        args: ['--branch=main']
+    -   id: check-added-large-files
+        args: ['--maxkb=1000']
+    -   id: end-of-file-fixer
+        exclude: '^(.*\.svg)$'
+
+-   repo: https://github.com/Lucas-C/pre-commit-hooks
+    rev: v1.5.4
+    hooks:
+    -   id: insert-license
+        files: \.py$|\.sh$
+        args:
+        - --license-filepath
+        - docs/license_header.txt
+
+-   repo: https://github.com/pycqa/flake8
+    rev: 34cbf8ef3950f43d09b85e2e45c15ae5717dc37b
+    hooks:
+    -   id: flake8
+        additional_dependencies:
+          - flake8-bugbear == 22.4.25
+          - pep8-naming == 0.12.1
+          - torchfix
+        args: ['--config=.flake8']
+
+-   repo: https://github.com/omnilib/ufmt
+    rev: v2.3.0
+    hooks:
+    -   id: ufmt
+        additional_dependencies:
+          - black == 22.12.0
+          - usort == 1.0.5
+
+- repo: https://github.com/jsh9/pydoclint
+  rev: 94efc5f989adbea30f3534b476b2931a02c1af90
+  hooks:
+    - id: pydoclint
+      args: [--config=pyproject.toml]
diff -ruN marc_original/third_party/torchtune/pyproject.toml marc/third_party/torchtune/pyproject.toml
--- marc_original/third_party/torchtune/pyproject.toml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/pyproject.toml	2025-02-20 17:49:29.106023516 -0500
@@ -0,0 +1,94 @@
+# ---- All project specifications ---- #
+[project]
+name = "torchtune"
+description = "A native-PyTorch library for LLM fine-tuning"
+readme = "README.md"
+requires-python = ">=3.9"
+license = {file = "LICENSE"}
+authors = [
+    { name = "PyTorch Team", email = "packages@pytorch.org" },
+]
+keywords = ["pytorch", "finetuning", "llm"]
+dependencies = [
+
+    # Hugging Face integrations
+    "datasets",
+    "huggingface_hub",
+    "safetensors",
+
+    # Tokenization
+    "sentencepiece",
+    "tiktoken",
+    "blobfile>=2",
+
+    # Miscellaneous
+    "numpy",
+    "tqdm",
+    "omegaconf",
+    "psutil",
+
+    # Multimodal
+    "Pillow>=9.4.0",
+
+]
+dynamic = ["version"]
+
+[project.urls]
+GitHub = "https://github.com/pytorch/torchtune"
+Documentation = "https://pytorch.org/torchtune/main/index.html"
+Issues = "https://github.com/pytorch/torchtune/issues"
+
+[project.scripts]
+tune = "torchtune._cli.tune:main"
+
+[project.optional-dependencies]
+dev = [
+    "bitsandbytes>=0.43.0",
+    "comet_ml>=3.44.2",
+    "pre-commit",
+    "pytest==7.4.0",
+    "pytest-cov",
+    "pytest-mock",
+    "pytest-integration",
+    "tensorboard",
+    # Pin urllib3 to avoid transient error from https://github.com/psf/requests/issues/6443
+    "urllib3<2.0.0",
+    "wandb",
+    "expecttest",
+]
+
+[tool.setuptools.dynamic]
+version = {attr = "torchtune.__version__"}
+
+
+# ---- Explicit project build information ---- #
+[build-system]
+requires = ["setuptools", "wheel"]
+build-backend = "setuptools.build_meta"
+
+[tool.setuptools.packages.find]
+where = [""]
+include = ["torchtune*", "recipes*"]
+
+[tool.setuptools.package-data]
+recipes = ["configs/*.yaml", "configs/*/*.yaml", "configs/*/*/*.yaml"]
+
+
+# ---- Tooling specifications ---- #
+[tool.usort]
+first_party_detection = false
+
+[tool.black]
+target-version = ["py38"]
+
+[tool.pydoclint]
+style = 'google'
+check-return-types = 'False'
+exclude = 'tests/torchtune/models/(\w+)/scripts/'
+
+[tool.pytest.ini_options]
+addopts = ["--showlocals", "--import-mode=prepend", "--without-integration", "--without-slow-integration"]
+# --showlocals will show local variables in tracebacks
+# --import-mode=prepend will add the root (the parent dir of torchtune/, tests/, recipes/)
+# to `sys.path` when invoking pytest, allowing us to treat `tests` as a package within the tests.
+# --without-integration and --without-slow-integration: default to running unit tests only
diff -ruN marc_original/third_party/torchtune/README.md marc/third_party/torchtune/README.md
--- marc_original/third_party/torchtune/README.md	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/README.md	2025-02-20 17:49:28.770022963 -0500
@@ -0,0 +1,300 @@
+
+
+
+# torchtune
+
+[![Unit Test](https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml/badge.svg?branch=main)](https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml)
+![Recipe Integration Test](https://github.com/pytorch/torchtune/actions/workflows/recipe_test.yaml/badge.svg)
+[![](https://dcbadge.vercel.app/api/server/4Xsdn8Rr9Q?style=flat)](https://discord.gg/4Xsdn8Rr9Q)
+
+[**Introduction**](#introduction) | [**Installation**](#installation) | [**Get Started**](#get-started) |  [**Documentation**](https://pytorch.org/torchtune/main/index.html) | [**Community**](#community) | [**License**](#license) | [**Citing torchtune**](#citing-torchtune)
+
+> [!IMPORTANT]
+> Update September 25, 2024: torchtune has support for **Llama 3.2 11B Vision**, **Llama 3.2 3B**, and **Llama 3.2 1B** models! Try them out by following our installation instructions [here](#Installation), then run any of the text configs [here](recipes/configs/llama3_2) or vision configs [here](recipes/configs/llama3_2_vision).
+
+
+&nbsp;
+
+## Introduction
+
+torchtune is a PyTorch library for easily authoring, finetuning and experimenting with LLMs.
+
+torchtune provides:
+
+- PyTorch implementations of popular LLMs from Llama, Gemma, Mistral, Phi, and Qwen model families
+- Hackable training recipes for full finetuning, LoRA, QLoRA, DPO, PPO, QAT, knowledge distillation, and more
+- Out-of-the-box memory efficiency, performance improvements, and scaling with the latest PyTorch APIs
+- YAML configs for easily configuring training, evaluation, quantization or inference recipes
+- Built-in support for many popular dataset formats and prompt templates
+
+
+&nbsp;
+
+### Models
+
+torchtune currently supports the following models.
+
+| Model                                         | Sizes     |
+|-----------------------------------------------|-----------|
+| [Llama3.2-Vision](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2#-llama-3.2-vision-models-(11b/90b)-)    | 11B [[models](torchtune/models/llama3_2_vision/_model_builders.py), [configs](recipes/configs/llama3_2_vision/)]        |
+| [Llama3.2](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2)    | 1B, 3B [[models](torchtune/models/llama3_2/_model_builders.py), [configs](recipes/configs/llama3_2/)]        |
+| [Llama3.1](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1)    | 8B, 70B, 405B [[models](torchtune/models/llama3_1/_model_builders.py), [configs](recipes/configs/llama3_1/)]        |
+| [Llama3](https://llama.meta.com/llama3)    | 8B, 70B [[models](torchtune/models/llama3/_model_builders.py), [configs](recipes/configs/llama3/)]        |
+| [Llama2](https://llama.meta.com/llama2/)   | 7B, 13B, 70B [[models](torchtune/models/llama2/_model_builders.py), [configs](recipes/configs/llama2/)]        |
+| [Code-Llama2](https://ai.meta.com/blog/code-llama-large-language-model-coding/)   | 7B, 13B, 70B [[models](torchtune/models/code_llama2/_model_builders.py), [configs](recipes/configs/code_llama2/)] |
+| [Mistral](https://huggingface.co/mistralai)   | 7B [[models](torchtune/models/mistral/_model_builders.py), [configs](recipes/configs/mistral/)] |
+| [Gemma](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b)   | 2B, 7B [[models](torchtune/models/gemma/_model_builders.py), [configs](recipes/configs/gemma/)] |
+| [Microsoft Phi3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3) | Mini [[models](torchtune/models/phi3/), [configs](recipes/configs/phi3/)]
+| [Qwen2](https://qwenlm.github.io/blog/qwen2/) | 0.5B, 1.5B, 7B [[models](torchtune/models/qwen2/), [configs](recipes/configs/qwen2/)]
+
+We're always adding new models, but feel free to [file an issue](https://github.com/pytorch/torchtune/issues/new) if there's a new one you would like to see in torchtune.
+
+&nbsp;
+
+### Finetuning recipes
+
+torchtune provides the following finetuning recipes for training on one or more devices.
+
+
+| Finetuning Method                          | Devices | Recipe  | Example Config(s) |
+|:-:|:-:|:-:|:-:|
+| Full Finetuning  | 1-8 | [full_finetune_single_device](recipes/full_finetune_single_device.py) <br> [full_finetune_distributed](recipes/full_finetune_distributed.py)| [Llama3.1 8B single-device](recipes/configs/llama3_1/8B_full_single_device.yaml) <br> [Llama 3.1 70B distributed](recipes/configs/llama3_1/70B_full.yaml)
+| LoRA Finetuning | 1-8  | [lora_finetune_single_device](recipes/lora_finetune_single_device.py) <br> [lora_finetune_distributed](recipes/lora_finetune_distributed.py) | [Qwen2 0.5B single-device](recipes/configs/qwen2/0.5B_lora_single_device.yaml) <br> [Gemma 7B distributed](recipes/configs/gemma/7B_lora.yaml)
+| QLoRA Finetuning | 1-8 | [lora_finetune_single_device](recipes/lora_finetune_single_device.py) <br> [lora_finetune_distributed](recipes/lora_finetune_distributed.py)| [Phi3 Mini single-device](recipes/configs/phi3/mini_qlora_single_device.yaml) <br> [Llama 3.1 405B distributed](recipes/configs/llama3_1/405B_qlora.yaml)
+| DoRA/QDoRA Finetuning | 1-8 | [lora_finetune_single_device](recipes/lora_finetune_single_device.py) <br> [lora_finetune_distributed](recipes/lora_finetune_distributed.py)| [Llama3 8B QDoRA single-device](recipes/configs/llama3/8B_qdora_single_device.yaml) <br> [Llama3 8B DoRA distributed](recipes/configs/llama3/8B_dora.yaml)
+| Quantization-Aware Training | 4-8 | [qat_distributed](recipes/qat_distributed.py)| [Llama3 8B QAT](recipes/configs/llama3/8B_qat_full.yaml)
+| Direct Preference Optimization |1-8 | [lora_dpo_single_device](recipes/lora_dpo_single_device.py) <br> [lora_dpo_distributed](recipes/lora_dpo_distributed.py) | [Llama2 7B single-device](recipes/configs/llama2/7B_lora_dpo_single_device.yaml) <br> [Llama2 7B distributed](recipes/configs/llama2/7B_lora_dpo.yaml)
+| Proximal Policy Optimization | 1 |  [ppo_full_finetune_single_device](recipes/ppo_full_finetune_single_device.py) | [Mistral 7B](recipes/configs/mistral/7B_full_ppo_low_memory.yaml)
+| Knowledge Distillation | 1 | [knowledge_distillation_single_device](recipes/knowledge_distillation_single_device.py) | [Qwen2 1.5B -> 0.5B](recipes/configs/qwen2/knowledge_distillation_single_device.yaml)
+
+
+The above configs are just examples to get you started. If you see a model above not listed here, we likely still support it. If you're unsure whether something is supported, please open an issue on the repo.
+
+&nbsp;
+
+### Memory and training speed
+
+Below is an example of the memory requirements and training speed for different Llama 3.1 models.
+
+> [!NOTE]
+> For ease of comparison, all the below numbers are provided for batch size 2 (without gradient accumulation), a dataset packed to sequence length 2048, and torch compile enabled.
+
+If you are interested in running on different hardware or with different models, check out our documentation on memory optimizations [here](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html) to find the right setup for you.
+
+| Model | Finetuning Method | Runnable On | Peak Memory per GPU | Tokens/sec * |
+|:-:|:-:|:-:|:-:|:-:|
+| Llama 3.1 8B | Full finetune | 1x 4090 | 18.9 GiB | 1650 |
+| Llama 3.1 8B | Full finetune | 1x A6000 | 37.4 GiB |  2579|
+| Llama 3.1 8B | LoRA | 1x 4090 |  16.2 GiB | 3083 |
+| Llama 3.1 8B | LoRA | 1x A6000 | 30.3 GiB  | 4699 |
+| Llama 3.1 8B | QLoRA | 1x 4090 | 7.4 GiB | 2413  |
+| Llama 3.1 70B | Full finetune | 8x A100  | 13.9 GiB ** | 1568  |
+| Llama 3.1 70B | LoRA | 8x A100 | 27.6 GiB  | 3497  |
+| Llama 3.1 405B | QLoRA | 8x A100 | 44.8 GB  | 653  |
+
+*= Measured over one full training epoch
+
+**= Uses CPU offload with fused optimizer
+
+&nbsp;
+
+## Installation
+
+torchtune is tested with the latest stable PyTorch release as well as the preview nightly version. torchtune leverages
+torchvision for finetuning multimodal LLMs and torchao for the latest in quantization techniques; you should install these as well.
+
+&nbsp;
+
+### Install stable release
+
+```bash
+# Install stable PyTorch, torchvision, torchao stable releases
+pip install torch torchvision torchao
+pip install torchtune
+```
+
+&nbsp;
+
+### Install nightly release
+
+```bash
+# Install PyTorch, torchvision, torchao nightlies
+pip install --pre --upgrade torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/cu121 # full options are cpu/cu118/cu121/cu124
+pip install --pre --upgrade torchtune --extra-index-url https://download.pytorch.org/whl/nightly/cpu
+```
+
+You can also check out our [install documentation](https://pytorch.org/torchtune/main/install.html) for more information, including installing torchtune from source.
+
+&nbsp;
+
+To confirm that the package is installed correctly, you can run the following command:
+
+```bash
+tune --help
+```
+
+And should see the following output:
+
+```bash
+usage: tune [-h] {ls,cp,download,run,validate} ...
+
+Welcome to the torchtune CLI!
+
+options:
+  -h, --help            show this help message and exit
+
+...
+```
+
+&nbsp;
+
+## Get Started
+
+To get started with torchtune, see our [First Finetune Tutorial](https://pytorch.org/torchtune/main/tutorials/first_finetune_tutorial.html). Our [End-to-End Workflow Tutorial](https://pytorch.org/torchtune/main/tutorials/e2e_flow.html) will show you how to evaluate, quantize and run inference with a Llama model. The rest of this section will provide a quick overview of these steps with Llama3.1.
+
+
+### Downloading a model
+
+Follow the instructions on the official [`meta-llama`](https://huggingface.co/meta-llama) repository to ensure you have access to the official Llama model weights. Once you have confirmed access, you can run the following command to download the weights to your local machine. This will also download the tokenizer model and a responsible use guide.
+
+To download Llama3.1, you can run:
+
+```bash
+tune download meta-llama/Meta-Llama-3.1-8B-Instruct \
+--output-dir /tmp/Meta-Llama-3.1-8B-Instruct \
+--hf-token <HF_TOKEN> \
+```
+
+> [!Tip]
+> Set your environment variable `HF_TOKEN` or pass in `--hf-token` to the command in order to validate your access. You can find your token at https://huggingface.co/settings/tokens
+
+&nbsp;
+
+### Running finetuning recipes
+
+You can finetune Llama3.1 8B with LoRA on a single GPU using the following command:
+
+```bash
+tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device
+```
+
+For distributed training, tune CLI integrates with [torchrun](https://pytorch.org/docs/stable/elastic/run.html).
+To run a full finetune of Llama3.1 8B on two GPUs:
+
+```bash
+tune run --nproc_per_node 2 full_finetune_distributed --config llama3_1/8B_full
+```
+
+> [!Tip]
+> Make sure to place any torchrun commands **before** the recipe specification. Any CLI args after this will override the config and not impact distributed training.
+
+&nbsp;
+
+### Modify Configs
+
+There are two ways in which you can modify configs:
+
+**Config Overrides**
+
+You can directly overwrite config fields from the command line:
+
+```bash
+tune run lora_finetune_single_device \
+--config llama2/7B_lora_single_device \
+batch_size=8 \
+enable_activation_checkpointing=True \
+max_steps_per_epoch=128
+```
+
+**Update a Local Copy**
+
+You can also copy the config to your local directory and modify the contents directly:
+
+```bash
+tune cp llama3_1/8B_full ./my_custom_config.yaml
+Copied to ./my_custom_config.yaml
+```
+
+Then, you can run your custom recipe by directing the `tune run` command to your local files:
+
+```bash
+tune run full_finetune_distributed --config ./my_custom_config.yaml
+```
+
+&nbsp;
+
+Check out `tune --help` for all possible CLI commands and options. For more information on using and updating configs, take a look at our [config deep-dive](https://pytorch.org/torchtune/main/deep_dives/configs.html).
+
+&nbsp;
+
+### Custom Datasets
+
+torchtune supports finetuning on a variety of different datasets, including [instruct-style](https://pytorch.org/torchtune/main/basics/instruct_datasets.html), [chat-style](https://pytorch.org/torchtune/main/basics/chat_datasets.html), [preference datasets](https://pytorch.org/torchtune/main/basics/preference_datasets.html), and more. If you want to learn more about how to apply these components to finetune on your own custom dataset, please check out the provided links along with our [API docs](https://pytorch.org/torchtune/main/api_ref_datasets.html).
+
+&nbsp;
+
+## Community
+
+torchtune focuses on integrating with popular tools and libraries from the ecosystem. These are just a few examples, with more under development:
+
+- [Hugging Face Hub](https://huggingface.co/docs/hub/en/index) for [accessing model weights](torchtune/_cli/download.py)
+- [EleutherAI's LM Eval Harness](https://github.com/EleutherAI/lm-evaluation-harness) for [evaluating](recipes/eleuther_eval.py) trained models
+- [Hugging Face Datasets](https://huggingface.co/docs/datasets/en/index) for [access](torchtune/datasets/_instruct.py) to training and evaluation datasets
+- [PyTorch FSDP2](https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md) for distributed training
+- [torchao](https://github.com/pytorch-labs/ao) for lower precision dtypes and [post-training quantization](recipes/quantize.py) techniques
+- [Weights & Biases](https://wandb.ai/site) for [logging](https://pytorch.org/torchtune/main/deep_dives/wandb_logging.html) metrics and checkpoints, and tracking training progress
+- [Comet](https://www.comet.com/site/) as another option for [logging](https://pytorch.org/torchtune/main/deep_dives/comet_logging.html)
+- [ExecuTorch](https://pytorch.org/executorch-overview) for [on-device inference](https://github.com/pytorch/executorch/tree/main/examples/models/llama2#optional-finetuning) using finetuned models
+- [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index) for low memory optimizers for our [single-device recipes](recipes/configs/llama2/7B_full_low_memory.yaml)
+- [PEFT](https://github.com/huggingface/peft) for continued finetuning or inference with torchtune models in the Hugging Face ecosystem
+
+&nbsp;
+
+### Community Contributions
+
+We really value our community and the contributions made by our wonderful users. We'll use this section to call out some of these contributions. If you'd like to help out as well, please see the [CONTRIBUTING](CONTRIBUTING.md) guide.
+
+- [@SalmanMohammadi](https://github.com/salmanmohammadi) for adding a comprehensive end-to-end recipe for [Reinforcement Learning from Human Feedback (RLHF)](recipes/ppo_full_finetune_single_device.py) finetuning with PPO to torchtune
+- [@fyabc](https://github.com/fyabc) for adding Qwen2 models, tokenizer, and recipe integration to torchtune
+- [@solitude-alive](https://github.com/solitude-alive) for adding the [Gemma 2B model](torchtune/models/gemma/) to torchtune, including recipe changes, numeric validations of the models and recipe correctness
+- [@yechenzhi](https://github.com/yechenzhi) for adding [Direct Preference Optimization (DPO)](recipes/lora_dpo_single_device.py) to torchtune, including the recipe and config along with correctness checks
+
+
+&nbsp;
+
+## Acknowledgements
+
+The Llama2 code in this repository is inspired by the original [Llama2 code](https://github.com/meta-llama/llama/blob/main/llama/model.py).
+
+We want to give a huge shout-out to EleutherAI, Hugging Face and Weights & Biases for being wonderful collaborators and for working with us on some of these integrations within torchtune.
+
+We also want to acknowledge some awesome libraries and tools from the ecosystem:
+- [gpt-fast](https://github.com/pytorch-labs/gpt-fast) for performant LLM inference techniques which we've adopted out-of-the-box
+- [llama recipes](https://github.com/meta-llama/llama-recipes) for spring-boarding the llama2 community
+- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) for bringing several memory and performance based techniques to the PyTorch ecosystem
+- [@winglian](https://github.com/winglian/) and [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) for early feedback and brainstorming on torchtune's design and feature set.
+- [lit-gpt](https://github.com/Lightning-AI/litgpt) for pushing the LLM finetuning community forward.
+- [HF TRL](https://github.com/huggingface/trl) for making reward modeling more accessible to the PyTorch community.
+
+&nbsp;
+
+
+## License
+
+torchtune is released under the [BSD 3 license](./LICENSE). However you may have other legal obligations that govern your use of other content, such as the terms of service for third-party models.
+
+
+## Citing torchtune
+
+If you find the torchtune library useful, please cite it in your work as below.
+
+```bibtex
+@software{torchtune,
+  title = {torchtune: PyTorch's finetuning library},
+  author = {torchtune maintainers and contributors},
+  url = {https//github.com/pytorch/torchtune},
+  license = {BSD-3-Clause},
+  month = apr,
+  year = {2024}
+}
+```
diff -ruN marc_original/third_party/torchtune/recipes/configs/code_llama2/7B_full_low_memory.yaml marc/third_party/torchtune/recipes/configs/code_llama2/7B_full_low_memory.yaml
--- marc_original/third_party/torchtune/recipes/configs/code_llama2/7B_full_low_memory.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/code_llama2/7B_full_low_memory.yaml	2025-02-20 17:49:29.118023536 -0500
@@ -0,0 +1,78 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a CodeLlama 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#     tune download meta-llama/CodeLlama-7b-hf --output-dir /tmp/CodeLlama-7b-hf
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config code_llama2/7B_full_low_memory
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config code_llama2/7B_full_low_memory checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model arguments
+model:
+  _component_: torchtune.models.code_llama2.code_llama2_7b
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/CodeLlama-7b-hf/tokenizer.model
+  max_seq_len: null
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/CodeLlama-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00003.bin,
+    pytorch_model-00002-of-00003.bin,
+    pytorch_model-00003-of-00003.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/CodeLlama-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 1
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW
+  lr: 2e-5
+optimizer_in_bwd: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+dtype: bf16
+
+# Logging
+output_dir: /tmp/codellama_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/CodeLlama-7b-hf/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/code_llama2/7B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/code_llama2/7B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/code_llama2/7B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/code_llama2/7B_lora_single_device.yaml	2025-02-20 17:49:29.118023536 -0500
@@ -0,0 +1,113 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a CodeLlama 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/CodeLlama-7b-hf --output-dir /tmp/CodeLlama-7b-hf
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config code_llama2/7B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config code_llama2/7B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model Arguments
+model:
+  _component_: torchtune.models.code_llama2.lora_code_llama2_7b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/CodeLlama-7b-hf/tokenizer.model
+  max_seq_len: null
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/CodeLlama-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00003.bin,
+    pytorch_model-00002-of-00003.bin,
+    pytorch_model-00003-of-00003.bin
+  ]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/CodeLlama-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 16
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+dtype: bf16
+
+# Logging
+output_dir: /tmp/codellama_lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/CodeLlama-7b-hf/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Showcase the usage of PyTorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: /tmp/CodeLlama-7b-hf/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/code_llama2/7B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/code_llama2/7B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/code_llama2/7B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/code_llama2/7B_qlora_single_device.yaml	2025-02-20 17:49:29.122023543 -0500
@@ -0,0 +1,116 @@
+# Config for single device QLoRA finetuning in lora_finetune_single_device.py
+# using a CodeLlama 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/CodeLlama-7b-hf --output-dir /tmp/CodeLlama-7b-hf
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config code_llama2/7B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config code_llama2/7B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model Arguments
+model:
+  _component_: torchtune.models.code_llama2.qlora_code_llama2_7b
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/CodeLlama-7b-hf/tokenizer.model
+  max_seq_len: null
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/CodeLlama-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00003.bin,
+    pytorch_model-00002-of-00003.bin,
+    pytorch_model-00003-of-00003.bin
+  ]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/CodeLlama-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 16
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+dtype: bf16
+
+# Logging
+output_dir: /tmp/codellama_qlora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/CodeLlama-7b-hf/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: /tmp/CodeLlama-7b-hf/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
+
+# For colab use True
+low_cpu_ram: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/dev/8B_full_experimental.yaml marc/third_party/torchtune/recipes/configs/dev/8B_full_experimental.yaml
--- marc_original/third_party/torchtune/recipes/configs/dev/8B_full_experimental.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/dev/8B_full_experimental.yaml	2025-02-20 17:49:29.130023555 -0500
@@ -0,0 +1,80 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Llama3 8B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B --output-dir /tmp/Meta-Llama-3-8B --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 8B_full_single_device.yaml for those cases
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3-8B/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.llama3_8b
+
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3-8B/original/
+  checkpoint_files: [
+    consolidated.00.pth
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3-8B/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+
+optimizer:
+  _component_: torch.optim.AdamW
+  lr: 2e-5
+  fused: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: False
+ac_mode: 'selective'  # ['selective', 'full']
+ac_option: 2 # [int] = ac every positive int layer
+memory_efficient_fsdp_wrap: False
+
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-llama3-finetune
+log_every_n_steps: null
diff -ruN marc_original/third_party/torchtune/recipes/configs/eleuther_evaluation.yaml marc/third_party/torchtune/recipes/configs/eleuther_evaluation.yaml
--- marc_original/third_party/torchtune/recipes/configs/eleuther_evaluation.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/eleuther_evaluation.yaml	2025-02-20 17:49:29.134023562 -0500
@@ -0,0 +1,39 @@
+# Config for EleutherEvalRecipe in eleuther_eval.py
+#
+# To launch, run the following command from root torchtune directory:
+#    tune run eleuther_eval --config eleuther_evaluation tasks=["truthfulqa_mc2","hellaswag"]
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.llama2_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin,
+  ]
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: null
+
+# Environment
+device: cuda
+dtype: bf16
+seed: 1234 # It is not recommended to change this seed, b/c it matches EleutherAI's default seed
+
+# EleutherAI specific eval args
+tasks: ["truthfulqa_mc2"]
+limit: null
+max_seq_length: 4096
+batch_size: 8
+enable_kv_cache: True
+
+# Quantization specific args
+quantizer: null
diff -ruN marc_original/third_party/torchtune/recipes/configs/gemma/2B_full.yaml marc/third_party/torchtune/recipes/configs/gemma/2B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/gemma/2B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/gemma/2B_full.yaml	2025-02-20 17:49:29.138023568 -0500
@@ -0,0 +1,73 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a gemma 2B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download google/gemma-2b --ignore-patterns "gemma-2b.gguf" --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config gemma/2B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config gemma/2B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only when the model is being fine-tuned on 2+ GPUs.
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.gemma.gemma_tokenizer
+  path: /tmp/gemma-2b/tokenizer.model
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.gemma.gemma_2b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/gemma-2b/
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/gemma-2b
+  model_type: GEMMA
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-gemma-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/gemma/2B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/gemma/2B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/gemma/2B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/gemma/2B_lora_single_device.yaml	2025-02-20 17:49:29.142023575 -0500
@@ -0,0 +1,112 @@
+# Config for multi-device LoRA finetuning in lora_finetune_single_device.py
+# using a gemma 2B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download google/gemma-2b --ignore-patterns "gemma-2b.gguf"  --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config gemma/2B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config gemma/2B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.gemma.gemma_tokenizer
+  path: /tmp/gemma-2b/tokenizer.model
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.gemma.lora_gemma_2b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/gemma-2b/
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/gemma-2b
+  model_type: GEMMA
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 10
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-gemma-lora
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/gemma/2B_lora.yaml marc/third_party/torchtune/recipes/configs/gemma/2B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/gemma/2B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/gemma/2B_lora.yaml	2025-02-20 17:49:29.142023575 -0500
@@ -0,0 +1,85 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a gemma 2B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download google/gemma-2b --ignore-patterns "gemma-2b.gguf" --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed --config gemma/2B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed --config gemma/2B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only when the model is being fine-tuned on 2+ GPUs.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.gemma.gemma_tokenizer
+  path: /tmp/gemma-2b/tokenizer.model
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.gemma.lora_gemma_2b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/gemma-2b/
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/gemma-2b
+  model_type: GEMMA
+resume_from_checkpoint: False
+
+save_adapter_weights_only: False
+
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 10
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-gemma-lora
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/gemma/2B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/gemma/2B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/gemma/2B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/gemma/2B_qlora_single_device.yaml	2025-02-20 17:49:29.146023582 -0500
@@ -0,0 +1,112 @@
+# Config for multi-device QLoRA finetuning in lora_finetune_single_device.py
+# using a gemma 2B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download google/gemma-2b --ignore-patterns "gemma-2b.gguf" --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config gemma/2B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config gemma/2B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.gemma.gemma_tokenizer
+  path: /tmp/gemma-2b/tokenizer.model
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.gemma.qlora_gemma_2b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/gemma-2b/
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/gemma-2b
+  model_type: GEMMA
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 10
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-gemma-lora
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/gemma/7B_full.yaml marc/third_party/torchtune/recipes/configs/gemma/7B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/gemma/7B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/gemma/7B_full.yaml	2025-02-20 17:49:29.150023588 -0500
@@ -0,0 +1,75 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a gemma 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download google/gemma-7b --ignore-patterns "gemma-7b.gguf"  --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config gemma/7B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config gemma/7B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only when the model is being fine-tuned on 2+ GPUs.
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.gemma.gemma_tokenizer
+  path: /tmp/gemma-7b/tokenizer.model
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.gemma.gemma_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/gemma-7b/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/gemma-7b
+  model_type: GEMMA
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 1
+epochs: 1
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-gemma-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/gemma/7B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/gemma/7B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/gemma/7B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/gemma/7B_lora_single_device.yaml	2025-02-20 17:49:29.158023601 -0500
@@ -0,0 +1,114 @@
+# Config for multi-device LoRA finetuning in lora_finetune_single_device.py
+# using a gemma 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run (torchtune does not use gguf so you can ignore it to save time and space):
+#   tune download google/gemma-7b --ignore-patterns "gemma-7b.gguf"  --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config gemma/7B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config gemma/7B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.gemma.gemma_tokenizer
+  path: /tmp/gemma-7b/tokenizer.model
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.gemma.lora_gemma_7b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/gemma-7b/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/gemma-7b/
+  model_type: GEMMA
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 5e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 10
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Fine-tuning arguments
+batch_size: 8
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 2
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-gemma-lora
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/gemma/7B_lora.yaml marc/third_party/torchtune/recipes/configs/gemma/7B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/gemma/7B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/gemma/7B_lora.yaml	2025-02-20 17:49:29.154023595 -0500
@@ -0,0 +1,87 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a gemma 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download google/gemma-7b --ignore-patterns "gemma-7b.gguf" --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed --config gemma/7B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed --config gemma/7B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only when the model is being fine-tuned on 2+ GPUs.
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.gemma.gemma_tokenizer
+  path: /tmp/gemma-7b/tokenizer.model
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.gemma.lora_gemma_7b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/gemma-7b/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/gemma-7b/
+  model_type: GEMMA
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 10
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-gemma-lora
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/gemma/7B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/gemma/7B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/gemma/7B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/gemma/7B_qlora_single_device.yaml	2025-02-20 17:49:29.162023608 -0500
@@ -0,0 +1,117 @@
+# Config for multi-device QLoRA finetuning in lora_finetune_single_device.py
+# using a gemma 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download google/gemma-7b --ignore-patterns "gemma-7b.gguf" --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config gemma/7B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config gemma/7B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.gemma.gemma_tokenizer
+  path: /tmp/gemma-7b/tokenizer.model
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.gemma.qlora_gemma_7b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/gemma-7b/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/gemma-7b/
+  model_type: GEMMA
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 10
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-gemma-lora
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
+
+# For colab use True
+low_cpu_ram: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/gemma/evaluation.yaml marc/third_party/torchtune/recipes/configs/gemma/evaluation.yaml
--- marc_original/third_party/torchtune/recipes/configs/gemma/evaluation.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/gemma/evaluation.yaml	2025-02-20 17:49:29.166023615 -0500
@@ -0,0 +1,39 @@
+# Config for EleutherEvalRecipe in eleuther_eval.py
+#
+# To launch, run the following command:
+#    tune run eleuther_eval --config gemma/evaluation
+
+# Model Arguments
+model:
+  _component_: torchtune.models.gemma.gemma_2b
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/gemma-2b
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  output_dir: ./ # Not needed
+  model_type: GEMMA
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.gemma.gemma_tokenizer
+  path: /tmp/gemma-2b/tokenizer.model
+
+# Environment
+device: cuda
+dtype: bf16
+seed: 1234 # It is not recommended to change this seed, b/c it matches EleutherAI's default seed
+
+# EleutherAI specific eval args
+tasks: ["truthfulqa_mc2"]
+limit: null
+max_seq_length: 4096
+batch_size: 8
+enable_kv_cache: True
+
+# Quantization specific args
+quantizer: null
diff -ruN marc_original/third_party/torchtune/recipes/configs/generation.yaml marc/third_party/torchtune/recipes/configs/generation.yaml
--- marc_original/third_party/torchtune/recipes/configs/generation.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/generation.yaml	2025-02-20 17:49:29.170023621 -0500
@@ -0,0 +1,41 @@
+# Config for running the InferenceRecipe in generate.py to generate output from an LLM
+#
+# To launch, run the following command from root torchtune directory:
+#    tune run generate --config generation
+
+# Model arguments
+model:
+  _component_: torchtune.models.llama2.llama2_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf/
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin,
+  ]
+  output_dir: /tmp/Llama-2-7b-hf/
+  model_type: LLAMA2
+
+device: cuda
+dtype: bf16
+
+seed: 1234
+
+# Tokenizer arguments
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: null
+
+# Generation arguments; defaults taken from gpt-fast
+prompt: "Tell me a joke?"
+instruct_template: null
+chat_format: null
+max_new_tokens: 300
+temperature: 0.6 # 0.8 and 0.6 are popular values to try
+top_k: 300
+
+enable_kv_cache: True
+
+quantizer: null
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/13B_full.yaml marc/third_party/torchtune/recipes/configs/llama2/13B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/13B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/13B_full.yaml	2025-02-20 17:49:29.174023628 -0500
@@ -0,0 +1,77 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Llama2 13B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-13b-hf --output-dir /tmp/Llama-2-13b-hf --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config llama2/13B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config llama2/13B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config should be used with 2+ GPUs. Single device full fine-tuning
+# requires several memory optimizations which are exposed through
+# 7B_full_single_device.yaml. Please update the model and checkpoints to 13B
+# in that config.
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.llama2_13b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-13b-hf/
+  checkpoint_files: [
+    pytorch_model-00001-of-00003.bin,
+    pytorch_model-00002-of-00003.bin,
+    pytorch_model-00003-of-00003.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-13b-hf/
+  model_type: LLAMA2
+resume_from_checkpoint: False
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-13b-hf/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-llama2-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/13B_lora.yaml marc/third_party/torchtune/recipes/configs/llama2/13B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/13B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/13B_lora.yaml	2025-02-20 17:49:29.178023634 -0500
@@ -0,0 +1,89 @@
+# Config for multi-device LoRA in lora_finetune_distributed.py
+# using a Llama2 13B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-13b-hf --output-dir /tmp/Llama-2-13b-hf --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed --config llama2/13B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 lora_finetune_distributed --config llama2/13B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 7B_lora_single_device.yaml
+# or 7B_qlora_single_device.yaml and update the model and checkpoints to
+# the 13B model.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.lora_llama2_13b
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: True
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-13b-hf/
+  checkpoint_files: [
+    pytorch_model-00001-of-00003.bin,
+    pytorch_model-00002-of-00003.bin,
+    pytorch_model-00003-of-00003.bin
+  ]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-13b-hf/
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-13b-hf/tokenizer.model
+  max_seq_len: null
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 2e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 16
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/13B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama2/13B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/13B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/13B_qlora_single_device.yaml	2025-02-20 17:49:29.182023641 -0500
@@ -0,0 +1,113 @@
+# Config for single device QLoRA with lora_finetune_single_device.py
+# using a Llama2 13B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-13b-hf --output-dir /tmp/Llama-2-13b-hf --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama2/13B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config 13_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.qlora_llama2_13b
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-13b-hf/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-13b-hf/
+  checkpoint_files: [
+    pytorch_model-00001-of-00003.bin,
+    pytorch_model-00002-of-00003.bin,
+    pytorch_model-00003-of-00003.bin
+  ]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-13b-hf/
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 16
+compile: False
+
+# Logging
+output_dir: /tmp/qlora_finetune_output/
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/70B_lora.yaml marc/third_party/torchtune/recipes/configs/llama2/70B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/70B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/70B_lora.yaml	2025-02-20 17:49:29.186023647 -0500
@@ -0,0 +1,89 @@
+# Config for multi-device LoRA in lora_finetune_distributed.py
+# using a Llama2 70B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-70b-hf --output-dir /tmp/Llama-2-70b-hf --hf-token <HF_TOKEN>
+#
+# This config needs 8 GPUs to run
+#   # tune run --nproc_per_node 8 lora_finetune_distributed --config llama2/70B_lora
+#
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.lora_llama2_70b
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 16
+  lora_alpha: 32
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-70b-hf/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir:  /tmp/Llama-2-70b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00015.bin,
+    pytorch_model-00002-of-00015.bin,
+    pytorch_model-00003-of-00015.bin,
+    pytorch_model-00004-of-00015.bin,
+    pytorch_model-00005-of-00015.bin,
+    pytorch_model-00006-of-00015.bin,
+    pytorch_model-00007-of-00015.bin,
+    pytorch_model-00008-of-00015.bin,
+    pytorch_model-00009-of-00015.bin,
+    pytorch_model-00010-of-00015.bin,
+    pytorch_model-00011-of-00015.bin,
+    pytorch_model-00012-of-00015.bin,
+    pytorch_model-00013-of-00015.bin,
+    pytorch_model-00014-of-00015.bin,
+    pytorch_model-00015-of-00015.bin,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-70b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/70B_qlora.yaml marc/third_party/torchtune/recipes/configs/llama2/70B_qlora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/70B_qlora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/70B_qlora.yaml	2025-02-20 17:49:29.190023654 -0500
@@ -0,0 +1,99 @@
+# Config for multi-device QLoRA in lora_finetune_distributed.py
+# using a Llama2 70B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-70b-hf --output-dir /tmp/Llama-2-70b-hf --hf-token <HF_TOKEN>
+#
+# This config needs 8 GPUs to run
+#   # tune run --nproc_per_node 8 lora_finetune_distributed --config llama2/70B_qlora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 8 lora_finetune_distributed --config llama2/70B_qlora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.qlora_llama2_70b
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 16
+  lora_alpha: 32
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-70b-hf/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir:  /tmp/Llama-2-70b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00015.bin,
+    pytorch_model-00002-of-00015.bin,
+    pytorch_model-00003-of-00015.bin,
+    pytorch_model-00004-of-00015.bin,
+    pytorch_model-00005-of-00015.bin,
+    pytorch_model-00006-of-00015.bin,
+    pytorch_model-00007-of-00015.bin,
+    pytorch_model-00008-of-00015.bin,
+    pytorch_model-00009-of-00015.bin,
+    pytorch_model-00010-of-00015.bin,
+    pytorch_model-00011-of-00015.bin,
+    pytorch_model-00012-of-00015.bin,
+    pytorch_model-00013-of-00015.bin,
+    pytorch_model-00014-of-00015.bin,
+    pytorch_model-00015-of-00015.bin,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-70b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+  train_on_input: True
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+fsdp:
+  cpu_offload: False
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+compile: False
+
+# Logging
+output_dir: /tmp/qlora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/7B_full_low_memory.yaml marc/third_party/torchtune/recipes/configs/llama2/7B_full_low_memory.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/7B_full_low_memory.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/7B_full_low_memory.yaml	2025-02-20 17:49:29.198023668 -0500
@@ -0,0 +1,82 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Llama2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config llama2/7B_full_low_memory
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config llama2/7B_full_low_memory checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.llama2_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW
+  lr: 1e-5
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+optimizer_in_bwd: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+compile: False
+
+# Training environment
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-llama2-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/7B_full.yaml marc/third_party/torchtune/recipes/configs/llama2/7B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/7B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/7B_full.yaml	2025-02-20 17:49:29.194023661 -0500
@@ -0,0 +1,77 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Llama2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config llama2/7B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config llama2/7B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 7B_full_single_device.yaml for those cases
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.llama2_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-llama2-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/7B_lora_dpo_single_device.yaml marc/third_party/torchtune/recipes/configs/llama2/7B_lora_dpo_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/7B_lora_dpo_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/7B_lora_dpo_single_device.yaml	2025-02-20 17:49:29.206023680 -0500
@@ -0,0 +1,83 @@
+# Config for single device LoRA DPO alignment in lora_dpo_single_device.py
+# using a Llama2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_dpo_single_device --config llama2/7B_lora_dpo_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_dpo_single_device --config llama2/7B_lora_dpo_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.lora_llama2_7b
+  lora_attn_modules: ["q_proj", "v_proj"]
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: 1024
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files:
+    [pytorch_model-00001-of-00002.bin, pytorch_model-00002-of-00002.bin]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.stack_exchange_paired_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.05
+  lr: 5e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.rlhf.loss.DPOLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: 1000
+gradient_accumulation_steps: 16
+compile: False
+
+# Logging
+output_dir: /tmp/lora_dpo_output/
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/7B_lora_dpo.yaml marc/third_party/torchtune/recipes/configs/llama2/7B_lora_dpo.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/7B_lora_dpo.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/7B_lora_dpo.yaml	2025-02-20 17:49:29.202023673 -0500
@@ -0,0 +1,85 @@
+# Config for multi-device LoRA DPO alignment in lora_dpo_distributed.py
+# using a Llama2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_dpo_distributed --config llama2/7B_lora_dpo
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_dpo_distributed --config llama2/7B_lora_dpo checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA DPO alignment please use 7B_lora_dpo_single_device.yaml
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.lora_llama2_7b
+  lora_attn_modules: ["q_proj", "v_proj"]
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: 1024
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files:
+    [pytorch_model-00001-of-00002.bin, pytorch_model-00002-of-00002.bin]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.stack_exchange_paired_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.05
+  lr: 5e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.rlhf.loss.DPOLoss
+  beta: 0.1
+  label_smoothing: 0
+
+# Training
+epochs: 1
+max_steps_per_epoch: 1000
+gradient_accumulation_steps: 8
+
+# Logging
+output_dir: /tmp/lora_dpo_output/
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/7B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama2/7B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/7B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/7B_lora_single_device.yaml	2025-02-20 17:49:29.210023687 -0500
@@ -0,0 +1,114 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Llama2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama2/7B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config 7B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.lora_llama2_7b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 64
+compile: False
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/7B_lora.yaml marc/third_party/torchtune/recipes/configs/llama2/7B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/7B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/7B_lora.yaml	2025-02-20 17:49:29.198023668 -0500
@@ -0,0 +1,113 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Llama2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 7B_lora_single_device.yaml
+# or 7B_qlora_single_device.yaml
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.lora_llama2_7b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 32
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/7B_qat_full.yaml marc/third_party/torchtune/recipes/configs/llama2/7B_qat_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/7B_qat_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/7B_qat_full.yaml	2025-02-20 17:49:29.214023693 -0500
@@ -0,0 +1,78 @@
+# Config for multi-device QAT finetuning in qat_distributed.py
+# using a Llama2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 qat_distributed --config llama2/7B_qat_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 qat_distributed --config llama2/7B_qat_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.llama2_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# QAT arguments
+quantizer:
+  _component_: torchtune.training.quantization.Int8DynActInt4WeightQATQuantizer
+  groupsize: 256
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+memory_efficient_fsdp_wrap: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-llama2-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/7B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama2/7B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/7B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/7B_qlora_single_device.yaml	2025-02-20 17:49:29.222023707 -0500
@@ -0,0 +1,116 @@
+# Config for single device QLoRA with lora_finetune_single_device.py
+# using a Llama2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama2/7B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config 7B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.qlora_llama2_7b
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 16
+compile: False
+
+# Logging
+output_dir: /tmp/qlora_finetune_output/
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
+
+# For colab use True
+low_cpu_ram: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/7B_qlora.yaml marc/third_party/torchtune/recipes/configs/llama2/7B_qlora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/7B_qlora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/7B_qlora.yaml	2025-02-20 17:49:29.218023700 -0500
@@ -0,0 +1,90 @@
+# Config for single device QLoRA with lora_finetune_distributed.py
+# using a Llama2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf --hf-token <HF_TOKEN>
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_qlora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_qlora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 7B_lora_single_device.yaml
+# or 7B_qlora_single_device.yaml
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama2.qlora_llama2_7b
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-hf/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  adapter_checkpoint: null
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+  train_on_input: True
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+fsdp:
+  cpu_offload: False
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 2
+compile: False
+
+# Logging
+output_dir: /tmp/qlora_finetune_output/
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama2/generation_v2.yaml marc/third_party/torchtune/recipes/configs/llama2/generation_v2.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama2/generation_v2.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama2/generation_v2.yaml	2025-02-20 17:49:29.226023714 -0500
@@ -0,0 +1,42 @@
+# Config for running the InferenceRecipe in generate_V2.py to generate output from an LLM
+#
+# This config assumes that you've run the following command before launching:
+#   tune download meta-llama/Llama-2-7b-chat-hf --output-dir /tmp/Llama-2-7b-chat-hf
+#
+# To launch, run the following command:
+#    tune run dev/generate_v2 --config llama2/generation_v2
+
+# Model arguments
+model:
+  _component_: torchtune.models.llama2.llama2_7b
+
+# Transform arguments
+tokenizer:
+  _component_: torchtune.models.llama2.llama2_tokenizer
+  path: /tmp/Llama-2-7b-chat-hf/tokenizer.model
+  max_seq_len: 2048
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-chat-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  output_dir: ./
+  model_type: LLAMA2
+
+# Device
+device: cuda
+dtype: bf16
+seed: 1234
+log_level: INFO
+
+# Generation arguments
+prompt:
+  system: You are a helpful and creative AI assistant.
+  user: What is the capital of France?
+max_new_tokens: 200
+temperature: 0.6 # 0.8 and 0.6 are popular values to try
+top_k: 300
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/70B_full.yaml marc/third_party/torchtune/recipes/configs/llama3/70B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/70B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/70B_full.yaml	2025-02-20 17:49:29.230023719 -0500
@@ -0,0 +1,113 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Llama3 70B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-70B-Instruct --output-dir /tmp/Meta-Llama-3-70B-Instruct  --ignore-patterns "original/consolidated*" --hf-token <HF_TOKEN>
+#
+# To launch on 8 devices, run the following command from root:
+#   tune run --nproc_per_node 8 full_finetune_distributed --config llama3/70B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 8 full_finetune_distributed --config llama3/70B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config is only tested on an 8xA100 machine.
+#
+# !!!!!!!!!!!!!
+# !!!!!!!!!!!!!
+# ATTENTION: It will only work with pytorch>=2.5 (nightlies). For other pytorch versions, it will OOM, even on 8xA100.
+# !!!!!!!!!!!!!
+# !!!!!!!!!!!!!
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3-70B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.llama3_70b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3-70B-Instruct
+  checkpoint_files: [
+    model-00001-of-00030.safetensors,
+    model-00002-of-00030.safetensors,
+    model-00003-of-00030.safetensors,
+    model-00004-of-00030.safetensors,
+    model-00005-of-00030.safetensors,
+    model-00006-of-00030.safetensors,
+    model-00007-of-00030.safetensors,
+    model-00008-of-00030.safetensors,
+    model-00009-of-00030.safetensors,
+    model-00010-of-00030.safetensors,
+    model-00011-of-00030.safetensors,
+    model-00012-of-00030.safetensors,
+    model-00013-of-00030.safetensors,
+    model-00014-of-00030.safetensors,
+    model-00015-of-00030.safetensors,
+    model-00016-of-00030.safetensors,
+    model-00017-of-00030.safetensors,
+    model-00018-of-00030.safetensors,
+    model-00019-of-00030.safetensors,
+    model-00020-of-00030.safetensors,
+    model-00021-of-00030.safetensors,
+    model-00022-of-00030.safetensors,
+    model-00023-of-00030.safetensors,
+    model-00024-of-00030.safetensors,
+    model-00025-of-00030.safetensors,
+    model-00026-of-00030.safetensors,
+    model-00027-of-00030.safetensors,
+    model-00028-of-00030.safetensors,
+    model-00029-of-00030.safetensors,
+    model-00030-of-00030.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3-70b
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+
+optimizer:
+  _component_: torch.optim.AdamW
+  lr: 2e-5
+  fused: True
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+custom_sharded_layers: ['tok_embeddings', 'output']
+fsdp_cpu_offload: True
+compile: False # set it to True for better memory and performance
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/full-llama3-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/70B_lora.yaml marc/third_party/torchtune/recipes/configs/llama3/70B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/70B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/70B_lora.yaml	2025-02-20 17:49:29.234023726 -0500
@@ -0,0 +1,105 @@
+# Config for multi-device LoRA in lora_finetune_distributed.py
+# using a Llama3 70B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-70B-Instruct --output-dir /tmp/Meta-Llama-3-70B-Instruct --ignore-patterns "original/consolidated*"  --hf-token <TOKEN>
+#
+# This config needs 8 GPUs to run
+#   # tune run --nproc_per_node 8 lora_finetune_distributed --config llama3/70B_lora
+#
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.lora_llama3_70b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 16
+  lora_alpha: 32
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3-70B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir:  /tmp/Meta-Llama-3-70B-Instruct
+  checkpoint_files: [
+    model-00001-of-00030.safetensors,
+    model-00002-of-00030.safetensors,
+    model-00003-of-00030.safetensors,
+    model-00004-of-00030.safetensors,
+    model-00005-of-00030.safetensors,
+    model-00006-of-00030.safetensors,
+    model-00007-of-00030.safetensors,
+    model-00008-of-00030.safetensors,
+    model-00009-of-00030.safetensors,
+    model-00010-of-00030.safetensors,
+    model-00011-of-00030.safetensors,
+    model-00012-of-00030.safetensors,
+    model-00013-of-00030.safetensors,
+    model-00014-of-00030.safetensors,
+    model-00015-of-00030.safetensors,
+    model-00016-of-00030.safetensors,
+    model-00017-of-00030.safetensors,
+    model-00018-of-00030.safetensors,
+    model-00019-of-00030.safetensors,
+    model-00020-of-00030.safetensors,
+    model-00021-of-00030.safetensors,
+    model-00022-of-00030.safetensors,
+    model-00023-of-00030.safetensors,
+    model-00024-of-00030.safetensors,
+    model-00025-of-00030.safetensors,
+    model-00026-of-00030.safetensors,
+    model-00027-of-00030.safetensors,
+    model-00028-of-00030.safetensors,
+    model-00029-of-00030.safetensors,
+    model-00030-of-00030.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3-70B-Instruct
+  model_type: LLAMA3
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/8B_dora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3/8B_dora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/8B_dora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/8B_dora_single_device.yaml	2025-02-20 17:49:29.242023739 -0500
@@ -0,0 +1,108 @@
+# Config for single device DoRA finetuning in lora_finetune_single_device.py
+# using a Llama3 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3/8B_dora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config llama3/8B_dora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.lora_llama3_8b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  use_dora: True
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/original/
+  checkpoint_files: [
+    consolidated.00.pth
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torch.nn.CrossEntropyLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 64
+compile: False
+
+# Logging
+output_dir: /tmp/dora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/8B_dora.yaml marc/third_party/torchtune/recipes/configs/llama3/8B_dora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/8B_dora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/8B_dora.yaml	2025-02-20 17:49:29.238023733 -0500
@@ -0,0 +1,79 @@
+# Config for multi-device DoRA finetuning in lora_finetune_distributed.py
+# using a Llama3 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama3/8B_dora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama3/8B_dora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.lora_llama3_8b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  use_dora: True
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/original/
+  checkpoint_files: [
+    consolidated.00.pth
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torch.nn.CrossEntropyLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Logging
+output_dir: /tmp/dora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/8B_full_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3/8B_full_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/8B_full_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/8B_full_single_device.yaml	2025-02-20 17:49:29.246023746 -0500
@@ -0,0 +1,81 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Llama3 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config llama3/8B_full_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config llama3/8B_full_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.llama3_8b
+
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/original/
+  checkpoint_files: [
+    consolidated.00.pth
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW8bit
+  lr: 1e-5
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+optimizer_in_bwd: True
+compile: False
+
+# Training environment
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/full-llama3-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/8B_full.yaml marc/third_party/torchtune/recipes/configs/llama3/8B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/8B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/8B_full.yaml	2025-02-20 17:49:29.246023746 -0500
@@ -0,0 +1,98 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Llama3 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 8B_full_single_device.yaml for those cases
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /raid/lingo/models/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+   _component_: torchtune.datasets.arc_dataset
+   source: /raid/lingo/akyurek/git/arc/data/tasks/more_both_aug_fix_drops
+   train_on_input: False
+   unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
+
+seed: 57
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.llama3_8b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /raid/lingo/akyurek/git/arc/experiments/fixed_aug_no_cc_train_on_input/model_weights/
+  checkpoint_files: [
+     hf_model_0001_1.pt,
+     hf_model_0002_1.pt,
+     hf_model_0003_1.pt,
+     hf_model_0004_1.pt,
+     # model-00001-of-00004.safetensors,
+     # model-00002-of-00004.safetensors,
+     # model-00003-of-00004.safetensors,
+     # model-00004-of-00004.safetensors,
+     # pytorch_model-0001-of-0004.bin,
+     # pytorch_model-0002-of-0004.bin,
+     # pytorch_model-0003-of-0004.bin,
+     # pytorch_model-0004-of-0004.bin,
+  ]
+  recipe_checkpoint: null
+  output_dir: /raid/lingo/akyurek/git/arc/experiments/fixed_aug_no_cc_train_on_input_ct/model_weights/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 4
+
+optimizer:
+  _component_: torch.optim.AdamW
+  lr: 2e-5
+  foreach: False
+  fused: True
+loss:
+  _component_: torchtune.modules.loss.CrossEntropyLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.WandBLogger
+  project: ARCPT
+  entity: akyurek
+  job_type: PT
+  group: pretrain
+
+output_dir: /raid/lingo/akyurek/git/arc/experiments/fixed_aug_no_cc_train_on_input_ct/logs/
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/8B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3/8B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/8B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/8B_lora_single_device.yaml	2025-02-20 17:49:29.254023759 -0500
@@ -0,0 +1,121 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Llama3 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3/8B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config llama3/8B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.lora_llama3_8b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 64
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /raid/lingo/models/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /raid/lingo/akyurek/git/arc/weights/hf_model_last/
+  checkpoint_files: [
+     pytorch_model-0001-of-0004.bin,
+     pytorch_model-0002-of-0004.bin,
+     pytorch_model-0003-of-0004.bin,
+     pytorch_model-0004-of-0004.bin,
+   ]
+   recipe_checkpoint: # recipe_state.pt
+   output_dir: /raid/lingo/akyurek/git/arc/experiments/lora/model_weights/
+   model_type: LLAMA3
+
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+   _component_: torchtune.datasets.arc_dataset
+   source: /raid/lingo/akyurek/git/arc/data/tasks/more_both_aug_fix_drops
+   train_on_input: False
+   unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
+
+seed: 57
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 5
+
+loss:
+  _component_: torch.nn.CrossEntropyLoss
+
+# Training
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 16
+compile: False
+
+# Logging
+output_dir: /raid/lingo/akyurek/git/arc/experiments/lora
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/8B_lora.yaml marc/third_party/torchtune/recipes/configs/llama3/8B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/8B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/8B_lora.yaml	2025-02-20 17:49:29.250023753 -0500
@@ -0,0 +1,84 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Llama3 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3/8B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3/8B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 8B_lora_single_device.yaml
+# or 8B_qlora_single_device.yaml
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.lora_llama3_8b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/original/
+  checkpoint_files: [
+    consolidated.00.pth
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 32
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/8B_qat_full.yaml marc/third_party/torchtune/recipes/configs/llama3/8B_qat_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/8B_qat_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/8B_qat_full.yaml	2025-02-20 17:49:29.258023765 -0500
@@ -0,0 +1,77 @@
+# Config for multi-device QAT finetuning in qat_distributed.py
+# using a Llama3 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nproc_per_node 4 qat_distributed --config llama3/8B_qat_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 4 qat_distributed --config llama3/8B_qat_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.llama3_8b
+
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/original/
+  checkpoint_files: [
+    consolidated.00.pth
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+
+# QAT arguments
+quantizer:
+  _component_: torchtune.training.quantization.Int8DynActInt4WeightQATQuantizer
+  groupsize: 256
+
+optimizer:
+  _component_: torch.optim.AdamW
+  lr: 2e-5
+  fused: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+memory_efficient_fsdp_wrap: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/alpaca-llama3-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/8B_qdora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3/8B_qdora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/8B_qdora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/8B_qdora_single_device.yaml	2025-02-20 17:49:29.262023772 -0500
@@ -0,0 +1,109 @@
+# Config for single device QDoRA finetuning in lora_finetune_single_device.py
+# using a Llama3 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3/8B_qdora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config llama3/8B_qdora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.lora_llama3_8b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  use_dora: True
+  quantize_base: True
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/original/
+  checkpoint_files: [
+    consolidated.00.pth
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torch.nn.CrossEntropyLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 64
+compile: False
+
+# Logging
+output_dir: /tmp/qdora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3/8B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3/8B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3/8B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3/8B_qlora_single_device.yaml	2025-02-20 17:49:29.266023779 -0500
@@ -0,0 +1,121 @@
+# Config for single device LoRA finetuning in qlora_finetune_single_device.py
+# using a Llama3 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3-8B-Instruct --output-dir /tmp/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run qlora_finetune_single_device --config llama3/8B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run qlora_finetune_single_device --config llama3/8B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3.qlora_llama3_8b
+  qlora_attn_modules: ['q_proj', 'v_proj']
+  apply_qlora_to_mlp: True
+  apply_qlora_to_output: False
+  qlora_rank: 64
+  qlora_alpha: 16
+  qlora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /raid/lingo/models/Meta-Llama-3-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /raid/lingo/akyurek/git/arc/weights/hf_model_last/
+  checkpoint_files: [
+     pytorch_model-0001-of-0004.bin,
+     pytorch_model-0002-of-0004.bin,
+     pytorch_model-0003-of-0004.bin,
+     pytorch_model-0004-of-0004.bin,
+   ]
+   recipe_checkpoint: # recipe_state.pt
+   output_dir: /raid/lingo/akyurek/git/arc/experiments/qlora/model_weights/
+   model_type: LLAMA3
+
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+   _component_: torchtune.datasets.arc_dataset
+   source: /raid/lingo/akyurek/git/arc/data/tasks/more_both_aug_fix_drops
+   train_on_input: False
+   unmask_outputs: True # we'll get loss from all outputs after the first demonstration, very hacky, tokenizer & formatting specific
+
+seed: 57
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 5
+
+loss:
+  _component_: torch.nn.CrossEntropyLoss
+
+# Training
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 16
+compile: False
+
+# Logging
+output_dir: /raid/lingo/akyurek/git/arc/experiments/qlora
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_1/405B_qlora.yaml marc/third_party/torchtune/recipes/configs/llama3_1/405B_qlora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_1/405B_qlora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_1/405B_qlora.yaml	2025-02-20 17:49:29.270023786 -0500
@@ -0,0 +1,87 @@
+# Config for multi-device QLoRA in lora_finetune_fsdp2.py
+# using a Llama3.1 405B model
+#
+# This config requires PyTorch nightlies to run.
+# See https://pytorch.org/torchtune/main/install.html#install-instructions
+# for setup instructions.
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3.1-405B-Instruct --ignore-patterns "original/consolidated*" --hf-token <HF_TOKEN>
+#
+# This config needs 8 GPUs to run
+#   # tune run --nproc_per_node 8 lora_finetune_distributed --config llama3_1/405B_qlora
+#
+# !!!!!!!!!!!!!
+# !!!!!!!!!!!!!
+# ATTENTION: It will only work with pytorch>=2.5 (nightlies). For other pytorch versions, it will OOM, even on 8xA100.
+# !!!!!!!!!!!!!
+# !!!!!!!!!!!!!
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_1.qlora_llama3_1_405b
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 16
+  lora_alpha: 32
+
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3.1-405B-Instruct/original/mp8/tokenizer.model
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3.1-405B-Instruct/
+  checkpoint_files:
+    filename_format: model-{}-of-{}.safetensors
+    max_filename: 00191
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3.1-405B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+save_adapter_weights_only: True
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+  train_on_input: True
+seed: null
+shuffle: True
+batch_size: 1
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  weight_decay: 0.01
+  lr: 3e-4
+  fused: True
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torch.nn.CrossEntropyLoss
+
+fsdp:
+  cpu_offload: False
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 16
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/qlora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: True
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_1/70B_full.yaml marc/third_party/torchtune/recipes/configs/llama3_1/70B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_1/70B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_1/70B_full.yaml	2025-02-20 17:49:29.274023792 -0500
@@ -0,0 +1,115 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Llama3.1 70B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3.1-70B-Instruct --output-dir /tmp/Meta-Llama-3.1-70B-Instruct --ignore-patterns "original/consolidated*"
+#
+# To launch on 8 devices, run the following command from root:
+#   tune run --nproc_per_node 8 full_finetune_distributed --config llama3_1/70B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 8 full_finetune_distributed --config llama3_1/70B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config is only tested on an 8xA100 machine.
+#
+# !!!!!!!!!!!!!
+# !!!!!!!!!!!!!
+# ATTENTION: It will only work with pytorch>=2.5 (nightlies). For other pytorch versions, it will OOM, even on 8xA100.
+# !!!!!!!!!!!!!
+# !!!!!!!!!!!!!
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3.1-70B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_1.llama3_1_70b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3.1-70B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00030.safetensors,
+    model-00002-of-00030.safetensors,
+    model-00003-of-00030.safetensors,
+    model-00004-of-00030.safetensors,
+    model-00005-of-00030.safetensors,
+    model-00006-of-00030.safetensors,
+    model-00007-of-00030.safetensors,
+    model-00008-of-00030.safetensors,
+    model-00009-of-00030.safetensors,
+    model-00010-of-00030.safetensors,
+    model-00011-of-00030.safetensors,
+    model-00012-of-00030.safetensors,
+    model-00013-of-00030.safetensors,
+    model-00014-of-00030.safetensors,
+    model-00015-of-00030.safetensors,
+    model-00016-of-00030.safetensors,
+    model-00017-of-00030.safetensors,
+    model-00018-of-00030.safetensors,
+    model-00019-of-00030.safetensors,
+    model-00020-of-00030.safetensors,
+    model-00021-of-00030.safetensors,
+    model-00022-of-00030.safetensors,
+    model-00023-of-00030.safetensors,
+    model-00024-of-00030.safetensors,
+    model-00025-of-00030.safetensors,
+    model-00026-of-00030.safetensors,
+    model-00027-of-00030.safetensors,
+    model-00028-of-00030.safetensors,
+    model-00029-of-00030.safetensors,
+    model-00030-of-00030.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3.1-70B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+
+optimizer:
+  _component_: torch.optim.AdamW
+  lr: 2e-5
+  # Note: highly recommended to use fused=True optimizer flag
+  # with CPU offload for faster optimizer step.
+  fused: True
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+custom_sharded_layers: ['tok_embeddings', 'output']
+fsdp_cpu_offload: True
+compile: False # set it to True for better memory and performance
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/full-llama3_1-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_1/70B_lora.yaml marc/third_party/torchtune/recipes/configs/llama3_1/70B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_1/70B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_1/70B_lora.yaml	2025-02-20 17:49:29.278023799 -0500
@@ -0,0 +1,104 @@
+# Config for multi-device LoRA in lora_finetune_distributed.py
+# using a Llama3.1 70B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3.1-70B-Instruct --output-dir /tmp/Meta-Llama-3.1-70B-Instruct --ignore-patterns "original/consolidated*"
+#
+# This config needs 8 GPUs to run
+#   tune run --nproc_per_node 8 lora_finetune_distributed --config llama3_1/70B_lora
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_1.lora_llama3_1_70b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 16
+  lora_alpha: 32
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3.1-70B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3.1-70B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00030.safetensors,
+    model-00002-of-00030.safetensors,
+    model-00003-of-00030.safetensors,
+    model-00004-of-00030.safetensors,
+    model-00005-of-00030.safetensors,
+    model-00006-of-00030.safetensors,
+    model-00007-of-00030.safetensors,
+    model-00008-of-00030.safetensors,
+    model-00009-of-00030.safetensors,
+    model-00010-of-00030.safetensors,
+    model-00011-of-00030.safetensors,
+    model-00012-of-00030.safetensors,
+    model-00013-of-00030.safetensors,
+    model-00014-of-00030.safetensors,
+    model-00015-of-00030.safetensors,
+    model-00016-of-00030.safetensors,
+    model-00017-of-00030.safetensors,
+    model-00018-of-00030.safetensors,
+    model-00019-of-00030.safetensors,
+    model-00020-of-00030.safetensors,
+    model-00021-of-00030.safetensors,
+    model-00022-of-00030.safetensors,
+    model-00023-of-00030.safetensors,
+    model-00024-of-00030.safetensors,
+    model-00025-of-00030.safetensors,
+    model-00026-of-00030.safetensors,
+    model-00027-of-00030.safetensors,
+    model-00028-of-00030.safetensors,
+    model-00029-of-00030.safetensors,
+    model-00030-of-00030.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3.1-70B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora-llama3_1-finetune-output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_full_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_1/8B_full_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_full_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_1/8B_full_single_device.yaml	2025-02-20 17:49:29.282023805 -0500
@@ -0,0 +1,106 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Llama3.1 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config llama3_1/8B_full_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config llama3_1/8B_full_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_1.llama3_1_8b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW8bit
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+optimizer_in_bwd: True
+compile: False # set it to True for better memory and performance
+
+# Training environment
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/full-llama3.1-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: True
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 1
+  warmup_steps: 2
+  active_steps: 1
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_full.yaml marc/third_party/torchtune/recipes/configs/llama3_1/8B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_1/8B_full.yaml	2025-02-20 17:49:29.282023805 -0500
@@ -0,0 +1,82 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Llama3.1 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3_1/8B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3_1/8B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 8B_full_single_device.yaml for those cases
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_1.llama3_1_8b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+
+optimizer:
+  _component_: torch.optim.AdamW
+  lr: 2e-5
+  fused: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+custom_sharded_layers: ['tok_embeddings', 'output']
+compile: False # set it to True for better memory and performance
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/full-llama3.1-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_1/8B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_1/8B_lora_single_device.yaml	2025-02-20 17:49:29.290023818 -0500
@@ -0,0 +1,115 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Llama3.1 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_1.lora_llama3_1_8b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 64
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_lora.yaml marc/third_party/torchtune/recipes/configs/llama3_1/8B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_1/8B_lora.yaml	2025-02-20 17:49:29.286023812 -0500
@@ -0,0 +1,88 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Llama3.1 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3_1/8B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3_1/8B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 8B_lora_single_device.yaml
+# or 8B_qlora_single_device.yaml
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_1.lora_llama3_1_8b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 32
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_1/8B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_1/8B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_1/8B_qlora_single_device.yaml	2025-02-20 17:49:29.294023825 -0500
@@ -0,0 +1,117 @@
+# Config for single device QLoRA with lora_finetune_single_device.py
+# using a Llama3.1 8B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3_1/8B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config llama3_1/8B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_1.qlora_llama3_1_8b
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 16
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/qlora_finetune_output/
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Offloading
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
+
+# For colab use True
+low_cpu_ram: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_full_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_2/1B_full_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_full_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/1B_full_single_device.yaml	2025-02-20 17:49:29.302023839 -0500
@@ -0,0 +1,103 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Llama3.2 1B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config llama3_2/1B_full_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config llama3_2/1B_full_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-1B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.llama3_2_1b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-1B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW8bit
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+optimizer_in_bwd: True
+compile: False # set it to True for better memory and performance
+
+# Training environment
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/full-llama3.2-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: True
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 1
+  warmup_steps: 2
+  active_steps: 1
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_full.yaml marc/third_party/torchtune/recipes/configs/llama3_2/1B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/1B_full.yaml	2025-02-20 17:49:29.298023832 -0500
@@ -0,0 +1,78 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Llama3.2 1B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3_2/1B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3_2/1B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 1B_full_single_device.yaml for those cases
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-1B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.llama3_2_1b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-1B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+
+optimizer:
+  _component_: torch.optim.AdamW
+  lr: 2e-5
+  fused: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: False
+compile: False # set it to True for better memory and performance
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/full-llama3.2-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_2/1B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/1B_lora_single_device.yaml	2025-02-20 17:49:29.306023844 -0500
@@ -0,0 +1,112 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Llama3.2 1B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3_2/1B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config llama3_2/1B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.lora_llama3_2_1b
+  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-1B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-1B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: False
+enable_activation_offloading: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_lora.yaml marc/third_party/torchtune/recipes/configs/llama3_2/1B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/1B_lora.yaml	2025-02-20 17:49:29.306023844 -0500
@@ -0,0 +1,85 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Llama3.2 1B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3_2/1B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3_2/1B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 1B_lora_single_device.yaml
+# or 1B_qlora_single_device.yaml
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-1B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.lora_llama3_2_1b
+  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-1B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_2/1B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/1B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/1B_qlora_single_device.yaml	2025-02-20 17:49:29.310023851 -0500
@@ -0,0 +1,111 @@
+# Config for single device QLoRA with lora_finetune_single_device.py
+# using a Llama3.2 1B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3_2/1B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config llama3_2/1B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.qlora_llama3_2_1b
+  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-1B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-1B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: False
+enable_activation_offloading: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_full_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_2/3B_full_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_full_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/3B_full_single_device.yaml	2025-02-20 17:49:29.318023864 -0500
@@ -0,0 +1,104 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Llama3.2 3B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-3B-Instruct --output-dir /tmp/Llama-3.2-3B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config llama3_2/3B_full_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config llama3_2/3B_full_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-3B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.llama3_2_3b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-3B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-3B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW8bit
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+optimizer_in_bwd: True
+compile: False # set it to True for better memory and performance
+
+# Training environment
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/full-llama3.2-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: True
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 1
+  warmup_steps: 2
+  active_steps: 1
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_full.yaml marc/third_party/torchtune/recipes/configs/llama3_2/3B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/3B_full.yaml	2025-02-20 17:49:29.314023858 -0500
@@ -0,0 +1,78 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Llama3.2 3B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-3B-Instruct --output-dir /tmp/Llama-3.2-3B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3_2/3B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3_2/3B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 3B_full_single_device.yaml for those cases
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-3B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.llama3_2_3b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-3B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-3B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+
+optimizer:
+  _component_: torch.optim.AdamW
+  lr: 2e-5
+  fused: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+compile: False # set it to True for better memory and performance
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/full-llama3.2-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_2/3B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/3B_lora_single_device.yaml	2025-02-20 17:49:29.326023878 -0500
@@ -0,0 +1,113 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Llama3.2 3B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-3B-Instruct --output-dir /tmp/Llama-3.2-3B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3_2/3B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config llama3_2/3B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.lora_llama3_2_3b
+  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-3B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-3B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-3B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_lora.yaml marc/third_party/torchtune/recipes/configs/llama3_2/3B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/3B_lora.yaml	2025-02-20 17:49:29.322023871 -0500
@@ -0,0 +1,86 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Llama3.2 3B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-3B-Instruct --output-dir /tmp/Llama-3.2-3B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3_2/3B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3_2/3B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 3B_lora_single_device.yaml
+# or 3B_qlora_single_device.yaml
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-3B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.lora_llama3_2_3b
+  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-3B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-3B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_2/3B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/3B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/3B_qlora_single_device.yaml	2025-02-20 17:49:29.330023885 -0500
@@ -0,0 +1,112 @@
+# Config for single device QLoRA with lora_finetune_single_device.py
+# using a Llama3.2 3B Instruct model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download meta-llama/Llama-3.2-3B-Instruct --output-dir /tmp/Llama-3.2-3B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3_2/3B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config llama3_2/3B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.qlora_llama3_2_3b
+  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-3B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-3B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-3B-Instruct/
+  model_type: LLAMA3_2
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False # set it to True for better memory and performance
+
+# Logging
+output_dir: /tmp/lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2/knowledge_distillation_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_2/knowledge_distillation_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2/knowledge_distillation_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2/knowledge_distillation_single_device.yaml	2025-02-20 17:49:29.330023885 -0500
@@ -0,0 +1,132 @@
+# Config for single device knowledge distillation (KD) in knowledge_distillation_single_device.py
+# using a LLAMA3 teacher and student model
+#
+# This config assumes that you've ran the following commands before launching KD:
+# First download the student and teacher models
+#   tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#   tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth"
+#
+# You get better results using KD if the teacher model has already been fine-tuned on the target dataset:
+#   tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device
+#
+# To launch on a single device, run the following command from root:
+#   tune run knowledge_distillation_single_device --config llama3_2/knowledge_distillation_single_device
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.llama3_2.lora_llama3_2_1b
+  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 64
+  lora_alpha: 128
+  lora_dropout: 0.0
+
+teacher_model:
+  _component_: torchtune.models.llama3_1.llama3_1_8b
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.llama3.llama3_tokenizer
+  path: /tmp/Llama-3.2-1B-Instruct/original/tokenizer.model
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-1B-Instruct/
+  model_type: LLAMA3
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Teacher checkpoint
+teacher_checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
+  model_type: LLAMA3
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+kd_loss:
+  _component_: torchtune.modules.loss.ForwardKLWithChunkedOutputLoss
+kd_ratio: 0.5
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 32
+compile: False
+
+# Logging
+output_dir: /tmp/kd_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: False
+enable_activation_offloading: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/11B_full_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_2_vision/11B_full_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/11B_full_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2_vision/11B_full_single_device.yaml	2025-02-20 17:49:29.342023904 -0500
@@ -0,0 +1,104 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Llama3.2 11B Vision Instruct model
+#
+# This config assumes that you've run the following command before launching:
+#   tune download meta-llama/Llama-3.2-11B-Vision-Instruct --output-dir /tmp/Llama-3.2-11B-Vision-Instruct
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with:
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config llama3_2_vision/11B_full_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training:
+#   tune run full_finetune_single_device --config llama3_2_vision/11B_full_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model arguments
+model:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_11b
+  decoder_trainable: False
+  encoder_trainable: True
+  fusion_trainable: True
+  image_size: 560 # Make sure this matches the image_size in tokenizer
+
+# Transform
+tokenizer:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_transform
+  path: /tmp/Llama-3.2-11B-Vision-Instruct/original/tokenizer.model
+  image_size: 560
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-11B-Vision-Instruct/original/
+  checkpoint_files: [consolidated.pth]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-11B-Vision-Instruct/
+  model_type: LLAMA3_VISION
+resume_from_checkpoint: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.multimodal.the_cauldron_dataset
+  subset: ocrvqa
+seed: null
+shuffle: True
+collate_fn: torchtune.data.padded_collate_tiled_images_and_mask
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 16
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW8bit
+  lr: 2e-5
+optimizer_in_bwd: False
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+clip_grad_norm: 1.0
+compile: False # set it to True for better memory and performance
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+dtype: bf16
+
+# Logging
+output_dir: /tmp/full-llama3.2-vision--finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/Llama-3.2-11B-Vision-Instruct/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Profiler (default is disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: True
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 1
+  warmup_steps: 2
+  active_steps: 1
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/11B_full.yaml marc/third_party/torchtune/recipes/configs/llama3_2_vision/11B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/11B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2_vision/11B_full.yaml	2025-02-20 17:49:29.338023897 -0500
@@ -0,0 +1,78 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Llama3.2 11B Vision Instruct model
+#
+# This config assumes that you've run the following command before launching:
+#   tune download meta-llama/Llama-3.2-11B-Vision-Instruct --output-dir /tmp/Llama-3.2-11B-Vision-Instruct
+#
+# To launch on a single device, run the following command from root:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config llama3_2_vision/11B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training:
+#    tune run --nproc_per_node 4 full_finetune_distributed --config llama3_2_vision/11B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 11B_full_single_device.yaml for those cases.
+
+# Model arguments
+model:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_11b
+  decoder_trainable: False
+  encoder_trainable: True
+  fusion_trainable: True
+  image_size: 560 # Make sure this matches the image_size in tokenizer
+
+# Transform
+tokenizer:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_transform
+  path: /tmp/Llama-3.2-11B-Vision-Instruct/original/tokenizer.model
+  image_size: 560
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-11B-Vision-Instruct/original/
+  checkpoint_files: [consolidated.pth]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-11B-Vision-Instruct/
+  model_type: LLAMA3_VISION
+resume_from_checkpoint: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.multimodal.the_cauldron_dataset
+  subset: ocrvqa
+seed: null
+shuffle: True
+collate_fn: torchtune.data.padded_collate_tiled_images_and_mask
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 4
+optimizer:
+  _component_: torch.optim.AdamW
+  lr: 2e-5
+  fused: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+clip_grad_norm: 1.0
+compile: False # set it to True for better memory and performance
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+custom_sharded_layers: ['tok_embeddings', 'output']
+dtype: bf16
+
+# Logging
+output_dir: /tmp/full-llama3.2-vision--finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/Llama-3.2-11B-Vision-Instruct/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/11B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/llama3_2_vision/11B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/11B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2_vision/11B_lora_single_device.yaml	2025-02-20 17:49:29.346023910 -0500
@@ -0,0 +1,112 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Llama3.2 11B Vision Instruct model
+#
+# This config assumes that you've run the following command before launching:
+#   tune download meta-llama/Llama-3.2-11B-Vision-Instruct --output-dir /tmp/Llama-3.2-11B-Vision-Instruct
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config llama3_2_vision/11B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training:
+#   tune run lora_finetune_single_device --config llama3_2_vision/11B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model arguments
+model:
+  _component_: torchtune.models.llama3_2_vision.lora_llama3_2_vision_11b
+  decoder_trainable: "frozen"
+  encoder_trainable: "lora"
+  fusion_trainable: "lora"
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+  image_size: 560 # Make sure this matches the image_size in tokenizer
+
+# Transform
+tokenizer:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_transform
+  path: /tmp/Llama-3.2-11B-Vision-Instruct/original/tokenizer.model
+  image_size: 560
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-11B-Vision-Instruct/original/
+  checkpoint_files: [consolidated.pth]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-11B-Vision-Instruct/
+  model_type: LLAMA3_VISION
+resume_from_checkpoint: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.multimodal.the_cauldron_dataset
+  subset: ocrvqa
+seed: null
+shuffle: True
+collate_fn: torchtune.data.padded_collate_tiled_images_and_mask
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 16
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 2e-5
+optimizer_in_bwd: False
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+clip_grad_norm: 1.0
+compile: False # set it to True for better memory and performance
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+dtype: bf16
+
+# Logging
+output_dir: /tmp/full-llama3.2-vision-finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/Llama-3.2-11B-Vision-Instruct/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Profiler (disabled)
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: True
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 1
+  warmup_steps: 2
+  active_steps: 1
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/11B_lora.yaml marc/third_party/torchtune/recipes/configs/llama3_2_vision/11B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/11B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2_vision/11B_lora.yaml	2025-02-20 17:49:29.342023904 -0500
@@ -0,0 +1,88 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Llama3.2 11B Vision Instruct model
+#
+# This config assumes that you've run the following command before launching:
+#   tune download meta-llama/Llama-3.2-11B-Vision-Instruct --output-dir /tmp/Llama-3.2-11B-Vision-Instruct
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3_2_vision/11B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training:
+#   tune run --nproc_per_node 2 lora_finetune_distributed --config llama3_2_vision/11B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 11B_lora_single_device.yaml
+# or 11B_qlora_single_device.yaml
+
+# Model arguments
+model:
+  _component_: torchtune.models.llama3_2_vision.lora_llama3_2_vision_11b
+  decoder_trainable: "frozen"
+  encoder_trainable: "lora"
+  fusion_trainable: "lora"
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+  image_size: 560 # Make sure this matches the image_size in tokenizer
+
+# Transform
+tokenizer:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_transform
+  path: /tmp/Llama-3.2-11B-Vision-Instruct/original/tokenizer.model
+  image_size: 560
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-11B-Vision-Instruct/original/
+  checkpoint_files: [consolidated.pth]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-3.2-11B-Vision-Instruct/
+  model_type: LLAMA3_VISION
+resume_from_checkpoint: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.multimodal.the_cauldron_dataset
+  subset: ocrvqa
+seed: null
+shuffle: True
+collate_fn: torchtune.data.padded_collate_tiled_images_and_mask
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 4
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 2e-5
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+clip_grad_norm: 1.0
+compile: False # set it to True for better memory and performance
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+dtype: bf16
+
+# Logging
+output_dir: /tmp/full-llama3.2-vision-finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/Llama-3.2-11B-Vision-Instruct/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/evaluation.yaml marc/third_party/torchtune/recipes/configs/llama3_2_vision/evaluation.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/evaluation.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2_vision/evaluation.yaml	2025-02-20 17:49:29.350023917 -0500
@@ -0,0 +1,48 @@
+# Config for running the InferenceRecipe in generate.py to generate output from an LLM
+#
+# This config assumes that you've run the following command before launching:
+#   tune download meta-llama/Llama-3.2-11B-Vision-Instruct --output-dir /tmp/Llama-3.2-11B-Vision-Instruct
+#
+# It also assumes that you've downloaded the EleutherAI Eval Harness (v0.4.5):
+#   pip install lm_eval==0.4.5
+#
+# To launch, run the following command from root torchtune directory:
+#    tune run eleuther_eval --config llama3_2_vision/evaluation
+
+# Model arguments
+model:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_11b
+
+# Transform arguments
+tokenizer:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_transform
+  path: /tmp/Llama-3.2-11B-Vision-Instruct/original/tokenizer.model
+  max_seq_len: 8192 # Limit the size of our inputs
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-11B-Vision-Instruct/original
+  checkpoint_files: [consolidated.pth]
+  output_dir: ./
+  model_type: LLAMA3_VISION
+
+# Environment
+device: cuda
+dtype: bf16
+seed: 1234 # It is not recommended to change this seed, b/c it matches EleutherAI's default seed
+log_level: INFO
+
+# EleutherAI specific eval args
+# Llama3.2 vision reports on MMMU Val using chain-of-thought reasoning
+# and image concatenation. This is not currently supported in the EletherAI
+# Eval Harness so results may not match the paper OOTB
+tasks: ["mmmu_val_science"] # Defaulting to science as a good subset
+limit: null
+batch_size: 1
+enable_kv_cache: True
+max_seq_length: 8192
+
+# Quantization specific args
+# Quantization is not supported in this specific config
+quantizer: null
diff -ruN marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/generation_v2.yaml marc/third_party/torchtune/recipes/configs/llama3_2_vision/generation_v2.yaml
--- marc_original/third_party/torchtune/recipes/configs/llama3_2_vision/generation_v2.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/llama3_2_vision/generation_v2.yaml	2025-02-20 17:49:29.354023924 -0500
@@ -0,0 +1,43 @@
+# Config for running the InferenceRecipe in dev/generate_v2.py to generate output
+# from a Llama3.2 11B Vision Instruct model
+#
+# This config assumes that you've run the following command before launching:
+#   tune download meta-llama/Llama-3.2-11B-Vision-Instruct --output-dir /tmp/Llama-3.2-11B-Vision-Instruct
+#
+# To launch, run the following command from root torchtune directory:
+#    tune run dev/generate_v2 --config llama3_2_vision/generation_v2
+
+# Model arguments
+model:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_11b
+
+# Transform arguments
+tokenizer:
+  _component_: torchtune.models.llama3_2_vision.llama3_2_vision_transform
+  path: /tmp/Llama-3.2-11B-Vision-Instruct/original/tokenizer.model
+  prompt_template: null
+  max_seq_len: 8192
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelMetaCheckpointer
+  checkpoint_dir: /tmp/Llama-3.2-11B-Vision-Instruct/original
+  checkpoint_files: [consolidated.pth]
+  output_dir: ./
+  model_type: LLAMA3_VISION
+
+# Device
+device: cuda
+dtype: bf16
+seed: 1234
+log_level: INFO
+
+# Generation arguments
+prompt:
+  system: You are a helpful assistant who responds like the author Shakespeare.
+  user:
+    image: https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg
+    text: What is in this image?
+max_new_tokens: 200
+temperature: 0.6 # 0.8 and 0.6 are popular values to try
+top_k: 300
diff -ruN marc_original/third_party/torchtune/recipes/configs/mistral/7B_full_low_memory.yaml marc/third_party/torchtune/recipes/configs/mistral/7B_full_low_memory.yaml
--- marc_original/third_party/torchtune/recipes/configs/mistral/7B_full_low_memory.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/mistral/7B_full_low_memory.yaml	2025-02-20 17:49:29.362023936 -0500
@@ -0,0 +1,84 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Mistral 7B model
+#
+# This config uses hyperparameters based on small set of experiments and information
+# available on various forums. These are not meant to replicate the numbers
+# from the paper
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download mistralai/Mistral-7B-v0.1 --hf-token <HF_TOKEN> --output-dir /tmp/Mistral-7B-v0.1
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config mistral/7B_full_low_memory
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config mistral/7B_full_low_memory checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.mistral.mistral_tokenizer
+  path: /tmp/Mistral-7B-v0.1/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.mistral.mistral_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Mistral-7B-v0.1
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Mistral-7B-v0.1/
+  model_type: MISTRAL
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW
+  lr: 5e-6
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+optimizer_in_bwd: True
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Model compilation
+compile: False
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Mistral-7B-v0.1/
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/mistral/7B_full_ppo_low_memory.yaml marc/third_party/torchtune/recipes/configs/mistral/7B_full_ppo_low_memory.yaml
--- marc_original/third_party/torchtune/recipes/configs/mistral/7B_full_ppo_low_memory.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/mistral/7B_full_ppo_low_memory.yaml	2025-02-20 17:49:29.366023943 -0500
@@ -0,0 +1,181 @@
+# Config for single device RLHF full finetuning using PPO in ppo_full_finetune_single_device.py
+# using a Mistral 7B model.
+#
+# This config has been tested on an A100 80GB.
+# This config uses hyperparameters based on small set of experiments and information
+# available from existing implementations.
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download weqweasdas/RM-Mistral-7B --output-dir /tmp/RM-Mistral-7B/ --ignore-patterns None
+#   tune download mistralai/Mistral-7B-Instruct-v0.2 --output-dir /tmp/Mistral-7B-Instruct-v0.2/ --hf-token HF_TOKEN
+#
+# You'll also need to ensure that {output_dir} exists beforehand, as checkpoints for policy and value models are saved in sub-folders.
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run ppo_full_finetune_single_device --config mistral/7B_full_ppo_low_memory
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run ppo_full_finetune_single_device --config mistral/7B_full_ppo_low_memory checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.mistral.mistral_tokenizer
+  path: /tmp/Mistral-7B-Instruct-v0.2/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.text_completion_dataset
+  source: trl-internal-testing/sentiment-trl-style
+  split: train
+  column: prompt
+  add_eos: False
+
+policy_model:
+  _component_: torchtune.models.mistral.mistral_7b
+
+# we need to manually build the mistral classifier model
+# because our reward model checkpoint has a larger vocabulary size (due to an added padding token)
+reward_and_value_model:
+  _component_: torchtune.models.mistral._component_builders.mistral_classifier
+  attn_dropout: 0.0
+  embed_dim: 4096
+  intermediate_dim: 14336
+  max_seq_len: 32768
+  norm_eps: 1.0e-05
+  num_classes: 1
+  num_heads: 32
+  num_kv_heads: 8
+  num_layers: 32
+  vocab_size: 32001
+
+# checkpointer for the policy model - update this if resuming from checkpoint
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Mistral-7B-Instruct-v0.2/
+  checkpoint_files:
+    [
+      "pytorch_model-00001-of-00003.bin",
+      "pytorch_model-00002-of-00003.bin",
+      "pytorch_model-00003-of-00003.bin",
+    ]
+  # this is the only place where you should update `recipe_checkpoint` if resuming training
+  recipe_checkpoint: null
+  output_dir: ${output_dir}/policy
+  model_type: MISTRAL
+
+# this should be setup identically to the policy model checkpointer at the start of training
+# ensure `checkpoint_files` always points to the original policy weights, even if resuming training
+ref_policy_checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Mistral-7B-Instruct-v0.2/
+  checkpoint_files:
+    [
+      "pytorch_model-00001-of-00003.bin",
+      "pytorch_model-00002-of-00003.bin",
+      "pytorch_model-00003-of-00003.bin",
+    ]
+  output_dir: ${output_dir}/policy
+  model_type: MISTRAL
+
+# checkpointer for the value model - update `checkpoint_files` if resuming from checkpoint
+# since this model will be identical to the reward model it's helpful to initialise this
+# from the trained reward model weights
+value_checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/RM-Mistral-7B/
+  checkpoint_files:
+    [
+      "model-00001-of-00003.safetensors",
+      "model-00002-of-00003.safetensors",
+      "model-00003-of-00003.safetensors",
+    ]
+  output_dir: ${output_dir}/value
+  model_type: REWARD
+
+# checkpointer for the reward model, ensure `checkpoint_files`
+# always points to the original reward model weights, even if resuming training
+reward_checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/RM-Mistral-7B/
+  checkpoint_files:
+    [
+      "model-00001-of-00003.safetensors",
+      "model-00002-of-00003.safetensors",
+      "model-00003-of-00003.safetensors",
+    ]
+  output_dir: ${output_dir}/value
+  model_type: REWARD
+
+resume_from_checkpoint: False
+output_dir: /tmp/mistral7b-ppo-finetune
+seed: null
+shuffle: True
+
+# Training env
+device: cuda
+
+# Training arguments
+batch_size: 64
+num_steps: 10000
+ppo_epochs: 2
+ppo_batch_size: 32
+gradient_accumulation_steps: 1
+
+# Memory management and performance
+compile: True
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW
+  lr: 3e-6
+optimizer_in_bwd: True
+log_peak_memory_stats: False
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# batch size for forward pass during generation
+forward_batch_size: 16
+max_generated_tokens: 58
+temperature: 0.7
+top_k: null
+
+# parameter for penalising generations shorter than `min_response_length`
+min_response_length: 18
+# parameter for penalising generations without a stop token
+penalise_no_eos: True
+# scalar penalty to apply when penalising
+reward_penalty: -3
+
+# tokens to consider as "end of sequence" tokens
+stop_token_ids: [
+    2, # eos_id
+    28723, # mistral "." token
+  ]
+whiten_rewards: False
+
+# GAE hyperparameters
+gamma: 1
+lmbda: 0.95
+
+# PPO hyperparameters
+loss:
+  _component_: torchtune.rlhf.loss.PPOLoss
+  epsilon: 0.2
+  value_coeff: 0.1
+  value_clip_range: 0.2
+kl_coeff: 0.01
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+
+log_every_n_steps: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/mistral/7B_full.yaml marc/third_party/torchtune/recipes/configs/mistral/7B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/mistral/7B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/mistral/7B_full.yaml	2025-02-20 17:49:29.358023930 -0500
@@ -0,0 +1,79 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Mistral 7B model
+#
+# This config uses hyperparameters based on small set of experiments and information
+# available on various forums. These are not meant to replicate the numbers
+# from the paper
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download mistralai/Mistral-7B-v0.1 --hf-token <HF_TOKEN> --output-dir /tmp/Mistral-7B-v0.1
+#
+# Run this config on 4 GPUs using the following:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config mistral/7B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config mistral/7B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 7B_full_single_device.yaml for those cases
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.mistral.mistral_tokenizer
+  path: /tmp/Mistral-7B-v0.1/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.mistral.mistral_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Mistral-7B-v0.1
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Mistral-7B-v0.1/
+  model_type: MISTRAL
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 5e-6
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Mistral-7B-v0.1/
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/mistral/7B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/mistral/7B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/mistral/7B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/mistral/7B_lora_single_device.yaml	2025-02-20 17:49:29.374023956 -0500
@@ -0,0 +1,118 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Mistral 7B model
+#
+# This config uses hyperparameters based on small set of experiments and information
+# available on various forums. These are not meant to replicate the numbers
+# from the paper
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download mistralai/Mistral-7B-v0.1 --hf-token <HF_TOKEN> --output-dir /tmp/Mistral-7B-v0.1
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config mistral/7B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config mistral/7B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.mistral.mistral_tokenizer
+  path: /tmp/Mistral-7B-v0.1/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.mistral.lora_mistral_7b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: True
+  lora_rank: 64
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Mistral-7B-v0.1
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Mistral-7B-v0.1
+  model_type: MISTRAL
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Mistral-7B-v0.1
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/mistral/7B_lora.yaml marc/third_party/torchtune/recipes/configs/mistral/7B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/mistral/7B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/mistral/7B_lora.yaml	2025-02-20 17:49:29.370023950 -0500
@@ -0,0 +1,93 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Mistral 7B model
+#
+# This config uses hyperparameters based on small set of experiments and information
+# available on various forums. These are not meant to replicate the numbers
+# from the paper
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download mistralai/Mistral-7B-v0.1 --hf-token <HF_TOKEN> --output-dir /tmp/Mistral-7B-v0.1
+#
+# Run this config on 2 GPUs using the following:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config mistral/7B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config mistral/7B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 7B_lora_single_device.yaml
+# or 7B_qlora_single_device.yaml for those cases
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.mistral.mistral_tokenizer
+  path: /tmp/Mistral-7B-v0.1/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.mistral.lora_mistral_7b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: True
+  lora_rank: 64
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Mistral-7B-v0.1
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Mistral-7B-v0.1
+  model_type: MISTRAL
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Mistral-7B-v0.1
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/mistral/7B_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/mistral/7B_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/mistral/7B_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/mistral/7B_qlora_single_device.yaml	2025-02-20 17:49:29.378023963 -0500
@@ -0,0 +1,122 @@
+# Config for single device QLoRA finetuning in lora_finetune_single_device.py
+# using a Mistral 7B model
+#
+# This config uses hyperparameters based on small set of experiments and information
+# available on various forums. These are not meant to replicate the numbers
+# from the paper
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download mistralai/Mistral-7B-v0.1 --hf-token <HF_TOKEN> --output-dir /tmp/Mistral-7B-v0.1
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config mistral/7B_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config mistral/7B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.mistral.mistral_tokenizer
+  path: /tmp/Mistral-7B-v0.1/tokenizer.model
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.mistral.qlora_mistral_7b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 64
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Mistral-7B-v0.1
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Mistral-7B-v0.1
+  model_type: MISTRAL
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Fine-tuning arguments
+batch_size: 4
+epochs: 3
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Mistral-7B-v0.1
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
+
+# For colab use True
+low_cpu_ram: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/mistral/evaluation.yaml marc/third_party/torchtune/recipes/configs/mistral/evaluation.yaml
--- marc_original/third_party/torchtune/recipes/configs/mistral/evaluation.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/mistral/evaluation.yaml	2025-02-20 17:49:29.382023970 -0500
@@ -0,0 +1,41 @@
+# Config for EleutherEvalRecipe in eleuther_eval.py
+#
+# To launch, run the following command:
+#    tune run eleuther_eval --config mistral/evaluation
+
+# Model Arguments
+model:
+  _component_: torchtune.models.mistral.mistral_7b
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Mistral-7B-v0.1/
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin
+  ]
+  output_dir: /tmp/Mistral-7B-v0.1/
+  model_type: MISTRAL
+resume_from_checkpoint: False
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.mistral.mistral_tokenizer
+  path: /tmp/Mistral-7B-v0.1/tokenizer.model
+  max_seq_len: null
+
+# Environment
+device: cuda
+dtype: bf16
+seed: 1234 # It is not recommended to change this seed, b/c it matches EleutherAI's default seed
+
+# EleutherAI specific eval args
+tasks: ["truthfulqa_mc2"]
+limit: null
+max_seq_length: 4096
+batch_size: 8
+enable_kv_cache: True
+
+# Quantization specific args
+quantizer: null
diff -ruN marc_original/third_party/torchtune/recipes/configs/phi3/evaluation.yaml marc/third_party/torchtune/recipes/configs/phi3/evaluation.yaml
--- marc_original/third_party/torchtune/recipes/configs/phi3/evaluation.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/phi3/evaluation.yaml	2025-02-20 17:49:29.386023976 -0500
@@ -0,0 +1,42 @@
+# Config for EleutherEvalRecipe in eleuther_eval.py
+#
+# To launch, run the following command:
+#    tune run eleuther_eval --config phi3/evaluation
+
+# Model Arguments
+model:
+  _component_: torchtune.models.phi3.phi3_mini
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Phi-3-mini-4k-instruct
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Phi-3-mini-4k-instruct
+  model_type: PHI3_MINI
+resume_from_checkpoint: False
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.phi3.phi3_mini_tokenizer
+  path: /tmp/Phi-3-mini-4k-instruct/tokenizer.model
+  max_seq_len: null
+
+# Environment
+device: cuda
+dtype: bf16
+seed: 1234 # It is not recommended to change this seed, b/c it matches EleutherAI's default seed
+
+# EleutherAI specific eval args
+tasks: ["truthfulqa_mc2"]
+limit: null
+max_seq_length: 4096
+batch_size: 8
+enable_kv_cache: True
+
+# Quantization specific args
+quantizer: null
diff -ruN marc_original/third_party/torchtune/recipes/configs/phi3/mini_full_low_memory.yaml marc/third_party/torchtune/recipes/configs/phi3/mini_full_low_memory.yaml
--- marc_original/third_party/torchtune/recipes/configs/phi3/mini_full_low_memory.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/phi3/mini_full_low_memory.yaml	2025-02-20 17:49:29.394023989 -0500
@@ -0,0 +1,77 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Phi3 Mini 4K Instruct
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download microsoft/Phi-3-mini-4k-instruct --output-dir /tmp/Phi-3-mini-4k-instruct --ignore-patterns None --hf-token <HF_TOKEN>
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config phi3/mini_full_low_memory
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config phi3/mini_full_low_memory checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model arguments
+model:
+  _component_: torchtune.models.phi3.phi3_mini
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.phi3.phi3_mini_tokenizer
+  path: /tmp/Phi-3-mini-4k-instruct/tokenizer.model
+  max_seq_len: null
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Phi-3-mini-4k-instruct
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Phi-3-mini-4k-instruct
+  model_type: PHI3_MINI
+resume_from_checkpoint: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 1
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW
+  lr: 5e-6
+optimizer_in_bwd: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+dtype: bf16
+
+# Logging
+output_dir: /tmp/phi3_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/Phi-3-mini-4k-instruct/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/phi3/mini_full.yaml marc/third_party/torchtune/recipes/configs/phi3/mini_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/phi3/mini_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/phi3/mini_full.yaml	2025-02-20 17:49:29.390023983 -0500
@@ -0,0 +1,74 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Phi3 Mini 4K Instruct
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download microsoft/Phi-3-mini-4k-instruct --output-dir /tmp/Phi-3-mini-4k-instruct --ignore-patterns None --hf-token <HF_TOKEN>
+#
+# Run this config on 4 GPUs using the following:
+#  tune run --nproc_per_node 4 full_finetune_distributed --config phi3/mini_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nproc_per_node 4 full_finetune_distributed --config phi3/mini_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use mini_low_memory.yaml for those cases
+
+# Model arguments
+model:
+  _component_: torchtune.models.phi3.phi3_mini
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.phi3.phi3_mini_tokenizer
+  path: /tmp/Phi-3-mini-4k-instruct/tokenizer.model
+  max_seq_len: null
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Phi-3-mini-4k-instruct
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Phi-3-mini-4k-instruct
+  model_type: PHI3_MINI
+resume_from_checkpoint: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 16
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 5e-6
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+dtype: bf16
+
+# Logging
+output_dir: /tmp/phi3_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/Phi-3-mini-4k-instruct/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/phi3/mini_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/phi3/mini_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/phi3/mini_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/phi3/mini_lora_single_device.yaml	2025-02-20 17:49:29.406024009 -0500
@@ -0,0 +1,113 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Phi3 mini (3.8B) model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download microsoft/Phi-3-mini-4k-instruct --output-dir /tmp/Phi-3-mini-4k-instruct --ignore-patterns None --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config phi3/mini_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config phi3/mini_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model arguments
+model:
+  _component_: torchtune.models.phi3.lora_phi3_mini
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.phi3.phi3_mini_tokenizer
+  path: /tmp/Phi-3-mini-4k-instruct/tokenizer.model
+  max_seq_len: null
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Phi-3-mini-4k-instruct
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Phi-3-mini-4k-instruct
+  model_type: PHI3_MINI
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 16
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+output_dir: /tmp/phi3_lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/Phi-3-mini-4k-instruct/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Showcase the usage of PyTorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: /tmp/Phi-3-mini-4k-instruct/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/phi3/mini_lora.yaml marc/third_party/torchtune/recipes/configs/phi3/mini_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/phi3/mini_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/phi3/mini_lora.yaml	2025-02-20 17:49:29.398023996 -0500
@@ -0,0 +1,85 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Phi3 mini (3.8B) model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download microsoft/Phi-3-mini-4k-instruct --output-dir /tmp/Phi-3-mini-4k-instruct --ignore-patterns None --hf-token <HF_TOKEN>
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config phi3/mini_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config phi3/mini_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use mini_lora_single_device.yaml
+# or mini_qlora_single_device.yaml
+
+# Model arguments
+model:
+  _component_: torchtune.models.phi3.lora_phi3_mini
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.phi3.phi3_mini_tokenizer
+  path: /tmp/Phi-3-mini-4k-instruct/tokenizer.model
+  max_seq_len: null
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Phi-3-mini-4k-instruct
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Phi-3-mini-4k-instruct
+  model_type: PHI3_MINI
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 16
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: False
+dtype: bf16
+
+# Logging
+output_dir: /tmp/phi3_lora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/Phi-3-mini-4k-instruct/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/phi3/mini_qlora_single_device.yaml marc/third_party/torchtune/recipes/configs/phi3/mini_qlora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/phi3/mini_qlora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/phi3/mini_qlora_single_device.yaml	2025-02-20 17:49:29.410024015 -0500
@@ -0,0 +1,116 @@
+# Config for single device QLoRA with lora_finetune_single_device.py
+# using a Phi3 mini (3.8B) model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download microsoft/Phi-3-mini-4k-instruct --output-dir /tmp/Phi-3-mini-4k-instruct --ignore-patterns None --hf-token <HF_TOKEN>
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config phi3/mini_qlora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config phi3/mini_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Model arguments
+model:
+  _component_: torchtune.models.phi3.qlora_phi3_mini
+  lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
+  apply_lora_to_mlp: True
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.phi3.phi3_mini_tokenizer
+  path: /tmp/Phi-3-mini-4k-instruct/tokenizer.model
+  max_seq_len: null
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Phi-3-mini-4k-instruct
+  checkpoint_files: [
+    model-00001-of-00002.safetensors,
+    model-00002-of-00002.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Phi-3-mini-4k-instruct
+  model_type: PHI3_MINI
+resume_from_checkpoint: False
+save_adapter_weights_only: False
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Fine-tuning arguments
+epochs: 1
+max_steps_per_epoch: null
+batch_size: 2
+gradient_accumulation_steps: 16
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+compile: False
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+output_dir: /tmp/phi3_qlora_finetune_output
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: /tmp/Phi-3-mini-4k-instruct/logs
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Showcase the usage of PyTorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  # Output directory of trace artifacts
+  output_dir: /tmp/Phi-3-mini-4k-instruct/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
+
+# For colab use True
+low_cpu_ram: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/quantization.yaml marc/third_party/torchtune/recipes/configs/quantization.yaml
--- marc_original/third_party/torchtune/recipes/configs/quantization.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/quantization.yaml	2025-02-20 17:49:29.414024022 -0500
@@ -0,0 +1,28 @@
+# Config for QuantizationRecipe in quantize.py
+#
+# To launch, run the following command from root torchtune directory:
+#    tune run quantize --config quantization
+
+#
+# Model arguments
+model:
+  _component_: torchtune.models.llama2.llama2_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Llama-2-7b-hf
+  checkpoint_files: [
+    pytorch_model-00001-of-00002.bin,
+    pytorch_model-00002-of-00002.bin,
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Llama-2-7b-hf
+  model_type: LLAMA2
+
+device: cuda
+dtype: bf16
+seed: 1234
+
+quantizer:
+  _component_: torchtune.training.quantization.Int8DynActInt4WeightQuantizer
+  groupsize: 256
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/0.5B_full_single_device.yaml marc/third_party/torchtune/recipes/configs/qwen2/0.5B_full_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/0.5B_full_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/0.5B_full_single_device.yaml	2025-02-20 17:49:29.422024035 -0500
@@ -0,0 +1,77 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Qwen2 0.5B
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-0.5B-Instruct --output-dir /tmp/Qwen2-0.5B-Instruct --ignore-patterns None
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config qwen2/0.5B_full_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config qwen2/0.5B_full_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-0.5B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.qwen2_0_5b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-0.5B-Instruct-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 1
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+optimizer_in_bwd: False
+
+max_steps_per_epoch: null
+gradient_accumulation_steps: 8
+compile: False
+
+# Training environment
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Qwen2-0.5B-Instruct-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/0.5B_full.yaml marc/third_party/torchtune/recipes/configs/qwen2/0.5B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/0.5B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/0.5B_full.yaml	2025-02-20 17:49:29.418024029 -0500
@@ -0,0 +1,76 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Qwen2 0.5B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-0.5B-Instruct --output-dir /tmp/Qwen2-0.5B-Instruct --ignore-patterns None
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config qwen2/0.5B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config qwen2/0.5B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 0.5B_full.yaml for those cases
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-0.5B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.qwen2_0_5b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-0.5B-Instruct-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 1
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 16
+
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Qwen2-0.5B-Instruct-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/0.5B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/qwen2/0.5B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/0.5B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/0.5B_lora_single_device.yaml	2025-02-20 17:49:29.430024049 -0500
@@ -0,0 +1,113 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Qwen2 0.5B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-0.5B-Instruct --output-dir /tmp/Qwen2-0.5B-Instruct --ignore-patterns None
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config qwen2/0.5B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config qwen2/0.5B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.lora_qwen2_0_5b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  lora_rank: 32
+  lora_alpha: 64
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-0.5B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-0.5B-Instruct-lora-finetune
+  model_type: QWEN2
+
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 2e-3
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+compile: False
+
+# Logging
+output_dir: /tmp/Qwen2-0.5B-Instruct-lora-finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/0.5B_lora.yaml marc/third_party/torchtune/recipes/configs/qwen2/0.5B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/0.5B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/0.5B_lora.yaml	2025-02-20 17:49:29.426024042 -0500
@@ -0,0 +1,113 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Qwen2 0.5B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-0.5B-Instruct --output-dir /tmp/Qwen2-0.5B-Instruct --ignore-patterns None
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config qwen2/0.5B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config qwen2/0.5B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 0.5B_lora_single_device.yaml
+# or 0.5B_qlora_single_device.yaml
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.lora_qwen2_0_5b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  lora_rank: 32
+  lora_alpha: 64
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-0.5B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-0.5B-Instruct-lora-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+
+seed: null
+shuffle: True
+batch_size: 4
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 2e-3
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 4
+
+# Logging
+output_dir: /tmp/Qwen2-0.5B-Instruct-lora-finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/1.5B_full_single_device.yaml marc/third_party/torchtune/recipes/configs/qwen2/1.5B_full_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/1.5B_full_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/1.5B_full_single_device.yaml	2025-02-20 17:49:29.438024061 -0500
@@ -0,0 +1,82 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Qwen2 1.5B
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-1.5B-Instruct --output-dir /tmp/Qwen2-1.5B-Instruct --ignore-patterns None
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config qwen2/1.5B_full_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config qwen2/1.5B_full_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-1.5B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-1.5B-Instruct/merges.txt
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.qwen2_1_5b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-1.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-1.5B-Instruct-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 1
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW
+  lr: 2e-5
+
+optimizer_in_bwd: True
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+compile: False
+
+# Training environment
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Qwen2-1.5B-Instruct-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/1.5B_full.yaml marc/third_party/torchtune/recipes/configs/qwen2/1.5B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/1.5B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/1.5B_full.yaml	2025-02-20 17:49:29.434024056 -0500
@@ -0,0 +1,76 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Qwen2 1.5B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-1.5B-Instruct --output-dir /tmp/Qwen2-1.5B-Instruct --ignore-patterns None
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config qwen2/1.5B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config qwen2/1.5B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 1.5B_full.yaml for those cases
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-1.5B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-1.5B-Instruct/merges.txt
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.qwen2_1_5b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-1.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-1.5B-Instruct-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 3
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: False
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Qwen2-1.5B-Instruct-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/1.5B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/qwen2/1.5B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/1.5B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/1.5B_lora_single_device.yaml	2025-02-20 17:49:29.446024075 -0500
@@ -0,0 +1,111 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Qwen2 1.5B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-1.5B-Instruct --output-dir /tmp/Qwen2-1.5B-Instruct --ignore-patterns None
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config qwen2/1.5B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config qwen2/1.5B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.lora_qwen2_1_5b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  lora_rank: 32
+  lora_alpha: 64
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-1.5B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-1.5B-Instruct/merges.txt
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-1.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-1.5B-Instruct-lora-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-3
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 8
+compile: False
+
+# Logging
+output_dir: /tmp/Qwen2-1.5B-Instruct-lora-finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Memory
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/1.5B_lora.yaml marc/third_party/torchtune/recipes/configs/qwen2/1.5B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/1.5B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/1.5B_lora.yaml	2025-02-20 17:49:29.442024068 -0500
@@ -0,0 +1,108 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Qwen2 1.5B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-1.5B-Instruct --output-dir /tmp/Qwen2-1.5B-Instruct --ignore-patterns None
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config qwen2/1.5B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config qwen2/1.5B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.lora_qwen2_1_5b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  lora_rank: 32
+  lora_alpha: 64
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-1.5B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-1.5B-Instruct/merges.txt
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-1.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-1.5B-Instruct-lora-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 2e-5
+
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 8
+
+# Logging
+output_dir: /tmp/Qwen2-1.5B-Instruct-lora-finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: True
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/7B_full_single_device.yaml marc/third_party/torchtune/recipes/configs/qwen2/7B_full_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/7B_full_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/7B_full_single_device.yaml	2025-02-20 17:49:29.454024088 -0500
@@ -0,0 +1,81 @@
+# Config for single device full finetuning in full_finetune_single_device.py
+# using a Qwen2 7B
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-7B-Instruct --output-dir /tmp/Qwen2-7B-Instruct --ignore-patterns None
+#
+# The default config uses an optimizer from bitsandbytes. If you do not have it installed,
+# you can install it with
+#   pip install bitsandbytes
+#
+# To launch on a single device, run the following command from root:
+#   tune run full_finetune_single_device --config qwen2/7B_full_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run full_finetune_single_device --config qwen2/7B_full_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-7B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-7B-Instruct/merges.txt
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.qwen2_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-7B-Instruct
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-7B-Instruct-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 1
+optimizer:
+  _component_: bitsandbytes.optim.PagedAdamW
+  lr: 5e-6
+optimizer_in_bwd: True
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 1
+compile: False
+
+# Training environment
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Qwen2-7B-Instruct-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/7B_full.yaml marc/third_party/torchtune/recipes/configs/qwen2/7B_full.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/7B_full.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/7B_full.yaml	2025-02-20 17:49:29.450024081 -0500
@@ -0,0 +1,79 @@
+# Config for multi-device full finetuning in full_finetune_distributed.py
+# using a Qwen2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-7B-Instruct --output-dir /tmp/Qwen2-7B-Instruct --ignore-patterns None
+#
+# To launch on 4 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config qwen2/7B_full
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 4 full_finetune_distributed --config qwen2/7B_full checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# Single device full finetuning requires more memory optimizations. It's
+# best to use 7B_full.yaml for those cases
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-7B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-7B-Instruct/merges.txt
+  max_seq_len: null
+
+# Dataset
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.qwen2_7b
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-7B-Instruct
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-7B-Instruct-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Fine-tuning arguments
+batch_size: 2
+epochs: 1
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  lr: 5e-6
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+max_steps_per_epoch: null
+gradient_accumulation_steps: 16
+
+
+# Training env
+device: cuda
+
+# Memory management
+enable_activation_checkpointing: True
+
+# Reduced precision
+dtype: bf16
+
+# Logging
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+output_dir: /tmp/Qwen2-7B-Instruct-finetune
+log_every_n_steps: 1
+log_peak_memory_stats: False
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/7B_lora_single_device.yaml marc/third_party/torchtune/recipes/configs/qwen2/7B_lora_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/7B_lora_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/7B_lora_single_device.yaml	2025-02-20 17:49:29.462024101 -0500
@@ -0,0 +1,115 @@
+# Config for single device LoRA finetuning in lora_finetune_single_device.py
+# using a Qwen2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-7B-Instruct --output-dir /tmp/Qwen2-7B-Instruct --ignore-patterns None
+#
+# To launch on a single device, run the following command from root:
+#   tune run lora_finetune_single_device --config qwen2/7B_lora_single_device
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run lora_finetune_single_device --config qwen2/7B_lora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works only for training on single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.lora_qwen2_7b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-7B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-7B-Instruct/merges.txt
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-7B-Instruct
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-7B-Instruct-lora-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 64
+compile: False
+
+# Logging
+output_dir: /tmp/Qwen2-7B-Instruct-lora-finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+
+# Activations Offloading
+enable_activation_checkpointing: True
+enable_activation_offloading: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/7B_lora.yaml marc/third_party/torchtune/recipes/configs/qwen2/7B_lora.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/7B_lora.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/7B_lora.yaml	2025-02-20 17:49:29.458024095 -0500
@@ -0,0 +1,114 @@
+# Config for multi-device LoRA finetuning in lora_finetune_distributed.py
+# using a Qwen2 7B model
+#
+# This config assumes that you've run the following command before launching
+# this run:
+#   tune download Qwen/Qwen2-7B-Instruct --output-dir /tmp/Qwen2-7B-Instruct --ignore-patterns None
+#
+# To launch on 2 devices, run the following command from root:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config qwen2/7B_lora
+#
+# You can add specific overrides through the command line. For example
+# to override the checkpointer directory while launching training
+# you can run:
+#   tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config qwen2/7B_lora checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
+#
+# This config works best when the model is being fine-tuned on 2+ GPUs.
+# For single device LoRA finetuning please use 7B_lora_single_device.yaml
+# or 7B_qlora_single_device.yaml
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.lora_qwen2_7b
+  lora_attn_modules: ['q_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  apply_lora_to_output: False
+  lora_rank: 8
+  lora_alpha: 16
+  lora_dropout: 0.0
+
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-7B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-7B-Instruct/merges.txt
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-7B-Instruct
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-7B-Instruct-lora-finetune
+  model_type: QWEN2
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 2
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  fused: True
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 32
+
+# Logging
+output_dir: /tmp/Qwen2-7B-Instruct-lora-finetune
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: False
+
+# Show case the usage of pytorch profiler
+# Set enabled to False as it's only needed for debugging training
+profiler:
+  _component_: torchtune.training.setup_torch_profiler
+
+  enabled: False
+
+  #Output directory of trace artifacts
+  output_dir: ${output_dir}/profiling_outputs
+
+  #`torch.profiler.ProfilerActivity` types to trace
+  cpu: True
+  cuda: True
+
+  #trace options passed to `torch.profiler.profile`
+  profile_memory: False
+  with_stack: False
+  record_shapes: True
+  with_flops: False
+
+  # `torch.profiler.schedule` options:
+  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+  wait_steps: 5
+  warmup_steps: 5
+  active_steps: 2
+  num_cycles: 1
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/evaluation.yaml marc/third_party/torchtune/recipes/configs/qwen2/evaluation.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/evaluation.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/evaluation.yaml	2025-02-20 17:49:29.466024107 -0500
@@ -0,0 +1,43 @@
+# Config for EleutherEvalRecipe in eleuther_eval.py
+#
+# To launch, run the following command:
+#    tune run eleuther_eval --config qwen2/evaluation
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.qwen2_7b
+
+# Checkpointer
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-7B-Instruct
+  checkpoint_files: [
+    model-00001-of-00004.safetensors,
+    model-00002-of-00004.safetensors,
+    model-00003-of-00004.safetensors,
+    model-00004-of-00004.safetensors
+  ]
+  output_dir: ./ # Not needed
+  model_type: QWEN2
+
+# Tokenizer
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-7B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-7B-Instruct/merges.txt
+  max_seq_len: null
+
+# Environment
+device: cuda
+dtype: bf16
+seed: 1234 # It is not recommended to change this seed, b/c it matches EleutherAI's default seed
+
+# EleutherAI specific eval args
+tasks: ["truthfulqa_mc2"]
+limit: null
+max_seq_length: 4096
+batch_size: 8
+enable_kv_cache: True
+
+# Quantization specific args
+quantizer: null
diff -ruN marc_original/third_party/torchtune/recipes/configs/qwen2/knowledge_distillation_single_device.yaml marc/third_party/torchtune/recipes/configs/qwen2/knowledge_distillation_single_device.yaml
--- marc_original/third_party/torchtune/recipes/configs/qwen2/knowledge_distillation_single_device.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/configs/qwen2/knowledge_distillation_single_device.yaml	2025-02-20 17:49:29.470024114 -0500
@@ -0,0 +1,97 @@
+# Config for single device knowledge distillation in kd_single_device.py
+# using a teacher and student model
+#
+# This config assumes that you've ran the following commands before launching KD:
+# First download the student and teacher models
+#   tune download Qwen/Qwen2-0.5B-Instruct --output-dir /tmp/Qwen2-0.5B-Instruct --ignore-patterns None
+#   tune download Qwen/Qwen2-1.5B-Instruct --output-dir /tmp/Qwen2-1.5B-Instruct --ignore-patterns None
+#
+# You get better results using KD if the teacher model has already been fine-tuned on the target dataset:
+#   tune run lora_finetune_single_device --config qwen2/1.5B_lora_single_device
+#
+# To launch on a single device, run the following command from root:
+#   tune run knowledge_distillation_single_device --config qwen2/knowledge_distillation_single_device
+#
+# This config works only for distilling on a single device.
+
+
+# Model Arguments
+model:
+  _component_: torchtune.models.qwen2.lora_qwen2_0_5b
+  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']
+  apply_lora_to_mlp: False
+  lora_rank: 32
+  lora_alpha: 64
+
+teacher_model:
+  _component_: torchtune.models.qwen2.qwen2_1_5b
+
+tokenizer:
+  _component_: torchtune.models.qwen2.qwen2_tokenizer
+  path: /tmp/Qwen2-0.5B-Instruct/vocab.json
+  merges_file: /tmp/Qwen2-0.5B-Instruct/merges.txt
+  max_seq_len: null
+
+checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-0.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-0.5B-Instruct
+  model_type: QWEN2
+
+teacher_checkpointer:
+  _component_: torchtune.training.FullModelHFCheckpointer
+  checkpoint_dir: /tmp/Qwen2-1.5B-Instruct
+  checkpoint_files: [
+    model.safetensors
+  ]
+  recipe_checkpoint: null
+  output_dir: /tmp/Qwen2-1.5B-Instruct
+  model_type: QWEN2
+
+resume_from_checkpoint: False
+
+# Dataset and Sampler
+dataset:
+  _component_: torchtune.datasets.alpaca_cleaned_dataset
+seed: null
+shuffle: True
+batch_size: 8
+
+# Optimizer and Scheduler
+optimizer:
+  _component_: torch.optim.AdamW
+  weight_decay: 0.01
+  lr: 3e-4
+lr_scheduler:
+  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
+  num_warmup_steps: 100
+
+loss:
+  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
+
+kd_loss:
+  _component_: torchtune.modules.loss.ForwardKLWithChunkedOutputLoss
+kd_ratio: 0.5
+
+# Training
+epochs: 1
+max_steps_per_epoch: null
+gradient_accumulation_steps: 2
+compile: False
+
+# Logging
+output_dir: /tmp/qwen_kd
+metric_logger:
+  _component_: torchtune.training.metric_logging.DiskLogger
+  log_dir: ${output_dir}
+log_every_n_steps: 1
+log_peak_memory_stats: False
+
+# Environment
+device: cuda
+dtype: bf16
+enable_activation_checkpointing: False
diff -ruN marc_original/third_party/torchtune/recipes/dev/generate_v2.py marc/third_party/torchtune/recipes/dev/generate_v2.py
--- marc_original/third_party/torchtune/recipes/dev/generate_v2.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/dev/generate_v2.py	2025-02-20 17:49:29.474024121 -0500
@@ -0,0 +1,215 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import itertools
+import sys
+import time
+from typing import Any, Dict, List
+
+import torch
+from omegaconf import DictConfig, OmegaConf
+
+from torchtune import config, training, utils
+from torchtune.data import load_image, Message, padded_collate_tiled_images_and_mask
+
+from torchtune.generation import sample
+
+from torchtune.modules.transforms import Transform
+
+
+class SingleTurnYAMLToMessages(Transform):
+    """
+    Converts a single turn conversation in YAML format to a list of messages.
+
+    Expects the YAML to look like:
+        system: You are a helpful AI assistant.
+        user: What is the capital of France?
+
+    or if it includes an image:
+        system: You are a helpful AI assistant.
+        user:
+            image: url or path_to_image
+            text: Describe the image in detail.
+    """
+
+    def __call__(self, prompt: Dict[str, Any]) -> List[Message]:
+        messages = []
+
+        # Iterate through roles and add content
+        for role, content in prompt.items():
+            if isinstance(content, str):
+                new_content = [{"type": "text", "content": content}]
+            else:
+                assert (
+                    "image" in content.keys()
+                ), "Multiple entries per role expect an image key"
+                image_loc = content["image"]
+                image = load_image(image_loc)
+                new_content = [
+                    {"type": "image", "content": image},
+                    {"type": "text", "content": content["text"]},
+                ]
+            messages.append(Message(role=role, content=new_content))
+
+        # Finally, add an empty assistant message to kick-start generation
+        messages.append(Message(role="assistant", content=""))
+        return messages
+
+
+class InferenceRecipe:
+    """
+    Recipe for generating tokens from a dense Transformer-based LLM.
+    This works for text-only generation and image-text generation.
+
+    This *does not* currently support the following features:
+        - torch.compile
+        - quantization through torchao
+        - multi-GPU generation
+        - batch generation
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        self._device = utils.get_device(device=cfg.device)
+        self._dtype = training.get_dtype(dtype=cfg.dtype, device=self._device)
+        self._logger = utils.get_logger(cfg.log_level)
+        training.set_seed(seed=cfg.seed)
+
+    def setup(self, cfg: DictConfig) -> None:
+        """Setup the model and transforms."""
+        # Load checkpointer and state_dict
+        _checkpointer = config.instantiate(cfg.checkpointer)
+        _ckpt_dict = _checkpointer.load_checkpoint()
+
+        # Instantiate model
+        with training.set_default_dtype(self._dtype), self._device:
+            model = config.instantiate(cfg.model)
+        model.load_state_dict(_ckpt_dict[training.MODEL_KEY])
+        self.model = model
+        self._logger.info(f"Model was initialized with precision {self._dtype}.")
+
+        # Instantiate transforms
+        self.model_transform = config.instantiate(cfg.tokenizer)
+        self.to_messages = SingleTurnYAMLToMessages()
+
+    def log_metrics(self, total_time: int, tokens_per_second: float) -> None:
+        """Logs the following metrics: total time for inference, tokens/sec,
+        bandwidth achieved, and max memory allocated.
+
+        Feel free to modify this function to log additional metrics.
+        """
+        model_size = sum(
+            [
+                p.numel() * p.dtype.itemsize
+                for p in itertools.chain(self.model.parameters(), self.model.buffers())
+            ]
+        )
+        self._logger.info(
+            f"Time for inference: {total_time:.02f} sec total, {tokens_per_second:.02f} tokens/sec"
+        )
+        self._logger.info(
+            f"Bandwidth achieved: {model_size * tokens_per_second / 1e9:.02f} GB/s"
+        )
+        self._logger.info(
+            f"Max memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB"
+        )
+
+    @torch.inference_mode()
+    def generate(self, cfg: DictConfig):
+        """The main entry point for generating tokens from a prompt."""
+        # 1. Convert input to messages
+        messages = self.to_messages(OmegaConf.to_container(cfg.prompt))
+        is_multimodal_input = any([m.contains_media for m in messages])
+
+        # 2. Apply model transform
+        model_inputs = self.model_transform({"messages": messages}, inference=True)
+        seq_len = len(model_inputs["tokens"])
+        total_response_length = seq_len + cfg.max_new_tokens
+
+        # 3. Setup KV cache
+        with self._device:
+            self.model.setup_caches(
+                batch_size=1,
+                dtype=self._dtype,
+                encoder_max_seq_len=(
+                    self.model_transform.image_seq_len if is_multimodal_input else None
+                ),
+                decoder_max_seq_len=total_response_length,
+            )
+
+        # 4. Pre-allocate causal mask and input_pos
+        causal_mask = torch.tril(
+            torch.ones(
+                size=(total_response_length, total_response_length),
+                dtype=torch.bool,
+                device=self._device,
+            )
+        )
+        input_pos = torch.arange(total_response_length)
+
+        # 5. Collate to batch size of 1 and tensor-ify
+        batch = {}
+        if is_multimodal_input:
+            batch = padded_collate_tiled_images_and_mask(
+                [model_inputs], pad_direction="left", pad_max_images=1
+            )
+            batch["encoder_mask"] = batch["encoder_mask"][:, :seq_len]
+            prompt = batch.pop("tokens").to(self._device)
+        else:
+            prompt = torch.tensor(
+                model_inputs["tokens"], device=self._device
+            ).unsqueeze(0)
+        batch["mask"] = causal_mask[None, :seq_len]
+        batch["input_pos"] = input_pos[None, :seq_len]
+        utils.batch_to_device(batch, self._device)
+
+        # 6. Prefill step
+        generated_tokens = []
+        t0 = time.perf_counter()
+        logits = self.model(prompt, **batch)[:, -1]
+        token = sample(logits, temperature=cfg.temperature, top_k=cfg.top_k)
+        generated_tokens.append(token.item())
+
+        if is_multimodal_input:
+            # Don't need image info b/c we only support 1 image and it's been
+            # processed by the model now
+            batch.pop("encoder_input")
+            batch["encoder_mask"] = batch["encoder_mask"][:, -1:]
+
+        # 7. Continue generating
+        for i in range(cfg.max_new_tokens):
+
+            # Update position and mask for incremental decoding
+            batch["input_pos"] = input_pos[None, seq_len]
+            batch["mask"] = causal_mask[None, seq_len, None, :]
+
+            if token.item() in self.model_transform.stop_tokens:
+                break
+
+            logits = self.model(token, **batch)[:, -1]
+            token = sample(logits, temperature=cfg.temperature, top_k=cfg.top_k)
+            generated_tokens.append(token.item())
+            seq_len += 1
+
+        t = time.perf_counter() - t0
+
+        # 8. Translate tokens back to text
+        decoded = self.model_transform.decode(generated_tokens)
+        self._logger.info(f"\n\n{decoded}\n")
+
+        # 9. Log metrics
+        tokens_per_second = len(generated_tokens) / t
+        self.log_metrics(total_time=t, tokens_per_second=tokens_per_second)
+
+
+@config.parse
+def main(cfg: DictConfig) -> None:
+    config.log_config(recipe_name="InferenceRecipe", cfg=cfg)
+    recipe = InferenceRecipe(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.generate(cfg=cfg)
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff -ruN marc_original/third_party/torchtune/recipes/eleuther_eval.py marc/third_party/torchtune/recipes/eleuther_eval.py
--- marc_original/third_party/torchtune/recipes/eleuther_eval.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/eleuther_eval.py	2025-02-20 17:49:29.478024127 -0500
@@ -0,0 +1,576 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import sys
+import time
+
+from typing import Dict, List, Tuple, Union
+
+import PIL
+
+import torch
+
+from lm_eval.evaluator import evaluate, get_task_list
+from lm_eval.models.hf_vlms import HFMultimodalLM
+from lm_eval.models.huggingface import HFLM
+from lm_eval.tasks import get_task_dict, TaskManager
+from lm_eval.utils import make_table
+from omegaconf import DictConfig
+
+from torchtune import config, training, utils
+from torchtune.data import (
+    format_content_with_images,
+    left_pad_sequence,
+    Message,
+    padded_collate_tiled_images_and_mask,
+)
+from torchtune.generation import generate, sample
+from torchtune.modules import TransformerDecoder
+from torchtune.modules.model_fusion import DeepFusionModel
+from torchtune.modules.tokenizers import ModelTokenizer
+from torchtune.modules.transforms import Transform
+from torchtune.recipe_interfaces import EvalRecipeInterface
+from torchtune.training import FullModelTorchTuneCheckpointer
+
+
+class _VLMEvalWrapper(HFMultimodalLM):
+    """An EvalWrapper for EleutherAI's eval harness based on gpt-fast's
+    EvalWrapper: https://github.com/pytorch-labs/gpt-fast/blob/main/eval.py.
+
+    Note:
+        This is ONLY for vision-language models.
+
+    Args:
+        model (DeepFusionModel): The VLM to evaluate.
+        transform (Transform): The transform (tokenizer) to use for preprocessing.
+        device (torch.device): The device to use.
+        max_seq_length (int): The maximum sequence length.
+        batch_size (int): The batch size.
+        dtype (torch.dtype): dtype for the model caches during generation.
+        enable_kv_cache (bool): Whether to enable KV cache for generation.
+        image_tag (str): The string to use for the image token. Default is "<image>", which
+            is the default used by the MMMU dataset.
+        max_images_per_sample (int): The maximum number of images per sample. Defaults to
+            the max number of images in MMMU.
+    """
+
+    def __init__(
+        self,
+        model: DeepFusionModel,
+        transform: Transform,
+        *,
+        device: torch.device,
+        max_seq_length: int = 4096,
+        batch_size: int = 8,
+        dtype: torch.dtype = torch.bfloat16,
+        enable_kv_cache: bool = True,
+        # TODO (@joecummings): Update these defaults once more multimodal
+        # tasks are added to the eval harness
+        image_tag: str = "<image>",
+        max_images_per_sample: int = 7,
+    ):
+        self._model = model
+        self._transform = transform
+        self._device = device
+        self._max_seq_length = max_seq_length
+        self._batch_size = batch_size
+        self._dtype = dtype
+        # Defaulting KV cache to True for multimodal
+        self._enable_kv_cache = True
+        self._image_tag = image_tag
+        self._max_images_per_sample = max_images_per_sample
+
+    @property
+    def model(self):
+        # Not actually changing the dtype here, just adding it as a
+        # property on the model
+        self._model.dtype = self._dtype
+        return self._model
+
+    @property
+    def model_transform(self):
+        return self._transform
+
+    @property
+    def device(self):
+        return self._device
+
+    @property
+    def cache_hook(self):
+        # Dummy class to appease the Harness
+        class DummyCacheHook:
+            def __init__(self):
+                self.add_partial = lambda x, y, z: True
+
+        return DummyCacheHook()
+
+    @property
+    def rank(self):
+        # Hardcoded for now b/c we only support single GPU eval
+        return 0
+
+    @property
+    def world_size(self):
+        # Hardcoded for now b/c we only support single GPU eval
+        return 1
+
+    @property
+    def batch_size(self):
+        return self._batch_size
+
+    @property
+    def eos_token_id(self):
+        return self._transform.tokenizer.eos_id
+
+    @property
+    def eot_token_id(self):
+        return self._transform.tokenizer.eot_id
+
+    @property
+    def max_length(self):
+        return self._max_seq_length
+
+    @property
+    def truncation(self):
+        return True
+
+    def tok_encode(self, string, **kwargs) -> List[int]:
+        # This is only used to get a number of tokens for use in sorting samples in dataset
+        # These values will not actually be used for eval
+        return self._transform.tokenizer.encode(string, add_bos=False, add_eos=False)
+
+    def tok_decode(self, tokens, skip_special_tokens=True) -> str:
+        if isinstance(tokens, int):
+            tokens = [tokens]
+        return self._transform.tokenizer.decode(
+            tokens, skip_special_tokens=skip_special_tokens
+        )
+
+    def tok_batch_multimodal_encode(
+        self,
+        all_texts: List[str],
+        all_images: List[List[PIL.Image.Image]],
+        left_truncate_len: int = None,
+        *args,
+        **kwargs,
+    ):
+        # Eleuther already parses out the text and images, so we just need to get
+        # it into a Message format for our tokenizer
+        all_encoded_messages = []
+
+        for text, images in zip(all_texts, all_images):
+            # Ensure images are all RGB
+            proper_images = []
+            for image in images:
+                if image.mode != "RGB":
+                    image = image.convert("RGB")
+                proper_images.append(image)
+
+            # Construct the messages
+            messages = []
+            content = format_content_with_images(
+                text, image_tag=self._image_tag, images=proper_images
+            )
+            messages.append(Message(role="user", content=content))
+            messages.append(Message(role="assistant", content=""))
+
+            # Transform the messages
+            tok_batch = self.model_transform({"messages": messages}, inference=True)
+            all_encoded_messages.append(tok_batch)
+
+        # Pad the encoded messages
+        tok_batch = padded_collate_tiled_images_and_mask(
+            all_encoded_messages,
+            pad_direction="left",
+            pad_max_images=self._max_images_per_sample,
+        )
+        utils.batch_to_device(tok_batch, self.device)
+
+        # Convert the batch to the format expected by the HF
+        tok_batch["input_ids"] = tok_batch.pop("tokens")
+
+        # the harness will use left_truncate_len to indicate that the current batch
+        # needs to be truncated to self.max_seq_len - self.max_gen_toks
+        if left_truncate_len is not None:
+            tok_batch["input_ids"] = tok_batch["input_ids"][:, -left_truncate_len:]
+
+        return tok_batch
+
+    @torch.inference_mode()
+    def _model_multimodal_generate(
+        self,
+        batch: Dict[str, torch.Tensor],
+        max_length: int,
+        stop: List[str],
+        **generation_kwargs,
+    ):
+        # 1. Validate inputs
+        prompt = batch.pop("input_ids")
+        bsz, seq_len = prompt.shape
+
+        temperature = generation_kwargs.get("temperature", 0.0)
+        do_sample = generation_kwargs.get("do_sample", False)
+        if do_sample or temperature != 0.0:
+            raise RuntimeError(
+                "Any decoding strategy other than greedy is not supported."
+            )
+
+        if bsz > 1:
+            raise ValueError(
+                f"Got a batch size of '{bsz}'. Batch size > 1 is not yet supported for "
+                "multimodal generation."
+            )
+
+        # 2. Setup KV cache and masks for bsz 1
+        with self.device:
+            if self.model.caches_are_enabled():
+                self.model.reset_caches()
+            else:
+                self.model.setup_caches(
+                    batch_size=1,
+                    dtype=self._dtype,
+                    encoder_max_seq_len=self.model_transform.image_seq_len
+                    * self._max_images_per_sample,
+                    decoder_max_seq_len=self.max_length,
+                )
+            causal_mask = torch.tril(
+                torch.ones(
+                    size=(self.max_length, self.max_length),
+                    dtype=torch.bool,
+                )
+            )
+            input_pos = torch.arange(self.max_length)
+
+        batch["input_pos"] = input_pos[None, :seq_len]
+        batch["mask"] = causal_mask[None, :seq_len]
+
+        # 3. Prefill step
+        generated_tokens = []
+        logits = self.model(prompt, **batch)[:, -1]
+        token = sample(logits, temperature=0.0, top_k=None)
+        generated_tokens.append(token.item())
+
+        cache_mask = batch["encoder_mask"][:, -1:]
+
+        # 4. Continue generating
+        for _ in range(max_length):
+            if token.item() in self.model_transform.stop_tokens:
+                break
+            logits = self.model(
+                token,
+                mask=causal_mask[None, seq_len, None, :],
+                encoder_input=None,
+                encoder_mask=cache_mask,
+                input_pos=input_pos[None, seq_len],
+            )[:, -1]
+            token = sample(logits, temperature=0.0, top_k=None)
+            generated_tokens.append(token.item())
+            seq_len += 1
+
+        # 5. Return generated tokens
+        return torch.tensor(generated_tokens, dtype=torch.int32).unsqueeze(0)
+
+
+class _LLMEvalWrapper(HFLM):
+    """An EvalWrapper for EleutherAI's eval harness based on gpt-fast's
+    EvalWrapper: https://github.com/pytorch-labs/gpt-fast/blob/main/eval.py.
+
+    Note:
+        This is for text-only decoder models.
+
+    Args:
+        model (TransformerDecoder): The model to evaluate.
+        tokenizer (ModelTokenizer): Tokenizer associated with the model being evaluated.
+            This should be the same tokenizer used when fine-tuning the model.
+        device (torch.device): The device to use.
+        max_seq_length (int): The maximum sequence length to use.
+        batch_size (int): The batch size per GPU to use.
+        dtype (torch.dtype): dtype for the model caches during generation.
+        enable_kv_cache (bool): Whether to enable KV cache for generation.
+    """
+
+    def __init__(
+        self,
+        model: TransformerDecoder,
+        tokenizer: ModelTokenizer,
+        *,
+        device: torch.device,
+        max_seq_length: int = 4096,
+        batch_size: int = 8,
+        dtype: torch.dtype = torch.float32,
+        enable_kv_cache: bool = True,
+    ):
+        # TODO (@joecummings): Remove this init function so we don't load in extraneous stuff
+        super().__init__(pretrained="gpt2", device=str(device))
+        self._model = model
+        self._tokenizer = tokenizer
+        self._max_seq_length = max_seq_length
+        self._batch_size = batch_size
+        self._dtype = dtype
+        self._enable_kv_cache = enable_kv_cache
+
+    @property
+    def model(self):
+        return self._model
+
+    @property
+    def eot_token_id(self):
+        return self._tokenizer.eos_id
+
+    @property
+    def max_length(self):
+        return self._max_seq_length
+
+    @property
+    def max_gen_toks(self):
+        return 256
+
+    @property
+    def batch_size(self):
+        return self._batch_size
+
+    @property
+    def device(self):
+        return self._device
+
+    @property
+    def enable_kv_cache(self):
+        return self._enable_kv_cache
+
+    def tok_encode(self, text: str, **kwargs) -> List[int]:
+        # Note on add_bos flag: setting to False as this gives better results, for example
+        # +1% on truthfulqa_mc2 with a LoRA finetune. lit-gpt also sets this to False,
+        # see https://github.com/Lightning-AI/lit-gpt/blob/main/eval/lm_eval_harness.py#L66,
+        # though notably fast-gpt does the opposite
+        # https://github.com/pytorch-labs/gpt-fast/blob/main/eval.py#L123.
+        return self._tokenizer.encode(text=text, add_bos=False, add_eos=False)
+
+    def tok_batch_encode(
+        self, text: List[str], left_truncate_len: int = None, **kwargs
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        tokenized_text = [self.tok_encode(x) for x in text]
+
+        # pad left
+        x = left_pad_sequence(
+            [torch.tensor(x) for x in tokenized_text],
+            batch_first=True,
+            padding_value=self._tokenizer.pad_id,
+        )
+
+        # the harness will use left_truncate_len to indicate that the current batch
+        # needs to be truncated to self.max_seq_len - self.max_gen_toks
+        if left_truncate_len is not None:
+            x = x[:, -left_truncate_len:]
+
+        return x, torch.ones_like(x)  # return 'mask' b/c it's expected by the harness
+
+    def tok_decode(self, tokens: Union[List[int], int], **kwargs) -> str:
+        if isinstance(tokens, int):
+            tokens = [tokens]
+        return self._tokenizer.decode(tokens)
+
+    def _model_call(self, inps: torch.Tensor, **kwargs) -> torch.Tensor:
+        return self._model(inps)
+
+    @torch.inference_mode()
+    def _model_generate(
+        self, context: torch.Tensor, **generation_kwargs
+    ) -> torch.Tensor:
+        bsz, seq_len = context.shape
+
+        temperature = generation_kwargs.get("temperature", 0.0)
+        do_sample = generation_kwargs.get("do_sample", False)
+        if do_sample or temperature != 0.0:
+            raise RuntimeError(
+                "Any decoding strategy other than greedy is not supported."
+            )
+
+        # Setup KV caches OR reset them if they're already set up
+        if self.enable_kv_cache:
+            if self.model.caches_are_enabled():
+                self.model.reset_caches()
+            else:
+                with self.device:
+                    self.model.setup_caches(
+                        batch_size=self.batch_size,
+                        dtype=self._dtype,
+                        decoder_max_seq_len=self.max_length,
+                    )
+
+        # if we've recieved fewer than self._batch_size samples in the current
+        # batch we need to pad the batch out. here we're padding the end of the
+        # current batch to the correct length. this is because when we use static
+        # KV-caches, the model will expect a fixed batch size for all samples.
+        maybe_padded_context = torch.nn.functional.pad(
+            context,
+            (0, 0, 0, self._batch_size - bsz),
+            value=self._tokenizer.eos_id,  # pad with one of the tokenizer's stop tokens so generation can stop early
+        )
+
+        toks, _ = generate(
+            self.model,
+            maybe_padded_context,
+            max_generated_tokens=self.max_gen_toks,
+            temperature=temperature,
+            top_k=None,
+            stop_tokens=self._tokenizer.stop_tokens,
+        )
+        return toks[:bsz]
+
+
+class EleutherEvalRecipe(EvalRecipeInterface):
+    """
+    This recipe runs evaluation on a trained model using EleutherAI's eval harness.
+    This assumes the user has the EleutherAI eval harness installed. See
+    https://github.com/EleutherAI/lm-evaluation-harness for more details.
+
+    Features:
+        - Single GPU evaluation. Multi-GPU evaluation is currently not supported.
+        - Quantization (for text-only models) is supported.
+        - Any task from the EleutherAI eval harness
+
+    We recommend launching evaluation using the tune CLI::
+
+        tune run eleuther_eval --config eleuther_evaluation \
+            tasks=["truthfulqa_mc2","hellaswag"] \
+            limit=50 \
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        # Double check we have the right Eval Harness version
+        from importlib.metadata import version
+
+        if version("lm-eval") != "0.4.5":
+            raise RuntimeError(
+                "This recipe requires EleutherAI Eval Harness v0.4.5. "
+                "Please install with `pip install lm-eval==0.4.5`"
+            )
+
+        # General variable initialization
+        self.device = utils.get_device(device=cfg.device)
+        self.dtype = training.get_dtype(dtype=cfg.dtype, device=self.device)
+        self.logger = utils.get_logger(cfg.get("log_level", "info"))
+        training.set_seed(seed=cfg.seed)
+
+        # Eval specific variables
+        self.limit = cfg.limit
+        self.tasks = list(cfg.tasks)
+        self.batch_size = cfg.batch_size
+        self.enable_kv_cache = cfg.get("enable_kv_cache", True)
+        self.include_path = cfg.get("include_path", None)
+
+    def setup(self, cfg: DictConfig) -> None:
+        # Initialize quantizer and quantization mode
+        quantizer = config.instantiate(cfg.quantizer)
+        quantization_mode = training.get_quantizer_mode(quantizer)
+
+        # Load checkpoint
+        checkpointer = config.instantiate(cfg.checkpointer)
+
+        # Initialize model
+        with training.set_default_dtype(self.dtype), self.device:
+            model = config.instantiate(cfg.model)
+
+        # Quantize model if requested
+        if quantization_mode is not None:
+            if not isinstance(checkpointer, FullModelTorchTuneCheckpointer):
+                raise ValueError(
+                    "Quantization is only supported for models quantized and saved with the "
+                    "FullModelTorchTuneCheckpointer - please ensure you have quantized your "
+                    "model and are using the quantized weights!"
+                )
+            if "qat" in quantization_mode:
+                raise ValueError(
+                    "You have specified a quantizer with 'QAT' - "
+                    "QAT quantizers should only be used during quantization aware training "
+                    "and when quantizing models. Please use the corresponding post-training "
+                    "quantizer e.g. Int8DynActInt4WeightQuantizer for Int8DynActInt4WeightQATQuantizer."
+                )
+            model = quantizer.quantize(model)
+            model = model.to(device=self.device, dtype=self.dtype)
+            ckpt_dict = checkpointer.load_checkpoint(weights_only=False)[
+                training.MODEL_KEY
+            ]
+            for k, v in ckpt_dict.items():
+                ckpt_dict[k] = v.to(self.device)
+            model.load_state_dict(ckpt_dict, assign=True)
+        else:
+            ckpt_dict = checkpointer.load_checkpoint()[training.MODEL_KEY]
+            model.load_state_dict(ckpt_dict)
+
+        # Load model weights into initialized model
+        self.logger.info(f"Model is initialized with precision {self.dtype}.")
+
+        # Put model in eval mode.
+        # Note: This will not disable the dropout applied in SDPA,
+        # see https://github.com/pytorch/pytorch/issues/124464
+        model.eval()
+
+        # Initialize tokenizer/transform
+        model_transform = config.instantiate(cfg.tokenizer)
+
+        # Finally, we setup the actual EvalWrapper class
+        if isinstance(model, DeepFusionModel):
+            eleuther_model_wrapper = _VLMEvalWrapper
+            if not self.enable_kv_cache:
+                self.logger.debug(
+                    "Received enable_kv_cache=False, but KV cache is required for running "
+                    "multimodal generation in a timely manner. Setting enable_kv_cache=True."
+                )
+        elif isinstance(model, TransformerDecoder):
+            eleuther_model_wrapper = _LLMEvalWrapper
+        self.eleuther_model_wrapper = eleuther_model_wrapper(
+            model,
+            model_transform,
+            device=self.device,
+            max_seq_length=cfg.max_seq_length,
+            batch_size=self.batch_size,
+            dtype=self.dtype,
+            enable_kv_cache=self.enable_kv_cache,
+        )
+
+    def evaluate(self) -> None:
+        # Initialize tasks for the harness
+        task_manager = TaskManager(include_path=self.include_path)
+        task_dict = get_task_dict(self.tasks, task_manager)
+        task_types = set([t.task.OUTPUT_TYPE for t in get_task_list(task_dict)])
+        if len(task_types) > 1 and "generate_until" in task_types:
+            raise RuntimeError(
+                "Evaluating on multiple task types where any one task involves "
+                "generation is currently not supported. See the issue below for more info: "
+                "https://github.com/pytorch/torchtune/issues/1621"
+            )
+
+        # Run evaluation
+        t0 = time.time()
+        self.logger.info(f"Running evaluation on the following tasks: {self.tasks}")
+        output = evaluate(
+            self.eleuther_model_wrapper,
+            task_dict,
+            limit=self.limit,
+        )
+        t1 = time.time() - t0
+
+        # Log metrics
+        self.logger.info(f"Eval completed in {t1:.02f} seconds.")
+        self.logger.info(
+            f"Max memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB"
+        )
+        formatted_output = make_table(output)
+        self.logger.info(f"\n\n{formatted_output}\n")
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """Entry point for the recipe."""
+    config.log_config(recipe_name="EleutherEvalRecipe", cfg=cfg)
+    recipe = EleutherEvalRecipe(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.evaluate()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
diff -ruN marc_original/third_party/torchtune/recipes/full_finetune_distributed.py marc/third_party/torchtune/recipes/full_finetune_distributed.py
--- marc_original/third_party/torchtune/recipes/full_finetune_distributed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/full_finetune_distributed.py	2025-02-20 17:49:29.482024134 -0500
@@ -0,0 +1,787 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import sys
+import time
+
+from functools import partial
+from typing import Any, Dict, List, Optional, Tuple, Union
+from warnings import warn
+
+import torch
+from omegaconf import DictConfig, ListConfig
+
+from torch import nn
+from torch.distributed import destroy_process_group, init_process_group
+
+from torch.optim import Optimizer
+from torch.utils.data import DataLoader, DistributedSampler
+from torchtune import config, modules, training, utils
+from torchtune.config._utils import _get_component_from_path
+from torchtune.data import padded_collate_packed
+from torchtune.datasets import ConcatDataset
+from torchtune.recipe_interfaces import FTRecipeInterface
+from torchtune.training import DummyProfiler, PROFILER_KEY
+from torchtune.training.activations import apply_selective_activation_checkpointing
+
+from tqdm import tqdm
+
+log = utils.get_logger("DEBUG")
+
+
+class FullFinetuneRecipeDistributed(FTRecipeInterface):
+    """
+    Full finetuning recipe for dense transformer-based LLMs such as Llama2. This recipe supports
+    distributed training and can be run on a single node (1 to 8 GPUs).
+
+    Features:
+        - FSDP. Supported using PyTorch's FSDP APIs. CPU offload of parameters, gradients, and optimizer states
+            is supported via ``fsdp_cpu_offload``. Resharding of parameters after the forward pass is
+            done by default (corresponding to FULL_SHARD sharding strategy), but can be disabled by setting the config
+            ``fsdp_reshard_after_forward`` to False (this corresponds to SHARD_GRAD_OP sharding strategy).
+            DDP is currently not supported. Training on CPU is not supported.
+
+        - Activation Checkpointing. This can be controlled using the ``activation_checkpointing``
+            flag. Activation checkpointing helps reduce the memory footprint since we no longer keep
+            activations in memory and instead recompute them during the backward pass. This is especially
+            helpful for larger batch sizes when you're memory constrained. But these savings in memory
+            come at the cost of training performance. In most cases training can slow-down quite a bit as
+            a result of this activation recomputation.
+
+        - Precision. Full fp32 and bf16 training are supported. Precision is controlled using the ``dtype``
+            flag. When ``dtype=bf16``, all activations, gradients and optimizer states are in bfloat16. In
+            most cases this should halve the memory footprint of full precision (fp32) training, without
+            loss in model quality (will depend on the model, training data and other settings). For
+            GPUs which do not support bfloat16, we fall back to fp32. Mixed precision training and fp16
+            precision are currently not supported.
+
+        - Gradient Accumulation. You can simulate larger batch sizes by accumulating gradients. This is
+            controlled using the ``gradient_accumulation_steps`` flag.
+
+                Total Batch Size = batch_size * number of GPUs * gradient accumulation steps.
+
+            For example: with batch_size=1, nproc_per_node=2 and gradient_accumulation_steps=32 we get a
+            total batch size of 64.
+
+            Gradient accumulation is especially useful when you are memory constrained. In this case,
+            accumulating gradients might give you better training speed than enabling activation
+            checkpointing.
+
+        - Checkpointing. Model weights are checkpointed both at the end of each epoch and at the end of
+            training. Optimizer state and recipe state (seed, total_epochs, number of epochs run etc) are
+            only saved at the end of a given epoch and used in case of resuming training.
+
+            Resuming training is controlled by the ``resume_from_checkpoint`` flag. Mid-epoch checkpointing is
+            currently not supported.
+
+            For more details on the checkpointer, please take a look at
+            our checkpointer deepdive (https://pytorch.org/torchtune/main/deep_dives/checkpointer.html).
+
+        - Logging. Terminal, Disk, WandB and TensorBoard are all supported.
+
+        - Gradient Clipping. Gradient clipping is supported using the ``clip_grad_norm`` flag. By default,
+            ``clip_grad_norm`` is set to ``None``. If you only want to log the grad norm, you can set
+            ``clip_grad_norm='inf'``.
+
+    For a full list of example configs for this recipe, run ``tune ls`` on the command line. Each config
+    has example commands for how to kick-off training.
+
+    Args:
+        cfg (DictConfig): OmegaConf object parsed from yaml file
+
+    Raises:
+        ValueError: If ``dtype`` is set to fp16.
+        RuntimeError: If ``dtype`` is set to bf16 and the hardware does not support bf16.
+        RuntimeError: If ``left_pad_sequence`` is set as the data collator.
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        self._device = utils.get_device(device=cfg.device)
+        self._dtype = training.get_dtype(cfg.dtype, device=self._device)
+
+        if self._dtype == torch.float16:
+            raise ValueError(
+                "full fp16 training is not supported with this recipe. Please use bf16 or fp32 instead."
+            )
+
+        if (
+            cfg.get("fsdp_cpu_offload", False)
+            and cfg.optimizer.get("fused", False)
+            and not utils.torch_version_ge("2.4.0")
+        ):
+            raise RuntimeError(
+                "Using fused optimizer on CPU is only supported in PyTorch nightly."
+            )
+
+        # logging attributes
+        self._output_dir = cfg.output_dir
+        self._log_every_n_steps = cfg.get("log_every_n_steps", 1)
+        self._log_peak_memory_stats = cfg.get("log_peak_memory_stats", False)
+
+        # _is_rank_zero is used primarily for logging. In the future, the logger
+        # should directly take care of this
+        _, rank = training.get_world_size_and_rank()
+        self._is_rank_zero = rank == 0
+
+        # Training cfg
+        self._resume_from_checkpoint = cfg.resume_from_checkpoint
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+
+        # These are public properties which are updated by the checkpoint loader
+        # when ``resume_from_checkpoint`` is `True` or validated in tests
+        self.seed = training.set_seed(seed=cfg.seed)
+        self.epochs_run = 0
+        self.total_epochs = cfg.epochs
+        self.max_steps_per_epoch = cfg.max_steps_per_epoch
+        self.global_step = 0
+        self._clip_grad_norm = cfg.get("clip_grad_norm", None)
+
+    def load_checkpoint(self, cfg_checkpointer: DictConfig) -> Dict[str, Any]:
+        """
+        Extract the checkpoint state from file and validate. If resume_from_checkpoint
+        is True, this also includes the recipe state.
+        """
+        self._checkpointer = config.instantiate(
+            cfg_checkpointer,
+            resume_from_checkpoint=self._resume_from_checkpoint,
+        )
+        checkpoint_dict = self._checkpointer.load_checkpoint()
+
+        if self._resume_from_checkpoint:
+            self._update_recipe_state(checkpoint_dict)
+        return checkpoint_dict
+
+    def _update_recipe_state(self, ckpt_dict: Dict[str, Any]) -> None:
+        """
+        Updates the recipe state from checkpoint.
+        """
+        try:
+            self.epochs_run = ckpt_dict[training.EPOCHS_KEY]
+
+            # on mismatch, warn the user and prevent the override
+            if self.seed != ckpt_dict[training.SEED_KEY]:
+                warn(
+                    message=(
+                        "Config value for seed does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.SEED_KEY]}"
+                    )
+                )
+                self.seed = ckpt_dict[training.SEED_KEY]
+            if self.max_steps_per_epoch != ckpt_dict[training.MAX_STEPS_KEY]:
+                warn(
+                    message=(
+                        "Config value for max_steps_per_epoch does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.MAX_STEPS_KEY]}"
+                    )
+                )
+                self.max_steps_per_epoch = ckpt_dict[training.MAX_STEPS_KEY]
+
+            # on mismatch, warn the user but allow the override
+            if self.total_epochs != ckpt_dict[training.TOTAL_EPOCHS_KEY]:
+                warn(
+                    message=(
+                        "Config value for total_epochs does not match the checkpoint value, "
+                        f"using the config value: {self.total_epochs}"
+                    )
+                )
+
+        except KeyError as e:
+            raise KeyError(
+                "Checkpoint does not contain the required keys needed for updating recipe state. "
+                "Are you sure you passed in the right recipe checkpoint?"
+            ) from e
+
+    def setup(self, cfg: DictConfig) -> None:
+        """
+        Setup the recipe. This includes training state (if resume_from_checkpoint is True),
+        model, tokenizer, loss, optimizer, sampler, and dataloader.
+        """
+        if self._is_rank_zero:
+            self._metric_logger = config.instantiate(cfg.metric_logger)
+
+            # log config with parameter override
+            self._metric_logger.log_config(cfg)
+
+        checkpoint_dict = self.load_checkpoint(cfg_checkpointer=cfg.checkpointer)
+
+        self._compile = cfg.get("compile", False)
+        self._model = self._setup_model(
+            cfg_model=cfg.model,
+            enable_activation_checkpointing=cfg.enable_activation_checkpointing,
+            custom_sharded_layers=cfg.get("custom_sharded_layers", None),
+            fsdp_cpu_offload=cfg.get("fsdp_cpu_offload", False),
+            reshard_after_forward=cfg.get("fsdp_reshard_after_forward", True),
+            model_state_dict=checkpoint_dict[training.MODEL_KEY],
+            ac_mode=cfg.get("ac_mode", None),
+            ac_option=cfg.get("ac_option", None),
+        )
+        self._tokenizer = config.instantiate(cfg.tokenizer)
+
+        self._optimizer = self._setup_optimizer(
+            cfg_optimizer=cfg.optimizer,
+            opt_state_dict=checkpoint_dict[training.OPT_KEY]
+            if self._resume_from_checkpoint
+            else None,
+        )
+
+        # initialize loss
+        self._loss_fn = config.instantiate(cfg.loss)
+
+        if self._compile:
+            training.compile_loss(self._loss_fn, verbose=self._is_rank_zero)
+
+        if self._loss_fn.__class__.__name__ == "CEWithChunkedOutputLoss":
+            # set num_output_chunks for model
+            self._model.set_num_output_chunks(self._loss_fn.num_output_chunks)
+
+        if self._is_rank_zero:
+            log.info("Loss is initialized.")
+
+        # sampler and dataloader depend on the tokenizer and loss_fn and should be
+        # setup after both of these are initialized
+        collate_name = cfg.get("collate_fn", "torchtune.data.padded_collate_sft")
+        self._sampler, self._dataloader = self._setup_data(
+            cfg_dataset=cfg.dataset,
+            shuffle=cfg.shuffle,
+            batch_size=cfg.batch_size,
+            collate_fn=collate_name,
+        )
+
+        # Finally update the recipe state which can only be correctly set after all of the
+        # other components have been initialized and updated.
+        #
+        # Number of training steps in each epoch depends on the number of batches produced
+        # by the dataloader, the max_steps_per_epoch param set by the user and the
+        # gradient_accumulation_steps param. This value is used for logging and tracking
+        # training state. The computation should happen after the dataloader has been setup
+        self._steps_per_epoch = (
+            len(self._dataloader) // self._gradient_accumulation_steps
+        )
+        if (
+            self.max_steps_per_epoch is not None
+            and self.max_steps_per_epoch < self._steps_per_epoch
+        ):
+            self._steps_per_epoch = self.max_steps_per_epoch
+        self.global_step = self.epochs_run * self._steps_per_epoch
+
+        self._lr_scheduler = self._setup_lr_scheduler(
+             cfg_lr_scheduler=cfg.lr_scheduler,
+             num_training_steps=2 * self.total_epochs * self._steps_per_epoch, # do not get to zero
+             last_epoch=self.global_step - 1,
+         )
+
+        # Set up profiler, returns DummyProfiler (nullcontext object with no-op `step` method)
+        # if cfg is missing profiler key or if `cfg.profiler.enabled = False`
+        self._profiler = self._setup_profiler(cfg.get(PROFILER_KEY, None))
+
+        # Used to ignore labels for loss computation
+        self.ignore_labels_cache = torch.full(
+            (cfg.batch_size, 1), self._loss_fn.ignore_index, device=self._device
+        )
+
+    def _setup_profiler(
+        self, cfg_profiler: Optional[DictConfig] = None
+    ) -> Union[torch.profiler.profile, DummyProfiler]:
+        """
+        Parses the `profiler` section of top-level `cfg` and sets up profiler
+
+        Args:
+            cfg_profiler (Optional[DictConfig]): ``profiler`` section of the top-level ``cfg`` (the main config passed to
+                `recipe.main`). Default None.
+
+        Returns:
+            profiler: Union[torch.profiler.profile, DummyProfiler] - DummyProfiler is a nullcontext with no-op methods
+            for `start`, `stop`, and `step` that can be used in place of `torch.profiler.profile` if profiler is not enabled such
+            that the instrumented training loop does not need to be changed profiling is disabled.
+
+        The profiler config can be provided in configs under the `profiler` key with the following layout:
+
+        .. code-block:: yaml
+            profiler:
+                enabled: bool
+
+                #Output directory of trace artifacts
+                output_dir: str
+
+            #`torch.profiler.ProfilerActivity` types to trace
+            cpu: bool
+            cuda: bool
+
+                #Trace options
+                profile_memory: bool
+                with_stack: bool
+                record_shapes: bool
+                with_flops: bool
+
+            # `torch.profiler.schedule` options:
+            # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+            wait_steps: int
+            warmup_steps: int
+            active_steps: int
+            num_cycles: int
+        """
+        # Missing profiler section in config, assume disabled
+        if cfg_profiler is None:
+            cfg_profiler = DictConfig({"enabled": False})
+
+        # Check that component is included and set correctly
+        if cfg_profiler.get("_component_", None) is None:
+            cfg_profiler["_component_"] = "torchtune.training.setup_torch_profiler"
+        else:
+            assert (
+                cfg_profiler.get("_component_")
+                == "torchtune.training.setup_torch_profiler"
+            ), "Only torch profiler supported currently: component must be `torchtune.training.setup_torch_profiler`"
+
+        profiler, profiler_cfg = config.instantiate(cfg_profiler)
+
+        if self._is_rank_zero:
+            log.info(f" Profiler config after instantiation: {profiler_cfg}")
+
+            self.profiler_profile_memory = profiler_cfg.get("profile_memory", False)
+            if profiler_cfg["enabled"]:
+                self.profiler_wait_steps = profiler_cfg["wait_steps"]
+                self.profiler_warmup_steps = profiler_cfg["warmup_steps"]
+                self.profiler_active_steps = profiler_cfg["active_steps"]
+
+        return profiler
+
+    def _setup_model(
+        self,
+        cfg_model: DictConfig,
+        enable_activation_checkpointing: bool,
+        custom_sharded_layers: Optional[List[str]],
+        fsdp_cpu_offload: bool,
+        reshard_after_forward: bool,
+        model_state_dict: Dict[str, Any],
+        ac_mode: Optional[str] = None,
+        ac_option: Optional[int] = None,
+    ) -> nn.Module:
+        """
+        Model initialization has some important considerations:
+           a. To minimize GPU peak memory, we initialize the model on meta device with
+              the right dtype
+           b. All ranks calls ``load_state_dict`` without peaking CPU RAMs since
+              full state dicts are loaded with ``torch.load(mmap=True)``
+        """
+
+        if self._is_rank_zero:
+            log.info(
+                "FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ..."
+            )
+            init_start = time.perf_counter()
+
+        with training.set_default_dtype(self._dtype), torch.device("meta"):
+            model = config.instantiate(cfg_model)
+
+        if self._compile:
+            training.compile_model(model, verbose=self._is_rank_zero)
+
+        # We currently have two versions of activation checkpointing in this recipe
+        # for testing and BC purposes. ``enable_activation_checkpointing`` controls
+        # the older version of AC and this behavior is unchanged
+        # ac_mode and ac_option together control selective AC. This is only enabled
+        # when these are set AND ``enable_activation_checkpointing`` is set to False
+        # We'll clean this up as soon as testing of AC is complete
+        if (not enable_activation_checkpointing) and (ac_mode is not None):
+            apply_selective_activation_checkpointing(
+                model,
+                ac_mode,
+                ac_option,
+            )
+
+        # original activation checkpointing (full) - flip the condition above
+        if enable_activation_checkpointing and ac_mode is None:
+            training.set_activation_checkpointing(
+                model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        # For FSDP sharding, we can condition on either the module or its name
+        # Shard conditions should be callables taking name (relative to model root)
+        # and the module itself and returning a bool on whether to shard the given module
+        fsdp_shard_conditions = []
+
+        # Shard transformer decoder layers (or AC-wrapped versions)
+        # Alternatively we could condition on the module type (TransformerDecoder or CheckpointWrapper)
+        # But directly using the name is more concise
+        def _is_layer_fqn(s: str) -> bool:
+            """
+            Return True for layers.i and False for all other module names
+            Covers sharding for both AC-wrapped and non-AC-wrapped modules in one shot
+            """
+            s_list = s.split(".")
+            return len(s_list) == 2 and s_list[0] == "layers" and str.isdigit(s_list[1])
+
+        fsdp_shard_conditions = [lambda n, m: _is_layer_fqn(n)]
+
+        # If wrapping any layers separately, we can add another shard condition
+        # A layer will be sharded if any of the fsdp_shard_conditions are met
+        if custom_sharded_layers:
+            fsdp_shard_conditions += [lambda n, m: n in custom_sharded_layers]
+
+        training.shard_model(
+            model=model,
+            shard_conditions=fsdp_shard_conditions,
+            cpu_offload=fsdp_cpu_offload,
+            reshard_after_forward=reshard_after_forward,
+        )
+
+        with training.set_default_dtype(self._dtype), self._device:
+            for m in model.modules():
+                # RoPE is not covered in state dict
+                if hasattr(m, "rope_init"):
+                    m.rope_init()
+
+        # This method will convert the full model state dict into a sharded state
+        # dict and load into the model
+        training.load_from_full_model_state_dict(
+            model,
+            model_state_dict,
+            self._device,
+            self._is_rank_zero,
+            strict=True,
+            cpu_offload=fsdp_cpu_offload,
+        )
+
+        # Ensure no params and buffers are on meta device
+        training.validate_no_params_on_meta_device(model)
+
+        if self._is_rank_zero:
+            log.info(
+                f"Instantiating model and loading checkpoint took {time.perf_counter() - init_start:.2f} secs"
+            )
+            memory_stats = training.get_memory_stats(device=self._device)
+            training.log_memory_stats(memory_stats)
+
+        # synchronize before training begins
+        torch.distributed.barrier()
+
+        return model
+
+    def _setup_optimizer(
+        self, cfg_optimizer: DictConfig, opt_state_dict: Optional[Dict[str, Any]] = None
+    ) -> Optimizer:
+        optimizer = config.instantiate(cfg_optimizer, self._model.parameters())
+        if opt_state_dict:
+            training.load_from_full_optimizer_state_dict(
+                optimizer,
+                opt_state_dict,
+                self._device,
+            )
+
+        if self._is_rank_zero:
+            log.info("Optimizer is initialized.")
+        return optimizer
+
+    def _setup_lr_scheduler(
+         self,
+         cfg_lr_scheduler: DictConfig,
+         num_training_steps: int,
+         last_epoch: int,
+     ) -> Optimizer:
+         lr_scheduler = config.instantiate(
+             cfg_lr_scheduler,
+             self._optimizer,
+             num_training_steps=num_training_steps,
+             last_epoch=last_epoch,
+         )
+         if self._is_rank_zero:
+             log.info("Learning rate scheduler is initialized.")
+         return lr_scheduler
+
+    def _setup_data(
+        self,
+        cfg_dataset: DictConfig,
+        shuffle: bool,
+        batch_size: int,
+        collate_fn: str,
+    ) -> Tuple[DistributedSampler, DataLoader]:
+        """
+        All data related setup happens here. Currently this recipe only supports the
+        DistributedSamplers with Map-style Datasets which fit into memory. Other samplers,
+        iterable datasets and streaming datasets are not supported.
+        """
+        world_size, rank = training.get_world_size_and_rank()
+
+        if isinstance(cfg_dataset, ListConfig):
+            datasets = [
+                config.instantiate(single_cfg_dataset, self._tokenizer)
+                for single_cfg_dataset in cfg_dataset
+            ]
+            ds = ConcatDataset(datasets=datasets)
+            packed = False
+        else:
+            ds = config.instantiate(cfg_dataset, self._tokenizer)
+            packed = cfg_dataset.get("packed", False)
+
+        # Instantiate collate_fn
+        if "left_pad_sequence" in collate_fn:
+            raise RuntimeError("left_pad_sequence collator is only for inference.")
+        collate_fn = _get_component_from_path(collate_fn)
+
+        sampler = DistributedSampler(
+            ds, num_replicas=world_size, rank=rank, shuffle=shuffle, seed=0
+        )
+        dataloader = DataLoader(
+            dataset=ds,
+            batch_size=batch_size,
+            sampler=sampler,
+            # dropping last avoids shape issues with compile + flex attention
+            drop_last=True,
+            collate_fn=partial(
+                collate_fn,
+                padding_idx=self._tokenizer.pad_id,
+                ignore_idx=self._loss_fn.ignore_index,
+            )
+            if not packed
+            else padded_collate_packed,
+        )
+
+        if self._is_rank_zero:
+            log.info("Dataset and Sampler are initialized.")
+
+        return sampler, dataloader
+
+    def save_checkpoint(
+        self,
+        epoch: int,
+    ) -> None:
+        """
+        Checkpoint the state of the recipe. The constructed checkpoint state dict
+        contains the following information:
+        - Model weights with key training.MODEL_KEY
+        - Relevant recipe state if training is not complete
+
+        Checkpointer will save the model weights and recipe state in
+        different checkpoint files. To correctly resume training from an intermediate checkpoint,
+        the model weights and recipe state must be provided.
+        """
+        # final dict passed onto the checkpointer
+        checkpoint_dict = {}
+
+        intermediate_checkpoint = epoch + 1 < self.total_epochs
+        # To prevent GPU memory from spiking during checkpoint save,
+        # we consolidate the full model and optim state dicts on CPU for rank 0
+        cpu_state_dict = training.get_full_model_state_dict(
+            self._model,
+            self._is_rank_zero,
+            device=self._device,
+        )
+
+        if intermediate_checkpoint:
+            opt_state_dict = training.get_full_optimizer_state_dict(
+                self._optimizer,
+                self._is_rank_zero,
+                device=self._device,
+            )
+        else:
+            opt_state_dict = None
+
+        # Now that we have the model and opt state dict, create the actual checkpoint dict
+        # to be sent to the checkpointer and ultimately written to file
+        if self._is_rank_zero:
+
+            checkpoint_dict.update({training.MODEL_KEY: cpu_state_dict})
+
+            # if training is in-progress, checkpoint the optimizer state and recipe state
+            # as well.
+            if intermediate_checkpoint:
+                checkpoint_dict.update(
+                    {
+                        training.OPT_KEY: opt_state_dict,
+                        training.SEED_KEY: self.seed,
+                        training.EPOCHS_KEY: self.epochs_run,
+                        training.TOTAL_EPOCHS_KEY: self.total_epochs,
+                        training.MAX_STEPS_KEY: self.max_steps_per_epoch,
+                    }
+                )
+
+            self._checkpointer.save_checkpoint(
+                checkpoint_dict,
+                epoch=epoch,
+                intermediate_checkpoint=intermediate_checkpoint,
+            )
+
+    def train(self) -> None:
+        """
+        The core training loop.
+        """
+        # clean up before training begins
+        training.cleanup_before_training()
+
+        _, rank = training.get_world_size_and_rank()
+
+        # zero out the gradients before starting training
+        self._optimizer.zero_grad()
+
+        # Initialize tokens count and running loss (for grad accumulation)
+        t0 = time.perf_counter()
+        running_loss = 0
+        num_tokens = 0
+
+        self._profiler.start()
+        # self.epochs_run should be non-zero when we're resuming from a checkpoint
+        for curr_epoch in range(self.epochs_run, self.total_epochs):
+
+            # Update the sampler to ensure data is correctly shuffled across epochs
+            # in case shuffle is True
+            self._sampler.set_epoch(curr_epoch)
+
+            pbar = tqdm(total=self._steps_per_epoch, disable=not (rank == 0))
+            for idx, batch in enumerate(self._dataloader):
+                if (
+                    self.max_steps_per_epoch is not None
+                    and (idx // self._gradient_accumulation_steps)
+                    == self.max_steps_per_epoch
+                ):
+                    break
+
+                # Start tracking CUDA memory for active steps for just the first epoch
+                if (
+                    self._is_rank_zero
+                    and curr_epoch == 0
+                    and self.profiler_profile_memory
+                    and idx == self.profiler_wait_steps + self.profiler_warmup_steps
+                ):
+                    torch.cuda.memory._record_memory_history()
+
+                utils.batch_to_device(batch, self._device)
+
+                num_tokens += batch["tokens"].numel()
+
+                # Shape [b, s], needed for the loss not the model
+                labels = batch.pop("labels")
+
+                logits = self._model(**batch)
+
+                # Shift labels to compute loss
+                # equivalent to doing labels[..., 1:] and logits[..., :-1, :]
+                # But this way we dont need to slice the logits. We just add an ignore index to labels.
+                labels = torch.hstack(
+                    (labels[..., 1:], self.ignore_labels_cache[: labels.shape[0]])
+                )
+                if not isinstance(logits, list):
+                    labels = labels.reshape(-1)
+                    logits = logits.reshape(-1, logits.size(-1))
+
+                # Compute loss
+                loss = self._loss_fn(logits, labels)
+
+                # free logits otherwise it peaks backward memory
+                del logits
+
+                loss = loss / self._gradient_accumulation_steps
+                running_loss += loss
+                loss.backward()
+
+                # Step with optimizer
+                if (idx + 1) % self._gradient_accumulation_steps == 0:
+                    if self._clip_grad_norm is not None:
+                        grad_norm = torch.nn.utils.clip_grad_norm_(
+                            self._model.parameters(),
+                            max_norm=float(self._clip_grad_norm),
+                        )
+                    self._optimizer.step()
+                    self._optimizer.zero_grad(set_to_none=True)
+                    self._lr_scheduler.step()
+
+                    # Update the number of steps when the weights are updated
+                    self.global_step += 1
+
+                    loss_to_log = running_loss.item()
+                    pbar.update(1)
+                    pbar.set_description(
+                        f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
+                    )
+
+                    # Log per-step metrics
+                    if (
+                        self.global_step % self._log_every_n_steps == 0
+                        and self._is_rank_zero
+                    ):
+                        time_per_step = time.perf_counter() - t0
+                        log_dict = {
+                            "loss": loss_to_log,
+                            "lr": self._optimizer.param_groups[0]["lr"],
+                            "tokens_per_second_per_gpu": num_tokens / time_per_step,
+                        }
+                        if self._log_peak_memory_stats:
+                            log_dict.update(
+                                training.get_memory_stats(device=self._device)
+                            )
+                        if self._clip_grad_norm is not None:
+                            log_dict.update({"grad_norm": grad_norm})
+                        self._metric_logger.log_dict(
+                            log_dict,
+                            step=self.global_step,
+                        )
+
+                    # Reset running stats for the next step
+                    running_loss = 0
+                    num_tokens = 0
+                    t0 = time.perf_counter()
+
+                    if self.global_step % 1000 == 0:
+                        self.save_checkpoint(epoch=curr_epoch)
+
+                    # Stop tracking CUDA memory now that active steps are complete
+                    if (
+                        self._is_rank_zero
+                        and curr_epoch == 0
+                        and self.profiler_profile_memory
+                        and idx
+                        == self.profiler_wait_steps
+                        + self.profiler_warmup_steps
+                        + self.profiler_active_steps
+                    ):
+                        torch.cuda.memory._record_memory_history(enabled=None)
+
+                    # Step profiler
+                    # Note that this is called within gradient accumulation block, hence
+                    # will include multiple forward / backward passes if gradient accumulation > 1
+                    self._profiler.step()
+
+            self.epochs_run += 1
+            self.save_checkpoint(epoch=curr_epoch)
+
+        self._profiler.stop()
+
+    def cleanup(self) -> None:
+        if self._is_rank_zero:
+            self._metric_logger.close()
+        destroy_process_group()
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """
+    Entry point for the recipe.
+
+    Configurable parameters are read in the following order:
+        - Parameters specified in config (see available configs through ``tune ls``)
+        - Overwritten by arguments from the command-line
+    """
+    if not training.is_distributed():
+        raise RuntimeError(
+            "Distributed finetune recipe should be run via a distributed launcher."
+            "If using tune CLI, please specify --nnodes 1 and --nproc_per_node [num_gpus]"
+        )
+    init_process_group(backend="gloo" if cfg.device == "cpu" else "nccl")
+    if cfg.get("fsdp_cpu_offload", False):
+        # Utilize all available CPU cores for intra-op parallelism. This provides ~2x
+        # speed up when benchmarking fused AdamW on CPU
+        training.set_torch_num_threads()
+
+    config.log_config(recipe_name="FullFinetuneRecipeDistributed", cfg=cfg)
+
+    recipe = FullFinetuneRecipeDistributed(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
diff -ruN marc_original/third_party/torchtune/recipes/full_finetune_single_device.py marc/third_party/torchtune/recipes/full_finetune_single_device.py
--- marc_original/third_party/torchtune/recipes/full_finetune_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/full_finetune_single_device.py	2025-02-20 17:49:29.486024141 -0500
@@ -0,0 +1,726 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import sys
+import time
+from functools import partial
+from typing import Any, Dict, Optional, Tuple, Union
+from warnings import warn
+
+import torch
+from omegaconf import DictConfig, ListConfig
+
+from torch import nn
+from torch.optim import Optimizer
+from torch.utils.data import DataLoader, DistributedSampler
+
+from torchtune import config, modules, training, utils
+from torchtune.config._utils import _get_component_from_path
+from torchtune.data import padded_collate_packed
+from torchtune.datasets import ConcatDataset
+from torchtune.recipe_interfaces import FTRecipeInterface
+from torchtune.training import DummyProfiler, PROFILER_KEY
+
+from tqdm import tqdm
+
+
+log = utils.get_logger("DEBUG")
+
+
+class FullFinetuneRecipeSingleDevice(FTRecipeInterface):
+    """
+    Full finetuning recipe for dense transformer-based LLMs such as Llama2. This recipe is optimized
+    for single GPU training. Training on CPU is not supported.
+
+    Features:
+        - Activation Checkpointing. This can be controlled using the ``activation_checkpointing``
+            flag. Activation checkpointing helps reduce the memory footprint since we no longer keep
+            activations in memory and instead recompute them during the backward pass. This is especially
+            helpful for larger batch sizes when you're memory constrained. But these savings in memory
+            come at the cost of training performance. In most cases training can slow-down quite a bit as
+            a result of this activation recomputation.
+
+        - Precision. Full fp32 and bf16 training are supported. Precision is controlled using the ``dtype``
+            flag. When ``dtype=bf16``, all activations, gradients and optimizer states are in bfloat16. In
+            most cases this should halve the memory footprint of full precision (fp32) training, without
+            loss in model quality (will depend on the model, training data and other settings). For
+            GPUs which do not support bfloat16, we fall back to fp32. Mixed precision training and fp16
+            precision are currently not supported.
+
+        - Gradient Accumulation. You can simulate larger batch sizes by accumulating gradients. This is
+            controlled using the ``gradient_accumulation_steps`` flag.
+
+                Total Batch Size = batch_size * gradient accumulation steps.
+
+            For example: with batch_size=1 and gradient_accumulation_steps=32 we get a total batch size of 32.
+
+            Gradient accumulation is especially useful when you are memory constrained. In this case,
+            accumulating gradients might give you better training speed than enabling activation
+            checkpointing.
+
+        - Optimizer in Backward. Fusing the optimizer step into the backward pass helps reduce the memory
+            footprint associated with gradients. This can be especially helpful when you are memory
+            constrained. Note that users can only use ONE of gradient accumulation or optimizer in backward.
+            These features currently do not work together. For more details on optimizer in backward, please
+            see this tutorial: https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html
+
+        - Lower precision optimizers. This recipe supports lower-precision optimizers from the bitsandbytes
+            library (https://huggingface.co/docs/bitsandbytes/main/en/index). We've tested the recipe with
+            8-bit AdamW and Paged AdamW. These optimizers are especially helpful when you are memory constrained
+            since they help reduce the memory footprint associated with the optimizer states.
+
+        - Checkpointing. Model weights are checkpointed both at the end of each epoch and at the end of
+            training. Optimizer State and recipe state (seed, total_epochs, number of epochs run etc) are
+            only saved at the end of a given epoch and used in case of resuming training.
+
+            Resuming training is controlled by the ``resume_from_checkpoint`` flag. Mid-epoch checkpointing is
+            currently not supported.
+
+            For more details on the checkpointer, please take a look at
+            our checkpointer deepdive (https://pytorch.org/torchtune/main/deep_dives/checkpointer.html).
+
+        - Logging. Terminal, Disk, WandB and TensorBoard are all supported.
+
+        - Gradient Clipping. Gradient clipping is supported using the ``clip_grad_norm`` flag. By default,
+            ``clip_grad_norm`` is set to ``None``. If you only want to log the grad norm, you can set
+            ``clip_grad_norm='inf'``.
+
+    For a full list of example configs for this recipe, run ``tune ls`` on the command line. Each config
+    has example commands for how to kick-off training.
+
+    Args:
+        cfg (DictConfig): OmegaConf object parsed from yaml file
+
+    Raises:
+        ValueError: If ``dtype`` is set to fp16.
+        RuntimeError: If ``dtype`` is set to bf16 and the hardware does not support bf16.
+        RuntimeError: If ``gradient_accumulation_steps > 1`` and ``optimizer_in_bwd`` is `True`.
+        RuntimeError: If ``left_pad_sequence`` is set as the data collator.
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        self._device = utils.get_device(device=cfg.device)
+        self._dtype = training.get_dtype(cfg.dtype, device=self._device)
+        # Disable for fp16, as we haven't validated "full" fp16 with this recipe, nor
+        # enabled necessary features such as gradient scaling.
+        if self._dtype == torch.float16:
+            raise ValueError(
+                "full fp16 training is not supported with this recipe. Please use bf16 or fp32 instead."
+            )
+
+        # logging attributes
+        self._output_dir = cfg.output_dir
+        self._log_every_n_steps = cfg.get("log_every_n_steps", 1)
+        self._log_peak_memory_stats = cfg.get("log_peak_memory_stats", False)
+
+        # Training cfg
+        self._resume_from_checkpoint = cfg.resume_from_checkpoint
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+        self._optimizer_in_bwd = cfg.optimizer_in_bwd
+
+        # TODO: find a better place / way to perform validation of args that don't yet
+        # compose with each other.
+        if self._gradient_accumulation_steps > 1 and self._optimizer_in_bwd:
+            raise RuntimeError(
+                "Gradient accumulation is not supported with optimizer in bwd."
+                "Please set gradient_accumulation_steps=1, or optimizer_in_bwd=False."
+            )
+
+        # These are public properties which are updated by the checkpoint loader
+        # when ``resume_from_checkpoint`` is `True` or validated in tests
+        self.seed = training.set_seed(seed=cfg.seed)
+        self.epochs_run = 0
+        self.total_epochs = cfg.epochs
+        self.max_steps_per_epoch = cfg.max_steps_per_epoch
+        self.global_step = 0
+        self._clip_grad_norm = cfg.get("clip_grad_norm", None)
+
+    def load_checkpoint(self, cfg_checkpointer: DictConfig) -> Dict[str, Any]:
+        """
+        Extract the checkpoint state from file and validate. If resume_from_checkpoint
+        is True, this also includes the recipe state.
+        """
+        self._checkpointer = config.instantiate(
+            cfg_checkpointer,
+            resume_from_checkpoint=self._resume_from_checkpoint,
+        )
+        checkpoint_dict = self._checkpointer.load_checkpoint()
+
+        if self._resume_from_checkpoint:
+            self._update_recipe_state(checkpoint_dict)
+        return checkpoint_dict
+
+    def _update_recipe_state(self, ckpt_dict: Dict[str, Any]) -> None:
+        """
+        Updates the recipe state from checkpoint.
+        """
+        try:
+            self.epochs_run = ckpt_dict[training.EPOCHS_KEY]
+
+            # on mismatch, warn the user and prevent the override
+            if self.seed != ckpt_dict[training.SEED_KEY]:
+                warn(
+                    message=(
+                        "Config value for seed does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.SEED_KEY]}"
+                    )
+                )
+                self.seed = ckpt_dict[training.SEED_KEY]
+            if self.max_steps_per_epoch != ckpt_dict[training.MAX_STEPS_KEY]:
+                warn(
+                    message=(
+                        "Config value for max_steps_per_epoch does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.MAX_STEPS_KEY]}"
+                    )
+                )
+                self.max_steps_per_epoch = ckpt_dict[training.MAX_STEPS_KEY]
+
+            # on mismatch, warn the user but allow the override
+            if self.total_epochs != ckpt_dict[training.TOTAL_EPOCHS_KEY]:
+                warn(
+                    message=(
+                        "Config value for total_epochs does not match the checkpoint value, "
+                        f"using the config value: {self.total_epochs}"
+                    )
+                )
+
+        except KeyError as e:
+            raise KeyError(
+                "Checkpoint does not contain the required keys needed for updating recipe state. "
+                "Are you sure you passed in the right recipe checkpoint?"
+            ) from e
+
+    def setup(self, cfg: DictConfig) -> None:
+        """
+        Sets up the recipe state correctly. This includes setting recipe attributes based
+        on the ``resume_from_checkpoint`` flag.
+        """
+        self._metric_logger = config.instantiate(cfg.metric_logger)
+
+        # log config with parameter override
+        self._metric_logger.log_config(cfg)
+
+        ckpt_dict = self.load_checkpoint(cfg.checkpointer)
+
+        # ``_setup_model`` handles initialization and loading the state dict. This method
+        # should be called before ``_setup_optimizer`` since transforming the optimizer
+        # state dict requires the model
+        self._compile = cfg.compile
+        self._model = self._setup_model(
+            cfg_model=cfg.model,
+            enable_activation_checkpointing=cfg.enable_activation_checkpointing,
+            compile_model=self._compile,
+            model_state_dict=ckpt_dict[training.MODEL_KEY],
+        )
+        self._tokenizer = config.instantiate(cfg.tokenizer)
+        log.info("Tokenizer is initialized from file.")
+
+        # _setup_optimizer should take in ckpt_dict only if training is resumed from
+        # checkpoint. Transforming the opt state dict is handled by this method
+        self._optimizer = self._setup_optimizer(
+            cfg_optimizer=cfg.optimizer,
+            optimizer_in_bwd=cfg.optimizer_in_bwd,
+            opt_state_dict=(
+                ckpt_dict[training.OPT_KEY] if self._resume_from_checkpoint else None
+            ),
+        )
+
+        # initialize loss
+        self._loss_fn = config.instantiate(cfg.loss)
+
+        if self._compile:
+            training.compile_loss(self._loss_fn)
+
+        if self._loss_fn.__class__.__name__ == "CEWithChunkedOutputLoss":
+            # set num_output_chunks for model
+            self._model.set_num_output_chunks(self._loss_fn.num_output_chunks)
+
+        log.info("Loss is initialized.")
+
+        # sampler and dataloader depend on the tokenizer and loss_fn and should be
+        # setup after both of these are initialized
+        collate_name = cfg.get("collate_fn", "torchtune.data.padded_collate_sft")
+        self._sampler, self._dataloader = self._setup_data(
+            cfg_dataset=cfg.dataset,
+            shuffle=cfg.shuffle,
+            batch_size=cfg.batch_size,
+            collate_fn=collate_name,
+        )
+
+        # Finally update the recipe state which can only be correctly set after all of the
+        # other components have been initialized and updated.
+        #
+        # Number of training steps in each epoch depends on the number of batches produced
+        # by the dataloader, the max_steps_per_epoch param set by the user and the
+        # gradient_accumulation_steps param. This value is used for logging and tracking
+        # training state. The computation should happen after the dataloader has been setup
+        self._steps_per_epoch = (
+            len(self._dataloader) // self._gradient_accumulation_steps
+        )
+        if (
+            self.max_steps_per_epoch is not None
+            and self.max_steps_per_epoch < self._steps_per_epoch
+        ):
+            self._steps_per_epoch = self.max_steps_per_epoch
+        self.global_step = self.epochs_run * self._steps_per_epoch
+
+        # Setup lr scheduler
+        self._lr_scheduler = self._setup_lr_scheduler(
+            cfg_lr_scheduler=cfg.get("lr_scheduler", None),
+            num_training_steps=self.total_epochs * self._steps_per_epoch,
+            last_epoch=self.global_step - 1,
+        )
+
+        # Set up profiler, returns DummyProfiler (nullcontext object with no-op `step` method)
+        # if cfg is missing profiler key or if `cfg.profiler.enabled = False`
+        self._profiler = self._setup_profiler(cfg.get(PROFILER_KEY, None))
+
+        # Used to ignore labels for loss computation
+        self.ignore_labels_cache = torch.full(
+            (cfg.batch_size, 1), self._loss_fn.ignore_index, device=self._device
+        )
+
+    def _setup_profiler(
+        self, cfg_profiler: Optional[DictConfig] = None
+    ) -> Union[torch.profiler.profile, DummyProfiler]:
+        """
+        Parses the `profiler` section of top-level `cfg` and sets up profiler
+
+        Args:
+            cfg_profiler (Optional[DictConfig]): ``profiler`` section of the top-level ``cfg`` (the main config passed to
+                `recipe.main`). Default None.
+
+        Returns:
+            profiler: Union[torch.profiler.profile, DummyProfiler] - DummyProfiler is a nullcontext with no-op methods
+            for `start`, `stop`, and `step` that can be used in place of `torch.profiler.profile` if profiler is not enabled such
+            that the instrumented training loop does not need to be changed profiling is disabled.
+
+        The profiler config can be provided in configs under the `profiler` key with the following layout:
+
+        .. code-block:: yaml
+            profiler:
+                enabled: bool
+
+                #Output directory of trace artifacts
+                output_dir: str
+
+            #`torch.profiler.ProfilerActivity` types to trace
+            cpu: bool
+            cuda: bool
+
+                #Trace options
+                profile_memory: bool
+                with_stack: bool
+                record_shapes: bool
+                with_flops: bool
+
+            # `torch.profiler.schedule` options:
+            # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+            wait_steps: int
+            warmup_steps: int
+            active_steps: int
+            num_cycles: int
+        """
+
+        # Missing profiler section in config, assume disabled
+        if cfg_profiler is None:
+            cfg_profiler = DictConfig({"enabled": False})
+
+        # Check that component is included and set correctly
+        if cfg_profiler.get("_component_", None) is None:
+            cfg_profiler["_component_"] = "torchtune.training.setup_torch_profiler"
+        else:
+            assert (
+                cfg_profiler.get("_component_")
+                == "torchtune.training.setup_torch_profiler"
+            ), "Only torch profiler supported currently: component must be `torchtune.training.setup_torch_profiler`"
+
+        profiler, profiler_cfg = config.instantiate(cfg_profiler)
+
+        log.info(f" Profiler config after instantiation: {profiler_cfg}")
+
+        self.profiler_profile_memory = profiler_cfg.get("profile_memory", False)
+        if profiler_cfg["enabled"]:
+            self.profiler_wait_steps = profiler_cfg["wait_steps"]
+            self.profiler_warmup_steps = profiler_cfg["warmup_steps"]
+            self.profiler_active_steps = profiler_cfg["active_steps"]
+
+        return profiler
+
+    def _setup_model(
+        self,
+        cfg_model: DictConfig,
+        enable_activation_checkpointing: bool,
+        compile_model: bool,
+        model_state_dict: Dict[str, Any],
+    ) -> nn.Module:
+        """
+        Set up the model including enabling activation checkpointing.
+        """
+        with training.set_default_dtype(self._dtype), self._device:
+            model = config.instantiate(cfg_model)
+
+        if compile_model:
+            training.compile_model(model)
+
+        if enable_activation_checkpointing:
+            training.set_activation_checkpointing(
+                model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        model.load_state_dict(model_state_dict)
+
+        # Validate model was loaded in with the expected dtype.
+        training.validate_expected_param_dtype(
+            model.named_parameters(), dtype=self._dtype
+        )
+        log.info(f"Model is initialized with precision {self._dtype}.")
+
+        if self._device.type == "cuda":
+            memory_stats = training.get_memory_stats(device=self._device)
+            training.log_memory_stats(memory_stats)
+
+        return model
+
+    def _setup_optimizer(
+        self,
+        cfg_optimizer: DictConfig,
+        optimizer_in_bwd: bool = False,
+        opt_state_dict: Optional[Dict[str, Any]] = None,
+    ) -> Optional[Optimizer]:
+        """
+        Set up the optimizer. This method also handles loading the optimizer state_dict, if specified.
+        """
+        if optimizer_in_bwd:
+            # Maintain a dict of optims for every parameter.
+            optim_dict = {
+                p: config.instantiate(cfg_optimizer, [p])
+                for p in self._model.parameters()
+            }
+            # Register optimizer step hooks on the model to run optimizer in backward.
+            training.register_optim_in_bwd_hooks(
+                model=self._model, optim_dict=optim_dict
+            )
+            # Create a wrapper for checkpoint save/load of optimizer states when running in backward.
+            self._optim_ckpt_wrapper = training.create_optim_in_bwd_wrapper(
+                model=self._model, optim_dict=optim_dict
+            )
+            # Load optimizer states. If optimizer states are being restored in an optimizer in backward
+            # run, these need to have been saved with the same setting. Cannot restore from runs that did not
+            # use optimizer in backward.
+            if opt_state_dict is not None:
+                try:
+                    self._optim_ckpt_wrapper.load_state_dict(opt_state_dict)
+                except BaseException as e:
+                    raise RuntimeError(
+                        "Failed loading in-backward optimizer checkpoints."
+                        "Please make sure run being restored from was using in-backward optimizer."
+                    ) from e
+            log.info("In-backward optimizers are set up.")
+            return None
+        else:
+            optimizer = config.instantiate(cfg_optimizer, self._model.parameters())
+
+            if opt_state_dict:
+                optimizer.load_state_dict(opt_state_dict)
+            log.info("Optimizer is initialized.")
+            return optimizer
+
+    def _setup_lr_scheduler(
+        self,
+        cfg_lr_scheduler: Optional[DictConfig],
+        num_training_steps: int,
+        last_epoch: int,
+    ) -> Optional[Optimizer]:
+        """
+        Set up the learning rate scheduler based on the provided configuration.
+        It handles both standard optimization and optimizer-in-backward cases, and supports
+        schedulers from both torchtune.modules and torch.optim.
+
+        Args:
+            cfg_lr_scheduler (Optional[DictConfig]): The learning rate scheduler configuration.
+            num_training_steps (int): The total number of training steps.
+            last_epoch (int): The index of the last epoch.
+
+        Returns:
+            lr_scheduler (Optional[Optimizer]): The learning rate scheduler.
+        """
+        if cfg_lr_scheduler is None:
+            log.info(
+                "No learning rate scheduler configured. Using constant learning rate."
+            )
+            return None
+
+        if self._optimizer_in_bwd:
+            # Use the first optimizer from the wrapper to represent the learning rate
+            optimizer = next(iter(self._optim_ckpt_wrapper.optim_map.values()))
+        else:
+            # Standard case: use the single optimizer
+            optimizer = self._optimizer
+
+        # Instantiate the learning rate scheduler
+        lr_scheduler = config.instantiate(
+            cfg_lr_scheduler,
+            optimizer,
+            num_training_steps=num_training_steps,
+            last_epoch=last_epoch,
+        )
+
+        if self._optimizer_in_bwd:
+            # Modify the scheduler for optimizer_in_bwd case
+            self._optim_ckpt_wrapper.set_lr_scheduler(lr_scheduler)
+
+        log.info("Learning rate scheduler is initialized.")
+        return lr_scheduler
+
+    def _setup_data(
+        self,
+        cfg_dataset: DictConfig,
+        shuffle: bool,
+        batch_size: int,
+        collate_fn: str,
+    ) -> Tuple[DistributedSampler, DataLoader]:
+        """
+        All data related setup happens here. Currently this recipe only supports the
+        DistributedSamplers with Map-style Datasets which fit into memory. Other samplers,
+        iterable datasets and streaming datasets are not supported.
+        """
+        if isinstance(cfg_dataset, ListConfig):
+            datasets = [
+                config.instantiate(single_cfg_dataset, self._tokenizer)
+                for single_cfg_dataset in cfg_dataset
+            ]
+            ds = ConcatDataset(datasets=datasets)
+            packed = False
+        else:
+            ds = config.instantiate(cfg_dataset, self._tokenizer)
+            packed = cfg_dataset.get("packed", False)
+
+        # Instantiate collate_fn
+        if "left_pad_sequence" in collate_fn:
+            raise RuntimeError("left_pad_sequence collator is only for inference.")
+        collate_fn = _get_component_from_path(collate_fn)
+
+        sampler = DistributedSampler(
+            ds,
+            num_replicas=1,
+            rank=0,
+            shuffle=shuffle,
+            seed=0,
+        )
+        dataloader = DataLoader(
+            dataset=ds,
+            batch_size=batch_size,
+            sampler=sampler,
+            # dropping last avoids shape issues with compile + flex attention
+            drop_last=True,
+            collate_fn=partial(
+                collate_fn,
+                padding_idx=self._tokenizer.pad_id,
+                ignore_idx=self._loss_fn.ignore_index,
+            )
+            if not packed
+            else padded_collate_packed,
+        )
+
+        log.info("Dataset and Sampler are initialized.")
+
+        return sampler, dataloader
+
+    def save_checkpoint(self, epoch: int) -> None:
+        """
+        Save state dict to file. The recipe save_checkpoint method is responsible for
+        correctly creating the checkpoint dict and passing to the checkpointer.
+        """
+        ckpt_dict = {training.MODEL_KEY: self._model.state_dict()}
+        # if training is in-progress, checkpoint the optimizer state as well
+        if epoch + 1 < self.total_epochs:
+            ckpt_dict.update(
+                {
+                    training.SEED_KEY: self.seed,
+                    training.EPOCHS_KEY: self.epochs_run,
+                    training.TOTAL_EPOCHS_KEY: self.total_epochs,
+                    training.MAX_STEPS_KEY: self.max_steps_per_epoch,
+                }
+            )
+            if not self._optimizer_in_bwd:
+                ckpt_dict[training.OPT_KEY] = self._optimizer.state_dict()
+            else:
+                ckpt_dict[training.OPT_KEY] = self._optim_ckpt_wrapper.state_dict()
+        self._checkpointer.save_checkpoint(
+            ckpt_dict,
+            epoch=epoch,
+            intermediate_checkpoint=(epoch + 1 < self.total_epochs),
+        )
+
+    def _loss_step(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
+        # Shape [b, s], needed for the loss not the model
+        labels = batch.pop("labels")
+
+        logits = self._model(**batch)
+
+        # Shift labels to compute loss
+        # equivalent to doing labels[..., 1:] and logits[..., :-1, :]
+        # But this way we dont need to slice the logits. We just add an ignore index to labels.
+        labels = torch.hstack(
+            (labels[..., 1:], self.ignore_labels_cache[: labels.shape[0]])
+        )
+        if not isinstance(logits, list):
+            labels = labels.reshape(-1)
+            logits = logits.reshape(-1, logits.size(-1))
+
+        # Compute loss
+        loss = self._loss_fn(logits, labels)
+        # free logits otherwise it peaks backward memory
+        del logits
+
+        return loss
+
+    def train(self) -> None:
+        """
+        The core training loop. Supports training on subsets of the dataset using the
+        ``max_steps_per_epoch``.
+        """
+        if self._compile:
+            log.info(
+                "NOTE: torch.compile is enabled and model is compiled in first forward. Expect a relatively slow first iteration."
+            )
+        # zero out the gradients before starting training
+        if not self._optimizer_in_bwd:
+            self._optimizer.zero_grad()
+
+        # Initialize tokens count and running loss (for grad accumulation)
+        t0 = time.perf_counter()
+        running_loss = 0
+        num_tokens = 0
+
+        self._profiler.start()
+        # self.epochs_run should be non-zero when we're resuming from a checkpoint
+        for curr_epoch in range(self.epochs_run, self.total_epochs):
+            # Update the sampler to ensure data is correctly shuffled across epochs
+            # in case shuffle is True
+            self._sampler.set_epoch(curr_epoch)
+
+            pbar = tqdm(total=self._steps_per_epoch)
+            for idx, batch in enumerate(self._dataloader):
+                if (
+                    self.max_steps_per_epoch is not None
+                    and (idx // self._gradient_accumulation_steps)
+                    == self.max_steps_per_epoch
+                ):
+                    break
+
+                # Start tracking CUDA memory for active steps for just the first epoch
+                if (
+                    curr_epoch == 0
+                    and self.profiler_profile_memory
+                    and idx == self.profiler_wait_steps + self.profiler_warmup_steps
+                ):
+                    torch.cuda.memory._record_memory_history()
+
+                utils.batch_to_device(batch, self._device)
+                num_tokens += batch["tokens"].numel()
+
+                loss = self._loss_step(batch)
+                loss = loss / self._gradient_accumulation_steps
+                running_loss += loss
+                loss.backward()
+
+                # Step with optimizer
+                if (idx + 1) % self._gradient_accumulation_steps == 0:
+                    if self._clip_grad_norm is not None:
+                        grad_norm = torch.nn.utils.clip_grad_norm_(
+                            self._model.parameters(),
+                            max_norm=float(self._clip_grad_norm),
+                        )
+                    if not self._optimizer_in_bwd:
+                        self._optimizer.step()
+                        self._optimizer.zero_grad(set_to_none=True)
+
+                    # Need to fix `lr_scheduler.step()` before `optimizer.step()` warning
+                    if self._lr_scheduler is not None:
+                        self._lr_scheduler.step()
+                    self.global_step += 1
+
+                    loss_to_log = running_loss.item()
+                    pbar.update(1)
+                    pbar.set_description(
+                        f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
+                    )
+
+                    # Log per-step metrics
+                    if self.global_step % self._log_every_n_steps == 0:
+                        time_per_step = time.perf_counter() - t0
+                        log_dict = {
+                            "loss": loss_to_log,
+                            # NOTE: for optim in backward, this assumes all optimizers have the same LR. This is currently
+                            # true since we don't expose the ability to configure this yet.
+                            "lr": (
+                                self._optim_ckpt_wrapper.get_optim_key("lr")
+                                if self._optimizer_in_bwd
+                                else self._optimizer.param_groups[0]["lr"]
+                            ),
+                            "tokens_per_second_per_gpu": num_tokens / time_per_step,
+                        }
+                        if self._device.type == "cuda" and self._log_peak_memory_stats:
+                            log_dict.update(
+                                training.get_memory_stats(device=self._device)
+                            )
+                        if self._clip_grad_norm is not None:
+                            log_dict.update({"grad_norm": grad_norm})
+                        self._metric_logger.log_dict(
+                            log_dict,
+                            step=self.global_step,
+                        )
+
+                    # Reset running stats for the next step
+                    running_loss = 0
+                    num_tokens = 0
+                    t0 = time.perf_counter()
+
+                # Stop tracking CUDA memory now that active steps are complete
+                if (
+                    curr_epoch == 0
+                    and self.profiler_profile_memory
+                    and idx
+                    == self.profiler_wait_steps
+                    + self.profiler_warmup_steps
+                    + self.profiler_active_steps
+                ):
+                    torch.cuda.memory._record_memory_history(enabled=None)
+
+                # Step the profiler
+                # Note we are stepping each batch, which might not include optimizer step in the trace
+                # if the schedule cycle doesn't align with gradient accumulation.
+                self._profiler.step()
+
+            self.epochs_run += 1
+            self.save_checkpoint(epoch=curr_epoch)
+
+        self._profiler.stop()
+
+    def cleanup(self) -> None:
+        self._metric_logger.close()
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """
+    Entry point for the recipe.
+
+    Configurable parameters are read in the following order:
+        - Parameters specified in config (see available configs through ``tune ls``)
+        - Overwritten by arguments from the command-line
+    """
+    config.log_config(recipe_name="FullFinetuneRecipeSingleDevice", cfg=cfg)
+    recipe = FullFinetuneRecipeSingleDevice(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
diff -ruN marc_original/third_party/torchtune/recipes/generate.py marc/third_party/torchtune/recipes/generate.py
--- marc_original/third_party/torchtune/recipes/generate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/generate.py	2025-02-20 17:49:29.490024147 -0500
@@ -0,0 +1,229 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import itertools
+import sys
+import time
+from typing import Any, Dict, List, Optional, Union
+
+import torch
+from omegaconf import DictConfig
+from torch import nn
+
+from torchtune import config, generation, training, utils
+from torchtune.config._utils import _get_component_from_path
+from torchtune.data import ChatFormat, InstructTemplate, Message
+from torchtune.training import FullModelTorchTuneCheckpointer
+
+logger = utils.get_logger("DEBUG")
+
+
+class InferenceRecipe:
+    """
+    Recipe for generating tokens from a dense Transformer-based LLM.
+
+    Currently this recipe supports single-GPU generation only. Speculative
+    decoding is not supported.
+
+    For more details on how to use this recipe for generation, please see our
+    tutorial: https://pytorch.org/torchtune/main/tutorials/e2e_flow.html#generation
+
+    For using this recipe with a quantized model, please the following section of
+    the above tutorial:
+    https://pytorch.org/torchtune/main/tutorials/e2e_flow.html#speeding-up-generation-using-quantization
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        self._device = utils.get_device(device=cfg.device)
+        self._dtype = training.get_dtype(dtype=cfg.dtype, device=self._device)
+        self._quantizer = config.instantiate(cfg.quantizer)
+        self._quantization_mode = training.get_quantizer_mode(self._quantizer)
+
+        training.set_seed(seed=cfg.seed)
+
+    def setup(self, cfg: DictConfig) -> None:
+        checkpointer = config.instantiate(cfg.checkpointer)
+
+        if self._quantization_mode is not None:
+            if not isinstance(checkpointer, FullModelTorchTuneCheckpointer):
+                raise ValueError(
+                    "Quantization is only supported for models quantized and saved with the "
+                    "FullModelTorchTuneCheckpointer - please ensure you have quantized your "
+                    "model and are using the quantized weights!"
+                )
+            if "qat" in self._quantization_mode:
+                raise ValueError(
+                    "You have specified a quantizer with 'QAT' - "
+                    "QAT quantizers should only be used during quantization aware training "
+                    "and when quantizing models. Please use the corresponding post-training "
+                    "quantizer e.g. Int8DynActInt4WeightQuantizer for Int8DynActInt4WeightQATQuantizer."
+                )
+
+        if self._quantization_mode is None:
+            ckpt_dict = checkpointer.load_checkpoint()
+        else:
+            # weights_only needs to be False when loading a quantized model
+            ckpt_dict = checkpointer.load_checkpoint(weights_only=False)
+
+        self._model = self._setup_model(
+            model_cfg=cfg.model,
+            model_state_dict=ckpt_dict[training.MODEL_KEY],
+        )
+        self._tokenizer = config.instantiate(cfg.tokenizer)
+
+    def _setup_model(
+        self,
+        model_cfg: DictConfig,
+        model_state_dict: Dict[str, Any],
+    ) -> nn.Module:
+        with training.set_default_dtype(self._dtype), self._device:
+            model = config.instantiate(model_cfg)
+
+        if self._quantization_mode is not None:
+            model = self._quantizer.quantize(model)
+            model = model.to(device=self._device, dtype=self._dtype)
+            for k, v in model_state_dict.items():
+                model_state_dict[k] = v.to(self._device)
+            model.load_state_dict(model_state_dict, assign=True)
+        else:
+            model.load_state_dict(model_state_dict)
+
+        # Validate model was loaded in with the expected dtype.
+        training.validate_expected_param_dtype(
+            model.named_parameters(), dtype=self._dtype
+        )
+        logger.info(f"Model is initialized with precision {self._dtype}.")
+
+        return model
+
+    def convert_prompt_to_tokens(
+        self,
+        prompt: Union[DictConfig, str],
+        chat_format: Optional[ChatFormat],
+        instruct_template: Optional[InstructTemplate],
+    ) -> List[Message]:
+        """
+        Either:
+        (1) a raw string is passed as the prompt, in which case we call tokenizer.encode directly, or
+        (2) a DictConfig is passed as the prompt. In this case there are three possibilities:
+            (a) an InstructTemplate is provided. Since instruct templates output a string, we will
+                call tokenizer.encode on the output of the instruct template.
+            (b) a ChatFormat is provided. Since chat formats output a list of messages, we will
+                call tokenizer.tokenize_messages on the output of the chat format.
+            (c) neither an InstructTemplate nor a ChatFormat is provided. In this case we will
+                convert the DictConfig to a list of messages and call tokenizer.tokenize_messages directly.
+        """
+
+        # Should only be chat-style prompt or instruct-style prompt
+        if chat_format and instruct_template:
+            raise ValueError(
+                "Cannot pass both chat format and instruct template for generation"
+            )
+
+        # If instruct template is provided, assert that the prompt is a DictConfig
+        # and apply it
+        if instruct_template:
+            if not isinstance(prompt, DictConfig):
+                raise ValueError("Cannot apply instruct template to raw string")
+            instruct_template = _get_component_from_path(instruct_template)
+            prompt = instruct_template.format(prompt)
+
+        # To hit this block, either the raw prompt is a string or an
+        # instruct template has been provided to convert it to a string
+        if isinstance(prompt, str):
+            return self._tokenizer.encode(prompt, add_bos=True, add_eos=False)
+
+        # dict.items() will respect order for Python >= 3.7
+        else:
+            messages = [Message(role=k, content=v) for k, v in prompt.items()]
+            messages += [Message(role="assistant", content="")]
+            if chat_format:
+                chat_format = _get_component_from_path(chat_format)
+                messages = chat_format.format(messages)
+            return self._tokenizer.tokenize_messages(messages)[0]
+
+    @torch.inference_mode()
+    def generate(self, cfg: DictConfig):
+        tokens = self.convert_prompt_to_tokens(
+            cfg.prompt, cfg.get("chat_format", None), cfg.get("instruct_template", None)
+        )
+        prompt = torch.tensor(tokens, dtype=torch.int, device=self._device)
+
+        custom_generate_next_token = None
+
+        # Ensure the cache is setup on the right device, with only as many tokens as we need
+        if cfg.enable_kv_cache:
+            with self._device:
+                self._model.setup_caches(
+                    batch_size=1,
+                    dtype=self._dtype,
+                    decoder_max_seq_len=prompt.numel() + cfg.max_new_tokens,
+                )
+
+        # since quantized model uses torch.compile to get speedup, it needs a warm up / prefill run
+        # to get the accurate performance measurement
+        if self._quantization_mode is not None:
+            logger.info("Starting compilation to improve generation performance ...")
+            custom_generate_next_token = torch.compile(
+                generation.generate_next_token, mode="max-autotune", fullgraph=True
+            )
+            t0 = time.perf_counter()
+            _ = generation.generate(
+                model=self._model,
+                prompt=prompt,
+                max_generated_tokens=2,
+                temperature=cfg.temperature,
+                top_k=cfg.top_k,
+                stop_tokens=self._tokenizer.stop_tokens,
+                custom_generate_next_token=custom_generate_next_token,
+            )
+            t = time.perf_counter() - t0
+            logger.info(f"Warmup run for quantized model takes: {t:.02f} sec")
+            self._model.reset_caches()
+
+        t0 = time.perf_counter()
+        generated_tokens, _ = generation.generate(
+            model=self._model,
+            prompt=prompt,
+            max_generated_tokens=cfg.max_new_tokens,
+            pad_id=self._tokenizer.pad_id,
+            temperature=cfg.temperature,
+            top_k=cfg.top_k,
+            stop_tokens=self._tokenizer.stop_tokens,
+            custom_generate_next_token=custom_generate_next_token,
+        )
+        generated_tokens = generated_tokens.tolist()
+        t = time.perf_counter() - t0
+
+        logger.info(self._tokenizer.decode(generated_tokens[0]))
+
+        model_size = sum(
+            [
+                p.numel() * p.dtype.itemsize
+                for p in itertools.chain(
+                    self._model.parameters(), self._model.buffers()
+                )
+            ]
+        )
+
+        tokens_generated = len(generated_tokens[0]) - prompt.size(0)
+        tokens_sec = tokens_generated / t
+        logger.info(
+            f"Time for inference: {t:.02f} sec total, {tokens_sec:.02f} tokens/sec"
+        )
+        logger.info(f"Bandwidth achieved: {model_size * tokens_sec / 1e9:.02f} GB/s")
+        logger.info(f"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB")
+
+
+@config.parse
+def main(cfg: DictConfig) -> None:
+    config.log_config(recipe_name="InferenceRecipe", cfg=cfg)
+    recipe = InferenceRecipe(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.generate(cfg=cfg)
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff -ruN marc_original/third_party/torchtune/recipes/__init__.py marc/third_party/torchtune/recipes/__init__.py
--- marc_original/third_party/torchtune/recipes/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/__init__.py	2025-02-20 17:49:29.110023522 -0500
@@ -0,0 +1,23 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+# This file mainly exists because we want to ensure that `recipes` aren't
+# importable *from the tests*.
+# We're using the `prepend` pytest import mode which adds the root dir (i.e. the
+# parent of torchtune/, tests/, recipes/) to the pythonpath during pytest
+# sessions
+# (https://docs.pytest.org/en/7.1.x/explanation/pythonpath.html#import-modes).
+# This has the positive effect that the `tests` folder becomes importable when
+# testing (we need that, considering how tests are currently set up) but ALSO
+# has the negative effect of making the `recipes/` importable when testing.
+# Since we don't want the tests to to incorrectly assume that recipes are
+# importable, we have to explicitly raise an error here.
+
+raise ModuleNotFoundError(
+    "The torchtune recipes directory isn't a package and you should not import anything from here. "
+    "Refer to our docs for detailed instructions on how to use recipes: "
+    "https://pytorch.org/torchtune/main/deep_dives/recipe_deepdive.html"
+)
diff -ruN marc_original/third_party/torchtune/recipes/knowledge_distillation_single_device.py marc/third_party/torchtune/recipes/knowledge_distillation_single_device.py
--- marc_original/third_party/torchtune/recipes/knowledge_distillation_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/knowledge_distillation_single_device.py	2025-02-20 17:49:29.490024147 -0500
@@ -0,0 +1,792 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import sys
+import time
+
+from functools import partial
+from typing import Any, Dict, Optional, Tuple, Union
+from warnings import warn
+
+import torch
+import torchtune.modules.common_utils as common_utils
+from omegaconf import DictConfig, ListConfig
+
+from torch import nn
+from torch.optim import Optimizer
+from torch.utils.data import DataLoader, DistributedSampler
+from torchtune import config, modules, training, utils
+from torchtune.data import padded_collate_sft
+from torchtune.datasets import ConcatDataset
+from torchtune.modules.peft import (
+    get_adapter_params,
+    get_lora_module_names,
+    get_merged_lora_ckpt,
+    load_dora_magnitudes,
+    set_trainable_params,
+    validate_missing_and_unexpected_for_lora,
+)
+from torchtune.recipe_interfaces import FTRecipeInterface
+from torchtune.training import DummyProfiler, PROFILER_KEY
+
+from tqdm import tqdm
+
+log = utils.get_logger("DEBUG")
+
+
+class KDRecipeSingleDevice(FTRecipeInterface):
+    """
+    Knowledge distillation recipe for dense transformer-based LLMs such as Llama3. This recipe is optimized
+    for single GPU training. Training on CPU is not supported.
+
+    Features:
+        - Activation Checkpointing. This can be controlled using the ``activation_checkpointing``
+            flag. Activation checkpointing helps reduce the memory footprint since we no longer keep
+            activations in memory and instead recompute them during the backward pass. This is especially
+            helpful for larger batch sizes when you're memory constrained. But these savings in memory
+            come at the cost of training performance. In most cases training can slow-down quite a bit as
+            a result of this activation recomputation.
+
+        - Precision. Full fp32 and bf16 training are supported. Precision is controlled using the ``dtype``
+            flag. When ``dtype=bf16``, all activations, gradients and optimizer states are in bfloat16. In
+            most cases this should halve the memory footprint of full precision (fp32) training, without
+            loss in model quality (will depend on the model, training data and other settings). For
+            GPUs which do not support bfloat16, we fall back to fp32. Mixed precision training and fp16
+            precision are currently not supported.g
+
+        - Gradient Accumulation. You can simulate larger batch sizes by accumulating gradients. This is
+            controlled using the ``gradient_accumulation_steps`` flag.
+
+                Total Batch Size = batch_size * gradient accumulation steps.
+
+            For example: with batch_size=1 and gradient_accumulation_steps=32 we get a total batch size of 32.
+
+            Gradient accumulation is especially useful when you are memory constrained. In this case,
+            accumulating gradients might give you better training speed than enabling activation
+            checkpointing.
+
+        - Lower precision optimizers. This recipe supports lower-precision optimizers from the bitsandbytes
+            library (https://huggingface.co/docs/bitsandbytes/main/en/index). We've tested the recipe with
+            8-bit AdamW and Paged AdamW.
+
+        - Checkpointing. Model weights are checkpointed both at the end of each epoch and at the end of
+            training. Currently we checkpoint both the adapter weights (trainable params only) and the
+            complete merged weights (adapter weights added back to the base model). For more details
+            please take a look at our LoRA tutorial
+            (https://pytorch.org/torchtune/main/tutorials/lora_finetune.html).
+
+            Optimizer State and recipe state (seed, total_epochs, number of epochs run etc) are
+            only saved at the end of a given epoch and used in case of resuming training. Resuming
+            training is controlled by the ``resume_from_checkpoint`` flag. Mid-epoch checkpointing is
+            currently not supported.
+
+            For more details on the checkpointer, please take a look at
+            our checkpointer deepdive (https://pytorch.org/torchtune/main/tutorials/checkpointer.html).
+
+        - Logging. Terminal, Disk, WandB and TensorBoard are all supported.
+
+        - Gradient Clipping. Gradient clipping is supported using the ``clip_grad_norm`` flag. By default,
+            ``clip_grad_norm`` is set to ``None``. If you only want to log the grad norm, you can set
+            ``clip_grad_norm='inf'``.
+
+    For a full list of example configs for this recipe, run ``tune ls`` on the command line. Each config
+    has example commands for how to kick-off training.
+
+    Args:
+        cfg (DictConfig): OmegaConf object parsed from yaml file
+
+    Raises:
+        ValueError: If ``dtype`` is set to fp16.
+        RuntimeError: If ``dtype`` is set to bf16 and the hardware does not support bf16.
+
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        self._device = utils.get_device(device=cfg.device)
+        # Reduced precision logic
+        self._dtype = training.get_dtype(cfg.dtype, device=self._device)
+        # fp16 precision is explicitly disabled as it is not supported in this
+        # recipe (for example, no gradient scaling).
+        if self._dtype == torch.float16:
+            raise ValueError(
+                "fp16 precision is not supported in this recipe. Please use fp32 or bf16."
+            )
+
+        # logging attributes
+        self._output_dir = cfg.output_dir
+        self._log_every_n_steps = cfg.get("log_every_n_steps", 1)
+        self._log_peak_memory_stats = cfg.get("log_peak_memory_stats", False)
+
+        # These are public properties which are updated by the checkpoint loader
+        # when ``resume_from_checkpoint`` is `True` or validated in tests
+        self.seed = training.set_seed(seed=cfg.seed)
+        self.epochs_run = 0
+        self.total_epochs = cfg.epochs
+        self.max_steps_per_epoch = cfg.max_steps_per_epoch
+        self.global_step = 0
+        self._resume_from_checkpoint = cfg.resume_from_checkpoint
+        self._save_adapter_weights_only = cfg.get("save_adapter_weights_only", False)
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+        self._clip_grad_norm = cfg.get("clip_grad_norm", None)
+        self._kd_ratio = cfg.get("kd_ratio", 0.5)
+
+    def load_checkpoint(self, cfg_checkpointer: DictConfig) -> Dict[str, Any]:
+        """
+        Extract the checkpoint state from file and validate. This includes the
+        base model weights. If resume_from_checkpoint is True, this also includes
+        the adapter weights and recipe state
+        """
+        self._checkpointer = config.instantiate(
+            cfg_checkpointer,
+            resume_from_checkpoint=self._resume_from_checkpoint,
+        )
+        checkpoint_dict = self._checkpointer.load_checkpoint()
+
+        if self._resume_from_checkpoint:
+            if training.ADAPTER_KEY not in checkpoint_dict:
+                raise ValueError(
+                    "Adapter weights not found. Please ensure a valid adapter checkpoint is provided."
+                )
+            # _update_recipe_state will throw an exception if the recipe state is not corrctly loaded
+            # no need to check here
+            self._update_recipe_state(checkpoint_dict)
+        return checkpoint_dict
+
+    def load_teacher_checkpoint(self, cfg_checkpointer: DictConfig) -> Dict[str, Any]:
+        """
+        Extract the teacher checkpoint state from file.
+        """
+        teacher_checkpointer = config.instantiate(
+            cfg_checkpointer,
+        )
+        checkpoint_dict = teacher_checkpointer.load_checkpoint()
+        return checkpoint_dict
+
+    def _update_recipe_state(self, ckpt_dict: Dict[str, Any]) -> None:
+        """
+        Updates the recipe state from checkpoint.
+        """
+        try:
+            self.epochs_run = ckpt_dict[training.EPOCHS_KEY]
+
+            # on mismatch, warn the user and prevent the override
+            if self.seed != ckpt_dict[training.SEED_KEY]:
+                warn(
+                    message=(
+                        "Config value for seed does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.SEED_KEY]}"
+                    )
+                )
+                self.seed = ckpt_dict[training.SEED_KEY]
+            if self.max_steps_per_epoch != ckpt_dict[training.MAX_STEPS_KEY]:
+                warn(
+                    message=(
+                        "Config value for max_steps_per_epoch does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.MAX_STEPS_KEY]}"
+                    )
+                )
+                self.max_steps_per_epoch = ckpt_dict[training.MAX_STEPS_KEY]
+
+            # on mismatch, warn the user but allow the override
+            if self.total_epochs != ckpt_dict[training.TOTAL_EPOCHS_KEY]:
+                warn(
+                    message=(
+                        "Config value for total_epochs does not match the checkpoint value, "
+                        f"using the config value: {self.total_epochs}"
+                    )
+                )
+
+        except KeyError as e:
+            raise KeyError(
+                "Checkpoint does not contain the required keys needed for updating recipe state. "
+                "Are you sure you passed in the right recipe checkpoint?"
+            ) from e
+
+    def setup(self, cfg: DictConfig) -> None:
+        """
+        Setup the recipe state. This includes recipe state (if resume_from_checkpoint is True),
+        model, tokenizer, loss, optimizer, learning rate scheduler, sampler, and dataloader.
+        """
+
+        self._metric_logger = config.instantiate(cfg.metric_logger)
+
+        # log config with parameter override
+        self._metric_logger.log_config(cfg)
+
+        self._compile = cfg.compile
+        checkpoint_dict = self.load_checkpoint(cfg_checkpointer=cfg.checkpointer)
+        teacher_checkpoint_dict = self.load_teacher_checkpoint(
+            cfg_checkpointer=cfg.teacher_checkpointer
+        )
+
+        common_utils._use_low_cpu_ram = cfg.get("low_cpu_ram", False)
+
+        # set up model
+        self._model = self._setup_model(
+            cfg_model=cfg.model,
+            enable_activation_checkpointing=cfg.enable_activation_checkpointing,
+            compile_model=cfg.compile,
+            base_model_state_dict=checkpoint_dict[training.MODEL_KEY],
+            lora_weights_state_dict=(
+                checkpoint_dict[training.ADAPTER_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+
+        self._teacher_model = self._setup_teacher_model(
+            model_cfg=cfg.teacher_model,
+            model_state_dict=teacher_checkpoint_dict[training.MODEL_KEY],
+        )
+
+        self._tokenizer = config.instantiate(cfg.tokenizer)
+        log.info("Tokenizer is initialized from file.")
+
+        self._optimizer = self._setup_optimizer(
+            cfg_optimizer=cfg.optimizer,
+            opt_state_dict=(
+                checkpoint_dict[training.OPT_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+
+        # initialize loss
+        self._loss_fn = config.instantiate(cfg.loss)
+        self._kd_loss_fn = config.instantiate(cfg.kd_loss)
+        if self._compile:
+            self._loss_fn = training.compile_loss(self._loss_fn)
+            self._kd_loss_fn = training.compile_loss(self._kd_loss_fn)
+        if self._loss_fn.__class__.__name__ == "CEWithChunkedOutputLoss":
+            # set num_output_chunks for model
+            self._model.set_num_output_chunks(self._loss_fn.num_output_chunks)
+            self._teacher_model.set_num_output_chunks(self._loss_fn.num_output_chunks)
+            # assert _loss_fn and _kd_loss_fn have the same num_output_chunks
+            assert (
+                self._loss_fn.num_output_chunks == self._kd_loss_fn.num_output_chunks
+            ), "Number of output chunks for loss_fn and kd_loss_fn must be the same."
+
+        log.info("Loss is initialized.")
+
+        # Dataloader depends on the tokenizer and loss_fn and should be
+        # setup after all of these are setup
+        self._sampler, self._dataloader = self._setup_data(
+            cfg_dataset=cfg.dataset,
+            shuffle=cfg.shuffle,
+            batch_size=cfg.batch_size,
+        )
+
+        # Finally update the recipe state which can only be correctly set after all of the
+        # other components have been initialized and updated.
+
+        # Number of training steps in each epoch depends on the number of batches produced
+        # by the dataloader and the max_steps_per_epoch param set by the user and is used
+        # for logging and tracking training state. This should be computed after the dataloader
+        # has been setup
+        self._steps_per_epoch = (
+            len(self._dataloader) // self._gradient_accumulation_steps
+        )
+        if (
+            self.max_steps_per_epoch is not None
+            and self.max_steps_per_epoch < self._steps_per_epoch
+        ):
+            self._steps_per_epoch = self.max_steps_per_epoch
+            self.global_step = self.epochs_run * self._steps_per_epoch
+
+        # Learning rate scheduler can only be set up after number of steps
+        # has been computed
+        self._lr_scheduler = self._setup_lr_scheduler(
+            cfg_lr_scheduler=cfg.lr_scheduler,
+            num_training_steps=self.total_epochs * self._steps_per_epoch,
+            last_epoch=self.global_step - 1,
+        )
+
+        # Set up profiler, returns DummyProfiler (nullcontext object with no-op `step` method)
+        # if cfg is missing profiler key or if `cfg.profiler.enabled = False
+        self._profiler = self._setup_profiler(cfg.get(PROFILER_KEY, None))
+
+        # Used to ignore labels for loss computation
+        self.ignore_labels_cache = torch.full(
+            (cfg.batch_size, 1), self._loss_fn.ignore_index, device=self._device
+        )
+
+    def _setup_profiler(
+        self, cfg_profiler: Optional[DictConfig] = None
+    ) -> Union[torch.profiler.profile, DummyProfiler]:
+        """
+        Parses the `profiler` section of top-level `cfg` and sets up profiler
+
+        Args:
+            cfg_profiler (Optional[DictConfig]): ``profiler`` section of the top-level ``cfg`` (the main config passed to
+                `recipe.main`). Default None.
+
+        Returns:
+            profiler: Union[torch.profiler.profile, DummyProfiler] - DummyProfiler is a nullcontext with no-op methods
+            for `start`, `stop`, and `step` that can be used in place of `torch.profiler.profile` if profiler is not enabled such
+            that the instrumented training loop does not need to be changed profiling is disabled.
+
+        The profiler config can be provided in configs under the `profiler` key with the following layout:
+
+        .. code-block:: yaml
+            profiler:
+                enabled: bool
+
+                #Output directory of trace artifacts
+                output_dir: str
+
+            #`torch.profiler.ProfilerActivity` types to trace
+            cpu: bool
+            cuda: bool
+
+                #Trace options
+                profile_memory: bool
+                with_stack: bool
+                record_shapes: bool
+                with_flops: bool
+
+            # `torch.profiler.schedule` options:
+            # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+            wait_steps: int
+            warmup_steps: int
+            active_steps: int
+            num_cycles: int
+        """
+
+        # Missing profiler section in config, assume disabled
+        if cfg_profiler is None:
+            cfg_profiler = DictConfig({"enabled": False})
+
+        # Check that component is included and set correctly
+        if cfg_profiler.get("_component_", None) is None:
+            cfg_profiler["_component_"] = "torchtune.training.setup_torch_profiler"
+        else:
+            assert (
+                cfg_profiler.get("_component_")
+                == "torchtune.training.setup_torch_profiler"
+            ), "Only torch profiler supported currently: component must be `torchtune.training.setup_torch_profiler`"
+
+        profiler, profiler_cfg = config.instantiate(cfg_profiler)
+
+        log.info(f" Profiler config after instantiation: {profiler_cfg}")
+
+        self.profiler_profile_memory = profiler_cfg.get("profile_memory", False)
+        if profiler_cfg["enabled"]:
+            self.profiler_wait_steps = profiler_cfg["wait_steps"]
+            self.profiler_warmup_steps = profiler_cfg["warmup_steps"]
+            self.profiler_active_steps = profiler_cfg["active_steps"]
+
+        return profiler
+
+    def _setup_model(
+        self,
+        cfg_model: DictConfig,
+        enable_activation_checkpointing: bool,
+        compile_model: bool,
+        base_model_state_dict: Dict[str, Any],
+        lora_weights_state_dict: Optional[Dict[str, Any]] = None,
+    ) -> nn.Module:
+        with training.set_default_dtype(self._dtype), self._device:
+            model = config.instantiate(cfg_model)
+
+        self._lora_rank = cfg_model.lora_rank
+        self._lora_alpha = cfg_model.lora_alpha
+        self._lora_attn_modules = list(cfg_model.lora_attn_modules)
+        self._apply_lora_to_mlp = cfg_model.apply_lora_to_mlp
+        self._apply_lora_to_output = getattr(cfg_model, "apply_lora_to_output", False)
+        self.adapter_params = get_adapter_params(model)
+        self._is_dora = any(["magnitude" in k for k in self.adapter_params.keys()])
+        set_trainable_params(model, self.adapter_params)
+
+        if compile_model:
+            training.compile_model(model)
+
+        if enable_activation_checkpointing:
+            training.set_activation_checkpointing(
+                model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        base_missing, base_unexpected = model.load_state_dict(
+            base_model_state_dict, strict=False
+        )
+        # This is for any adapters that need to be initialized after base weights
+        # have been loaded (e.g. DoRA).
+        if self._is_dora:
+            load_dora_magnitudes(model)
+        if lora_weights_state_dict:
+            lora_missing, lora_unexpected = model.load_state_dict(
+                lora_weights_state_dict, strict=False
+            )
+        else:
+            lora_missing, lora_unexpected = None, None
+        validate_missing_and_unexpected_for_lora(
+            lora_attn_modules=self._lora_attn_modules,
+            apply_lora_to_mlp=self._apply_lora_to_mlp,
+            apply_lora_to_output=self._apply_lora_to_output,
+            base_missing=base_missing,
+            base_unexpected=base_unexpected,
+            lora_missing=lora_missing,
+            lora_unexpected=lora_unexpected,
+        )
+        # Validate model adapter params were loaded in with the expected dtype
+        # TODO (rohan-varma): Further validation to ensure the appropriate base params
+        # are NF4 vs bf16 based on the quantization config.
+        training.validate_expected_param_dtype(
+            self.adapter_params.items(), dtype=self._dtype
+        )
+
+        log.info(f"Model is initialized with precision {self._dtype}.")
+
+        if self._device.type == "cuda":
+            memory_stats = training.get_memory_stats(device=self._device)
+            training.log_memory_stats(memory_stats)
+        return model
+
+    def _setup_teacher_model(
+        self,
+        model_cfg: DictConfig,
+        model_state_dict: Dict[str, Any],
+    ) -> nn.Module:
+        with training.set_default_dtype(self._dtype), self._device:
+            model = config.instantiate(model_cfg)
+
+        model.load_state_dict(model_state_dict)
+
+        # Put model in eval mode.
+        # Note: This will not disable the dropout applied in SDPA,
+        # see https://github.com/pytorch/pytorch/issues/124464
+        model.eval()
+
+        # Validate model was loaded in with the expected dtype.
+        training.validate_expected_param_dtype(
+            model.named_parameters(), dtype=self._dtype
+        )
+        log.info(f"Teacher model is initialized with precision {self._dtype}.")
+        return model
+
+    def _setup_optimizer(
+        self, cfg_optimizer: DictConfig, opt_state_dict: Optional[Dict[str, Any]] = None
+    ) -> Optimizer:
+        optimizer = config.instantiate(cfg_optimizer, self._model.parameters())
+        if opt_state_dict:
+            optimizer.load_state_dict(opt_state_dict)
+
+        log.info("Optimizer and loss are initialized.")
+        return optimizer
+
+    def _setup_lr_scheduler(
+        self,
+        cfg_lr_scheduler: DictConfig,
+        num_training_steps: int,
+        last_epoch: int,
+    ) -> Optimizer:
+        lr_scheduler = config.instantiate(
+            cfg_lr_scheduler,
+            self._optimizer,
+            num_training_steps=num_training_steps,
+            last_epoch=last_epoch,
+        )
+
+        log.info("Learning rate scheduler is initialized.")
+        return lr_scheduler
+
+    def _setup_data(
+        self,
+        cfg_dataset: DictConfig,
+        shuffle: bool,
+        batch_size: int,
+    ) -> Tuple[DistributedSampler, DataLoader]:
+        """
+        All data related setup happens here. Currently this recipe only supports
+        Map-style Datasets which fit into memory and an option for random shuffling.
+        Samplers, iterable datasets, and streaming datasets are not supported.
+        """
+        if isinstance(cfg_dataset, ListConfig):
+            datasets = [
+                config.instantiate(single_cfg_dataset, self._tokenizer)
+                for single_cfg_dataset in cfg_dataset
+            ]
+            ds = ConcatDataset(datasets=datasets)
+            packed = False
+        else:
+            ds = config.instantiate(cfg_dataset, self._tokenizer)
+            packed = cfg_dataset.get("packed", False)
+
+        sampler = DistributedSampler(
+            ds,
+            num_replicas=1,
+            rank=0,
+            shuffle=shuffle,
+            seed=0,
+        )
+        dataloader = DataLoader(
+            dataset=ds,
+            sampler=sampler,
+            batch_size=batch_size,
+            # dropping last avoids shape issues with compile + flex attention
+            drop_last=True,
+            collate_fn=(
+                partial(
+                    padded_collate_sft,
+                    padding_idx=self._tokenizer.pad_id,
+                    ignore_idx=self._loss_fn.ignore_index,
+                )
+                if not packed
+                else None
+            ),
+        )
+
+        log.info("Dataset and Sampler are initialized.")
+
+        return sampler, dataloader
+
+    def save_checkpoint(self, epoch: int) -> None:
+        """
+        Checkpoint the state of the recipe. The constructed checkpoint state dict
+        contains the following information:
+        - Merged weights with key MODEL_KEY
+        - Adapter weights with key ADAPTER_KEY
+        - Relevant recipe state if training is not complete
+        - If the `self._save_adapter_weights_only` option is True, the checkpointer will save only the adapter weights
+
+        To correctly resume from training, the adapter weights and recipe state must be provided along with the base model weights.
+        """
+        ckpt_dict = {}
+
+        intermediate_checkpoint = epoch + 1 < self.total_epochs
+        # if training is in-progress, checkpoint the optimizer state as well
+        if intermediate_checkpoint:
+            ckpt_dict.update(
+                {
+                    training.OPT_KEY: self._optimizer.state_dict(),
+                    training.SEED_KEY: self.seed,
+                    training.EPOCHS_KEY: self.epochs_run,
+                    training.TOTAL_EPOCHS_KEY: self.total_epochs,
+                    training.MAX_STEPS_KEY: self.max_steps_per_epoch,
+                }
+            )
+
+        # Move to CPU to avoid a copy on GPU
+        state_dict = {k: v.cpu() for k, v in self._model.state_dict().items()}
+
+        # Construct the full state dict with LoRA weights merged into base LLM weights
+        merged_state_dict = get_merged_lora_ckpt(
+            state_dict,
+            rank=self._lora_rank,
+            alpha=self._lora_alpha,
+        )
+        ckpt_dict.update({training.MODEL_KEY: merged_state_dict})
+
+        # Construct the adapter weights
+        adapter_key_filter = lambda x: x in self.adapter_params
+        adapter_state_dict = {
+            k: v for k, v in self._model.state_dict().items() if adapter_key_filter(k)
+        }
+        ckpt_dict.update({training.ADAPTER_KEY: adapter_state_dict})
+        adapter_config = {
+            "r": self._lora_rank,
+            "lora_alpha": self._lora_alpha,
+            "target_modules": get_lora_module_names(
+                self._lora_attn_modules,
+                self._apply_lora_to_mlp,
+                self._apply_lora_to_output,
+            ),
+            "peft_type": "LORA",
+        }
+        ckpt_dict.update({training.ADAPTER_CONFIG: adapter_config})
+
+        self._checkpointer.save_checkpoint(
+            ckpt_dict,
+            epoch=epoch,
+            intermediate_checkpoint=intermediate_checkpoint,
+            adapter_only=self._save_adapter_weights_only,
+        )
+
+    def _loss_step(
+        self, batch: Dict[str, torch.Tensor]
+    ) -> (torch.Tensor, torch.Tensor):
+
+        # Both are shape [b, s]
+        tokens, labels = batch["tokens"], batch["labels"]
+
+        # Get the attention mask and position ids from the dataset if they
+        # exist. Currently, only sample packing in PackedDataset returns these
+        mask = batch.get("mask", None)  # shape [b, s, s]
+        input_pos = batch.get("input_pos", None)  # shape [b, s]
+
+        # run model
+        logits = self._model(tokens, mask=mask, input_pos=input_pos)
+
+        # Compute teacher logits
+        with torch.no_grad():
+            teacher_logits = self._teacher_model(tokens, mask=mask, input_pos=input_pos)
+
+        # Shift labels to compute loss
+        # equivalent to doing labels[..., 1:] and logits[..., :-1, :]
+        # But this way we dont need to slice the logits. We just add an ignore index to labels.
+        labels = torch.hstack(
+            (labels[..., 1:], self.ignore_labels_cache[: labels.shape[0]])
+        )
+        if not isinstance(logits, list):
+            labels = labels.reshape(-1)
+            logits = logits.reshape(-1, logits.size(-1))
+            teacher_logits = teacher_logits.reshape(-1, teacher_logits.size(-1))
+
+        # Compute kd loss
+        kd_loss = self._kd_loss_fn(logits, teacher_logits, labels)
+
+        # Compute loss
+        loss = self._loss_fn(logits, labels)
+
+        # free logits otherwise it peaks backward memory
+        del logits
+        del teacher_logits
+
+        return loss, kd_loss
+
+    def train(self) -> None:
+        """
+        The core training loop.
+        """
+
+        if self._compile:
+            log.info(
+                "NOTE: torch.compile is enabled and model is compiled in first forward. Expect a relatively slow first iteration."
+            )
+
+        # Initialize tokens count and running loss (for grad accumulation)
+        t0 = time.perf_counter()
+        running_class_loss = 0
+        running_kd_loss = 0
+        num_tokens = 0
+
+        with self._profiler as prof:
+            # self.epochs_run should be non-zero when we're resuming from a checkpoint
+            for curr_epoch in range(self.epochs_run, self.total_epochs):
+                # Update the sampler to ensure data is correctly shuffled across epochs
+                # in case shuffle is True
+                self._sampler.set_epoch(curr_epoch)
+
+                pbar = tqdm(total=self._steps_per_epoch)
+                for idx, batch in enumerate(self._dataloader):
+                    if (
+                        self.max_steps_per_epoch is not None
+                        and (idx // self._gradient_accumulation_steps)
+                        == self.max_steps_per_epoch
+                    ):
+                        break
+
+                    # Start tracking CUDA memory for active steps for just the first epoch
+                    if (
+                        curr_epoch == 0
+                        and self.profiler_profile_memory
+                        and idx == self.profiler_wait_steps + self.profiler_warmup_steps
+                    ):
+                        torch.cuda.memory._record_memory_history()
+
+                    batch = {k: v.to(self._device) for k, v in batch.items()}
+                    num_tokens += batch["tokens"].numel()
+
+                    class_loss, kd_loss = self._loss_step(batch)
+                    loss = (1 - self._kd_ratio) * class_loss + self._kd_ratio * kd_loss
+                    loss = loss / self._gradient_accumulation_steps
+                    running_class_loss += class_loss / self._gradient_accumulation_steps
+                    running_kd_loss += kd_loss / self._gradient_accumulation_steps
+                    loss.backward()
+
+                    # Step with optimizer
+                    if (idx + 1) % self._gradient_accumulation_steps == 0:
+                        if self._clip_grad_norm is not None:
+                            grad_norm = torch.nn.utils.clip_grad_norm_(
+                                self._model.parameters(),
+                                max_norm=float(self._clip_grad_norm),
+                            )
+                        self._optimizer.step()
+                        self._optimizer.zero_grad(set_to_none=True)
+                        self._lr_scheduler.step()
+                        # Update the number of steps when the weights are updated
+                        self.global_step += 1
+
+                        class_loss_to_log = running_class_loss.item()
+                        kd_loss_to_log = running_kd_loss.item()
+                        loss_to_log = (
+                            1 - self._kd_ratio
+                        ) * class_loss_to_log + self._kd_ratio * kd_loss_to_log
+                        pbar.update(1)
+                        pbar.set_description(
+                            f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
+                        )
+
+                        # Log per-step metrics
+                        if self.global_step % self._log_every_n_steps == 0:
+                            time_per_step = time.perf_counter() - t0
+                            log_dict = {
+                                "loss": loss_to_log,
+                                "class_loss": class_loss_to_log,
+                                "kd_loss": kd_loss_to_log,
+                                "lr": self._optimizer.param_groups[0]["lr"],
+                                "tokens_per_second_per_gpu": num_tokens / time_per_step,
+                            }
+                            if (
+                                self._device.type == "cuda"
+                                and self._log_peak_memory_stats
+                            ):
+                                log_dict.update(
+                                    training.get_memory_stats(device=self._device)
+                                )
+                            if self._clip_grad_norm is not None:
+                                log_dict.update({"grad_norm": grad_norm})
+                            self._metric_logger.log_dict(
+                                log_dict,
+                                step=self.global_step,
+                            )
+
+                        # Reset running stats for the next step
+                        running_class_loss = 0
+                        running_kd_loss = 0
+                        num_tokens = 0
+                        t0 = time.perf_counter()
+
+                    # Stop tracking CUDA memory now that active steps are complete
+                    if (
+                        curr_epoch == 0
+                        and self.profiler_profile_memory
+                        and idx
+                        == self.profiler_wait_steps
+                        + self.profiler_warmup_steps
+                        + self.profiler_active_steps
+                    ):
+                        torch.cuda.memory._record_memory_history(enabled=None)
+
+                    # Step the profiler
+                    # Note we are stepping each batch, which might not include optimizer step in the trace
+                    # if the schedule cycle doesn't align with gradient accumulation.
+                    prof.step()
+
+                self.epochs_run += 1
+                self.save_checkpoint(epoch=curr_epoch)
+
+    def cleanup(self) -> None:
+        self._metric_logger.close()
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """
+    Entry point for the recipe.
+
+    Configurable parameters are read in the following order:
+        - Parameters specified in config (see available configs through ``tune ls``)
+        - Overwritten by arguments from the command-line
+    """
+    config.log_config(recipe_name="KDRecipeSingleDevice", cfg=cfg)
+    recipe = KDRecipeSingleDevice(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
diff -ruN marc_original/third_party/torchtune/recipes/lora_dpo_distributed.py marc/third_party/torchtune/recipes/lora_dpo_distributed.py
--- marc_original/third_party/torchtune/recipes/lora_dpo_distributed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/lora_dpo_distributed.py	2025-02-20 17:49:29.494024153 -0500
@@ -0,0 +1,773 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import sys
+import time
+
+from functools import partial
+from typing import Any, Dict, Optional, Tuple
+from warnings import warn
+
+import torch
+from omegaconf import DictConfig, ListConfig
+
+from torch import nn
+from torch.distributed import destroy_process_group, init_process_group
+from torch.optim import Optimizer
+from torch.utils.data import DataLoader, DistributedSampler
+from torchtune import config, modules, rlhf, training, utils
+from torchtune.data import CROSS_ENTROPY_IGNORE_IDX, padded_collate_dpo
+from torchtune.datasets import ConcatDataset
+from torchtune.modules.peft import (
+    disable_adapter,
+    DoRALinear,
+    get_adapter_params,
+    get_merged_lora_ckpt,
+    load_dora_magnitudes,
+    LoRALinear,
+    set_trainable_params,
+    validate_missing_and_unexpected_for_lora,
+)
+from torchtune.recipe_interfaces import FTRecipeInterface
+from torchtune.rlhf.loss import SimPOLoss
+from tqdm import tqdm
+
+log = utils.get_logger("DEBUG")
+
+
+class LoRADPORecipeDistributed(FTRecipeInterface):
+    """
+    Distributed LoRA DPO recipe for dense transformer-based LLMs such as Llama2. This recipe supports
+    distributed training and can be run on a single node (1 to 8 GPUs). This is based on HF's DPOTrainer
+    in the TRL library: https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L65
+
+    Features:
+        - FSDP. Supported using PyTorch's FSDP APIs. CPU offload of parameters, gradients, and optimizer states
+            is supported via ``fsdp_cpu_offload``. Resharding of parameters after the forward pass is
+            done by default (corresponding to FULL_SHARD sharding strategy), but can be disabled by setting the config
+            ``fsdp_reshard_after_forward`` to False (this corresponds to SHARD_GRAD_OP sharding strategy).
+            DDP is currently not supported. Training on CPU is not supported.
+
+        - Activation Checkpointing. This can be controlled using the ``activation_checkpointing``
+            flag. Activation checkpointing helps reduce the memory footprint since we no longer keep
+            activations in memory and instead recompute them during the backward pass. This is especially
+            helpful for larger batch sizes when you're memory constrained. But these savings in memory
+            come at the cost of training performance. In most cases training can slow-down quite a bit as
+            a result of this activation recomputation.
+
+        - Precision. Full fp32 and bf16 training are supported. Precision is controlled using the ``dtype``
+            flag. When ``dtype=bf16``, all activations, gradients and optimizer states are in bfloat16. In
+            most cases this should halve the memory footprint of full precision (fp32) training, without
+            loss in model quality (will depend on the model, training data and other settings). For
+            GPUs which do not support bfloat16, we fall back to fp32. Mixed precision training and fp16
+            precision are currently not supported.
+
+        - Gradient Accumulation. You can simulate larger batch sizes by accumulating gradients. This is
+            controlled using the ``gradient_accumulation_steps`` flag.
+
+                Total Batch Size = batch_size * number of GPUs * gradient accumulation steps.
+
+            For example: with batch_size=1, nproc_per_node=2 and gradient_accumulation_steps=32 we get a
+            total batch size of 64.
+
+            Gradient accumulation is especially useful when you are memory constrained. In this case,
+            accumulating gradients might give you better training speed than enabling activation
+            checkpointing.
+
+        - Checkpointing. Model weights are checkpointed both at the end of each epoch and at the end of
+            training. Currently we checkpoint both the adapter weights (trainable params only) and the
+            complete merged weights (adapter weights added back to the base model). For more details
+            please take a look at our LoRA tutorial
+            (https://pytorch.org/torchtune/main/tutorials/lora_finetune.html).
+
+            Optimizer State and recipe state (seed, total_epochs, number of epochs run etc) are
+            only saved at the end of a given epoch and used in case of resuming training. Resuming
+            training is controlled by the ``resume_from_checkpoint`` flag. Mid-epoch checkpointing is
+            currently not supported.
+
+            For more details on the checkpointer, please take a look at
+            our checkpointer deepdive (https://pytorch.org/torchtune/main/tutorials/checkpointer.html).
+
+        - Logging. Terminal, Disk, WandB and TensorBoard are all supported.
+
+    The following losses are supported in this recipe:
+        - :class:`~torchtune.rlhf.loss.DPOLoss`: Direct Preference Optimization (DPO).
+        - :class:`~torchtune.rlhf.loss.RSOPLoss`: Rejection Sampling Optimization (RSO).
+        - :class:`~torchtune.rlhf.loss.SimPOLoss`: Simple Preference Optimization (SimPO).
+
+    For a full list of example configs for this recipe, run ``tune ls`` on the command line. Each config
+    has example commands for how to kick-off training.
+
+    Args:
+        cfg (DictConfig): OmegaConf object parsed from yaml file
+
+    Raises:
+        ValueError: If ``dtype`` is set to fp16.
+        ValueError: If world_size is 1
+        RuntimeError: If ``dtype`` is set to bf16 and the hardware does not support bf16.
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        self._device = utils.get_device(device=cfg.device)
+        self._dtype = training.get_dtype(cfg.dtype, device=self._device)
+
+        if self._dtype == torch.float16:
+            raise ValueError(
+                "full fp16 training is not supported with this recipe. Please use bf16 or fp32 instead."
+            )
+
+        _, rank = training.get_world_size_and_rank()
+
+        # _is_rank_zero is used primarily for logging. In the future, the logger
+        # should directly take care of this
+        self._is_rank_zero = rank == 0
+
+        # logging attributes
+        self._output_dir = cfg.output_dir
+        self._log_every_n_steps = cfg.get("log_every_n_steps", 1)
+        self._log_peak_memory_stats = cfg.get("log_peak_memory_stats", False)
+
+        # training attributes
+        self._enable_activation_checkpointing = cfg.enable_activation_checkpointing
+
+        # These attributes constitute the recipe state and are updated by ``load_checkpoint``
+        # when ``resume_from_checkpoint`` is ``True``
+        self.seed = training.set_seed(seed=cfg.seed)
+        self.epochs_run = 0
+        self.total_epochs = cfg.epochs
+        self.max_steps_per_epoch = cfg.max_steps_per_epoch
+        self.global_step = 0
+        self._resume_from_checkpoint = cfg.resume_from_checkpoint
+        self._save_adapter_weights_only = cfg.get("save_adapter_weights_only", False)
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+
+    def load_checkpoint(self, cfg_checkpointer: DictConfig) -> Dict[str, Any]:
+        """
+        Extract the checkpoint state from file and validate. This includes the
+        base model weights. If resume_from_checkpoint is True, this also includes
+        the adapter weights and recipe state
+        """
+        self._checkpointer = config.instantiate(
+            cfg_checkpointer,
+            resume_from_checkpoint=self._resume_from_checkpoint,
+        )
+        checkpoint_dict = self._checkpointer.load_checkpoint()
+
+        # When resuming from checkpoint for LoRA, the recipe expects the adapter weights
+        # and recipe state to be present. The keys should match up with what ``save_checkpoint``
+        # used to create these intermediate checkpoints
+        if self._resume_from_checkpoint:
+            if training.ADAPTER_KEY not in checkpoint_dict:
+                raise ValueError(
+                    "Adapter weights not found. Please ensure a valid adapter checkpoint is provided."
+                )
+            # _update_recipe_state will throw an exception if the recipe state is not corrctly loaded
+            # no need to check here
+            self._update_recipe_state(checkpoint_dict)
+        return checkpoint_dict
+
+    def _update_recipe_state(self, ckpt_dict: Dict[str, Any]) -> None:
+        """
+        Updates the recipe state from checkpoint.
+        """
+        try:
+            self.epochs_run = ckpt_dict[training.EPOCHS_KEY]
+
+            # on mismatch, warn the user and prevent the override
+            if self.seed != ckpt_dict[training.SEED_KEY]:
+                warn(
+                    message=(
+                        "Config value for seed does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.SEED_KEY]}"
+                    )
+                )
+                self.seed = ckpt_dict[training.SEED_KEY]
+            if self.max_steps_per_epoch != ckpt_dict[training.MAX_STEPS_KEY]:
+                warn(
+                    message=(
+                        "Config value for max_steps_per_epoch does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.MAX_STEPS_KEY]}"
+                    )
+                )
+                self.max_steps_per_epoch = ckpt_dict[training.MAX_STEPS_KEY]
+
+            # on mismatch, warn the user but allow the override
+            if self.total_epochs != ckpt_dict[training.TOTAL_EPOCHS_KEY]:
+                warn(
+                    message=(
+                        "Config value for total_epochs does not match the checkpoint value, "
+                        f"using the config value: {self.total_epochs}"
+                    )
+                )
+
+        except KeyError as e:
+            raise KeyError(
+                "Checkpoint does not contain the required keys needed for updating recipe state. "
+                "Are you sure you passed in the right recipe checkpoint?"
+            ) from e
+
+    def setup(self, cfg: DictConfig) -> None:
+        """
+        Setup the recipe state. This includes recipe state (if resume_from_checkpoint is True),
+        model, tokenizer, loss, optimizer, learning rate scheduler, sampler, and dataloader.
+        """
+        if self._is_rank_zero:
+            self._metric_logger = config.instantiate(cfg.metric_logger)
+
+            # log config with parameter override
+            self._metric_logger.log_config(cfg)
+            log.info("_metric_logger is initialized.")
+
+        checkpoint_dict = self.load_checkpoint(cfg_checkpointer=cfg.checkpointer)
+
+        self._model = self._setup_model(
+            cfg_model=cfg.model,
+            enable_activation_checkpointing=cfg.enable_activation_checkpointing,
+            fsdp_cpu_offload=cfg.get("fsdp_cpu_offload", False),
+            reshard_after_forward=cfg.get("fsdp_reshard_after_forward", True),
+            base_model_state_dict=checkpoint_dict[training.MODEL_KEY],
+            lora_weights_state_dict=(
+                checkpoint_dict[training.ADAPTER_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+        self._tokenizer = config.instantiate(cfg.tokenizer)
+
+        self._optimizer = self._setup_optimizer(
+            cfg_optimizer=cfg.optimizer,
+            opt_state_dict=(
+                checkpoint_dict[training.OPT_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+
+        self._loss_fn = config.instantiate(cfg.loss)
+        if self._is_rank_zero:
+            log.info("Loss is initialized.")
+
+        # sampler and dataloader depend on the tokenizer and loss_fn and should be
+        # setup after all of these are setup
+        self._sampler, self._dataloader = self._setup_data(
+            cfg_dataset=cfg.dataset,
+            shuffle=cfg.shuffle,
+            batch_size=cfg.batch_size,
+        )
+
+        # Finally update the recipe state which can only be correctly set after all of the
+        # other components have been initialized and updated.
+
+        # Number of training steps in each epoch depends on the number of batches produced
+        # by the dataloader and the max_steps_per_epoch param set by the user and is used
+        # for logging and tracking training state. This should be computed after the dataloader
+        # has been setup
+        self._steps_per_epoch = (
+            len(self._dataloader) // self._gradient_accumulation_steps
+        )
+        if (
+            self.max_steps_per_epoch is not None
+            and self.max_steps_per_epoch < self._steps_per_epoch
+        ):
+            self._steps_per_epoch = self.max_steps_per_epoch
+        self.global_step = self.epochs_run * self._steps_per_epoch
+
+        # Learning rate scheduler can only be set up after number of steps
+        # has been computed
+        self._lr_scheduler = self._setup_lr_scheduler(
+            cfg_lr_scheduler=cfg.lr_scheduler,
+            num_training_steps=self.total_epochs * self._steps_per_epoch,
+            last_epoch=self.global_step - 1,
+        )
+
+    def _setup_model(
+        self,
+        cfg_model: DictConfig,
+        enable_activation_checkpointing: bool,
+        fsdp_cpu_offload: bool,
+        reshard_after_forward: bool,
+        base_model_state_dict: Dict[str, Any],
+        lora_weights_state_dict: Optional[Dict[str, Any]] = None,
+    ) -> nn.Module:
+        """
+        Model initialization has some important considerations:
+           a. To minimize GPU peak memory, we initialize the model on meta device with
+              the right dtype
+           b. All ranks calls ``load_state_dict`` without peaking CPU RAMs since
+              full state dicts are loaded with ``torch.load(mmap=True)``
+           c. We register (pre-)forward hooks with ``fully_shard`` instead of wrapping `nn.Module`
+        """
+        self._lora_rank = cfg_model.lora_rank
+        self._lora_alpha = cfg_model.lora_alpha
+        self._lora_attn_modules = list(cfg_model.lora_attn_modules)
+        self._apply_lora_to_mlp = cfg_model.apply_lora_to_mlp
+        self._apply_lora_to_output = getattr(cfg_model, "apply_lora_to_output", False)
+
+        if self._is_rank_zero:
+            log.info(
+                "FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ..."
+            )
+            init_start = time.perf_counter()
+
+        with training.set_default_dtype(self._dtype), torch.device("meta"):
+            model = config.instantiate(cfg_model)
+
+        self.adapter_params = get_adapter_params(model)
+        set_trainable_params(model, self.adapter_params)
+
+        if enable_activation_checkpointing:
+            training.set_activation_checkpointing(
+                model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        # For FSDP sharding, we can condition on either the module or its name
+        # Shard conditions should be callables taking name (relative to model root)
+        # and the module itself and returning a bool on whether to shard the given module
+
+        # Shard transformer decoder layers (or AC-wrapped versions)
+        # Alternatively we could condition on the module type (TransformerDecoder or CheckpointWrapper)
+        # But directly using the name is more concise
+        def _is_layer_name(name: str, module: nn.Module) -> bool:
+            """
+            Return True for layers.i and False for all other module names
+            Covers sharding for both AC-wrapped and non-AC-wrapped modules in one shot
+            """
+            name_list = name.split(".")
+            return (
+                len(name_list) == 2
+                and name_list[0] == "layers"
+                and str.isdigit(name_list[1])
+            )
+
+        training.shard_model(
+            model=model,
+            shard_conditions=[_is_layer_name],
+            cpu_offload=fsdp_cpu_offload,
+            reshard_after_forward=reshard_after_forward,
+        )
+
+        if lora_weights_state_dict:
+            lora_missing, lora_unexpected = training.load_from_full_model_state_dict(
+                model,
+                lora_weights_state_dict,
+                self._device,
+                self._is_rank_zero,
+                cpu_offload=fsdp_cpu_offload,
+            )
+        else:
+            lora_missing, lora_unexpected = None, None
+
+        # Initialize LoRA params and RoPE buffers
+        with training.set_default_dtype(self._dtype), self._device:
+            lora_device = "cpu" if fsdp_cpu_offload else self._device
+            for m in model.modules():
+                if (
+                    isinstance(m, LoRALinear) or isinstance(m, DoRALinear)
+                ) and not lora_weights_state_dict:
+                    # lora may not be covered in state dict
+                    # if finetune for the 1st time
+                    m.lora_a.to_empty(device=lora_device)
+                    m.lora_b.to_empty(device=lora_device)
+                    m.initialize_parameters()
+                # RoPE is not covered in state dict
+                if hasattr(m, "rope_init"):
+                    m.rope_init()
+
+        base_missing, base_unexpected = training.load_from_full_model_state_dict(
+            model,
+            base_model_state_dict,
+            self._device,
+            self._is_rank_zero,
+            cpu_offload=fsdp_cpu_offload,
+        )
+        is_dora = False
+        for m in model.modules():
+            if hasattr(m, "initialize_dora_magnitude"):
+                is_dora = True
+                m.initialize_dora_magnitude()
+        if is_dora:
+            load_dora_magnitudes(model)
+        validate_missing_and_unexpected_for_lora(
+            lora_attn_modules=self._lora_attn_modules,
+            apply_lora_to_mlp=self._apply_lora_to_mlp,
+            apply_lora_to_output=self._apply_lora_to_output,
+            base_missing=base_missing,
+            base_unexpected=base_unexpected,
+            lora_missing=lora_missing,
+            lora_unexpected=lora_unexpected,
+        )
+        # Ensure no params and buffers are on meta device
+        training.validate_no_params_on_meta_device(model)
+        if self._is_rank_zero:
+            log.info(
+                f"Instantiating model and loading checkpoint took {time.perf_counter() - init_start:.2f} secs"
+            )
+            memory_stats = training.get_memory_stats(device=self._device)
+            training.log_memory_stats(memory_stats)
+
+        # synchronize before training begins
+        torch.distributed.barrier()
+
+        return model
+
+    def _setup_optimizer(
+        self, cfg_optimizer: DictConfig, opt_state_dict: Optional[Dict[str, Any]] = None
+    ) -> Optimizer:
+        optimizer = config.instantiate(cfg_optimizer, self._model.parameters())
+        if opt_state_dict:
+            training.load_from_full_optimizer_state_dict(
+                optimizer,
+                opt_state_dict,
+                self._device,
+            )
+
+        if self._is_rank_zero:
+            log.info("Optimizer and loss are initialized.")
+        return optimizer
+
+    def _setup_lr_scheduler(
+        self,
+        cfg_lr_scheduler: DictConfig,
+        num_training_steps: int,
+        last_epoch: int,
+    ) -> Optimizer:
+        lr_scheduler = config.instantiate(
+            cfg_lr_scheduler,
+            self._optimizer,
+            num_training_steps=num_training_steps,
+            last_epoch=last_epoch,
+        )
+        if self._is_rank_zero:
+            log.info("Learning rate scheduler is initialized.")
+        return lr_scheduler
+
+    def _setup_data(
+        self,
+        cfg_dataset: DictConfig,
+        shuffle: bool,
+        batch_size: int,
+    ) -> Tuple[DistributedSampler, DataLoader]:
+        """
+        All data related setup happens here. Currently this recipe only supports the
+        DistributedSamplers with Map-style Datasets which fit into memory. Other samplers,
+        iterable datasets and streaming datasets are not supported.
+        """
+        world_size, rank = training.get_world_size_and_rank()
+
+        if isinstance(cfg_dataset, ListConfig):
+            datasets = [
+                config.instantiate(single_cfg_dataset, tokenizer=self._tokenizer)
+                for single_cfg_dataset in cfg_dataset
+            ]
+            ds = ConcatDataset(datasets=datasets)
+        else:
+            ds = config.instantiate(cfg_dataset, tokenizer=self._tokenizer)
+
+        sampler = DistributedSampler(
+            ds, num_replicas=world_size, rank=rank, shuffle=shuffle, seed=0
+        )
+
+        dataloader = DataLoader(
+            dataset=ds,
+            batch_size=batch_size,
+            sampler=sampler,
+            # dropping last avoids shape issues with compile + flex attention
+            drop_last=True,
+            collate_fn=partial(
+                padded_collate_dpo,
+                padding_idx=self._tokenizer.pad_id,
+                ignore_idx=CROSS_ENTROPY_IGNORE_IDX,
+            ),
+        )
+
+        if self._is_rank_zero:
+            log.info("Dataset and Sampler are initialized.")
+
+        return sampler, dataloader
+
+    def save_checkpoint(
+        self,
+        epoch: int,
+    ) -> None:
+        """
+        Checkpoint the state of the recipe. The constructed checkpoint state dict
+        contains the following information:
+        - Merged weights with key MODEL_KEY
+        - Adapter weights with key ADAPTER_KEY
+        - Relevant recipe state if training is not complete
+        - If the `self._save_adapter_weights_only` option is True, the checkpointer will save only the adapter weights
+
+        Checkpointer will save the merged weights, adapter weights and recipe state in
+        different checkpoint files. To correctly resume from training, the adapter weights
+        and recipe state must be provided along with the base model weights."""
+        # final dict passed onto the checkpointer
+        checkpoint_dict = {}
+
+        intermediate_checkpoint = epoch + 1 < self.total_epochs
+        # To prevent GPU memory from spiking during checkpoint save,
+        # we consolidate the full model and optim state dicts on CPU for rank 0
+        cpu_state_dict = training.get_full_model_state_dict(
+            self._model,
+            self._is_rank_zero,
+            device=self._device,
+        )
+        if intermediate_checkpoint:
+            opt_state_dict = training.get_full_optimizer_state_dict(
+                self._optimizer,
+                self._is_rank_zero,
+                device=self._device,
+            )
+        else:
+            opt_state_dict = None
+
+        # Now that we have the model and opt state dict, create the actual checkpoint dict
+        # to be sent to the checkpointer and ultimately written to file
+        if self._is_rank_zero:
+
+            # Filter out the adapter keys and weights from the model state dict. These will
+            # be saved separately
+            adapter_key_filter = lambda x: x in self.adapter_params
+            adapter_state_dict = {
+                k: v for k, v in cpu_state_dict.items() if adapter_key_filter(k)
+            }
+            checkpoint_dict.update({training.ADAPTER_KEY: adapter_state_dict})
+
+            # merge the adapter weights and base weights to create the model checkpoint
+            if not self._save_adapter_weights_only:
+                merged_state_dict = get_merged_lora_ckpt(
+                    cpu_state_dict,
+                    rank=self._lora_rank,
+                    alpha=self._lora_alpha,
+                )
+                checkpoint_dict.update({training.MODEL_KEY: merged_state_dict})
+
+            # if training is in-progress, checkpoint the optimizer state and recipe state
+            # as well.
+            if intermediate_checkpoint:
+                checkpoint_dict.update(
+                    {
+                        training.OPT_KEY: opt_state_dict,
+                        training.SEED_KEY: self.seed,
+                        training.EPOCHS_KEY: self.epochs_run,
+                        training.TOTAL_EPOCHS_KEY: self.total_epochs,
+                        training.MAX_STEPS_KEY: self.max_steps_per_epoch,
+                    }
+                )
+
+            self._checkpointer.save_checkpoint(
+                checkpoint_dict,
+                epoch=epoch,
+                intermediate_checkpoint=intermediate_checkpoint,
+                adapter_only=self._save_adapter_weights_only,
+            )
+
+    def concatenated_forward(
+        self, model: nn.Module, batch: Tuple[torch.Tensor, torch.Tensor]
+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
+        """
+        Run forward pass of the model with chosen and rejected samples concatenated.
+
+        Args:
+            model (nn.Module): The model to be used for the forward pass.
+            batch (Tuple[torch.Tensor, torch.Tensor]): Tuple of input_ids and labels.
+
+        Returns:
+            Tuple of chosen log probs, rejected log probs, chosen logits, rejected logits.
+        """
+        concatenated_input_ids, concatenated_labels = batch
+        concatenated_input_ids = concatenated_input_ids.to(self._device)
+        concatenated_labels = concatenated_labels.to(self._device)
+
+        # formed by concatenating an equal number of "chosen" and "rejected".
+        len_chosen = concatenated_input_ids.shape[0] // 2
+
+        all_logits = model(concatenated_input_ids)
+
+        all_log_probs = rlhf.get_batch_log_probs(
+            all_logits,
+            concatenated_labels,
+            # see :class:`~torchtune.rlhf.loss.dpo.SimPOLoss`
+            return_average_logprobs=isinstance(self._loss_fn, SimPOLoss),
+        )
+
+        chosen_log_probs = all_log_probs[:len_chosen]
+        rejected_log_probs = all_log_probs[len_chosen:]
+
+        chosen_logits = all_logits[:len_chosen]
+        rejected_logits = all_logits[len_chosen:]
+
+        return (chosen_log_probs, rejected_log_probs, chosen_logits, rejected_logits)
+
+    def train(self) -> None:
+        """
+        The core training loop.
+        """
+        # clean up before training begins
+        training.cleanup_before_training()
+
+        _, rank = training.get_world_size_and_rank()
+
+        # zero out the gradients before starting training
+        self._optimizer.zero_grad()
+
+        # Initialize tokens count and running loss (for grad accumulation)
+        t0 = time.perf_counter()
+        running_loss = 0
+        num_tokens = 0
+
+        # self.epochs_run should be non-zero when we're resuming from a checkpoint
+        for curr_epoch in range(self.epochs_run, self.total_epochs):
+
+            # Update the sampler to ensure data is correctly shuffled across epochs
+            # in case shuffle is True
+            self._sampler.set_epoch(curr_epoch)
+
+            pbar = tqdm(total=self._steps_per_epoch, disable=not (rank == 0))
+            for idx, batch in enumerate(self._dataloader):
+                if (
+                    self.max_steps_per_epoch is not None
+                    and (idx // self._gradient_accumulation_steps)
+                    == self.max_steps_per_epoch
+                ):
+                    break
+
+                # batch is input_ids, labels
+                num_tokens += batch[0].numel()
+
+                (
+                    policy_chosen_log_probs,
+                    policy_rejected_log_probs,
+                    policy_chosen_logits,
+                    policy_rejected_logits,
+                ) = self.concatenated_forward(self._model, batch)
+
+                policy_chosen_logits_mean = policy_chosen_logits.detach().mean()
+                policy_rejected_logits_mean = policy_rejected_logits.detach().mean()
+
+                # deleting logits here helps reduce (peak) memory usage - we only need them for metric logging
+                del policy_chosen_logits, policy_rejected_logits
+
+                if isinstance(self._loss_fn, SimPOLoss):
+                    loss, chosen_rewards, rejected_rewards = self._loss_fn(
+                        policy_chosen_log_probs, policy_rejected_log_probs
+                    )
+                else:
+                    # reference based losses (e.g. DPO) explicitly regularize the objective fn based on
+                    # the reference model's output - reference-free losses (such as SimPO) don't require this.
+                    with torch.no_grad(), disable_adapter(self._model):
+                        (
+                            reference_chosen_log_probs,
+                            reference_rejected_log_probs,
+                            _,
+                            _,
+                        ) = self.concatenated_forward(self._model, batch)
+                    loss, chosen_rewards, rejected_rewards = self._loss_fn(
+                        policy_chosen_log_probs,
+                        policy_rejected_log_probs,
+                        reference_chosen_log_probs,
+                        reference_rejected_log_probs,
+                    )
+
+                loss = loss.mean()
+                reward_accuracies = (chosen_rewards > rejected_rewards).float()
+
+                loss = loss / self._gradient_accumulation_steps
+                running_loss += loss
+                loss.backward()
+
+                # Step with optimizer
+                if (idx + 1) % self._gradient_accumulation_steps == 0:
+                    self._optimizer.step()
+                    self._optimizer.zero_grad(set_to_none=True)
+                    self._lr_scheduler.step()
+
+                    # Update the number of steps when the weights are updated
+                    self.global_step += 1
+
+                    loss_to_log = running_loss.item()
+                    pbar.update(1)
+                    pbar.set_description(
+                        f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
+                    )
+
+                    # Log per-step metrics
+                    if (
+                        self.global_step % self._log_every_n_steps == 0
+                        and self._is_rank_zero
+                    ):
+                        time_per_step = time.perf_counter() - t0
+                        log_dict = {
+                            "loss": loss_to_log,
+                            "lr": self._optimizer.param_groups[0]["lr"],
+                            "tokens_per_second_per_gpu": num_tokens / time_per_step,
+                            "rewards/chosen": chosen_rewards.mean().cpu(),
+                            "rewards/rejected": rejected_rewards.mean().cpu(),
+                            "rewards/accuracies": reward_accuracies.mean().cpu(),
+                            "rewards/margins": (chosen_rewards - rejected_rewards)
+                            .mean()
+                            .cpu(),
+                            "log_probs/rejected": policy_rejected_log_probs.detach()
+                            .mean()
+                            .cpu(),
+                            "log_probs/chosen": policy_chosen_log_probs.detach()
+                            .mean()
+                            .cpu(),
+                            "logits/rejected": policy_rejected_logits_mean.cpu(),
+                            "logits/chosen": policy_chosen_logits_mean.cpu(),
+                        }
+                        if self._log_peak_memory_stats:
+                            log_dict.update(
+                                training.get_memory_stats(device=self._device)
+                            )
+                        self._metric_logger.log_dict(
+                            log_dict,
+                            step=self.global_step,
+                        )
+
+                    # Reset running stats for the next step
+                    running_loss = 0
+                    num_tokens = 0
+                    t0 = time.perf_counter()
+
+            self.epochs_run += 1
+            self.save_checkpoint(epoch=curr_epoch)
+
+    def cleanup(self) -> None:
+        if self._is_rank_zero:
+            self._metric_logger.close()
+        destroy_process_group()
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """
+    Entry point for the recipe.
+
+    Configurable parameters are read in the following order:
+        - Parameters specified in config (see available configs through ``tune ls``)
+        - Overwritten by arguments from the command-line
+    """
+    if not training.is_distributed():
+        raise RuntimeError(
+            "Distributed finetune recipe should be run via a distributed launcher."
+            "If using tune CLI, please specify --nnodes 1 and --nproc_per_node [num_gpus]"
+        )
+    if cfg.get("fsdp_cpu_offload", False):
+        # Utilize all available CPU cores for intra-op parallelism. This provides ~2x
+        # speed up when benchmarking fused AdamW on CPU
+        training.set_torch_num_threads()
+    init_process_group(backend="gloo" if cfg.device == "cpu" else "nccl")
+
+    config.log_config(recipe_name="LoRADPORecipeDistributed", cfg=cfg)
+
+    recipe = LoRADPORecipeDistributed(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
diff -ruN marc_original/third_party/torchtune/recipes/lora_dpo_single_device.py marc/third_party/torchtune/recipes/lora_dpo_single_device.py
--- marc_original/third_party/torchtune/recipes/lora_dpo_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/lora_dpo_single_device.py	2025-02-20 17:49:29.498024160 -0500
@@ -0,0 +1,609 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import sys
+import time
+
+from functools import partial
+from typing import Any, Dict, Optional, Tuple
+from warnings import warn
+
+import torch
+from omegaconf import DictConfig, ListConfig
+
+from torch import nn
+from torch.optim import Optimizer
+from torch.utils.data import DataLoader, DistributedSampler
+from torchtune import config, modules, rlhf, training, utils
+from torchtune.data import CROSS_ENTROPY_IGNORE_IDX, padded_collate_dpo
+from torchtune.datasets import ConcatDataset
+from torchtune.modules.peft import (
+    disable_adapter,
+    get_adapter_params,
+    get_merged_lora_ckpt,
+    set_trainable_params,
+    validate_missing_and_unexpected_for_lora,
+    validate_state_dict_for_lora,
+)
+from torchtune.recipe_interfaces import FTRecipeInterface
+
+from torchtune.rlhf.loss import SimPOLoss
+from tqdm import tqdm
+
+log = utils.get_logger("DEBUG")
+
+
+class LoRADPORecipeSingleDevice(FTRecipeInterface):
+    """
+    LoRA DPO recipe for dense transformer-based LLMs such as Llama2 for
+    single device training. This is based on HF's DPOTrainer in the
+    TRL library: https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L65
+
+    This recipe supports:
+        - Activation checkpointing. This is enabled by default but is configurable.
+        - Full bf16 training for supported HW architectures. We currently check bf16 support via
+        the `torch.cuda.is_bf16_supported` API. This is disabled by default but can be enabled via
+        setting `dtype=bf16` in configuration.
+        - Checkpointing: of LoRA adapter parameters and their optimizer states. When resuming
+            from a checkpoint, the adapter parameters are loaded from the checkpoint along
+            with the base model weights. Note that intra-epoch resumption is not supported.
+        - Logging to terminal, WandB, or TensorBoard.
+
+
+    The following losses are supported in this recipe:
+        - :class:`~torchtune.rlhf.loss.DPOLoss`: Direct Preference Optimization (DPO).
+        - :class:`~torchtune.rlhf.loss.RSOPLoss`: Rejection Sampling Optimization (RSO).
+        - :class:`~torchtune.rlhf.loss.SimPOLoss`: Simple Preference Optimization (SimPO).
+
+    Assumptions:
+        - Checkpoints are ONLY saved at epoch boundaries. In case of failure, work done
+            in ongoing epoch is lost.
+        - Datasets are Map-style and data fits in memory (not streamed).
+
+    The following configs can be used to run this recipe:
+        >>> tune ls
+        RECIPE                          CONFIG
+        lora_dpo_single_device          llama2/7B_lora_dpo_single_device
+
+    Args:
+        cfg (DictConfig): OmegaConf object parsed from yaml file
+
+    Raises:
+        ValueError: If ``dtype`` is set to fp16.
+        RuntimeError: If ``dtype`` is set to bf16 and the hardware does not support bf16.
+
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+
+        self._device = utils.get_device(device=cfg.device)
+        # Reduced precision logic
+        self._dtype = training.get_dtype(cfg.dtype, device=self._device)
+
+        # fp16 precision is explicitly disabled as it is not supported in this
+        # recipe (for example, no gradient scaling).
+        if self._dtype == torch.float16:
+            raise ValueError(
+                "fp16 precision is not supported in this recipe. Please use fp32 or bf16."
+            )
+
+        # logging attributes
+        self._output_dir = cfg.output_dir
+        self._log_every_n_steps = cfg.get("log_every_n_steps", 1)
+        self._log_peak_memory_stats = cfg.get("log_peak_memory_stats", False)
+
+        # These are public properties which are updated by the checkpoint loader
+        # when ``resume_from_checkpoint`` is `True` or validated in tests
+        self.seed = training.set_seed(seed=cfg.seed)
+        self.epochs_run = 0
+        self.total_epochs = cfg.epochs
+        self.max_steps_per_epoch = cfg.max_steps_per_epoch
+        self.global_step = 0
+        self._resume_from_checkpoint = cfg.resume_from_checkpoint
+        self._save_adapter_weights_only = cfg.get("save_adapter_weights_only", False)
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+
+    def load_checkpoint(self, cfg_checkpointer: DictConfig) -> Dict[str, Any]:
+        """
+        Extract the checkpoint state from file and validate. This includes the
+        base model weights. If resume_from_checkpoint is True, this also includes
+        the adapter weights and recipe state
+        """
+        self._checkpointer = config.instantiate(
+            cfg_checkpointer,
+            resume_from_checkpoint=self._resume_from_checkpoint,
+        )
+        checkpoint_dict = self._checkpointer.load_checkpoint()
+
+        if self._resume_from_checkpoint:
+            if training.ADAPTER_KEY not in checkpoint_dict:
+                raise ValueError(
+                    "Adapter weights not found. Please ensure a valid adapter checkpoint is provided."
+                )
+            # _update_recipe_state will throw an exception if the recipe state is not correctly loaded
+            # no need to check here
+            self._update_recipe_state(checkpoint_dict)
+        return checkpoint_dict
+
+    def _update_recipe_state(self, ckpt_dict: Dict[str, Any]) -> None:
+        """
+        Updates the recipe state from checkpoint.
+        """
+        try:
+            self.epochs_run = ckpt_dict[training.EPOCHS_KEY]
+
+            # on mismatch, warn the user and prevent the override
+            if self.seed != ckpt_dict[training.SEED_KEY]:
+                warn(
+                    message=(
+                        "Config value for seed does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.SEED_KEY]}"
+                    )
+                )
+                self.seed = ckpt_dict[training.SEED_KEY]
+            if self.max_steps_per_epoch != ckpt_dict[training.MAX_STEPS_KEY]:
+                warn(
+                    message=(
+                        "Config value for max_steps_per_epoch does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.MAX_STEPS_KEY]}"
+                    )
+                )
+                self.max_steps_per_epoch = ckpt_dict[training.MAX_STEPS_KEY]
+
+            # on mismatch, warn the user but allow the override
+            if self.total_epochs != ckpt_dict[training.TOTAL_EPOCHS_KEY]:
+                warn(
+                    message=(
+                        "Config value for total_epochs does not match the checkpoint value, "
+                        f"using the config value: {self.total_epochs}"
+                    )
+                )
+
+        except KeyError as e:
+            raise KeyError(
+                "Checkpoint does not contain the required keys needed for updating recipe state. "
+                "Are you sure you passed in the right recipe checkpoint?"
+            ) from e
+
+    def setup(self, cfg: DictConfig) -> None:
+        """
+        Setup the recipe state. This includes recipe state (if resume_from_checkpoint is True),
+        model, tokenizer, loss, optimizer, learning rate scheduler, sampler, and dataloader.
+        """
+        self._metric_logger = config.instantiate(cfg.metric_logger)
+
+        # log config with parameter override
+        self._metric_logger.log_config(cfg)
+
+        self._model_compile = cfg.compile
+        checkpoint_dict = self.load_checkpoint(cfg_checkpointer=cfg.checkpointer)
+
+        self._model = self._setup_model(
+            cfg_model=cfg.model,
+            enable_activation_checkpointing=cfg.enable_activation_checkpointing,
+            compile_model=cfg.compile,
+            base_model_state_dict=checkpoint_dict[training.MODEL_KEY],
+            lora_weights_state_dict=(
+                checkpoint_dict[training.ADAPTER_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+
+        self._tokenizer = config.instantiate(cfg.tokenizer)
+        log.info("Tokenizer is initialized from file.")
+
+        self._optimizer = self._setup_optimizer(
+            cfg_optimizer=cfg.optimizer,
+            opt_state_dict=(
+                checkpoint_dict[training.OPT_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+
+        self._loss_fn = config.instantiate(cfg.loss)
+        log.info("Loss function is initialized.")
+
+        # Dataloader depends on the tokenizer and loss_fn and should be
+        # setup after all of these are setup
+        self._sampler, self._dataloader = self._setup_data(
+            cfg_dataset=cfg.dataset,
+            shuffle=cfg.shuffle,
+            batch_size=cfg.batch_size,
+        )
+
+        # Finally update the recipe state which can only be correctly set after all of the
+        # other components have been initialized and updated.
+
+        # Number of training steps in each epoch depends on the number of batches produced
+        # by the dataloader and the max_steps_per_epoch param set by the user and is used
+        # for logging and tracking training state. This should be computed after the dataloader
+        # has been setup
+        self._steps_per_epoch = (
+            len(self._dataloader) // self._gradient_accumulation_steps
+        )
+        if (
+            self.max_steps_per_epoch is not None
+            and self.max_steps_per_epoch < self._steps_per_epoch
+        ):
+            self._steps_per_epoch = self.max_steps_per_epoch
+            self.global_step = self.epochs_run * self._steps_per_epoch
+
+        # Learning rate scheduler can only be set up after number of steps
+        # has been computed
+        self._lr_scheduler = self._setup_lr_scheduler(
+            cfg_lr_scheduler=cfg.lr_scheduler,
+            num_training_steps=self.total_epochs * self._steps_per_epoch,
+            last_epoch=self.global_step - 1,
+        )
+
+    def _setup_model(
+        self,
+        cfg_model: DictConfig,
+        enable_activation_checkpointing: bool,
+        compile_model: bool,
+        base_model_state_dict: Dict[str, Any],
+        lora_weights_state_dict: Optional[Dict[str, Any]] = None,
+    ) -> nn.Module:
+        with training.set_default_dtype(self._dtype), self._device:
+            model = config.instantiate(cfg_model)
+        self._lora_rank = cfg_model.lora_rank
+        self._lora_alpha = cfg_model.lora_alpha
+        self._lora_attn_modules = list(cfg_model.lora_attn_modules)
+        self._apply_lora_to_mlp = cfg_model.apply_lora_to_mlp
+        self._apply_lora_to_output = getattr(cfg_model, "apply_lora_to_output", False)
+        self.adapter_params = get_adapter_params(model)
+        set_trainable_params(model, self.adapter_params)
+
+        if enable_activation_checkpointing:
+            training.set_activation_checkpointing(
+                model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        validate_state_dict_for_lora(
+            lora_attn_modules=cfg_model.lora_attn_modules,
+            apply_lora_to_mlp=cfg_model.apply_lora_to_mlp,
+            apply_lora_to_output=getattr(cfg_model, "apply_lora_to_output", False),
+            full_model_state_dict_keys=model.state_dict().keys(),
+            lora_state_dict_keys=(
+                lora_weights_state_dict.keys()
+                if lora_weights_state_dict is not None
+                else None
+            ),
+            base_model_state_dict_keys=base_model_state_dict.keys(),
+        )
+
+        base_missing, base_unexpected = model.load_state_dict(
+            base_model_state_dict, strict=False
+        )
+        if lora_weights_state_dict:
+            lora_missing, lora_unexpected = model.load_state_dict(
+                lora_weights_state_dict, strict=False
+            )
+        else:
+            lora_missing, lora_unexpected = None, None
+        validate_missing_and_unexpected_for_lora(
+            lora_attn_modules=self._lora_attn_modules,
+            apply_lora_to_mlp=self._apply_lora_to_mlp,
+            apply_lora_to_output=self._apply_lora_to_output,
+            base_missing=base_missing,
+            base_unexpected=base_unexpected,
+            lora_missing=lora_missing,
+            lora_unexpected=lora_unexpected,
+        )
+
+        log.info(f"Model is initialized with precision {self._dtype}.")
+
+        # Compile model, if enabled.
+        if compile_model:
+            training.compile_model(model)
+        if self._device == torch.device("cuda"):
+            memory_stats = training.get_memory_stats(device=self._device)
+            training.log_memory_stats(memory_stats)
+        return model
+
+    def _setup_optimizer(
+        self, cfg_optimizer: DictConfig, opt_state_dict: Optional[Dict[str, Any]] = None
+    ) -> Optimizer:
+        optimizer = config.instantiate(cfg_optimizer, self._model.parameters())
+        if opt_state_dict:
+            optimizer.load_state_dict(opt_state_dict)
+
+        log.info("Optimizer and loss are initialized.")
+        return optimizer
+
+    def _setup_lr_scheduler(
+        self,
+        cfg_lr_scheduler: DictConfig,
+        num_training_steps: int,
+        last_epoch: int,
+    ) -> Optimizer:
+        lr_scheduler = config.instantiate(
+            cfg_lr_scheduler,
+            self._optimizer,
+            num_training_steps=num_training_steps,
+            last_epoch=last_epoch,
+        )
+
+        log.info("Learning rate scheduler is initialized.")
+        return lr_scheduler
+
+    def _setup_data(
+        self,
+        cfg_dataset: DictConfig,
+        shuffle: bool,
+        batch_size: int,
+    ) -> Tuple[DistributedSampler, DataLoader]:
+        """
+        All data related setup happens here. Currently this recipe only supports
+        Map-style Datasets which fit into memory and an option for random shuffling.
+        Samplers, iterable datasets, and streaming datasets are not supported.
+        """
+        if isinstance(cfg_dataset, ListConfig):
+            datasets = [
+                config.instantiate(single_cfg_dataset, tokenizer=self._tokenizer)
+                for single_cfg_dataset in cfg_dataset
+            ]
+            ds = ConcatDataset(datasets=datasets)
+        else:
+            ds = config.instantiate(cfg_dataset, tokenizer=self._tokenizer)
+
+        sampler = DistributedSampler(
+            ds,
+            num_replicas=1,
+            rank=0,
+            shuffle=shuffle,
+            seed=0,
+        )
+        dataloader = DataLoader(
+            dataset=ds,
+            sampler=sampler,
+            batch_size=batch_size,
+            # dropping last avoids shape issues with compile + flex attention
+            drop_last=True,
+            collate_fn=partial(
+                padded_collate_dpo,
+                padding_idx=self._tokenizer.pad_id,
+                ignore_idx=CROSS_ENTROPY_IGNORE_IDX,
+            ),
+        )
+        log.info("Dataset and Sampler are initialized.")
+
+        return sampler, dataloader
+
+    def save_checkpoint(self, epoch: int) -> None:
+        """
+        Checkpoint the state of the recipe. The constructed checkpoint state dict
+        contains the following information:
+        - Merged weights with key MODEL_KEY
+        - Adapter weights with key ADAPTER_KEY
+        - Relevant recipe state if training is not complete
+        - If the `self._save_adapter_weights_only` option is True, the checkpointer will save only the adapter weights
+
+        To correctly resume from training, the adapter weights and recipe state must be provided along with the base model weights.
+        """
+        ckpt_dict = {}
+
+        intermediate_checkpoint = epoch + 1 < self.total_epochs
+        # if training is in-progress, checkpoint the optimizer state as well
+        if intermediate_checkpoint:
+            ckpt_dict.update(
+                {
+                    training.OPT_KEY: self._optimizer.state_dict(),
+                    training.SEED_KEY: self.seed,
+                    training.EPOCHS_KEY: self.epochs_run,
+                    training.TOTAL_EPOCHS_KEY: self.total_epochs,
+                    training.MAX_STEPS_KEY: self.max_steps_per_epoch,
+                }
+            )
+
+        adapter_state_dict = {k: v.cpu() for k, v in self.adapter_params.items()}
+        ckpt_dict.update({training.ADAPTER_KEY: adapter_state_dict})
+        if not self._save_adapter_weights_only:
+            # Construct the full state dict with LoRA weights merged into base LLM weights
+
+            # Move to CPU to avoid a copy on GPU
+            state_dict = {k: v.cpu() for k, v in self._model.state_dict().items()}
+
+            merged_state_dict = get_merged_lora_ckpt(
+                state_dict,
+                rank=self._lora_rank,
+                alpha=self._lora_alpha,
+            )
+
+            ckpt_dict.update({training.MODEL_KEY: merged_state_dict})
+
+        self._checkpointer.save_checkpoint(
+            ckpt_dict,
+            epoch=epoch,
+            intermediate_checkpoint=intermediate_checkpoint,
+            adapter_only=self._save_adapter_weights_only,
+        )
+
+    def concatenated_forward(
+        self, model: nn.Module, batch: Tuple[torch.Tensor, torch.Tensor]
+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
+        """
+        Run forward pass of the model with chosen and rejected samples concatenated.
+
+        Args:
+            model (nn.Module): The model to be used for the forward pass.
+            batch (Tuple[torch.Tensor, torch.Tensor]): Tuple of input_ids and labels.
+
+        Returns:
+            Tuple of chosen log probs, rejected log probs, chosen logits, rejected logits.
+        """
+        concatenated_input_ids, concatenated_labels = batch
+        concatenated_input_ids = concatenated_input_ids.to(self._device)
+        concatenated_labels = concatenated_labels.to(self._device)
+
+        # formed by concatenating an equal number of "chosen" and "rejected".
+        len_chosen = concatenated_input_ids.shape[0] // 2
+
+        all_logits = model(concatenated_input_ids)
+
+        all_log_probs = rlhf.get_batch_log_probs(
+            all_logits,
+            concatenated_labels,
+            # see :class:`~torchtune.rlhf.loss.dpo.SimPOLoss`
+            return_average_logprobs=isinstance(self._loss_fn, SimPOLoss),
+        )
+
+        chosen_log_probs = all_log_probs[:len_chosen]
+        rejected_log_probs = all_log_probs[len_chosen:]
+
+        chosen_logits = all_logits[:len_chosen]
+        rejected_logits = all_logits[len_chosen:]
+
+        return (chosen_log_probs, rejected_log_probs, chosen_logits, rejected_logits)
+
+    def train(self) -> None:
+        """
+        The core training loop.
+        """
+        if self._model_compile:
+            log.info(
+                "NOTE: torch.compile is enabled and model is compiled in first forward. Expect a relatively slow first iteration."
+            )
+
+        # Initialize tokens count and running loss (for grad accumulation)
+        t0 = time.perf_counter()
+        running_loss = 0
+        num_tokens = 0
+
+        # self.epochs_run should be non-zero when we're resuming from a checkpoint
+        for curr_epoch in range(self.epochs_run, self.total_epochs):
+            # Update the sampler to ensure data is correctly shuffled across epochs
+            # in case shuffle is True
+            self._sampler.set_epoch(curr_epoch)
+            pbar = tqdm(total=self._steps_per_epoch)
+            for idx, batch in enumerate(self._dataloader):
+                if (
+                    self.max_steps_per_epoch is not None
+                    and (idx // self._gradient_accumulation_steps)
+                    == self.max_steps_per_epoch
+                ):
+                    break
+
+                # batch is input_ids, labels
+                num_tokens += batch[0].numel()
+                (
+                    policy_chosen_log_probs,
+                    policy_rejected_log_probs,
+                    policy_chosen_logits,
+                    policy_rejected_logits,
+                ) = self.concatenated_forward(self._model, batch)
+
+                policy_chosen_logits_mean = policy_chosen_logits.detach().mean()
+                policy_rejected_logits_mean = policy_rejected_logits.detach().mean()
+
+                # deleting logits here helps reduce (peak) memory usage - we only need them for metric logging
+                del policy_chosen_logits, policy_rejected_logits
+
+                if isinstance(self._loss_fn, SimPOLoss):
+                    loss, chosen_rewards, rejected_rewards = self._loss_fn(
+                        policy_chosen_log_probs, policy_rejected_log_probs
+                    )
+                else:
+                    # reference based losses (e.g. DPO) explicitly regularize the objective fn based on
+                    # the reference model's output - reference-free losses (such as SimPO) don't require this.
+                    with torch.no_grad(), disable_adapter(self._model):
+                        (
+                            reference_chosen_log_probs,
+                            reference_rejected_log_probs,
+                            _,
+                            _,
+                        ) = self.concatenated_forward(self._model, batch)
+                    loss, chosen_rewards, rejected_rewards = self._loss_fn(
+                        policy_chosen_log_probs,
+                        policy_rejected_log_probs,
+                        reference_chosen_log_probs,
+                        reference_rejected_log_probs,
+                    )
+
+                loss = loss.mean()
+                reward_accuracies = (chosen_rewards > rejected_rewards).float()
+
+                loss = loss / self._gradient_accumulation_steps
+                running_loss += loss
+                loss.backward()
+
+                # Step with optimizer
+                if (idx + 1) % self._gradient_accumulation_steps == 0:
+                    self._optimizer.step()
+                    self._optimizer.zero_grad(set_to_none=True)
+                    self._lr_scheduler.step()
+                    # Update the number of steps when the weights are updated
+                    self.global_step += 1
+
+                    loss_to_log = running_loss.item()
+                    pbar.update(1)
+                    pbar.set_description(
+                        f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
+                    )
+
+                    # Log per-step metrics
+                    if self.global_step % self._log_every_n_steps == 0:
+                        time_per_step = time.perf_counter() - t0
+                        log_dict = {
+                            "loss": loss_to_log,
+                            "lr": self._optimizer.param_groups[0]["lr"],
+                            "tokens_per_second_per_gpu": num_tokens / time_per_step,
+                            "rewards/chosen": chosen_rewards.mean().cpu(),
+                            "rewards/rejected": rejected_rewards.mean().cpu(),
+                            "rewards/accuracies": reward_accuracies.mean().cpu(),
+                            "rewards/margins": (chosen_rewards - rejected_rewards)
+                            .mean()
+                            .cpu(),
+                            "log_probs/rejected": policy_rejected_log_probs.detach()
+                            .mean()
+                            .cpu(),
+                            "log_probs/chosen": policy_chosen_log_probs.detach()
+                            .mean()
+                            .cpu(),
+                            "logits/rejected": policy_rejected_logits_mean.cpu(),
+                            "logits/chosen": policy_chosen_logits_mean.cpu(),
+                        }
+                        if self._log_peak_memory_stats:
+                            log_dict.update(
+                                training.get_memory_stats(device=self._device)
+                            )
+                        self._metric_logger.log_dict(
+                            log_dict,
+                            step=self.global_step,
+                        )
+
+                    # Reset running stats for the next step
+                    running_loss = 0
+                    num_tokens = 0
+                    t0 = time.perf_counter()
+
+            self.epochs_run += 1
+            self.save_checkpoint(epoch=curr_epoch)
+
+    def cleanup(self) -> None:
+        self._metric_logger.close()
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """
+    Entry point for the recipe.
+
+    Configurable parameters are read in the following order:
+        - Parameters specified in config (see available configs through ``tune ls``)
+        - Overwritten by arguments from the command-line
+    """
+    config.log_config(recipe_name="LoRADPORecipeSingleDevice", cfg=cfg)
+    recipe = LoRADPORecipeSingleDevice(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
diff -ruN marc_original/third_party/torchtune/recipes/lora_finetune_distributed.py marc/third_party/torchtune/recipes/lora_finetune_distributed.py
--- marc_original/third_party/torchtune/recipes/lora_finetune_distributed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/lora_finetune_distributed.py	2025-02-20 17:49:29.502024167 -0500
@@ -0,0 +1,908 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import contextlib
+import sys
+import time
+
+from functools import partial
+from typing import Any, Dict, Optional, Tuple, Union
+from warnings import warn
+
+import torch
+from omegaconf import DictConfig, ListConfig
+
+from torch import nn
+from torch.distributed import destroy_process_group, init_process_group
+
+from torch.optim import Optimizer
+from torch.utils.data import DataLoader, DistributedSampler
+from torchtune import config, modules, training, utils
+from torchtune.config._utils import _get_component_from_path
+from torchtune.data import padded_collate_packed
+from torchtune.datasets import ConcatDataset
+from torchtune.modules.peft import (
+    DoRALinear,
+    get_adapter_params,
+    get_lora_module_names,
+    get_merged_lora_ckpt,
+    load_dora_magnitudes,
+    LoRALinear,
+    set_trainable_params,
+    validate_missing_and_unexpected_for_lora,
+)
+from torchtune.recipe_interfaces import FTRecipeInterface
+from torchtune.training import (
+    DummyProfiler,
+    NoOpManager,
+    OffloadActivations,
+    PROFILER_KEY,
+)
+
+from tqdm import tqdm
+
+log = utils.get_logger("DEBUG")
+
+
+class LoRAFinetuneRecipeDistributed(FTRecipeInterface):
+    """
+    Distributed LoRA finetuning recipe for dense transformer-based LLMs such as Llama2. This recipe supports
+    distributed training and can be run on a single node (1 to 8 GPUs).
+
+    Features:
+        - FSDP. Supported using PyTorch's FSDP APIs. CPU offload of parameters, gradients, and optimizer states
+            is supported via ``fsdp_cpu_offload``. Resharding of parameters after the forward pass is
+            done by default (corresponding to FULL_SHARD sharding strategy), but can be disabled by setting the config
+            ``fsdp_reshard_after_forward`` to False (this corresponds to SHARD_GRAD_OP sharding strategy).
+            DDP is currently not supported. Training on CPU is not supported.
+
+        - Activation Checkpointing. This can be controlled using the ``enable_activation_checkpointing``
+            flag. Activation checkpointing helps reduce the memory footprint since we no longer keep
+            activations in memory and instead recompute them during the backward pass. This is especially
+            helpful for larger batch sizes when you're memory constrained. But these savings in memory
+            come at the cost of training performance. In most cases training can slow-down quite a bit as
+            a result of this activation recomputation.
+
+        - Activation Offloading. This can be controlled using the ``enable_activation_offloading``
+            flag. Activation offloading is a technique similar to activations checkpointing that helps
+            reduce the memory footprint to prevent OOMs on CUDA and enable bigger batches. Where activations
+            checkpointing drops the activation in the forward to recompute it later in the backward,
+            activations offloading will drop the activation in the forward to the CPU and bring it
+            back during the backward pass. As always, there is a tradeoff--these savings in memory can
+            come at the cost of training performance and CPU resources. To recover some runtime cost,
+            we've added an option to enable offloading on a different stream to permit overlapping with
+            the computation. This option is currently only available on PyTorch nightly 2.5.0.dev20240907
+            or later and will be enabled by default if an acceptable torch version is found. Activation
+            offloading can be used in conjunction with activation checkpointing.
+
+        - Precision. Full fp32 and bf16 training are supported. Precision is controlled using the ``dtype``
+            flag. When ``dtype=bf16``, all activations, gradients and optimizer states are in bfloat16. In
+            most cases this should halve the memory footprint of full precision (fp32) training, without
+            loss in model quality (will depend on the model, training data and other settings). For
+            GPUs which do not support bfloat16, we fall back to fp32. Mixed precision training and fp16
+            precision are currently not supported.
+
+        - Gradient Accumulation. You can simulate larger batch sizes by accumulating gradients. This is
+            controlled using the ``gradient_accumulation_steps`` flag.
+
+                Total Batch Size = batch_size * number of GPUs * gradient accumulation steps.
+
+            For example: with batch_size=1, nproc_per_node=2 and gradient_accumulation_steps=32 we get a
+            total batch size of 64.
+
+            Gradient accumulation is especially useful when you are memory constrained. In this case,
+            accumulating gradients might give you better training speed than enabling activation
+            checkpointing.
+
+        - Checkpointing. Model weights are checkpointed both at the end of each epoch and at the end of
+            training. Currently we checkpoint both the adapter weights (trainable params only) and the
+            complete merged weights (adapter weights added back to the base model). For more details
+            please take a look at our LoRA tutorial
+            (https://pytorch.org/torchtune/main/tutorials/lora_finetune.html).
+
+            Optimizer State and recipe state (seed, total_epochs, number of epochs run etc) are
+            only saved at the end of a given epoch and used in case of resuming training. Resuming
+            training is controlled by the ``resume_from_checkpoint`` flag. Mid-epoch checkpointing is
+            currently not supported.
+
+            For more details on the checkpointer, please take a look at
+            our checkpointer deepdive (https://pytorch.org/torchtune/main/tutorials/checkpointer.html).
+
+        - Logging. Terminal, Disk, WandB and TensorBoard are all supported.
+
+        - Gradient Clipping. Gradient clipping is supported using the ``clip_grad_norm`` flag. By default,
+            ``clip_grad_norm`` is set to ``None``. If you only want to log the grad norm, you can set
+            ``clip_grad_norm='inf'``.
+
+    For a full list of example configs for this recipe, run ``tune ls`` on the command line. Each config
+    has example commands for how to kick-off training.
+
+    Args:
+        cfg (DictConfig): OmegaConf object parsed from yaml file
+
+    Raises:
+        ValueError: If ``dtype`` is set to fp16.
+        ValueError: If world_size is 1
+        RuntimeError: If ``dtype`` is set to bf16 and the hardware does not support bf16.
+        RuntimeError: If ``left_pad_sequence`` is set as the data collator.
+        RuntimeError: If ``enable_activation_offloading`` is True and device is not CUDA.
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        self._device = utils.get_device(device=cfg.device)
+        self._dtype = training.get_dtype(cfg.dtype, device=self._device)
+
+        if self._dtype == torch.float16:
+            raise ValueError(
+                "full fp16 training is not supported with this recipe. Please use bf16 or fp32 instead."
+            )
+
+        _, rank = training.get_world_size_and_rank()
+
+        # _is_rank_zero is used primarily for logging. In the future, the logger
+        # should directly take care of this
+        self._is_rank_zero = rank == 0
+
+        # logging attributes
+        self._output_dir = cfg.output_dir
+        self._log_every_n_steps = cfg.get("log_every_n_steps", 1)
+        self._log_peak_memory_stats = cfg.get("log_peak_memory_stats", False)
+
+        # training attributes
+        self._enable_activation_checkpointing = cfg.enable_activation_checkpointing
+        self._enable_activation_offloading = cfg.get(
+            "enable_activation_offloading", False
+        )
+        if self._enable_activation_offloading and self._device.type != "cuda":
+            raise RuntimeError(
+                "enable_activation_offloading should only be enabled for training on CUDA"
+            )
+
+        # These attributes constitute the recipe state and are updated by ``load_checkpoint``
+        # when ``resume_from_checkpoint`` is ``True``
+        self.seed = training.set_seed(seed=cfg.seed)
+        self.epochs_run = 0
+        self.total_epochs = cfg.epochs
+        self.max_steps_per_epoch = cfg.max_steps_per_epoch
+        self.global_step = 0
+        self._clip_grad_norm = cfg.get("clip_grad_norm", None)
+
+        self._save_adapter_weights_only = cfg.get("save_adapter_weights_only", False)
+        self._resume_from_checkpoint = cfg.resume_from_checkpoint
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+
+    def load_checkpoint(self, cfg_checkpointer: DictConfig) -> Dict[str, Any]:
+        """
+        Extract the checkpoint state from file and validate. This includes the
+        base model weights. If resume_from_checkpoint is True, this also includes
+        the adapter weights and recipe state
+        """
+        self._checkpointer = config.instantiate(
+            cfg_checkpointer,
+            resume_from_checkpoint=self._resume_from_checkpoint,
+        )
+        checkpoint_dict = self._checkpointer.load_checkpoint()
+
+        # When resuming from checkpoint for LoRA, the recipe expects the adapter weights
+        # and recipe state to be present. The keys should match up with what ``save_checkpoint``
+        # used to create these intermediate checkpoints
+        if self._resume_from_checkpoint:
+            if training.ADAPTER_KEY not in checkpoint_dict:
+                raise ValueError(
+                    "Adapter weights not found. Please ensure a valid adapter checkpoint is provided."
+                )
+            # _update_recipe_state will throw an exception if the recipe state is not corrctly loaded
+            # no need to check here
+            self._update_recipe_state(checkpoint_dict)
+        return checkpoint_dict
+
+    def _update_recipe_state(self, ckpt_dict: Dict[str, Any]) -> None:
+        """
+        Updates the recipe state from checkpoint.
+        """
+        try:
+            self.epochs_run = ckpt_dict[training.EPOCHS_KEY]
+
+            # on mismatch, warn the user and prevent the override
+            if self.seed != ckpt_dict[training.SEED_KEY]:
+                warn(
+                    message=(
+                        "Config value for seed does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.SEED_KEY]}"
+                    )
+                )
+                self.seed = ckpt_dict[training.SEED_KEY]
+            if self.max_steps_per_epoch != ckpt_dict[training.MAX_STEPS_KEY]:
+                warn(
+                    message=(
+                        "Config value for max_steps_per_epoch does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.MAX_STEPS_KEY]}"
+                    )
+                )
+                self.max_steps_per_epoch = ckpt_dict[training.MAX_STEPS_KEY]
+
+            # on mismatch, warn the user but allow the override
+            if self.total_epochs != ckpt_dict[training.TOTAL_EPOCHS_KEY]:
+                warn(
+                    message=(
+                        "Config value for total_epochs does not match the checkpoint value, "
+                        f"using the config value: {self.total_epochs}"
+                    )
+                )
+
+        except KeyError as e:
+            raise KeyError(
+                "Checkpoint does not contain the required keys needed for updating recipe state. "
+                "Are you sure you passed in the right recipe checkpoint?"
+            ) from e
+
+    def setup(self, cfg: DictConfig) -> None:
+        """
+        Setup the recipe state. This includes recipe state (if resume_from_checkpoint is True),
+        model, tokenizer, loss, optimizer, learning rate scheduler, sampler, and dataloader.
+        """
+        if self._is_rank_zero:
+            self._metric_logger = config.instantiate(cfg.metric_logger)
+
+            # log config with parameter override
+            self._metric_logger.log_config(cfg)
+
+        checkpoint_dict = self.load_checkpoint(cfg_checkpointer=cfg.checkpointer)
+        self._compile = cfg.get("compile", False)
+
+        self._model = self._setup_model(
+            cfg_model=cfg.model,
+            enable_activation_checkpointing=cfg.enable_activation_checkpointing,
+            enable_activation_offloading=self._enable_activation_offloading,
+            fsdp_cpu_offload=cfg.get("fsdp_cpu_offload", False),
+            reshard_after_forward=cfg.get("fsdp_reshard_after_forward", True),
+            base_model_state_dict=checkpoint_dict[training.MODEL_KEY],
+            lora_weights_state_dict=(
+                checkpoint_dict[training.ADAPTER_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+        self._tokenizer = config.instantiate(cfg.tokenizer)
+
+        self._optimizer = self._setup_optimizer(
+            cfg_optimizer=cfg.optimizer,
+            opt_state_dict=(
+                checkpoint_dict[training.OPT_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+
+        # initialize loss
+        self._loss_fn = config.instantiate(cfg.loss)
+
+        if self._compile:
+            training.compile_loss(self._loss_fn, verbose=self._is_rank_zero)
+
+        if self._loss_fn.__class__.__name__ == "CEWithChunkedOutputLoss":
+            # set num_output_chunks for model
+            self._model.set_num_output_chunks(self._loss_fn.num_output_chunks)
+        if self._is_rank_zero:
+            log.info("Loss is initialized.")
+
+        # sampler and dataloader depend on the tokenizer and loss_fn and should be
+        # setup after all of these are setup
+        collate_name = cfg.get("collate_fn", "torchtune.data.padded_collate_sft")
+        self._sampler, self._dataloader = self._setup_data(
+            cfg_dataset=cfg.dataset,
+            shuffle=cfg.shuffle,
+            batch_size=cfg.batch_size,
+            collate_fn=collate_name,
+        )
+
+        # Finally update the recipe state which can only be correctly set after all of the
+        # other components have been initialized and updated.
+
+        # Number of training steps in each epoch depends on the number of batches produced
+        # by the dataloader and the max_steps_per_epoch param set by the user and is used
+        # for logging and tracking training state. This should be computed after the dataloader
+        # has been setup
+        self._steps_per_epoch = (
+            len(self._dataloader) // self._gradient_accumulation_steps
+        )
+        if (
+            self.max_steps_per_epoch is not None
+            and self.max_steps_per_epoch < self._steps_per_epoch
+        ):
+            self._steps_per_epoch = self.max_steps_per_epoch
+        self.global_step = self.epochs_run * self._steps_per_epoch
+
+        # Learning rate scheduler can only be set up after number of steps
+        # has been computed
+        self._lr_scheduler = self._setup_lr_scheduler(
+            cfg_lr_scheduler=cfg.lr_scheduler,
+            num_training_steps=self.total_epochs * self._steps_per_epoch,
+            last_epoch=self.global_step - 1,
+        )
+
+        # Set up profiler, returns DummyProfiler (nullcontext object with no-op `step` method)
+        # if cfg is missing profiler key or if `cfg.profiler.enabled = False`
+        self._profiler = self._setup_profiler(cfg.get(PROFILER_KEY, None))
+
+        # Used to ignore labels for loss computation
+        self.ignore_labels_cache = torch.full(
+            (cfg.batch_size, 1), self._loss_fn.ignore_index, device=self._device
+        )
+
+    def _setup_profiler(
+        self, cfg_profiler: Optional[DictConfig] = None
+    ) -> Union[torch.profiler.profile, DummyProfiler]:
+        """
+        Parses the `profiler` section of top-level `cfg` and sets up profiler
+
+        Args:
+            cfg_profiler (Optional[DictConfig]): ``profiler`` section of the top-level ``cfg`` (the main config passed to
+                `recipe.main`). Default None.
+
+        Returns:
+            profiler: Union[torch.profiler.profile, DummyProfiler] - DummyProfiler is a nullcontext with no-op methods
+            for `start`, `stop`, and `step` that can be used in place of `torch.profiler.profile` if profiler is not enabled such
+            that the instrumented training loop does not need to be changed profiling is disabled.
+
+        The profiler config can be provided in configs under the `profiler` key with the following layout:
+
+        .. code-block:: yaml
+            profiler:
+                enabled: bool
+
+                #Output directory of trace artifacts
+                output_dir: str
+
+            #`torch.profiler.ProfilerActivity` types to trace
+            cpu: bool
+            cuda: bool
+
+                #Trace options
+                profile_memory: bool
+                with_stack: bool
+                record_shapes: bool
+                with_flops: bool
+
+            # `torch.profiler.schedule` options:
+            # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+            wait_steps: int
+            warmup_steps: int
+            active_steps: int
+            num_cycles: int
+        """
+        # Missing profiler section in config, assume disabled
+        if cfg_profiler is None:
+            cfg_profiler = DictConfig({"enabled": False})
+
+        # Check that component is included and set correctly
+        if cfg_profiler.get("_component_", None) is None:
+            cfg_profiler["_component_"] = "torchtune.training.setup_torch_profiler"
+        else:
+            assert (
+                cfg_profiler.get("_component_")
+                == "torchtune.training.setup_torch_profiler"
+            ), "Only torch profiler supported currently: component must be `torchtune.training.setup_torch_profiler`"
+
+        profiler, profiler_cfg = config.instantiate(cfg_profiler)
+
+        if self._is_rank_zero:
+            log.info(f" Profiler config after instantiation: {profiler_cfg}")
+
+            self.profiler_profile_memory = profiler_cfg.get("profile_memory", False)
+            if profiler_cfg["enabled"]:
+                self.profiler_wait_steps = profiler_cfg["wait_steps"]
+                self.profiler_warmup_steps = profiler_cfg["warmup_steps"]
+                self.profiler_active_steps = profiler_cfg["active_steps"]
+
+        return profiler
+
+    def _setup_model(
+        self,
+        cfg_model: DictConfig,
+        enable_activation_checkpointing: bool,
+        enable_activation_offloading: bool,
+        fsdp_cpu_offload: bool,
+        reshard_after_forward: bool,
+        base_model_state_dict: Dict[str, Any],
+        lora_weights_state_dict: Optional[Dict[str, Any]] = None,
+    ) -> nn.Module:
+        """
+        Model initialization has some important considerations:
+           a. To minimize GPU peak memory, we initialize the model on meta device with
+              the right dtype
+           b. All ranks calls ``load_state_dict`` without peaking CPU RAMs since
+              full state dicts are loaded with ``torch.load(mmap=True)``
+           c. We register (pre-)forward hooks with ``fully_shard`` instead of wrapping `nn.Module`
+        """
+
+        self._lora_rank = cfg_model.lora_rank
+        self._lora_alpha = cfg_model.lora_alpha
+        self._lora_attn_modules = list(cfg_model.lora_attn_modules)
+        self._apply_lora_to_mlp = cfg_model.apply_lora_to_mlp
+        self._apply_lora_to_output = getattr(cfg_model, "apply_lora_to_output", False)
+
+        if self._is_rank_zero:
+            log.info(
+                "FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ..."
+            )
+            init_start = time.perf_counter()
+
+        with training.set_default_dtype(self._dtype), torch.device("meta"):
+            model = config.instantiate(cfg_model)
+
+        self.adapter_params = get_adapter_params(model)
+        set_trainable_params(model, self.adapter_params)
+
+        if self._compile:
+            training.compile_model(model, verbose=self._is_rank_zero)
+
+        if enable_activation_checkpointing:
+            training.set_activation_checkpointing(
+                model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        # For FSDP sharding, we can condition on either the module or its name
+        # Shard conditions should be callables taking name (relative to model root)
+        # and the module itself and returning a bool on whether to shard the given module
+
+        # Shard transformer decoder layers (or AC-wrapped versions)
+        # Alternatively we could condition on the module type (TransformerDecoder or CheckpointWrapper)
+        # But directly using the name is more concise
+        def _is_layer_name(name: str, module: nn.Module) -> bool:
+            """
+            Return True for layers.i and False for all other module names
+            Covers sharding for both AC-wrapped and non-AC-wrapped modules in one shot
+            """
+            name_list = name.split(".")
+            return (
+                len(name_list) == 2
+                and name_list[0] == "layers"
+                and str.isdigit(name_list[1])
+            )
+
+        training.shard_model(
+            model=model,
+            shard_conditions=[_is_layer_name],
+            cpu_offload=fsdp_cpu_offload,
+            reshard_after_forward=reshard_after_forward,
+        )
+
+        if lora_weights_state_dict:
+            lora_missing, lora_unexpected = training.load_from_full_model_state_dict(
+                model,
+                lora_weights_state_dict,
+                self._device,
+                self._is_rank_zero,
+                cpu_offload=fsdp_cpu_offload,
+            )
+        else:
+            lora_missing, lora_unexpected = None, None
+
+        # Initialize LoRA params and RoPE buffers
+        with training.set_default_dtype(self._dtype), self._device:
+            lora_device = "cpu" if fsdp_cpu_offload else self._device
+            for m in model.modules():
+                if (
+                    isinstance(m, LoRALinear) or isinstance(m, DoRALinear)
+                ) and not lora_weights_state_dict:
+                    # lora may not be covered in state dict
+                    # if finetune for the 1st time
+                    m.lora_a.to_empty(device=lora_device)
+                    m.lora_b.to_empty(device=lora_device)
+                    m.initialize_parameters()
+                # RoPE is not covered in state dict
+                if hasattr(m, "rope_init"):
+                    m.rope_init()
+
+        base_missing, base_unexpected = training.load_from_full_model_state_dict(
+            model,
+            base_model_state_dict,
+            self._device,
+            self._is_rank_zero,
+            cpu_offload=fsdp_cpu_offload,
+        )
+        is_dora = False
+        for m in model.modules():
+            if hasattr(m, "initialize_dora_magnitude"):
+                is_dora = True
+                m.initialize_dora_magnitude()
+        if is_dora:
+            load_dora_magnitudes(model)
+        validate_missing_and_unexpected_for_lora(
+            lora_attn_modules=self._lora_attn_modules,
+            apply_lora_to_mlp=self._apply_lora_to_mlp,
+            apply_lora_to_output=self._apply_lora_to_output,
+            base_missing=base_missing,
+            base_unexpected=base_unexpected,
+            lora_missing=lora_missing,
+            lora_unexpected=lora_unexpected,
+        )
+        # Ensure no params and buffers are on meta device
+        training.validate_no_params_on_meta_device(model)
+
+        self.activations_handling_ctx = contextlib.nullcontext()
+        if enable_activation_offloading:
+            self.activations_handling_ctx = OffloadActivations()
+
+            # Below is our hack to disable offloading the last output Linear in every
+            # step, as the cost for offloading the activation and then soon after bringing
+            # it back is expensive. Moreover, due to heuristics in our streaming API,
+            # we actually use more memory if we offload it as it interferes with chunkedCE.
+            if hasattr(model, "output") and isinstance(model.output, nn.Module):
+                noop_ctx = NoOpManager()
+                model.output.register_forward_pre_hook(
+                    lambda *args: noop_ctx.__enter__()
+                )
+                model.output.register_forward_hook(
+                    lambda *args: noop_ctx.__exit__(), always_call=True
+                )
+
+        if self._is_rank_zero:
+            log.info(
+                f"Instantiating model and loading checkpoint took {time.perf_counter() - init_start:.2f} secs"
+            )
+            memory_stats = training.get_memory_stats(device=self._device)
+            training.log_memory_stats(memory_stats)
+
+        # synchronize before training begins
+        torch.distributed.barrier()
+
+        return model
+
+    def _setup_optimizer(
+        self, cfg_optimizer: DictConfig, opt_state_dict: Optional[Dict[str, Any]] = None
+    ) -> Optimizer:
+        optimizer = config.instantiate(cfg_optimizer, self._model.parameters())
+        if opt_state_dict:
+            training.load_from_full_optimizer_state_dict(
+                optimizer,
+                opt_state_dict,
+                self._device,
+            )
+
+        if self._is_rank_zero:
+            log.info("Optimizer is initialized.")
+        return optimizer
+
+    def _setup_lr_scheduler(
+        self,
+        cfg_lr_scheduler: DictConfig,
+        num_training_steps: int,
+        last_epoch: int,
+    ) -> Optimizer:
+        lr_scheduler = config.instantiate(
+            cfg_lr_scheduler,
+            self._optimizer,
+            num_training_steps=num_training_steps,
+            last_epoch=last_epoch,
+        )
+        if self._is_rank_zero:
+            log.info("Learning rate scheduler is initialized.")
+        return lr_scheduler
+
+    def _setup_data(
+        self,
+        cfg_dataset: DictConfig,
+        shuffle: bool,
+        batch_size: int,
+        collate_fn: str,
+    ) -> Tuple[DistributedSampler, DataLoader]:
+        """
+        All data related setup happens here. Currently this recipe only supports the
+        DistributedSamplers with Map-style Datasets which fit into memory. Other samplers,
+        iterable datasets and streaming datasets are not supported.
+        """
+        world_size, rank = training.get_world_size_and_rank()
+
+        if isinstance(cfg_dataset, ListConfig):
+            datasets = [
+                config.instantiate(single_cfg_dataset, self._tokenizer)
+                for single_cfg_dataset in cfg_dataset
+            ]
+            ds = ConcatDataset(datasets=datasets)
+            packed = False
+        else:
+            ds = config.instantiate(cfg_dataset, self._tokenizer)
+            packed = cfg_dataset.get("packed", False)
+
+        # Instantiate collate_fn
+        if "left_pad_sequence" in collate_fn:
+            raise RuntimeError("left_pad_sequence collator is only for inference.")
+        collate_fn = _get_component_from_path(collate_fn)
+
+        sampler = DistributedSampler(
+            ds, num_replicas=world_size, rank=rank, shuffle=shuffle, seed=0
+        )
+
+        dataloader = DataLoader(
+            dataset=ds,
+            batch_size=batch_size,
+            sampler=sampler,
+            # dropping last avoids shape issues with compile + flex attention
+            drop_last=True,
+            collate_fn=partial(
+                collate_fn,
+                padding_idx=self._tokenizer.pad_id,
+                ignore_idx=self._loss_fn.ignore_index,
+            )
+            if not packed
+            else padded_collate_packed,
+        )
+
+        if self._is_rank_zero:
+            log.info("Dataset and Sampler are initialized.")
+
+        return sampler, dataloader
+
+    def save_checkpoint(
+        self,
+        epoch: int,
+    ) -> None:
+        """
+        Checkpoint the state of the recipe. The constructed checkpoint state dict
+        contains the following information:
+        - Merged weights with key MODEL_KEY
+        - Adapter weights with key ADAPTER_KEY
+        - Relevant recipe state if training is not complete
+        - If the `self._save_adapter_weights_only` option is True, the checkpointer will save only the adapter weights
+
+        Checkpointer will save the merged weights, adapter weights and recipe state in
+        different checkpoint files. To correctly resume from training, the adapter weights
+        and recipe state must be provided along with the base model weights.
+        """
+        # final dict passed onto the checkpointer
+        checkpoint_dict = {}
+
+        intermediate_checkpoint = epoch + 1 < self.total_epochs
+        # To prevent GPU memory from spiking during checkpoint save,
+        # we consolidate the full model and optim state dicts on CPU for rank 0
+        cpu_state_dict = training.get_full_model_state_dict(
+            self._model,
+            self._is_rank_zero,
+            device=self._device,
+            trainable_only=self._save_adapter_weights_only,
+        )
+
+        if intermediate_checkpoint:
+            opt_state_dict = training.get_full_optimizer_state_dict(
+                self._optimizer,
+                self._is_rank_zero,
+                device=self._device,
+            )
+        else:
+            opt_state_dict = None
+
+        # Now that we have the model and opt state dict, create the actual checkpoint dict
+        # to be sent to the checkpointer and ultimately written to file
+        if self._is_rank_zero:
+
+            # Filter out the adapter keys and weights from the model state dict. These will
+            # be saved separately
+            adapter_key_filter = lambda x: x in self.adapter_params
+            adapter_state_dict = {
+                k: v for k, v in cpu_state_dict.items() if adapter_key_filter(k)
+            }
+            checkpoint_dict.update({training.ADAPTER_KEY: adapter_state_dict})
+
+            # merge the adapter weights and base weights to create the model checkpoint
+            if not self._save_adapter_weights_only:
+                merged_state_dict = get_merged_lora_ckpt(
+                    cpu_state_dict,
+                    rank=self._lora_rank,
+                    alpha=self._lora_alpha,
+                )
+                checkpoint_dict.update({training.MODEL_KEY: merged_state_dict})
+
+            # if training is in-progress, checkpoint the optimizer state and recipe state
+            # as well.
+            if intermediate_checkpoint:
+                checkpoint_dict.update(
+                    {
+                        training.OPT_KEY: opt_state_dict,
+                        training.SEED_KEY: self.seed,
+                        training.EPOCHS_KEY: self.epochs_run,
+                        training.TOTAL_EPOCHS_KEY: self.total_epochs,
+                        training.MAX_STEPS_KEY: self.max_steps_per_epoch,
+                    }
+                )
+
+            adapter_config = {
+                "r": self._lora_rank,
+                "lora_alpha": self._lora_alpha,
+                "target_modules": get_lora_module_names(
+                    self._lora_attn_modules,
+                    self._apply_lora_to_mlp,
+                    self._apply_lora_to_output,
+                ),
+                "peft_type": "LORA",
+            }
+            checkpoint_dict.update({training.ADAPTER_CONFIG: adapter_config})
+            print("saving checkpoint")
+            self._checkpointer.save_checkpoint(
+                checkpoint_dict,
+                epoch=epoch,
+                intermediate_checkpoint=intermediate_checkpoint,
+                adapter_only=self._save_adapter_weights_only,
+            )
+
+    def train(self) -> None:
+        """
+        The core training loop.
+        """
+        # clean up before training begins
+        training.cleanup_before_training()
+
+        _, rank = training.get_world_size_and_rank()
+
+        # zero out the gradients before starting training
+        self._optimizer.zero_grad()
+
+        # Initialize tokens count and running loss (for grad accumulation)
+        t0 = time.perf_counter()
+        running_loss = 0
+        num_tokens = 0
+
+        self._profiler.start()
+        # self.epochs_run should be non-zero when we're resuming from a checkpoint
+        for curr_epoch in range(self.epochs_run, self.total_epochs):
+
+            # Update the sampler to ensure data is correctly shuffled across epochs
+            # in case shuffle is True
+            self._sampler.set_epoch(curr_epoch)
+
+            pbar = tqdm(total=self._steps_per_epoch, disable=not (rank == 0))
+            for idx, batch in enumerate(self._dataloader):
+                if (
+                    self.max_steps_per_epoch is not None
+                    and (idx // self._gradient_accumulation_steps)
+                    == self.max_steps_per_epoch
+                ):
+                    break
+
+                # Start tracking CUDA memory for active steps for just the first epoch
+                if (
+                    self._is_rank_zero
+                    and curr_epoch == 0
+                    and self.profiler_profile_memory
+                    and idx == self.profiler_wait_steps + self.profiler_warmup_steps
+                ):
+                    torch.cuda.memory._record_memory_history()
+
+                utils.batch_to_device(batch, self._device)
+                num_tokens += batch["tokens"].numel()
+
+                # Shape [b, s], needed for the loss not the model
+                labels = batch.pop("labels")
+
+                with self.activations_handling_ctx:
+                    logits = self._model(**batch)
+
+                # Shift labels to compute loss
+                # equivalent to doing labels[..., 1:] and logits[..., :-1, :]
+                # But this way we dont need to slice the logits. We just add an ignore index to labels.
+                labels = torch.hstack(
+                    (labels[..., 1:], self.ignore_labels_cache[: labels.shape[0]])
+                )
+                if not isinstance(logits, list):
+                    labels = labels.reshape(-1)
+                    logits = logits.reshape(-1, logits.size(-1))
+
+                # Compute loss
+                loss = self._loss_fn(logits, labels)
+
+                # free logits otherwise it peaks backward memory
+                del logits
+
+                loss = loss / self._gradient_accumulation_steps
+                running_loss += loss
+                loss.backward()
+
+                # Step with optimizer
+                if (idx + 1) % self._gradient_accumulation_steps == 0:
+                    if self._clip_grad_norm is not None:
+                        grad_norm = torch.nn.utils.clip_grad_norm_(
+                            self._model.parameters(),
+                            max_norm=float(self._clip_grad_norm),
+                        )
+                    self._optimizer.step()
+                    self._optimizer.zero_grad(set_to_none=True)
+                    self._lr_scheduler.step()
+
+                    # Update the number of steps when the weights are updated
+                    self.global_step += 1
+
+                    loss_to_log = running_loss.item()
+                    pbar.update(1)
+                    pbar.set_description(
+                        f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
+                    )
+
+                    # Log per-step metrics
+                    if (
+                        self.global_step % self._log_every_n_steps == 0
+                        and self._is_rank_zero
+                    ):
+                        time_per_step = time.perf_counter() - t0
+                        log_dict = {
+                            "loss": loss_to_log,
+                            "lr": self._optimizer.param_groups[0]["lr"],
+                            "tokens_per_second_per_gpu": num_tokens / time_per_step,
+                        }
+                        if self._log_peak_memory_stats:
+                            log_dict.update(
+                                training.get_memory_stats(device=self._device)
+                            )
+                        if self._clip_grad_norm is not None:
+                            log_dict.update({"grad_norm": grad_norm})
+                        self._metric_logger.log_dict(
+                            log_dict,
+                            step=self.global_step,
+                        )
+
+                    # Reset running stats for the next step
+                    running_loss = 0
+                    num_tokens = 0
+                    t0 = time.perf_counter()
+
+                    # Stop tracking CUDA memory now that active steps are complete
+                    if (
+                        self._is_rank_zero
+                        and curr_epoch == 0
+                        and self.profiler_profile_memory
+                        and idx
+                        == self.profiler_wait_steps
+                        + self.profiler_warmup_steps
+                        + self.profiler_active_steps
+                    ):
+                        torch.cuda.memory._record_memory_history(enabled=None)
+
+                    # Step profiler
+                    # Note that this is called within gradient accumulation block, hence
+                    # will include multiple forward / backward passes if gradient accumulation > 1
+                    self._profiler.step()
+
+            self.epochs_run += 1
+            self.save_checkpoint(epoch=curr_epoch)
+
+        self._profiler.stop()
+
+    def cleanup(self) -> None:
+        if self._is_rank_zero:
+            self._metric_logger.close()
+        destroy_process_group()
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """
+    Entry point for the recipe.
+
+    Configurable parameters are read in the following order:
+        - Parameters specified in config (see available configs through ``tune ls``)
+        - Overwritten by arguments from the command-line
+    """
+    if not training.is_distributed():
+        raise RuntimeError(
+            "Distributed finetune recipe should be run via a distributed launcher."
+            "If using tune CLI, please specify --nnodes 1 and --nproc_per_node [num_gpus]"
+        )
+    if cfg.get("fsdp_cpu_offload", False):
+        # Utilize all available CPU cores for intra-op parallelism. This provides ~2x
+        # speed up when benchmarking fused AdamW on CPU
+        training.set_torch_num_threads()
+    init_process_group(backend="gloo" if cfg.device == "cpu" else "nccl")
+
+    config.log_config(recipe_name="LoRAFinetuneRecipeDistributed", cfg=cfg)
+
+    recipe = LoRAFinetuneRecipeDistributed(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
diff -ruN marc_original/third_party/torchtune/recipes/lora_finetune_single_device.py marc/third_party/torchtune/recipes/lora_finetune_single_device.py
--- marc_original/third_party/torchtune/recipes/lora_finetune_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/lora_finetune_single_device.py	2025-02-20 17:49:29.506024173 -0500
@@ -0,0 +1,801 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import contextlib
+import sys
+import time
+
+from functools import partial
+from typing import Any, Dict, Optional, Tuple, Union
+from warnings import warn
+
+import torch
+import torchtune.modules.common_utils as common_utils
+from omegaconf import DictConfig, ListConfig
+
+from torch import nn
+from torch.optim import Optimizer
+from torch.utils.data import DataLoader, DistributedSampler
+from torchtune import config, modules, training, utils
+from torchtune.config._utils import _get_component_from_path
+from torchtune.data import padded_collate_packed
+from torchtune.datasets import ConcatDataset
+from torchtune.modules.peft import (
+    get_adapter_params,
+    get_lora_module_names,
+    get_merged_lora_ckpt,
+    load_dora_magnitudes,
+    set_trainable_params,
+    validate_missing_and_unexpected_for_lora,
+)
+from torchtune.recipe_interfaces import FTRecipeInterface
+from torchtune.training import (
+    DummyProfiler,
+    NoOpManager,
+    OffloadActivations,
+    PROFILER_KEY,
+)
+from tqdm import tqdm
+
+log = utils.get_logger("DEBUG")
+
+
+class LoRAFinetuneRecipeSingleDevice(FTRecipeInterface):
+    """
+    LoRA finetuning recipe for dense transformer-based LLMs such as Llama2. This recipe is optimized
+    for single GPU training. Training on CPU is not supported.
+
+    Features:
+        - Activation Checkpointing. This can be controlled using the ``enable_activation_checkpointing``
+            flag. Activation checkpointing helps reduce the memory footprint since we no longer keep
+            activations in memory and instead recompute them during the backward pass. This is especially
+            helpful for larger batch sizes when you're memory constrained. But these savings in memory
+            come at the cost of training performance. In most cases training can slow-down quite a bit as
+            a result of this activation recomputation.
+
+        - Activation Offloading. This can be controlled using the ``enable_activation_offloading``
+            flag. Activation offloading is a technique similar to activations checkpointing that helps
+            reduce the memory footprint to prevent OOMs on CUDA and enable bigger batches. Where activations
+            checkpointing drops the activation in the forward to recompute it later in the backward,
+            activations offloading will drop the activation in the forward to the CPU and bring it
+            back during the backward pass. As always, there is a tradeoff--these savings in memory can
+            come at the cost of training performance and CPU resources. To recover some runtime cost,
+            we've added an option to enable offloading on a different stream to permit overlapping with
+            the computation. This option is currently only available on PyTorch nightly 2.5.0.dev20240907
+            or later and will be enabled by default if an acceptable torch version is found. Activation
+            offloading can be used in conjunction with activation checkpointing.
+
+        - Precision. Full fp32 and bf16 training are supported. Precision is controlled using the ``dtype``
+            flag. When ``dtype=bf16``, all activations, gradients and optimizer states are in bfloat16. In
+            most cases this should halve the memory footprint of full precision (fp32) training, without
+            loss in model quality (will depend on the model, training data and other settings). For
+            GPUs which do not support bfloat16, we fall back to fp32. Mixed precision training and fp16
+            precision are currently not supported.
+
+        - Gradient Accumulation. You can simulate larger batch sizes by accumulating gradients. This is
+            controlled using the ``gradient_accumulation_steps`` flag.
+
+                Total Batch Size = batch_size * gradient accumulation steps.
+
+            For example: with batch_size=1 and gradient_accumulation_steps=32 we get a total batch size of 32.
+
+            Gradient accumulation is especially useful when you are memory constrained. In this case,
+            accumulating gradients might give you better training speed than enabling activation
+            checkpointing.
+
+        - Lower precision optimizers. This recipe supports lower-precision optimizers from the bitsandbytes
+            library (https://huggingface.co/docs/bitsandbytes/main/en/index). We've tested the recipe with
+            8-bit AdamW and Paged AdamW.
+
+        - Checkpointing. Model weights are checkpointed both at the end of each epoch and at the end of
+            training. Currently we checkpoint both the adapter weights (trainable params only) and the
+            complete merged weights (adapter weights added back to the base model). For more details
+            please take a look at our LoRA tutorial
+            (https://pytorch.org/torchtune/main/tutorials/lora_finetune.html).
+
+            Optimizer State and recipe state (seed, total_epochs, number of epochs run etc) are
+            only saved at the end of a given epoch and used in case of resuming training. Resuming
+            training is controlled by the ``resume_from_checkpoint`` flag. Mid-epoch checkpointing is
+            currently not supported.
+
+            For more details on the checkpointer, please take a look at
+            our checkpointer deepdive (https://pytorch.org/torchtune/main/tutorials/checkpointer.html).
+
+        - Logging. Terminal, Disk, WandB and TensorBoard are all supported.
+
+        - Gradient Clipping. Gradient clipping is supported using the ``clip_grad_norm`` flag. By default,
+            ``clip_grad_norm`` is set to ``None``. If you only want to log the grad norm, you can set
+            ``clip_grad_norm='inf'``.
+
+    For a full list of example configs for this recipe, run ``tune ls`` on the command line. Each config
+    has example commands for how to kick-off training.
+
+    Args:
+        cfg (DictConfig): OmegaConf object parsed from yaml file
+
+    Raises:
+        ValueError: If ``dtype`` is set to fp16.
+        RuntimeError: If ``dtype`` is set to bf16 and the hardware does not support bf16.
+        RuntimeError: If ``enable_activation_offloading`` is True and device is not CUDA.
+        RuntimeError: If ``left_pad_sequence`` is set as the data collator
+
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+
+        self._device = utils.get_device(device=cfg.device)
+        # Reduced precision logic
+        self._dtype = training.get_dtype(cfg.dtype, device=self._device)
+        # fp16 precision is explicitly disabled as it is not supported in this
+        # recipe (for example, no gradient scaling).
+        if self._dtype == torch.float16:
+            raise ValueError(
+                "fp16 precision is not supported in this recipe. Please use fp32 or bf16."
+            )
+
+        # logging attributes
+        self._output_dir = cfg.output_dir
+        self._log_every_n_steps = cfg.get("log_every_n_steps", 1)
+        self._log_peak_memory_stats = cfg.get("log_peak_memory_stats", False)
+
+        # These are public properties which are updated by the checkpoint loader
+        # when ``resume_from_checkpoint`` is `True` or validated in tests
+        self.seed = training.set_seed(seed=cfg.seed)
+        self.epochs_run = 0
+        self.total_epochs = cfg.epochs
+        self.max_steps_per_epoch = cfg.max_steps_per_epoch
+        self.global_step = 0
+        self._resume_from_checkpoint = cfg.resume_from_checkpoint
+        self._save_adapter_weights_only = cfg.get("save_adapter_weights_only", False)
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+        self._clip_grad_norm = cfg.get("clip_grad_norm", None)
+        self._enable_activation_offloading = cfg.get(
+            "enable_activation_offloading", False
+        )
+        if self._enable_activation_offloading and self._device.type != "cuda":
+            raise RuntimeError(
+                "enable_activation_offloading should only be enabled for training on CUDA"
+            )
+
+    def load_checkpoint(self, cfg_checkpointer: DictConfig) -> Dict[str, Any]:
+        """
+        Extract the checkpoint state from file and validate. This includes the
+        base model weights. If resume_from_checkpoint is True, this also includes
+        the adapter weights and recipe state
+        """
+        self._checkpointer = config.instantiate(
+            cfg_checkpointer,
+            resume_from_checkpoint=self._resume_from_checkpoint,
+        )
+        checkpoint_dict = self._checkpointer.load_checkpoint()
+
+        if self._resume_from_checkpoint:
+            if training.ADAPTER_KEY not in checkpoint_dict:
+                raise ValueError(
+                    "Adapter weights not found. Please ensure a valid adapter checkpoint is provided."
+                )
+            # _update_recipe_state will throw an exception if the recipe state is not corrctly loaded
+            # no need to check here
+            self._update_recipe_state(checkpoint_dict)
+        return checkpoint_dict
+
+    def _update_recipe_state(self, ckpt_dict: Dict[str, Any]) -> None:
+        """
+        Updates the recipe state from checkpoint.
+        """
+        try:
+            self.epochs_run = ckpt_dict[training.EPOCHS_KEY]
+
+            # on mismatch, warn the user and prevent the override
+            if self.seed != ckpt_dict[training.SEED_KEY]:
+                warn(
+                    message=(
+                        "Config value for seed does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.SEED_KEY]}"
+                    )
+                )
+                self.seed = ckpt_dict[training.SEED_KEY]
+            if self.max_steps_per_epoch != ckpt_dict[training.MAX_STEPS_KEY]:
+                warn(
+                    message=(
+                        "Config value for max_steps_per_epoch does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.MAX_STEPS_KEY]}"
+                    )
+                )
+                self.max_steps_per_epoch = ckpt_dict[training.MAX_STEPS_KEY]
+
+            # on mismatch, warn the user but allow the override
+            if self.total_epochs != ckpt_dict[training.TOTAL_EPOCHS_KEY]:
+                warn(
+                    message=(
+                        "Config value for total_epochs does not match the checkpoint value, "
+                        f"using the config value: {self.total_epochs}"
+                    )
+                )
+
+        except KeyError as e:
+            raise KeyError(
+                "Checkpoint does not contain the required keys needed for updating recipe state. "
+                "Are you sure you passed in the right recipe checkpoint?"
+            ) from e
+
+    def setup(self, cfg: DictConfig, model=None, adapter=None) -> None:
+        """
+        Setup the recipe state. This includes recipe state (if resume_from_checkpoint is True),
+        model, tokenizer, loss, optimizer, learning rate scheduler, sampler, and dataloader.
+        """
+        self._metric_logger = config.instantiate(cfg.metric_logger)
+
+        # log config with parameter override
+        self._metric_logger.log_config(cfg)
+
+        self._compile = cfg.compile
+        checkpoint_dict = self.load_checkpoint(cfg_checkpointer=cfg.checkpointer)
+
+        # hack to toggle to the low cpu ram version of the reparametrize_as_dtype
+        # hook based on the config.
+        common_utils._use_low_cpu_ram = cfg.get("low_cpu_ram", False)
+
+        if model is None:
+            # set up model
+            self._model = self._setup_model(
+                cfg_model=cfg.model,
+                enable_activation_checkpointing=cfg.enable_activation_checkpointing,
+                enable_activation_offloading=self._enable_activation_offloading,
+                compile_model=cfg.compile,
+                base_model_state_dict=checkpoint_dict[training.MODEL_KEY],
+                lora_weights_state_dict=(
+                    checkpoint_dict[training.ADAPTER_KEY]
+                    if self._resume_from_checkpoint
+                    else None
+                ),
+            )
+
+            self._tokenizer = config.instantiate(cfg.tokenizer)
+            log.info("Tokenizer is initialized from file.")
+        else:
+            # set grad to None for all parameters
+            for param in model.parameters():
+                param.grad = None
+
+            lora_parameters = adapter #get_adapter_params(model)
+            for name, param in model.named_parameters():
+                if name in lora_parameters:
+                    # copy
+                    param.data.copy_(lora_parameters[name].data)
+                    param.grad = None
+
+
+            self._model = model
+
+            checkpoint_dict = {}
+
+
+        self._optimizer = self._setup_optimizer(
+            cfg_optimizer=cfg.optimizer,
+            opt_state_dict=(
+                checkpoint_dict[training.OPT_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+
+        # initialize loss
+        self._loss_fn = config.instantiate(cfg.loss)
+        if self._compile:
+            self._loss_fn = training.compile_loss(self._loss_fn)
+
+        if self._loss_fn.__class__.__name__ == "CEWithChunkedOutputLoss":
+            # set num_output_chunks for model
+            self._model.set_num_output_chunks(self._loss_fn.num_output_chunks)
+
+        log.info("Loss is initialized.")
+
+        # Dataloader depends on the tokenizer and loss_fn and should be
+        # setup after all of these are setup
+        collate_name = cfg.get("collate_fn", "torchtune.data.padded_collate_sft")
+        self._sampler, self._dataloader = self._setup_data(
+            cfg_dataset=cfg.dataset,
+            shuffle=cfg.shuffle,
+            batch_size=cfg.batch_size,
+            collate_fn=collate_name,
+        )
+
+        # Finally update the recipe state which can only be correctly set after all of the
+        # other components have been initialized and updated.
+
+        # Number of training steps in each epoch depends on the number of batches produced
+        # by the dataloader and the max_steps_per_epoch param set by the user and is used
+        # for logging and tracking training state. This should be computed after the dataloader
+        # has been setup
+        self._steps_per_epoch = (
+            len(self._dataloader) // self._gradient_accumulation_steps
+        )
+        if (
+            self.max_steps_per_epoch is not None
+            and self.max_steps_per_epoch < self._steps_per_epoch
+        ):
+            self._steps_per_epoch = self.max_steps_per_epoch
+            self.global_step = self.epochs_run * self._steps_per_epoch
+
+        # Learning rate scheduler can only be set up after number of steps
+        # has been computed
+        self._lr_scheduler = self._setup_lr_scheduler(
+            cfg_lr_scheduler=cfg.lr_scheduler,
+            num_training_steps=self.total_epochs * self._steps_per_epoch,
+            last_epoch=self.global_step - 1,
+        )
+
+        # Set up profiler, returns DummyProfiler (nullcontext object with no-op `step` method)
+        # if cfg is missing profiler key or if `cfg.profiler.enabled = False
+        self._profiler = self._setup_profiler(cfg.get(PROFILER_KEY, None))
+
+        # Used to ignore labels for loss computation
+        self.ignore_labels_cache = torch.full(
+            (cfg.batch_size, 1), self._loss_fn.ignore_index, device=self._device
+        )
+
+    def _setup_profiler(
+        self, cfg_profiler: Optional[DictConfig] = None
+    ) -> Union[torch.profiler.profile, DummyProfiler]:
+        """
+        Parses the `profiler` section of top-level `cfg` and sets up profiler
+
+        Args:
+            cfg_profiler (Optional[DictConfig]): ``profiler`` section of the top-level ``cfg`` (the main config passed to
+                `recipe.main`). Default None.
+
+        Returns:
+            profiler: Union[torch.profiler.profile, DummyProfiler] - DummyProfiler is a nullcontext with no-op methods
+            for `start`, `stop`, and `step` that can be used in place of `torch.profiler.profile` if profiler is not enabled such
+            that the instrumented training loop does not need to be changed profiling is disabled.
+
+        The profiler config can be provided in configs under the `profiler` key with the following layout:
+
+        .. code-block:: yaml
+            profiler:
+                enabled: bool
+
+                #Output directory of trace artifacts
+                output_dir: str
+
+            #`torch.profiler.ProfilerActivity` types to trace
+            cpu: bool
+            cuda: bool
+
+                #Trace options
+                profile_memory: bool
+                with_stack: bool
+                record_shapes: bool
+                with_flops: bool
+
+            # `torch.profiler.schedule` options:
+            # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+            wait_steps: int
+            warmup_steps: int
+            active_steps: int
+            num_cycles: int
+        """
+
+        # Missing profiler section in config, assume disabled
+        if cfg_profiler is None:
+            cfg_profiler = DictConfig({"enabled": False})
+
+        # Check that component is included and set correctly
+        if cfg_profiler.get("_component_", None) is None:
+            cfg_profiler["_component_"] = "torchtune.training.setup_torch_profiler"
+        else:
+            assert (
+                cfg_profiler.get("_component_")
+                == "torchtune.training.setup_torch_profiler"
+            ), "Only torch profiler supported currently: component must be `torchtune.training.setup_torch_profiler`"
+
+        profiler, profiler_cfg = config.instantiate(cfg_profiler)
+
+        log.info(f" Profiler config after instantiation: {profiler_cfg}")
+
+        self.profiler_profile_memory = profiler_cfg.get("profile_memory", False)
+        if profiler_cfg["enabled"]:
+            self.profiler_wait_steps = profiler_cfg["wait_steps"]
+            self.profiler_warmup_steps = profiler_cfg["warmup_steps"]
+            self.profiler_active_steps = profiler_cfg["active_steps"]
+
+        return profiler
+
+    def _setup_model(
+        self,
+        cfg_model: DictConfig,
+        enable_activation_checkpointing: bool,
+        enable_activation_offloading: bool,
+        compile_model: bool,
+        base_model_state_dict: Dict[str, Any],
+        lora_weights_state_dict: Optional[Dict[str, Any]] = None,
+    ) -> nn.Module:
+        with training.set_default_dtype(self._dtype), self._device:
+            model = config.instantiate(cfg_model)
+
+        self._lora_rank = cfg_model.lora_rank
+        self._lora_alpha = cfg_model.lora_alpha
+        self._lora_attn_modules = list(cfg_model.lora_attn_modules)
+        self._apply_lora_to_mlp = cfg_model.apply_lora_to_mlp
+        self._apply_lora_to_output = getattr(cfg_model, "apply_lora_to_output", False)
+        self.adapter_params = get_adapter_params(model)
+        self._is_dora = any(["magnitude" in k for k in self.adapter_params.keys()])
+        set_trainable_params(model, self.adapter_params)
+
+        if compile_model:
+            training.compile_model(model)
+
+        if enable_activation_checkpointing:
+            training.set_activation_checkpointing(
+                model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        base_missing, base_unexpected = model.load_state_dict(
+            base_model_state_dict, strict=False
+        )
+        # This is for any adapters that need to be initialized after base weights
+        # have been loaded (e.g. DoRA).
+        if self._is_dora:
+            load_dora_magnitudes(model)
+        if lora_weights_state_dict:
+            lora_missing, lora_unexpected = model.load_state_dict(
+                lora_weights_state_dict, strict=False
+            )
+        else:
+            lora_missing, lora_unexpected = None, None
+        validate_missing_and_unexpected_for_lora(
+            lora_attn_modules=self._lora_attn_modules,
+            apply_lora_to_mlp=self._apply_lora_to_mlp,
+            apply_lora_to_output=self._apply_lora_to_output,
+            base_missing=base_missing,
+            base_unexpected=base_unexpected,
+            lora_missing=lora_missing,
+            lora_unexpected=lora_unexpected,
+        )
+        # Validate model adapter params were loaded in with the expected dtype
+        # TODO (rohan-varma): Further validation to ensure the appropriate base params
+        # are NF4 vs bf16 based on the quantization config.
+        training.validate_expected_param_dtype(
+            self.adapter_params.items(), dtype=self._dtype
+        )
+
+        self.activations_handling_ctx = contextlib.nullcontext()
+        if enable_activation_offloading:
+            self.activations_handling_ctx = OffloadActivations()
+
+            # Below is our hack to disable offloading the last output Linear in every
+            # step, as the cost for offloading the activation and then soon after bringing
+            # it back is expensive. Moreover, due to heuristics in our streaming API,
+            # we actually use more memory if we offload it as it interferes with chunkedCE.
+            if hasattr(model, "output") and isinstance(model.output, nn.Module):
+                noop_ctx = NoOpManager()
+                model.output.register_forward_pre_hook(
+                    lambda *args: noop_ctx.__enter__()
+                )
+                model.output.register_forward_hook(
+                    lambda *args: noop_ctx.__exit__(), always_call=True
+                )
+
+        log.info(f"Model is initialized with precision {self._dtype}.")
+
+        if self._device.type == "cuda":
+            memory_stats = training.get_memory_stats(device=self._device)
+            training.log_memory_stats(memory_stats)
+        return model
+
+    def _setup_optimizer(
+        self, cfg_optimizer: DictConfig, opt_state_dict: Optional[Dict[str, Any]] = None
+    ) -> Optimizer:
+        optimizer = config.instantiate(cfg_optimizer, self._model.parameters())
+        if opt_state_dict:
+            optimizer.load_state_dict(opt_state_dict)
+
+        log.info("Optimizer and loss are initialized.")
+        return optimizer
+
+    def _setup_lr_scheduler(
+        self,
+        cfg_lr_scheduler: DictConfig,
+        num_training_steps: int,
+        last_epoch: int,
+    ) -> Optimizer:
+        lr_scheduler = config.instantiate(
+            cfg_lr_scheduler,
+            self._optimizer,
+            num_training_steps=num_training_steps,
+            last_epoch=last_epoch,
+        )
+
+        log.info("Learning rate scheduler is initialized.")
+        return lr_scheduler
+
+    def _setup_data(
+        self,
+        cfg_dataset: DictConfig,
+        shuffle: bool,
+        batch_size: int,
+        collate_fn: str,
+    ) -> Tuple[DistributedSampler, DataLoader]:
+        """
+        All data related setup happens here. Currently this recipe only supports
+        Map-style Datasets which fit into memory and an option for random shuffling.
+        Samplers, iterable datasets, and streaming datasets are not supported.
+        """
+        if isinstance(cfg_dataset, ListConfig):
+            datasets = [
+                config.instantiate(single_cfg_dataset, self._tokenizer)
+                for single_cfg_dataset in cfg_dataset
+            ]
+            ds = ConcatDataset(datasets=datasets)
+            packed = False
+        else:
+            ds = config.instantiate(cfg_dataset, self._tokenizer)
+            packed = cfg_dataset.get("packed", False)
+
+        # Instantiate collate_fn
+        if "left_pad_sequence" in collate_fn:
+            raise RuntimeError("left_pad_sequence collator is only for inference.")
+        collate_fn = _get_component_from_path(collate_fn)
+
+        sampler = DistributedSampler(
+            ds,
+            num_replicas=1,
+            rank=0,
+            shuffle=shuffle,
+            seed=0,
+        )
+        dataloader = DataLoader(
+            dataset=ds,
+            sampler=sampler,
+            batch_size=batch_size,
+            # dropping last avoids shape issues with compile + flex attention
+            drop_last=True,
+            collate_fn=(
+                partial(
+                    collate_fn,
+                    padding_idx=self._tokenizer.pad_id,
+                    ignore_idx=self._loss_fn.ignore_index,
+                )
+                if not packed
+                else padded_collate_packed
+            ),
+        )
+
+        log.info("Dataset and Sampler are initialized.")
+
+        return sampler, dataloader
+
+    def save_checkpoint(self, epoch: int) -> None:
+        """
+        Checkpoint the state of the recipe. The constructed checkpoint state dict
+        contains the following information:
+        - Merged weights with key MODEL_KEY
+        - Adapter weights with key ADAPTER_KEY
+        - Relevant recipe state if training is not complete
+        - If the `self._save_adapter_weights_only` option is True, the checkpointer will save only the adapter weights
+
+        To correctly resume from training, the adapter weights and recipe state must be provided along with the base model weights.
+        """
+        ckpt_dict = {}
+
+        intermediate_checkpoint = epoch + 1 < self.total_epochs
+        # if training is in-progress, checkpoint the optimizer state as well
+        if intermediate_checkpoint:
+            ckpt_dict.update(
+                {
+                    training.OPT_KEY: self._optimizer.state_dict(),
+                    training.SEED_KEY: self.seed,
+                    training.EPOCHS_KEY: self.epochs_run,
+                    training.TOTAL_EPOCHS_KEY: self.total_epochs,
+                    training.MAX_STEPS_KEY: self.max_steps_per_epoch,
+                }
+            )
+
+        adapter_state_dict = {k: v.cpu() for k, v in self.adapter_params.items()}
+        ckpt_dict.update({training.ADAPTER_KEY: adapter_state_dict})
+
+        if not self._save_adapter_weights_only:
+            # Construct the full state dict with LoRA weights merged into base LLM weights
+
+            # Move to CPU to avoid a copy on GPU
+            state_dict = {k: v.cpu() for k, v in self._model.state_dict().items()}
+
+            merged_state_dict = get_merged_lora_ckpt(
+                state_dict,
+                rank=self._lora_rank,
+                alpha=self._lora_alpha,
+            )
+
+            ckpt_dict.update({training.MODEL_KEY: merged_state_dict})
+
+        adapter_config = {
+            "r": self._lora_rank,
+            "lora_alpha": self._lora_alpha,
+            "target_modules": get_lora_module_names(
+                self._lora_attn_modules,
+                self._apply_lora_to_mlp,
+                self._apply_lora_to_output,
+            ),
+            "peft_type": "LORA",
+        }
+        ckpt_dict.update({training.ADAPTER_CONFIG: adapter_config})
+
+        self._checkpointer.save_checkpoint(
+            ckpt_dict,
+            epoch=epoch,
+            intermediate_checkpoint=intermediate_checkpoint,
+            adapter_only=self._save_adapter_weights_only,
+        )
+
+    def _loss_step(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
+        # Shape [b, s], needed for the loss not the model
+        labels = batch.pop("labels")
+
+        # run model
+        with self.activations_handling_ctx:
+            logits = self._model(**batch)
+
+        # Shift labels to compute loss
+        # equivalent to doing labels[..., 1:] and logits[..., :-1, :]
+        # But this way we dont need to slice the logits. We just add an ignore index to labels.
+        labels = torch.hstack(
+            (labels[..., 1:], self.ignore_labels_cache[: labels.shape[0]])
+        )
+        if not isinstance(logits, list):
+            labels = labels.reshape(-1)
+            logits = logits.reshape(-1, logits.size(-1))
+
+        # Compute loss
+        loss = self._loss_fn(logits, labels)
+
+        # free logits otherwise it peaks backward memory
+        del logits
+
+        return loss
+
+    def train(self) -> None:
+        """
+        The core training loop.
+        """
+
+        if self._compile:
+            log.info(
+                "NOTE: torch.compile is enabled and model is compiled in first forward. Expect a relatively slow first iteration."
+            )
+
+        # Initialize tokens count and running loss (for grad accumulation)
+        t0 = time.perf_counter()
+        running_loss = 0
+        num_tokens = 0
+        self._optimizer.zero_grad(set_to_none=True)
+
+        with self._profiler as prof:
+            # self.epochs_run should be non-zero when we're resuming from a checkpoint
+            for curr_epoch in range(self.epochs_run, self.total_epochs):
+                # Update the sampler to ensure data is correctly shuffled across epochs
+                # in case shuffle is True
+                self._sampler.set_epoch(curr_epoch)
+
+                pbar = tqdm(total=self._steps_per_epoch)
+                for idx, batch in enumerate(self._dataloader):
+                    if (
+                        self.max_steps_per_epoch is not None
+                        and (idx // self._gradient_accumulation_steps)
+                        == self.max_steps_per_epoch
+                    ):
+                        break
+
+                    # Start tracking CUDA memory for active steps for just the first epoch
+                    if (
+                        curr_epoch == 0
+                        and self.profiler_profile_memory
+                        and idx == self.profiler_wait_steps + self.profiler_warmup_steps
+                    ):
+                        torch.cuda.memory._record_memory_history()
+
+                    utils.batch_to_device(batch, self._device)
+                    num_tokens += batch["tokens"].numel()
+
+                    loss = self._loss_step(batch)
+                    loss = loss / self._gradient_accumulation_steps
+                    running_loss += loss
+                    loss.backward()
+
+                    # Step with optimizer
+                    if (idx + 1) % self._gradient_accumulation_steps == 0:
+                        if self._clip_grad_norm is not None:
+                            grad_norm = torch.nn.utils.clip_grad_norm_(
+                                self._model.parameters(),
+                                max_norm=float(self._clip_grad_norm),
+                            )
+                        self._optimizer.step()
+                        self._optimizer.zero_grad(set_to_none=True)
+                        self._lr_scheduler.step()
+                        # Update the number of steps when the weights are updated
+                        self.global_step += 1
+
+                        loss_to_log = running_loss.item()
+                        pbar.update(1)
+                        pbar.set_description(
+                            f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
+                        )
+
+                        # Log per-step metrics
+                        if self.global_step % self._log_every_n_steps == 0:
+                            time_per_step = time.perf_counter() - t0
+                            log_dict = {
+                                "loss": loss_to_log,
+                                "lr": self._optimizer.param_groups[0]["lr"],
+                                "tokens_per_second_per_gpu": num_tokens / time_per_step,
+                            }
+                            if (
+                                self._device.type == "cuda"
+                                and self._log_peak_memory_stats
+                            ):
+                                log_dict.update(
+                                    training.get_memory_stats(device=self._device)
+                                )
+                            if self._clip_grad_norm is not None:
+                                log_dict.update({"grad_norm": grad_norm})
+                            self._metric_logger.log_dict(
+                                log_dict,
+                                step=self.global_step,
+                            )
+
+                        # Reset running stats for the next step
+                        running_loss = 0
+                        num_tokens = 0
+                        t0 = time.perf_counter()
+
+                    # Stop tracking CUDA memory now that active steps are complete
+                    if (
+                        curr_epoch == 0
+                        and self.profiler_profile_memory
+                        and idx
+                        == self.profiler_wait_steps
+                        + self.profiler_warmup_steps
+                        + self.profiler_active_steps
+                    ):
+                        torch.cuda.memory._record_memory_history(enabled=None)
+
+                    # Step the profiler
+                    # Note we are stepping each batch, which might not include optimizer step in the trace
+                    # if the schedule cycle doesn't align with gradient accumulation.
+                    prof.step()
+
+                self.epochs_run += 1
+                # start_save_checkpoint = time.perf_counter()
+                # log.info("Starting checkpoint save...")
+                # self.save_checkpoint(epoch=curr_epoch)
+                # log.info(
+                #     "Checkpoint saved in {:.2f} seconds.".format(
+                #         time.perf_counter() - start_save_checkpoint
+                #     )
+                # )
+
+    def cleanup(self) -> None:
+        self._metric_logger.close()
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """
+    Entry point for the recipe.
+
+    Configurable parameters are read in the following order:
+        - Parameters specified in config (see available configs through ``tune ls``)
+        - Overwritten by arguments from the command-line
+    """
+    config.log_config(recipe_name="LoRAFinetuneRecipeSingleDevice", cfg=cfg)
+    recipe = LoRAFinetuneRecipeSingleDevice(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
diff -ruN marc_original/third_party/torchtune/recipes/ppo_full_finetune_single_device.py marc/third_party/torchtune/recipes/ppo_full_finetune_single_device.py
--- marc_original/third_party/torchtune/recipes/ppo_full_finetune_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/ppo_full_finetune_single_device.py	2025-02-20 17:49:29.510024180 -0500
@@ -0,0 +1,1088 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+import os
+import sys
+from functools import partial
+from itertools import chain
+from typing import Any, Dict, List, Optional, Tuple
+from warnings import warn
+
+import torch
+from omegaconf import DictConfig, ListConfig
+from torch import nn
+from torch.optim import Optimizer
+from torch.utils.data import DataLoader, DistributedSampler
+from torchtune import config, generation, modules, rlhf, training, utils
+from torchtune.data import padded_collate
+from torchtune.datasets import ConcatDataset
+from torchtune.recipe_interfaces import FTRecipeInterface
+from torchtune.rlhf import PPOStats, Trajectory
+from tqdm import tqdm
+
+log = utils.get_logger("DEBUG")
+
+
+class PPOFullFinetuneRecipeSingleDevice(FTRecipeInterface):
+    """
+    Full finetuning recipe for RLHF with PPO for dense transformer-based LLMs such as LLama2. This recipe is optimized
+    for single GPU training. Training on CPU is not supported.
+
+    This implementation is based on `Learning to summarize from human feedback <https://arxiv.org/abs/2009.01325`_ and
+    `Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback <https://arxiv.org/abs/2204.05862`_.
+
+    Features:
+        - Activation Checkpointing. This can be controlled using the ``activation_checkpointing``
+            flag. Activation checkpointing helps reduce the memory footprint since we no longer keep
+            activations in memory and instead recompute them during the backward pass. This is especially
+            helpful for larger batch sizes when you're memory constrained. But these savings in memory
+            come at the cost of training performance. In most cases training can slow-down quite a bit as
+            a result of this activation recomputation.
+
+        - Precision. Full fp32 and bf16 training are supported. Precision is controlled using the ``dtype``
+            flag. When ``dtype=bf16``, all activations, gradients and optimizer states are in bfloat16. In
+            most cases this should halve the memory footprint of full precision (fp32) training, without
+            loss in model quality (will depend on the model, training data and other settings). For
+            GPUs which do not support bfloat16, we fall back to fp32. Mixed precision training and fp16
+            precision are currently not supported.
+
+        - Adjusting batch sizes when memory constrained. This recipe uses three different batch sizes:
+            - ``batch_size`` controls the total number of samples which are sampled from the dataset for a single trajectory.
+            - ``forward_batch_size`` controls the mini-batch size for trajectory generation. Since gradients are disabled
+                during trajectory generation, memory consumption is lower and this can be higher than ``ppo_batch_size``.
+            - ``ppo_batch_size`` controls the number of samples used for a single optimization step during PPO optimization.
+                Since we're optimizing two models at once, adjusting this parameter can have a big impact during training.
+
+        - Gradient Accumulation. You can simulate larger ``ppo_batch_size`` sizes by accumulating gradients. This is
+            controlled using the ``gradient_accumulation_steps`` flag.
+
+            For example: with ``ppo_batch_size``=32 and ``gradient_accumulation_steps``=16, each backward pass during
+            PPO optimization uses a 'micro batch size' of 2.
+
+            Gradient accumulation is especially useful when you are memory constrained. In this case,
+            accumulating gradients might give you better training speed than enabling activation
+            checkpointing.
+
+        - Optimizer in Backward. Fusing the optimizer step into the backward pass helps reduce the memory
+            footprint associated with gradients. This can be especially helpful when you are memory
+            constrained. Note that users can only use ONE of gradient accumulation or optimizer in backward.
+            These features currently do not work together. For more details on optimizer in backward, please
+            see this tutorial: https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html
+
+            This paramater can provide significant performance gains, since there the number of optimization steps
+            scales with ``ppo_epochs`` and ``batch_size``. Depending on the maximum sequence length sampled from the dataset,
+            we've found that setting ``ppo_batch_size`` to the highest you can fit in memory, and `optimizer_in_bwd=True` to
+            provide significant memory savings.
+
+        - Lower precision optimizers. This recipe supports lower-precision optimizers from the bitsandbytes
+            library (https://huggingface.co/docs/bitsandbytes/main/en/index). We've tested the recipe with
+            8-bit AdamW and Paged AdamW. These optimizers are especially helpful when you are memory constrained
+            since they help reduce the memory footprint associated with the optimizer states.
+
+        - Checkpointing. Model weights are checkpointed both at the end of each epoch, and at the end of
+            training. Optimizer State and recipe state (seed, total_epochs, number of epochs run etc) are
+            only saved at the end of a given epoch and used in case of resuming training.
+
+            Resuming training is controlled by the ``resume_from_checkpoint`` flag. Mid-epoch checkpointing is
+            currently not supported.
+
+            For more details on the checkpointer, please take a look at
+            our checkpointer deepdive (https://pytorch.org/torchtune/main/deep_dives/checkpointer.html).
+
+        - Logging. Terminal, Disk, WandB and TensorBoard are all supported.
+
+    Args:
+        cfg (DictConfig): OmegaConf object parsed from yaml file
+
+    Raises:
+        RuntimeError: If ``dtype`` is set to fp16.
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+
+        self._device = utils.get_device(device=cfg.device)
+        self._dtype = training.get_dtype(cfg.dtype, device=self._device)
+
+        # Disable for fp16, as we haven't validated "full" fp16 with this recipe, nor
+        # enabled necessary features such as gradient scaling.
+        if self._dtype == torch.float16:
+            raise RuntimeError(
+                "full fp16 training is not supported with this recipe. Please use bf16 or fp32 instead."
+            )
+
+        # logging attributes
+        self._output_dir = cfg.output_dir
+        self._log_every_n_steps = cfg.get("log_every_n_steps", 1)
+        self._log_peak_memory_stats = cfg.get("log_peak_memory_stats", False)
+
+        # These are public properties which are updated by the checkpoint loader
+        # when ``resume_from_checkpoint`` is `True` or validated in tests
+        self.seed = training.set_seed(seed=cfg.seed)
+        # manually setting up a generator for the recipe
+        self._rng = torch.Generator(self._device).manual_seed(self.seed)
+        self._total_steps = 0
+        self._steps_run = 0
+        self._total_epochs = 0
+        self._epochs_run = 0
+        self.global_step = 0
+
+        # Training cfg
+        self._resume_from_checkpoint = cfg.resume_from_checkpoint
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+
+    def setup(self, cfg: DictConfig) -> None:
+        """
+        Sets up the recipe state correctly. This includes setting recipe attributes based
+        on the ``resume_from_checkpoint`` flag.
+        """
+        self._metric_logger = config.instantiate(cfg.metric_logger)
+
+        # log config with parameter override
+        self._metric_logger.log_config(cfg)
+
+        # setup checkpointers
+        (
+            self._policy_checkpointer,
+            ref_policy_checkpointer,
+            self._value_checkpointer,
+            reward_checkpointer,
+        ) = self._setup_checkpointers(
+            cfg.checkpointer,
+            cfg.ref_policy_checkpointer,
+            cfg.value_checkpointer,
+            cfg.reward_checkpointer,
+        )
+
+        # load policy checkpoints
+        policy_model_checkpoint_dict = self._policy_checkpointer.load_checkpoint()
+        ref_policy_state_dict = ref_policy_checkpointer.load_checkpoint()
+
+        # load reward and value model checkpoints
+        value_model_checkpoint_dict = self._value_checkpointer.load_checkpoint()
+        reward_model_state_dict = reward_checkpointer.load_checkpoint()
+
+        # update recipe state
+        # ``_setup_model`` handles initialization and loading the state dict. This method
+        # should be called before ``_setup_optimizer`` since transforming the optimizer
+        # state dict requires the model
+        self._model_compile = cfg.compile
+        self._optimizer_in_bwd = cfg.optimizer_in_bwd
+        (
+            self._policy_model,
+            self._value_model,
+            self._reward_model,
+            self._ref_policy_model,
+        ) = self._setup_models(
+            cfg_model=cfg.policy_model,
+            cfg_reward_value_model=cfg.reward_and_value_model,
+            enable_activation_checkpointing=cfg.enable_activation_checkpointing,
+            compile_model=self._model_compile,
+            policy_state_dict=policy_model_checkpoint_dict[training.MODEL_KEY],
+            ref_policy_state_dict=ref_policy_state_dict[training.MODEL_KEY],
+            value_model_state_dict=value_model_checkpoint_dict[training.MODEL_KEY],
+            reward_model_state_dict=reward_model_state_dict[training.MODEL_KEY],
+        )
+
+        # setup tokenizer
+        self._tokenizer = config.instantiate(cfg.tokenizer)
+        log.info("Tokenizer is initialized from file.")
+
+        # _setup_optimizer should take in ckpt_dict only if training is resumed from
+        # checkpoint. Transforming the opt state dict is handled by this method
+        self._optimizer = self._setup_optimizer(
+            cfg_optimizer=cfg.optimizer,
+            optimizer_in_bwd=cfg.optimizer_in_bwd,
+            opt_state_dict=(
+                policy_model_checkpoint_dict[training.OPT_KEY]
+                if self._resume_from_checkpoint
+                else None
+            ),
+        )
+
+        self._loss_fn = config.instantiate(cfg.loss)
+        log.info("Loss is initialized.")
+
+        # sampler and dataloader depends on the tokenizer and should be set
+        # setup afterit is initialized
+        self._sampler, self._dataloader = self._setup_data(
+            cfg_dataset=cfg.dataset,
+            shuffle=cfg.shuffle,
+            batch_size=cfg.batch_size,
+        )
+
+        self._setup_training_parameters(cfg)
+        self._setup_training_hyperparameters(cfg)
+
+        if self._resume_from_checkpoint:
+            self._update_recipe_state(policy_model_checkpoint_dict)
+
+        # one "step" is a single gradient update update over a minibatch of trajectories
+        self.global_step = (
+            self._steps_run
+            * self._ppo_epochs
+            * (self.batch_size // self._ppo_batch_size)
+        )
+
+    def _setup_training_hyperparameters(self, cfg) -> None:
+        """
+        Sets up the training hyperparameters for the recipe. This includes the GAE hyperparameters,
+        generation hyperparameters, reward masking hyperparameters, and stop token ids.
+        """
+
+        self._kl_coeff = cfg.kl_coeff
+        # GAE hyperparameters
+        self._gamma = cfg.gamma
+        self._lmbda = cfg.lmbda
+        self._whiten_rewards = cfg.whiten_rewards
+
+        # trajectory generation args
+        self._temperature = cfg.temperature
+        self._top_k = cfg.top_k
+        self._max_generated_tokens = cfg.max_generated_tokens
+
+        # reward masking args
+        self._min_response_length = cfg.min_response_length
+        self._penalise_no_eos = cfg.penalise_no_eos
+        self._reward_penalty = cfg.reward_penalty
+
+        # lots of hand holding for stop tokens
+        if cfg.get("stop_token_ids", False):
+            stop_token_ids = cfg.stop_token_ids
+            if self._tokenizer.eos_id not in stop_token_ids:
+                warn(
+                    f"tokenizer eos_id ({self._tokenizer.eos_id}) is not in stop_token_ids ({stop_token_ids})."
+                    "This may lead to unexpected behaviour."
+                )
+        else:
+            if not hasattr(self._tokenizer.stop_tokens):
+                warn(
+                    "No stop tokens defined in tokenizer, and no stop_token_ids provided. This may lead to unexpected behaviour."
+                )
+                stop_token_ids = []
+            else:
+                stop_token_ids = self._tokenizer.stop_tokens
+        self._stop_token_ids = torch.tensor(stop_token_ids, device=self._device)
+
+    def _setup_training_parameters(self, cfg: DictConfig) -> None:
+        """
+        Validates and sets up parameters for used during training and for tracking training state,
+        batch sizes for model forward passes during trajectory generation, PPO minibatches, and
+        PPO microbatches for gradient accumulation.
+
+        Raises
+            - ValueError if:
+                - batch_size is not divisible by forward_batch_size
+                - batch_size is not divisible by ppo_batch_size
+                - ppo_batch_size is not divisible by gradient_accumulation_steps
+                - num_steps is less than batch_size
+                - gradient_accumulation_steps > 1 and optimizer_in_bwd is True
+        """
+        self.batch_size = cfg.batch_size
+        self._forward_batch_size = cfg.forward_batch_size
+        self._ppo_epochs = cfg.ppo_epochs
+        self._ppo_batch_size = cfg.ppo_batch_size
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+        self._ppo_backward_batch_size = (
+            cfg.ppo_batch_size // self._gradient_accumulation_steps
+        )
+
+        if self.batch_size % self._forward_batch_size != 0:
+            raise ValueError(
+                f"batch_size ({self.batch_size}) must be exactly divisible by "
+                f"forward_batch_size ({self._forward_batch_size})."
+            )
+        if self.batch_size % self._ppo_batch_size != 0:
+            raise ValueError(
+                f"batch_size ({self.batch_size}) must be exactly divisible by "
+                f"ppo_batch_size ({self._ppo_batch_size})."
+            )
+        if self._ppo_batch_size % self._gradient_accumulation_steps != 0:
+            raise ValueError(
+                f"ppo_batch_size ({self._ppo_batch_size}) must be exactly divisible "
+                f"by gradient_accumulation_steps ({self._gradient_accumulation_steps})."
+            )
+
+        if self._gradient_accumulation_steps > 1 and self._optimizer_in_bwd:
+            raise RuntimeError(
+                "Gradient accumulation is not supported with optimizer in bwd."
+                "Please set gradient_accumulation_steps=1, or optimizer_in_bwd=False."
+            )
+
+        self._total_steps = cfg.num_steps // self.batch_size
+        batches_per_epoch = max(
+            1, len(self._dataloader)
+        )  # when we only have a single batch in the dataset
+
+        self._total_epochs = math.ceil(self._total_steps / batches_per_epoch)
+        if self._total_steps == 0:
+            raise ValueError(
+                f"num_steps {cfg.num_steps} must be greater than the batch size {self.batch_size}."
+            )
+        if self._total_steps < len(self._dataloader):
+            warn(
+                f"There are fewer total steps ({self._total_steps}, (num_steps//batch_size) "
+                f"than there are batches ({len(self._dataloader)}) in the dataset. "
+                f"Training will stop after ({self._total_steps}) steps without saving intermediate checkpoints"
+            )
+        if (self._total_steps > batches_per_epoch) and (
+            self._total_steps % batches_per_epoch != 0
+        ):
+            warn(
+                f"num_steps ({cfg.num_steps}) is not exactly divisible by "
+                f"the number of batches in the dataset ({batches_per_epoch}). "
+                f"Intermediate checkpoints will only be saved every {batches_per_epoch} steps."
+            )
+        log.info(
+            f"Total steps to run: {self._total_steps}, Total epochs to run: {self._total_epochs}"
+        )
+
+    def _setup_checkpointers(
+        self,
+        policy_cfg: DictConfig,
+        ref_policy_cfg: DictConfig,
+        value_cfg: DictConfig,
+        reward_cfg: DictConfig,
+    ) -> Tuple[
+        training.Checkpointer,
+        training.Checkpointer,
+        training.Checkpointer,
+        training.Checkpointer,
+    ]:
+        """
+        Sets up checkpointers for policy, reference policy, value, and reward models.
+        Only the policy checkpoint handles recipe state for resuming from checkpoints.
+        """
+
+        if not self._resume_from_checkpoint:
+            assert policy_cfg.checkpoint_dir == ref_policy_cfg.checkpoint_dir, (
+                "Policy and reference policy should be loaded from the same checkpoint directories"
+                f"at the start of training. Found: {policy_cfg.checkpoint_dir} and"
+                f"{ref_policy_cfg.checkpoint_dir}"
+            )
+            assert policy_cfg.checkpoint_files == ref_policy_cfg.checkpoint_files, (
+                "Policy and reference policy should be loaded from the same checkpoint files"
+                f"at the start of training. Found: {policy_cfg.checkpoint_files} and"
+                f"{ref_policy_cfg.checkpoint_files}"
+            )
+
+        policy_checkpointer = config.instantiate(
+            policy_cfg,
+            resume_from_checkpoint=self._resume_from_checkpoint,
+        )
+
+        ref_policy_checkpointer = config.instantiate(
+            ref_policy_cfg,
+            resume_from_checkpoint=False,
+        )
+
+        value_checkpointer = config.instantiate(
+            value_cfg,
+            resume_from_checkpoint=False,
+        )
+
+        reward_checkpointer = config.instantiate(
+            reward_cfg,
+            resume_from_checkpoint=False,
+        )
+
+        return (
+            policy_checkpointer,
+            ref_policy_checkpointer,
+            value_checkpointer,
+            reward_checkpointer,
+        )
+
+    def _setup_models(
+        self,
+        cfg_model: DictConfig,
+        cfg_reward_value_model: DictConfig,
+        enable_activation_checkpointing: bool,
+        compile_model: bool,
+        policy_state_dict: Dict[str, Any],
+        ref_policy_state_dict: Dict[str, Any],
+        value_model_state_dict: Dict[str, Any],
+        reward_model_state_dict: Dict[str, Any],
+    ) -> Tuple[nn.Module, nn.Module, nn.Module]:
+        """
+        Sets up the policy model, reference policy model, reward model, and value model.
+        """
+
+        with training.set_default_dtype(self._dtype), self._device:
+            policy_model = config.instantiate(cfg_model)
+            ref_policy_model = config.instantiate(cfg_model)
+            reward_model = config.instantiate(cfg_reward_value_model)
+            value_model = config.instantiate(cfg_reward_value_model)
+
+        if enable_activation_checkpointing:
+            training.set_activation_checkpointing(
+                policy_model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+            training.set_activation_checkpointing(
+                value_model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        policy_model.load_state_dict(policy_state_dict)
+        ref_policy_model.load_state_dict(ref_policy_state_dict)
+
+        # since we should be loading a classifier checkpoint into
+        # a classifier model, this function should just ensure
+        # output.weight appears in the state_dict and the model's parameters,
+        # and removes output.bias from the state dict if found
+        training.update_state_dict_for_classifier(
+            reward_model_state_dict, reward_model.named_parameters()
+        )
+        reward_model.load_state_dict(reward_model_state_dict)
+
+        # same as above
+        training.update_state_dict_for_classifier(
+            value_model_state_dict, value_model.named_parameters()
+        )
+        value_model.load_state_dict(value_model_state_dict)
+
+        # Validate models were loaded in with the expected dtype.
+        training.validate_expected_param_dtype(
+            value_model.named_parameters(), dtype=self._dtype
+        )
+        training.validate_expected_param_dtype(
+            reward_model.named_parameters(), dtype=self._dtype
+        )
+        training.validate_expected_param_dtype(
+            value_model.named_parameters(), dtype=self._dtype
+        )
+        training.validate_expected_param_dtype(
+            ref_policy_model.named_parameters(), dtype=self._dtype
+        )
+
+        log.info(f"Models are initialized with precision {self._dtype}.")
+
+        # disabling dropout if found - non-determinism leads to issues in e.g. comparing logprobs
+        # between ref policy and current policy
+        for module in policy_model.modules():
+            if isinstance(module, torch.nn.Dropout):
+                warn(
+                    f"Dropout found in {module}. This is likely to cause issues during training. Disabling."
+                )
+                module.p = 0
+        for module in value_model.modules():
+            if isinstance(module, torch.nn.Dropout):
+                warn(
+                    f"Dropout found in {module}. This is likely to cause issues during training. Disabling."
+                )
+                module.p = 0
+
+        # disabling grad and dropout in reward and reference policy models
+        reward_model.eval()
+        ref_policy_model.eval()
+
+        for p in reward_model.parameters():
+            p.requires_grad = False
+
+        for p in ref_policy_model.parameters():
+            p.requires_grad = False
+
+        # Compile model, if enabled.
+        if compile_model:
+            backend = os.environ.get("TORCH_COMPILE_BACKEND", "inductor")
+            log.info("Compiling models with torch.compile...")
+
+            policy_model.compile(backend=backend)
+            reward_model.compile(backend=backend)
+            ref_policy_model.compile(backend=backend)
+            value_model.compile(backend=backend)
+
+        if self._device.type == "cuda":
+            memory_stats = training.get_memory_stats(device=self._device)
+            training.log_memory_stats(memory_stats)
+
+        return policy_model, value_model, reward_model, ref_policy_model
+
+    def _setup_optimizer(
+        self,
+        cfg_optimizer: DictConfig,
+        optimizer_in_bwd: bool = False,
+        opt_state_dict: Optional[Dict[str, Any]] = None,
+    ) -> Optimizer:
+
+        if optimizer_in_bwd:
+            # Maintain a dict of optims for every parameter.
+            optim_dict = {
+                p: config.instantiate(cfg_optimizer, [p])
+                for p in chain(
+                    self._policy_model.parameters(), self._value_model.parameters()
+                )
+            }
+            # Register optimizer step hooks on the models to run optimizer in backward.
+            training.register_optim_in_bwd_hooks(
+                model=self._policy_model, optim_dict=optim_dict
+            )
+            training.register_optim_in_bwd_hooks(
+                model=self._value_model, optim_dict=optim_dict
+            )
+            # Create a wrapper for checkpoint save/load of optimizer states when running in backward.
+            self._optim_ckpt_wrapper = training.create_optim_in_bwd_wrapper(
+                model=self._policy_model, optim_dict=optim_dict
+            )
+            self._optim_ckpt_wrapper = training.create_optim_in_bwd_wrapper(
+                model=self._value_model, optim_dict=optim_dict
+            )
+            # Load optimizer states. If optimizer states are being restored in an optimizer in backward
+            # run, these need to have been saved with the same setting. Cannot restore from runs that did not
+            # use optimizer in backward.
+            if opt_state_dict is not None:
+                try:
+                    self._optim_ckpt_wrapper.load_state_dict(opt_state_dict)
+                except BaseException as e:
+                    raise RuntimeError(
+                        "Failed loading in-backward optimizer checkpoints."
+                        "Please make sure run being restored from was using in-backward optimizer."
+                    ) from e
+            log.info("In-backward optimizers are set up.")
+            return None
+        else:
+            optimizer = config.instantiate(
+                cfg_optimizer,
+                chain(self._policy_model.parameters(), self._value_model.parameters()),
+            )
+            if opt_state_dict:
+                optimizer.load_state_dict(opt_state_dict)
+
+            log.info("Optimizer is initialized.")
+            return optimizer
+
+    def _setup_data(
+        self, cfg_dataset: DictConfig, shuffle: bool, batch_size: int
+    ) -> Tuple[DistributedSampler, DataLoader]:
+        """
+        All data related setup happens here.
+        """
+        if isinstance(cfg_dataset, ListConfig):
+            datasets = [
+                config.instantiate(single_cfg_dataset, tokenizer=self._tokenizer)
+                for single_cfg_dataset in cfg_dataset
+            ]
+            ds = ConcatDataset(datasets=datasets)
+        else:
+            ds = config.instantiate(cfg_dataset, tokenizer=self._tokenizer)
+
+        sampler = DistributedSampler(
+            ds,
+            num_replicas=1,
+            rank=0,
+            shuffle=shuffle,
+            seed=0,
+        )
+        dataloader = DataLoader(
+            dataset=ds,
+            sampler=sampler,
+            batch_size=batch_size,
+            # dropping last avoids shape issues with compile + flex attention
+            drop_last=True,
+            collate_fn=partial(
+                padded_collate,
+                pad_direction="left",
+                keys_to_pad=["tokens", "labels"],
+                padding_idx=self._tokenizer.pad_id,
+            ),
+        )
+
+        return sampler, dataloader
+
+    def save_checkpoint(
+        self, epoch: int, is_intermediate_checkpoint: bool = False
+    ) -> None:
+        """
+        Save state dict to file. The recipe save_checkpoint method is responsible for
+        correctly creating the checkpoint dict and passing to the checkpointer.
+        """
+        policy_ckpt_dict = {training.MODEL_KEY: self._policy_model.state_dict()}
+        value_ckpt_dict = {training.MODEL_KEY: self._value_model.state_dict()}
+
+        # if training is in-progress, checkpoint the optimizer state and rng state as well
+        if is_intermediate_checkpoint:
+            policy_ckpt_dict.update(
+                {
+                    training.SEED_KEY: self.seed,
+                    training.EPOCHS_KEY: self._epochs_run,
+                    training.TOTAL_EPOCHS_KEY: self._total_epochs,
+                    training.MAX_STEPS_KEY: self._total_steps,
+                    training.STEPS_KEY: self._steps_run,
+                    training.RNG_KEY: self._rng.get_state(),
+                }
+            )
+            if not self._optimizer_in_bwd:
+                policy_ckpt_dict[training.OPT_KEY] = self._optimizer.state_dict()
+            else:
+                policy_ckpt_dict[
+                    training.OPT_KEY
+                ] = self._optim_ckpt_wrapper.state_dict()
+
+        self._policy_checkpointer.save_checkpoint(
+            policy_ckpt_dict,
+            epoch=epoch,
+            intermediate_checkpoint=is_intermediate_checkpoint,
+        )
+
+        self._value_checkpointer.save_checkpoint(
+            value_ckpt_dict,
+            epoch=epoch,
+            intermediate_checkpoint=False,
+        )
+
+    def _update_recipe_state(self, ckpt_dict: Dict[str, Any]) -> None:
+        """
+        Updates the recipe state from checkpoint.
+        """
+        # If seed or total_steps, or total_epochs don't match,
+        # warn the user and overwrite.
+        try:
+            if (
+                self.seed != ckpt_dict[training.SEED_KEY]
+                or self._total_steps != ckpt_dict[training.MAX_STEPS_KEY]
+                or self._total_epochs != ckpt_dict[training.TOTAL_EPOCHS_KEY]
+            ):
+                warn(
+                    message="""Configured value for seed, total_steps, or total_epochs
+                    does not match the value stored in checkpoint."""
+                )
+            self.seed = training.set_seed(seed=ckpt_dict[training.SEED_KEY])
+            self._rng.set_state(ckpt_dict[training.RNG_KEY])
+            self._steps_run = ckpt_dict[training.STEPS_KEY]
+            self._total_steps = ckpt_dict[training.MAX_STEPS_KEY]
+            self._total_epochs = ckpt_dict[training.TOTAL_EPOCHS_KEY]
+            self._epochs_run = ckpt_dict[training.EPOCHS_KEY]
+
+        except KeyError as e:
+            raise KeyError from e(
+                "Checkpoint does not contain the required keys needed for updating recipe state."
+                "Are you sure you passed in the right recipe checkpoint?"
+            )
+
+    def generate_trajectory(self, input_ids: torch.Tensor) -> Trajectory:
+        """
+        Generates a trajectory given the current policy and value models, the reference policy model, the reward model,
+        and batch of inputs. This is done over the following steps:
+
+        1: Generate responses, and logits corresponding to the responses using the current policy,
+            generating (query, response) pairs.
+        2. Estimate logprobs of the generated responses using the current policy.
+        3. Estimate values from the generated responses using the current value function.
+        4. Replace any tokens in the response after the first stop token (usually EOS token) with padding,
+            producting truncated responses.
+        5. Run the reward model on the (query, truncated-response) pairs.
+        6. Mask out all the invalid values in the trajectory due to padding tokens.
+
+        Args:
+            input_ids (torch.Tensor): tensor of input token IDs with shape [b, seq_length]
+
+        Returns:
+            Trajectory: An instance of :class:`~torchtune.rlhf.Trajectory` comprising
+                the current trajectory.
+        """
+        batch_size, context_length = input_ids.shape
+
+        # step 1: generate responses, and logits corresponding to the responses using the current policy
+        query_responses, logits = generation.generate(
+            model=self._policy_model,
+            prompt=input_ids,
+            max_generated_tokens=self._max_generated_tokens,
+            temperature=self._temperature,
+            top_k=self._top_k,
+            pad_id=self._tokenizer.pad_id,
+            rng=self._rng,
+        )
+
+        responses = query_responses[:, context_length:].clone()
+        query_response_padding_masks = query_responses != self._tokenizer.pad_id
+
+        # step 1.1 create attention masks and position IDs for any padding tokens in inputs, used for future forward passes
+        masks = generation.get_causal_mask_from_padding_mask(
+            query_response_padding_masks
+        )
+        position_ids = generation.get_position_ids_from_padding_mask(
+            query_response_padding_masks
+        )
+
+        del query_response_padding_masks
+
+        # step 2. estimate logprobs of the responses using the current policy
+        logits = logits[:, context_length - 1 :]
+        logprobs = rlhf.logits_to_logprobs(logits, responses, self._temperature)
+
+        del logits
+
+        # step 2.1 estimate logprobs of the responses using the reference policy
+        ref_logits = self._ref_policy_model(
+            query_responses, input_pos=position_ids, mask=masks
+        )
+        ref_logits = rlhf.truncate_sequence_for_logprobs(ref_logits, context_length)
+        ref_logprobs = rlhf.logits_to_logprobs(ref_logits, responses, self._temperature)
+
+        del ref_logits
+
+        # step 3. estimate values from the responses using the value function
+        values = self._value_model(query_responses, input_pos=position_ids, mask=masks)
+        values = rlhf.truncate_sequence_for_logprobs(values, context_length).squeeze(-1)
+
+        # step 4. replace any tokens in the responses after the first stop token (usually EOS token) with padding
+        # resulting in truncated responses
+        response_padding_masks, responses = rlhf.truncate_sequence_at_first_stop_token(
+            responses, self._stop_token_ids, self._tokenizer.pad_id
+        )
+
+        # step 5. run the reward model on the (query, truncated-response) pairs
+        scores = self._reward_model(
+            torch.cat([input_ids, responses], dim=1),
+            input_pos=position_ids,
+            mask=masks,
+        )
+
+        del responses
+
+        # step 5.1 the scores from the reward model are the logits for the last non-padding token in
+        # each (query, truncated-response) pair
+        seq_lens = training.get_unmasked_sequence_lengths(response_padding_masks)
+        scores = scores[torch.arange(batch_size), seq_lens + context_length].squeeze(-1)
+
+        # step 5.2 if configured, apply any penalties for sequences without EOS tokens
+        # or shorter than a certain length
+        if self._penalise_no_eos or self._min_response_length:
+            reward_penalty_mask = rlhf.get_reward_penalty_mask(
+                response_padding_masks,
+                seq_lens,
+                self._penalise_no_eos,
+                self._min_response_length,
+            )
+            scores[reward_penalty_mask] = self._reward_penalty
+
+        # step 6. mask out all the invalid values in the trajectory due to padding tokens
+        logprobs[response_padding_masks] = 1.0
+        ref_logprobs[response_padding_masks] = 1.0
+
+        # step 6.1 values are masked out *after* the last valid token in the response
+        value_seq_idxs = torch.where(
+            (seq_lens > 0) & (seq_lens < self._max_generated_tokens - 1),
+            seq_lens + 1,
+            seq_lens,
+        )
+        value_padding_masks = response_padding_masks.clone()
+        value_padding_masks[
+            torch.arange(batch_size, device=value_padding_masks.device),
+            value_seq_idxs,
+        ] = False
+
+        values[value_padding_masks] = 0.0
+
+        return Trajectory(
+            query_responses=query_responses,
+            logprobs=logprobs,
+            ref_logprobs=ref_logprobs,
+            values=values,
+            masks=masks,
+            position_ids=position_ids,
+            response_padding_masks=response_padding_masks,
+            value_padding_masks=value_padding_masks,
+            value_seq_idxs=value_seq_idxs,
+            scores=scores,
+            seq_lens=seq_lens,
+        )
+
+    def generate_trajectory_batched(self, input_ids: torch.Tensor) -> Trajectory:
+        """
+        Generates a ``self.batch_size`` batch of trajectories using `self._forward_batch_size` batch sizes.
+        See ``generate_trajectory`` for more details.
+
+        Args:
+            input_ids (torch.Tensor): tensor of input token IDs with shape [b, seq_length]
+
+        Returns:
+            Trajectory: An instance of :class:`~torchtune.rlhf.Trajectory`, comprising
+                the current trajectory.
+        """
+        trajectories: List[Trajectory] = []
+        with torch.no_grad():
+            for batch_start in range(0, self.batch_size, self._forward_batch_size):
+                batch_input_ids = input_ids[
+                    batch_start : batch_start + self._forward_batch_size
+                ]
+                trajectories.append(self.generate_trajectory(batch_input_ids))
+        return Trajectory(*map(torch.cat, zip(*trajectories)))
+
+    def train(self) -> None:
+        """
+        The core training loop."""
+
+        if self._model_compile:
+            log.info(
+                "NOTE: torch.compile is enabled and model is compiled in first forward."
+                "Expect a relatively slow first iteration."
+            )
+        # zero out the gradients before starting training
+        if not self._optimizer_in_bwd:
+            self._optimizer.zero_grad()
+
+        training_completed = False
+        pbar = tqdm(total=self._total_steps, initial=self._steps_run)
+        for curr_epoch in range(self._epochs_run, self._total_epochs):
+            # Update the sampler to ensure data is correctly shuffled across epochs
+            # in case shuffle is True
+            self._sampler.set_epoch(curr_epoch)
+
+            for _, batch in enumerate(self._dataloader):
+                batch = batch["tokens"].to(self._device)
+                _, context_length = batch.shape
+
+                # step 1. generate the trajectory using:
+                # - the current policy (pi_theta)
+                # - the current value function (V_phi)
+                # - the reference frozen policy model (pi_theta_0)
+                trajectory = self.generate_trajectory_batched(batch)
+
+                # step 2. get the rewards for the current trajectory. these are based on:
+                #   - the divergence between the current policy and the reference policy
+                #   - the scores from the reward model
+                rewards, kl, kl_rewards = rlhf.get_rewards_ppo(
+                    trajectory.scores,
+                    trajectory.logprobs,
+                    trajectory.ref_logprobs,
+                    self._kl_coeff,
+                    trajectory.value_seq_idxs,
+                )
+
+                # step 3. estimate the advantages using Generalized Advantage Estimation (GAE)
+                advantages, returns = rlhf.estimate_advantages(
+                    trajectory.values,
+                    rewards,
+                    self._gamma,
+                    self._lmbda,
+                    masks=~trajectory.response_padding_masks,
+                )
+
+                # step 4. optimise using the PPO objective over multiple epochs
+                ppo_stats: List[PPOStats] = []
+                for _ in range(self._ppo_epochs):
+                    batch_idxs = torch.randperm(self.batch_size, device=self._device)
+                    for i in range(0, self.batch_size, self._ppo_batch_size):
+                        mini_batch_idxs = batch_idxs[i : i + self._ppo_batch_size]
+
+                        batch_ppo_stats: List[PPOStats] = []
+                        for j in range(
+                            0, self._ppo_batch_size, self._ppo_backward_batch_size
+                        ):
+                            backward_batch_idxs = mini_batch_idxs[
+                                j : j + self._ppo_backward_batch_size
+                            ]
+
+                            batch_trajectory = Trajectory(
+                                *map(
+                                    partial(
+                                        torch.index_select,
+                                        dim=0,
+                                        index=backward_batch_idxs,
+                                    ),
+                                    trajectory,
+                                )
+                            )
+                            batch_ppo_stats.append(
+                                self._ppo_step(
+                                    batch_trajectory,
+                                    advantages[backward_batch_idxs],
+                                    returns[backward_batch_idxs],
+                                    context_length,
+                                )
+                            )
+                            del batch_trajectory
+
+                        ppo_stats.append(PPOStats(*map(sum, zip(*batch_ppo_stats))))
+
+                        if not self._optimizer_in_bwd:
+                            self._optimizer.step()
+                            self._optimizer.zero_grad(set_to_none=True)
+
+                        self.global_step += 1
+
+                # step 5. profit
+                self._steps_run += 1
+                if self._steps_run % self._log_every_n_steps == 0:
+                    self.log_metrics(
+                        trajectory,
+                        PPOStats(*map(torch.stack, zip(*ppo_stats))),
+                        kl,
+                        kl_rewards,
+                    )
+                self.cleanup_after_step(
+                    trajectory, ppo_stats, advantages, returns, kl, kl_rewards
+                )
+                pbar.update(1)
+                if self._steps_run == self._total_steps:
+                    training_completed = True
+                    break
+
+            # save checkpoint at current epoch
+            self._epochs_run += 1
+
+            self.save_checkpoint(
+                curr_epoch, is_intermediate_checkpoint=not training_completed
+            )
+            if training_completed:
+                return
+
+    def _ppo_step(
+        self,
+        trajectory: Trajectory,
+        advantages: torch.Tensor,
+        returns: torch.Tensor,
+        context_length: int,
+    ) -> PPOStats:
+        """
+        Perform a single PPO optimisation step over a batch of trajectories and corresponding advantages and returns.
+
+        Args:
+            trajectory (Trajectory): a batch of trajectories
+            advantages (torch.Tensor): advantages corresponding to the trajectories
+            returns (torch.Tensor): returns corresponding the trajectories
+            context_length (int): input ids sequence length
+
+        Returns:
+            PPOStats: An instance of :class:`~torchtune.rlhf.PPOStats`, a NamedTuple containing:
+               - loss (torch.Tensor): The total PPO loss.
+               - policy_loss (torch.Tensor): The policy function loss.
+               - value_loss (torch.Tensor): The value function loss.
+               - ratios (torch.Tensor): The ratio between the current and old policy probabilities.
+               - clipfrac (torch.Tensor): The fraction of ratios that were clipped.
+               - approx_policy_kls: Average estimated KL divergence between the policy before and after the optimisation step.
+
+        """
+        # estimate logprobs from the policy at the current optimisation step
+        pi_logits = self._policy_model(
+            trajectory.query_responses,
+            input_pos=trajectory.position_ids,
+            mask=trajectory.masks,
+        )
+        pi_logits = rlhf.truncate_sequence_for_logprobs(pi_logits, context_length)
+        pi_logprobs = rlhf.logits_to_logprobs(
+            pi_logits, trajectory.query_responses[:, context_length:], self._temperature
+        )
+        pi_logprobs[trajectory.response_padding_masks] = 1.0
+
+        del pi_logits
+
+        # estimate the values from the value function at the current optimisation step
+        phi_values = self._value_model(
+            trajectory.query_responses,
+            input_pos=trajectory.position_ids,
+            mask=trajectory.masks,
+        )
+
+        phi_values = rlhf.truncate_sequence_for_logprobs(
+            phi_values, context_length
+        ).squeeze(-1)
+        phi_values[trajectory.value_padding_masks] = 0.0
+
+        # calculate ppo loss
+        loss, policy_loss, value_loss, ratios, clipfrac = self._loss_fn(
+            trajectory.logprobs,
+            pi_logprobs,
+            advantages,
+            trajectory.values,
+            phi_values,
+            returns,
+            padding_masks=~trajectory.response_padding_masks,
+            value_padding_masks=~trajectory.value_padding_masks,
+        )
+
+        loss /= self._gradient_accumulation_steps
+        loss.backward()
+
+        with torch.no_grad():
+            approx_policy_kls = (
+                0.5 * (pi_logprobs - trajectory.logprobs).pow(2)
+            ).mean()
+
+        return PPOStats(
+            loss,
+            policy_loss / self._gradient_accumulation_steps,
+            value_loss / self._gradient_accumulation_steps,
+            ratios / self._gradient_accumulation_steps,
+            clipfrac / self._gradient_accumulation_steps,
+            approx_policy_kls / self._gradient_accumulation_steps,
+        )
+
+    def log_metrics(
+        self,
+        trajectory: Trajectory,
+        ppo_stats: PPOStats,
+        kl: torch.Tensor,
+        kl_rewards: torch.Tensor,
+    ) -> None:
+        """
+        Log metrics and statistics for the current step to the metric logger.
+        """
+        log_dict = {
+            "scores": trajectory.scores.mean(),
+            "num_stop_tokens": trajectory.response_padding_masks.any(-1).sum(),
+            "rlhf_reward": trajectory.scores.mean() + kl_rewards.sum(1).mean(),
+            "kl": kl.sum(1).mean(),
+            "kl_reward": kl_rewards.sum(1).mean(),
+            "loss": ppo_stats.loss.mean(),
+            "policy_loss": ppo_stats.policy_loss.mean(),
+            "value_loss": ppo_stats.value_loss.mean(),
+            "clipfrac": ppo_stats.clipfrac.mean(),
+            "ratios": ppo_stats.ratios.mean(),
+            "approx_policy_kl": ppo_stats.approx_policy_kls.mean(),
+            "response_lengths": trajectory.seq_lens.float().mean(),
+        }
+        if self._device.type == "cuda" and self._log_peak_memory_stats:
+            log_dict.update(training.get_memory_stats(device=self._device))
+
+        self._metric_logger.log_dict(log_dict, step=self.global_step)
+
+    def cleanup_after_step(
+        self,
+        trajectory: Trajectory,
+        ppo_stats: PPOStats,
+        advantages: torch.Tensor,
+        returns: torch.Tensor,
+        kl: torch.Tensor,
+        kl_rewards: torch.Tensor,
+    ) -> None:
+        """
+        Cleanup tensors after each PPO step to free up memory.
+        """
+        # there shouldn't be any floating references to the individual tensors at the this point, so gc can do its thing
+        for v in trajectory:
+            del v
+        del trajectory
+        for v in ppo_stats:
+            del v
+        del ppo_stats
+        del advantages
+        del returns
+        del kl
+        del kl_rewards
+
+    def cleanup(self, **kwargs) -> None:
+        self._metric_logger.close()
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """
+    Entry point for the recipe.
+
+    Configurable parameters are read in the following order:
+        - Parameters specified in config (see available configs through ``tune ls``)
+        - Overwritten by arguments from the command-line
+    """
+    config.log_config(recipe_name="PPOFullFinetuneRecipeSingleDevice", cfg=cfg)
+    recipe = PPOFullFinetuneRecipeSingleDevice(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
Binary files marc_original/third_party/torchtune/recipes/__pycache__/lora_finetune_single_device.cpython-312.pyc and marc/third_party/torchtune/recipes/__pycache__/lora_finetune_single_device.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/recipes/qat_distributed.py marc/third_party/torchtune/recipes/qat_distributed.py
--- marc_original/third_party/torchtune/recipes/qat_distributed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/qat_distributed.py	2025-02-20 17:49:29.514024186 -0500
@@ -0,0 +1,798 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+import sys
+import time
+
+from functools import partial
+from typing import Any, Dict, List, Optional, Tuple, Union
+from warnings import warn
+
+import torch
+from omegaconf import DictConfig, ListConfig
+
+from torch import nn
+from torch.distributed import destroy_process_group, init_process_group
+
+from torch.optim import Optimizer
+from torch.utils.data import DataLoader, DistributedSampler
+from torchtune import config, modules, training, utils
+from torchtune.data import padded_collate_packed, padded_collate_sft
+from torchtune.datasets import ConcatDataset
+from torchtune.recipe_interfaces import FTRecipeInterface
+from torchtune.training import DummyProfiler, PROFILER_KEY
+from torchtune.training.activations import apply_selective_activation_checkpointing
+
+from tqdm import tqdm
+
+log = utils.get_logger("DEBUG")
+
+
+class QATRecipeDistributed(FTRecipeInterface):
+    """
+    Quantization-aware training (QAT) recipe for dense transformer-based LLMs such as Llama2.
+    This recipe supports distributed training and can be run on a single node (1 to 8 GPUs).
+    Only compatible with PyTorch 2.4+.
+
+    Features:
+        - Quantization-aware training (QAT). Perform fake quantization on weights and/or activations
+            during finetuning, with the goal of ultimately producing a quantized model with minimal
+            accuracy degradation. This recipe produces an unquantized model in the original dtype,
+            which can then be quantized separately.
+
+        - Delayed fake quantization. Optionally specify the step after which fake quantization occurs.
+            Empirically, allowing the model to finetune without fake quantization initially allows the
+            weight and activation values to stabilize before fake quantizing them, potentially leading
+            to improved quantized accuracy. This can be specified through ``fake_quant_after_n_steps``.
+
+        - FSDP. Supported using PyTorch's FSDP APIs. CPU offload of parameters, gradients, and optimizer states
+            is supported via the ``fsdp_cpu_offload``. Resharding of parameters after the forward pass is
+            done by default (corresponding to FULL_SHARD sharding strategy), but can be disabled by setting the config
+            ``fsdp_reshard_after_forward`` to False (this corresponds to SHARD_GRAD_OP sharding strategy).
+            DDP is currently not supported. Training on CPU is not supported.
+
+        - Activation Checkpointing. This can be controlled using the ``activation_checkpointing``
+            flag. Activation checkpointing helps reduce the memory footprint since we no longer keep
+            activations in memory and instead recompute them during the backward pass. This is especially
+            helpful for larger batch sizes when you're memory constrained. But these savings in memory
+            come at the cost of training performance. In most cases training can slow-down quite a bit as
+            a result of this activation recomputation.
+
+        - Precision. Full fp32 and bf16 training are supported. Precision is controlled using the ``dtype``
+            flag. When ``dtype=bf16``, all activations, gradients and optimizer states are in bfloat16. In
+            most cases this should halve the memory footprint of full precision (fp32) training, without
+            loss in model quality (will depend on the model, training data and other settings). For
+            GPUs which do not support bfloat16, we fall back to fp32. Mixed precision training and fp16
+            precision are currently not supported.
+
+        - Gradient Accumulation. You can simulate larger batch sizes by accumulating gradients. This is
+            controlled using the ``gradient_accumulation_steps`` flag.
+
+                Total Batch Size = batch_size * number of GPUs * gradient accumulation steps.
+
+            For example: with batch_size=1, nproc_per_node=2 and gradient_accumulation_steps=32 we get a
+            total batch size of 64.
+
+            Gradient accumulation is especially useful when you are memory constrained. In this case,
+            accumulating gradients might give you better training speed than enabling activation
+            checkpointing.
+
+        - Checkpointing. Model weights are checkpointed both at the end of each epoch and at the end of
+            training. Optimizer state and recipe state (seed, total_epochs, number of epochs run etc) are
+            only saved at the end of a given epoch and used in case of resuming training.
+
+            Resuming training is controlled by the ``resume_from_checkpoint`` flag. Mid-epoch checkpointing is
+            currently not supported.
+
+            For more details on the checkpointer, please take a look at
+            our checkpointer deepdive (https://pytorch.org/torchtune/main/deep_dives/checkpointer.html).
+
+        - Logging. Terminal, Disk, WandB and TensorBoard are all supported.
+
+    For a full list of example configs for this recipe, run ``tune ls`` on the command line. Each config
+    has example commands for how to kick-off training.
+
+    Args:
+        cfg (DictConfig): OmegaConf object parsed from yaml file
+
+    Raises:
+        ValueError: If ``dtype`` is set to fp16.
+        RuntimeError: If ``dtype`` is set to bf16 and the hardware does not support bf16.
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        self._device = utils.get_device(device=cfg.device)
+        self._dtype = training.get_dtype(cfg.dtype, device=self._device)
+
+        if self._dtype == torch.float16:
+            raise ValueError(
+                "full fp16 training is not supported with this recipe. Please use bf16 or fp32 instead."
+            )
+
+        if (
+            cfg.get("fsdp_cpu_offload", False)
+            and cfg.optimizer.get("fused", False)
+            and not utils.torch_version_ge("2.4.0")
+        ):
+            raise RuntimeError(
+                "Using fused optimizer on CPU is only supported in PyTorch nightly."
+            )
+
+        # logging attributes
+        self._output_dir = cfg.output_dir
+        self._log_every_n_steps = cfg.get("log_every_n_steps", 1)
+        self._log_peak_memory_stats = cfg.get("log_peak_memory_stats", False)
+
+        # _is_rank_zero is used primarily for logging. In the future, the logger
+        # should directly take care of this
+        _, rank = training.get_world_size_and_rank()
+        self._is_rank_zero = rank == 0
+
+        # Training cfg
+        self._resume_from_checkpoint = cfg.resume_from_checkpoint
+        self._gradient_accumulation_steps = cfg.gradient_accumulation_steps
+        self._fsdp_sharding_strategy = torch.distributed.fsdp.ShardingStrategy[
+            cfg.get("fsdp_sharding_strategy", "FULL_SHARD")
+        ]
+        self._fake_quant_after_n_steps = cfg.get("fake_quant_after_n_steps", None)
+        self._quantizer_mode = None
+
+        # These are public properties which are updated by the checkpoint loader
+        # when ``resume_from_checkpoint`` is `True` or validated in tests
+        self.seed = training.set_seed(seed=cfg.seed)
+        self.epochs_run = 0
+        self.total_epochs = cfg.epochs
+        self.max_steps_per_epoch = cfg.max_steps_per_epoch
+        self.global_step = 0
+
+    def load_checkpoint(self, cfg_checkpointer: DictConfig) -> Dict[str, Any]:
+        """
+        Extract the checkpoint state from file and validate. If resume_from_checkpoint
+        is True, this also includes the recipe state.
+        """
+        self._checkpointer = config.instantiate(
+            cfg_checkpointer,
+            resume_from_checkpoint=self._resume_from_checkpoint,
+        )
+        checkpoint_dict = self._checkpointer.load_checkpoint()
+
+        if self._resume_from_checkpoint:
+            self._update_recipe_state(checkpoint_dict)
+        return checkpoint_dict
+
+    def _update_recipe_state(self, ckpt_dict: Dict[str, Any]) -> None:
+        """
+        Updates the recipe state from checkpoint.
+        """
+        try:
+            self.epochs_run = ckpt_dict[training.EPOCHS_KEY]
+
+            # on mismatch, warn the user and prevent the override
+            if self.seed != ckpt_dict[training.SEED_KEY]:
+                warn(
+                    message=(
+                        "Config value for seed does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.SEED_KEY]}"
+                    )
+                )
+                self.seed = ckpt_dict[training.SEED_KEY]
+            if self.max_steps_per_epoch != ckpt_dict[training.MAX_STEPS_KEY]:
+                warn(
+                    message=(
+                        "Config value for max_steps_per_epoch does not match the checkpoint value, "
+                        f"using the checkpoint value: {ckpt_dict[training.MAX_STEPS_KEY]}"
+                    )
+                )
+                self.max_steps_per_epoch = ckpt_dict[training.MAX_STEPS_KEY]
+
+            # on mismatch, warn the user but allow the override
+            if self.total_epochs != ckpt_dict[training.TOTAL_EPOCHS_KEY]:
+                warn(
+                    message=(
+                        "Config value for total_epochs does not match the checkpoint value, "
+                        f"using the config value: {self.total_epochs}"
+                    )
+                )
+
+        except KeyError as e:
+            raise KeyError(
+                "Checkpoint does not contain the required keys needed for updating recipe state. "
+                "Are you sure you passed in the right recipe checkpoint?"
+            ) from e
+
+    def setup(self, cfg: DictConfig) -> None:
+        """
+        Setup the recipe. This includes training state (if resume_from_checkpoint is True),
+        model, tokenizer, loss, optimizer, sampler, and dataloader.
+        """
+        if self._is_rank_zero:
+            self._metric_logger = config.instantiate(cfg.metric_logger)
+
+            # log config with parameter override
+            self._metric_logger.log_config(cfg)
+
+        checkpoint_dict = self.load_checkpoint(cfg_checkpointer=cfg.checkpointer)
+
+        self._model_compile = cfg.get("compile", False)
+        self._model = self._setup_model(
+            cfg_model=cfg.model,
+            enable_activation_checkpointing=cfg.enable_activation_checkpointing,
+            custom_sharded_layers=cfg.get("custom_sharded_layers", None),
+            fsdp_cpu_offload=cfg.get("fsdp_cpu_offload", False),
+            reshard_after_forward=cfg.get("fsdp_reshard_after_forward", True),
+            model_state_dict=checkpoint_dict[training.MODEL_KEY],
+            ac_mode=cfg.get("ac_mode", None),
+            ac_option=cfg.get("ac_option", None),
+            quantizer_cfg=cfg.get("quantizer", None),
+        )
+        self._tokenizer = config.instantiate(cfg.tokenizer)
+
+        self._optimizer = self._setup_optimizer(
+            cfg_optimizer=cfg.optimizer,
+            opt_state_dict=checkpoint_dict[training.OPT_KEY]
+            if self._resume_from_checkpoint
+            else None,
+        )
+
+        # initialize loss
+        self._loss_fn = config.instantiate(cfg.loss)
+        backend = os.environ.get("TORCH_COMPILE_BACKEND", "inductor")
+        if self._loss_fn.__class__.__name__ == "CEWithChunkedOutputLoss":
+            # set num_output_chunks for model
+            self._model.set_num_output_chunks(self._loss_fn.num_output_chunks)
+            if self._model_compile:
+                log.info("Compiling loss with torch.compile...")
+                # For CEWithChunkedOutputLoss, if we compile the entire class
+                # we lose the benefits from the chunked loss.
+                # Therefore, we only compile the cross entropy function + upcasting
+                self._loss_fn.compute_cross_entropy = torch.compile(
+                    self._loss_fn.compute_cross_entropy, backend=backend
+                )
+        else:
+            if self._model_compile:
+                log.info("Compiling loss with torch.compile...")
+                self._loss_fn = torch.compile(self._loss_fn, backend=backend)
+        log.info("Loss is initialized.")
+
+        # sampler and dataloader depend on the tokenizer and loss_fn and should be
+        # setup after both of these are initialized
+        self._sampler, self._dataloader = self._setup_data(
+            cfg_dataset=cfg.dataset,
+            shuffle=cfg.shuffle,
+            batch_size=cfg.batch_size,
+        )
+
+        # Finally update the recipe state which can only be correctly set after all of the
+        # other components have been initialized and updated.
+        #
+        # Number of training steps in each epoch depends on the number of batches produced
+        # by the dataloader, the max_steps_per_epoch param set by the user and the
+        # gradient_accumulation_steps param. This value is used for logging and tracking
+        # training state. The computation should happen after the dataloader has been setup
+        self._steps_per_epoch = (
+            len(self._dataloader) // self._gradient_accumulation_steps
+        )
+        if (
+            self.max_steps_per_epoch is not None
+            and self.max_steps_per_epoch < self._steps_per_epoch
+        ):
+            self._steps_per_epoch = self.max_steps_per_epoch
+        self.global_step = self.epochs_run * self._steps_per_epoch
+
+        # Set up profiler, returns DummyProfiler (nullcontext object with no-op `step` method)
+        # if cfg is missing profiler key or if `cfg.profiler.enabled = False`
+        self._profiler = self._setup_profiler(cfg.get(PROFILER_KEY, None))
+
+        # Used to ignore labels for loss computation
+        self.ignore_labels_cache = torch.full(
+            (cfg.batch_size, 1), self._loss_fn.ignore_index, device=self._device
+        )
+
+    def _setup_profiler(
+        self, cfg_profiler: Optional[DictConfig] = None
+    ) -> Union[torch.profiler.profile, DummyProfiler]:
+        """
+        Parses the `profiler` section of top-level `cfg` and sets up profiler
+
+        Args:
+            cfg_profiler (Optional[DictConfig]): ``profiler`` section of the top-level ``cfg`` (the main config passed to
+                `recipe.main`). Default None.
+
+        Returns:
+            profiler: Union[torch.profiler.profile, DummyProfiler] - DummyProfiler is a nullcontext with no-op methods
+            for `start`, `stop`, and `step` that can be used in place of `torch.profiler.profile` if profiler is not enabled such
+            that the instrumented training loop does not need to be changed profiling is disabled.
+
+        The profiler config can be provided in configs under the `profiler` key with the following layout:
+
+        .. code-block:: yaml
+            profiler:
+                enabled: bool
+
+                #Output directory of trace artifacts
+                output_dir: str
+
+            #`torch.profiler.ProfilerActivity` types to trace
+            cpu: bool
+            cuda: bool
+
+                #Trace options
+                profile_memory: bool
+                with_stack: bool
+                record_shapes: bool
+                with_flops: bool
+
+            # `torch.profiler.schedule` options:
+            # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
+            wait_steps: int
+            warmup_steps: int
+            active_steps: int
+            num_cycles: int
+        """
+        # Missing profiler section in config, assume disabled
+        if cfg_profiler is None:
+            cfg_profiler = DictConfig({"enabled": False})
+
+        # Check that component is included and set correctly
+        if cfg_profiler.get("_component_", None) is None:
+            cfg_profiler["_component_"] = "torchtune.training.setup_torch_profiler"
+        else:
+            assert (
+                cfg_profiler.get("_component_")
+                == "torchtune.training.setup_torch_profiler"
+            ), "Only torch profiler supported currently: component must be `torchtune.training.setup_torch_profiler`"
+
+        profiler, profiler_cfg = config.instantiate(cfg_profiler)
+
+        if self._is_rank_zero:
+            log.info(f" Profiler config after instantiation: {profiler_cfg}")
+
+            self.profiler_profile_memory = profiler_cfg.get("profile_memory", False)
+            if profiler_cfg["enabled"]:
+                self.profiler_wait_steps = profiler_cfg["wait_steps"]
+                self.profiler_warmup_steps = profiler_cfg["warmup_steps"]
+                self.profiler_active_steps = profiler_cfg["active_steps"]
+
+        return profiler
+
+    def _setup_model(
+        self,
+        cfg_model: DictConfig,
+        enable_activation_checkpointing: bool,
+        custom_sharded_layers: Optional[List[str]],
+        fsdp_cpu_offload: bool,
+        reshard_after_forward: bool,
+        model_state_dict: Dict[str, Any],
+        ac_mode: Optional[str] = None,
+        ac_option: Optional[int] = None,
+        quantizer_cfg: Optional[DictConfig] = None,
+    ) -> nn.Module:
+        """
+        Model initialization has some important considerations:
+           a. To minimize GPU peak memory, we initialize the model on meta device with
+              the right dtype
+           b. All ranks calls ``load_state_dict`` without peaking CPU RAMs since
+              full state dicts are loaded with ``torch.load(mmap=True)``
+        """
+
+        if self._is_rank_zero:
+            log.info(
+                "FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ..."
+            )
+            init_start = time.perf_counter()
+
+        with training.set_default_dtype(self._dtype), torch.device("meta"):
+            model = config.instantiate(cfg_model)
+
+        # We currently have two versions of activation checkpointing in this recipe
+        # for testing and BC purposes. ``enable_activation_checkpointing`` controls
+        # the older version of AC and this behavior is unchanged
+        # ac_mode and ac_option together control selective AC. This is only enabled
+        # when these are set AND ``enable_activation_checkpointing`` is set to False
+        # We'll clean this up as soon as testing of AC is complete
+        if (not enable_activation_checkpointing) and (ac_mode is not None):
+            apply_selective_activation_checkpointing(
+                model,
+                ac_mode,
+                ac_option,
+            )
+
+        # original activation checkpointing (full) - flip the condition above
+        if enable_activation_checkpointing and ac_mode is None:
+            training.set_activation_checkpointing(
+                model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        # Apply quantization-aware training during finetuning
+        if quantizer_cfg is None:
+            raise ValueError("Quantizer must be specified for QAT recipe.")
+        quantizer = config.instantiate(quantizer_cfg)
+        quantizer.precision = self._dtype
+        quantizer_mode = training.quantization.get_quantizer_mode(quantizer)
+        if "qat" not in quantizer_mode:
+            raise ValueError(
+                "Quantizer mode '%s' is not supported for finetuning" % quantizer_mode
+            )
+        self._quantizer_mode = quantizer_mode
+        model = quantizer.prepare(model)
+
+        # For FSDP sharding, we can condition on either the module or its name
+        # Shard conditions should be callables taking name (relative to model root)
+        # and the module itself and returning a bool on whether to shard the given module
+        fsdp_shard_conditions = []
+
+        # Shard transformer decoder layers (or AC-wrapped versions)
+        # Alternatively we could condition on the module type (TransformerDecoder or CheckpointWrapper)
+        # But directly using the name is more concise
+        def _is_layer_fqn(s: str) -> bool:
+            """
+            Return True for layers.i and False for all other module names
+            Covers sharding for both AC-wrapped and non-AC-wrapped modules in one shot
+            """
+            s_list = s.split(".")
+            return len(s_list) == 2 and s_list[0] == "layers" and str.isdigit(s_list[1])
+
+        fsdp_shard_conditions = [lambda n, m: _is_layer_fqn(n)]
+
+        # If wrapping any layers separately, we can add another shard condition
+        # A layer will be sharded if any of the fsdp_shard_conditions are met
+        if custom_sharded_layers:
+            fsdp_shard_conditions += [lambda n, m: n in custom_sharded_layers]
+
+        training.shard_model(
+            model=model,
+            shard_conditions=fsdp_shard_conditions,
+            cpu_offload=fsdp_cpu_offload,
+            reshard_after_forward=reshard_after_forward,
+        )
+
+        with training.set_default_dtype(self._dtype), self._device:
+            for m in model.modules():
+                # RoPE is not covered in state dict
+                if hasattr(m, "rope_init"):
+                    m.rope_init()
+
+        # This method will convert the full model state dict into a sharded state
+        # dict and load into the model
+        training.load_from_full_model_state_dict(
+            model, model_state_dict, self._device, self._is_rank_zero, strict=True
+        )
+
+        # Ensure no params and buffers are on meta device
+        training.validate_no_params_on_meta_device(model)
+
+        if self._is_rank_zero:
+            log.info(
+                f"Instantiating model and loading checkpoint took {time.perf_counter() - init_start:.2f} secs"
+            )
+            memory_stats = training.get_memory_stats(device=self._device)
+            training.log_memory_stats(memory_stats)
+
+        # synchronize before training begins
+        torch.distributed.barrier()
+
+        return model
+
+    def _setup_optimizer(
+        self, cfg_optimizer: DictConfig, opt_state_dict: Optional[Dict[str, Any]] = None
+    ) -> Optimizer:
+        optimizer = config.instantiate(cfg_optimizer, self._model.parameters())
+        if opt_state_dict:
+            training.load_from_full_optimizer_state_dict(
+                optimizer,
+                opt_state_dict,
+                self._device,
+            )
+
+        if self._is_rank_zero:
+            log.info("Optimizer is initialized.")
+        return optimizer
+
+    def _setup_data(
+        self,
+        cfg_dataset: DictConfig,
+        shuffle: bool,
+        batch_size: int,
+    ) -> Tuple[DistributedSampler, DataLoader]:
+        """
+        All data related setup happens here. Currently this recipe only supports the
+        DistributedSamplers with Map-style Datasets which fit into memory. Other samplers,
+        iterable datasets and streaming datasets are not supported.
+        """
+        world_size, rank = training.get_world_size_and_rank()
+
+        if isinstance(cfg_dataset, ListConfig):
+            datasets = [
+                config.instantiate(single_cfg_dataset, tokenizer=self._tokenizer)
+                for single_cfg_dataset in cfg_dataset
+            ]
+            ds = ConcatDataset(datasets=datasets)
+            packed = False
+        else:
+            ds = config.instantiate(cfg_dataset, tokenizer=self._tokenizer)
+            packed = cfg_dataset.get("packed", False)
+
+        sampler = DistributedSampler(
+            ds, num_replicas=world_size, rank=rank, shuffle=shuffle, seed=0
+        )
+        dataloader = DataLoader(
+            dataset=ds,
+            batch_size=batch_size,
+            sampler=sampler,
+            # dropping last avoids shape issues with compile + flex attention
+            drop_last=True,
+            collate_fn=partial(
+                padded_collate_sft,
+                padding_idx=self._tokenizer.pad_id,
+                ignore_idx=self._loss_fn.ignore_index,
+            )
+            if not packed
+            else partial(
+                padded_collate_packed,
+            ),
+        )
+
+        if self._is_rank_zero:
+            log.info("Dataset and Sampler are initialized.")
+
+        return sampler, dataloader
+
+    def save_checkpoint(
+        self,
+        epoch: int,
+    ) -> None:
+        """
+        Checkpoint the state of the recipe. The constructed checkpoint state dict
+        contains the following information:
+        - Model weights with key training.MODEL_KEY
+        - Relevant recipe state if training is not complete
+
+        Checkpointer will save the model weights and recipe state in
+        different checkpoint files. To correctly resume training from an intermediate checkpoint,
+        the model weights and recipe state must be provided.
+        """
+        # final dict passed onto the checkpointer
+        checkpoint_dict = {}
+
+        intermediate_checkpoint = epoch + 1 < self.total_epochs
+        # To prevent GPU memory from spiking during checkpoint save,
+        # we consolidate the full model and optim state dicts on CPU for rank 0
+        cpu_state_dict = training.get_full_model_state_dict(
+            self._model,
+            self._is_rank_zero,
+        )
+
+        if intermediate_checkpoint:
+            opt_state_dict = training.get_full_optimizer_state_dict(
+                self._optimizer,
+                self._is_rank_zero,
+            )
+        else:
+            opt_state_dict = None
+
+        # Now that we have the model and opt state dict, create the actual checkpoint dict
+        # to be sent to the checkpointer and ultimately written to file
+        if self._is_rank_zero:
+
+            checkpoint_dict.update({training.MODEL_KEY: cpu_state_dict})
+
+            # if training is in-progress, checkpoint the optimizer state and recipe state
+            # as well.
+            if intermediate_checkpoint:
+                checkpoint_dict.update(
+                    {
+                        training.OPT_KEY: opt_state_dict,
+                        training.SEED_KEY: self.seed,
+                        training.EPOCHS_KEY: self.epochs_run,
+                        training.TOTAL_EPOCHS_KEY: self.total_epochs,
+                        training.MAX_STEPS_KEY: self.max_steps_per_epoch,
+                    }
+                )
+
+            self._checkpointer.save_checkpoint(
+                checkpoint_dict,
+                epoch=epoch,
+                intermediate_checkpoint=intermediate_checkpoint,
+            )
+
+    def train(self) -> None:
+        """
+        The core training loop.
+        """
+        # clean up before training begins
+        training.cleanup_before_training()
+
+        _, rank = training.get_world_size_and_rank()
+
+        # zero out the gradients before starting training
+        self._optimizer.zero_grad()
+
+        # Initialize tokens count and running loss (for grad accumulation)
+        t0 = time.perf_counter()
+        running_loss = 0
+        num_tokens = 0
+
+        self._profiler.start()
+        # self.epochs_run should be non-zero when we're resuming from a checkpoint
+        for curr_epoch in range(self.epochs_run, self.total_epochs):
+
+            # Update the sampler to ensure data is correctly shuffled across epochs
+            # in case shuffle is True
+            self._sampler.set_epoch(curr_epoch)
+
+            pbar = tqdm(total=self._steps_per_epoch, disable=not (rank == 0))
+            for idx, batch in enumerate(self._dataloader):
+                if (
+                    self.max_steps_per_epoch is not None
+                    and (idx // self._gradient_accumulation_steps)
+                    == self.max_steps_per_epoch
+                ):
+                    break
+
+                # Start tracking CUDA memory for active steps for just the first epoch
+                if (
+                    self._is_rank_zero
+                    and curr_epoch == 0
+                    and self.profiler_profile_memory
+                    and idx == self.profiler_wait_steps + self.profiler_warmup_steps
+                ):
+                    torch.cuda.memory._record_memory_history()
+
+                # Both are shape [b, s]
+                tokens, labels = batch["tokens"], batch["labels"]
+                # Get the attention mask and position ids from the dataset if they
+                # exist. Currently, only sample packing in PackedDataset returns these
+                mask = batch.get("mask", None)  # shape [b, s, s]
+                input_pos = batch.get("input_pos", None)  # shape [b, s]
+
+                # Optionally wait N steps before enabling fake quant
+                if self._fake_quant_after_n_steps is not None:
+                    if self.global_step == 0:
+                        log.info(
+                            "Step 0: Disabling fake quant, will re-enable in step %s"
+                            % self._fake_quant_after_n_steps
+                        )
+                        disable_fq = training.quantization._get_disable_fake_quant(
+                            self._quantizer_mode
+                        )
+                        self._model.apply(disable_fq)
+                    elif self.global_step == self._fake_quant_after_n_steps:
+                        log.info(
+                            "Step %s: Enabling fake quant"
+                            % self._fake_quant_after_n_steps
+                        )
+                        enable_fq = training.quantization._get_enable_fake_quant(
+                            self._quantizer_mode
+                        )
+                        self._model.apply(enable_fq)
+
+                tokens = tokens.to(self._device)
+                num_tokens += tokens.numel()
+                labels = labels.to(self._device)
+                mask = mask.to(self._device) if mask is not None else None
+                input_pos = (
+                    input_pos.to(self._device) if input_pos is not None else None
+                )
+
+                logits = self._model(tokens, mask=mask, input_pos=input_pos)
+
+                # Shift labels to compute loss
+                # equivalent to doing labels[..., 1:] and logits[..., :-1, :]
+                # But this way we dont need to slice the logits. We just add an ignore index to labels.
+                labels = torch.hstack(
+                    (labels[..., 1:], self.ignore_labels_cache[: labels.shape[0]])
+                )
+                if not isinstance(logits, list):
+                    labels = labels.reshape(-1)
+                    logits = logits.reshape(-1, logits.size(-1))
+
+                # Compute loss
+                loss = self._loss_fn(logits, labels)
+                # free logits otherwise it peaks backward memory
+                del logits
+
+                loss = loss / self._gradient_accumulation_steps
+                running_loss += loss
+                loss.backward()
+
+                # Step with optimizer
+                if (idx + 1) % self._gradient_accumulation_steps == 0:
+                    self._optimizer.step()
+                    self._optimizer.zero_grad(set_to_none=True)
+
+                    # Update the number of steps when the weights are updated
+                    self.global_step += 1
+
+                    loss_to_log = running_loss.item()
+                    pbar.update(1)
+                    pbar.set_description(
+                        f"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}"
+                    )
+
+                    # Log per-step metrics
+                    if (
+                        self.global_step % self._log_every_n_steps == 0
+                        and self._is_rank_zero
+                    ):
+                        time_per_step = time.perf_counter() - t0
+                        log_dict = {
+                            "loss": loss_to_log,
+                            "lr": self._optimizer.param_groups[0]["lr"],
+                            "tokens_per_second_per_gpu": num_tokens / time_per_step,
+                        }
+                        if self._log_peak_memory_stats:
+                            log_dict.update(
+                                training.get_memory_stats(device=self._device)
+                            )
+                        self._metric_logger.log_dict(
+                            log_dict,
+                            step=self.global_step,
+                        )
+
+                    # Reset running stats for the next step
+                    running_loss = 0
+                    num_tokens = 0
+                    t0 = time.perf_counter()
+
+                    # Stop tracking CUDA memory now that active steps are complete
+                    if (
+                        self._is_rank_zero
+                        and curr_epoch == 0
+                        and self.profiler_profile_memory
+                        and idx
+                        == self.profiler_wait_steps
+                        + self.profiler_warmup_steps
+                        + self.profiler_active_steps
+                    ):
+                        torch.cuda.memory._record_memory_history(enabled=None)
+
+                    # Step profiler
+                    # Note that this is called within gradient accumulation block, hence
+                    # will include multiple forward / backward passes if gradient accumulation > 1
+                    self._profiler.step()
+
+            self.epochs_run += 1
+            self.save_checkpoint(epoch=curr_epoch)
+
+        self._profiler.stop()
+
+    def cleanup(self) -> None:
+        if self._is_rank_zero:
+            self._metric_logger.close()
+        destroy_process_group()
+
+
+@config.parse
+def recipe_main(cfg: DictConfig) -> None:
+    """
+    Entry point for the recipe.
+
+    Configurable parameters are read in the following order:
+        - Parameters specified in config (see available configs through ``tune ls``)
+        - Overwritten by arguments from the command-line
+    """
+    if not training.is_distributed():
+        raise RuntimeError(
+            "Distributed QAT recipe should be run via a distributed launcher."
+            "If using tune CLI, please specify --nnodes 1 and --nproc_per_node [num_gpus]"
+        )
+    init_process_group(backend="gloo" if cfg.device == "cpu" else "nccl")
+    if cfg.get("fsdp_cpu_offload", False):
+        # Utilize all available CPU cores for intra-op parallelism. This provides ~2x
+        # speed up when benchmarking fused AdamW on CPU
+        training.set_torch_num_threads()
+
+    config.log_config(recipe_name="QATRecipeDistributed", cfg=cfg)
+
+    recipe = QATRecipeDistributed(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.train()
+    recipe.cleanup()
+
+
+if __name__ == "__main__":
+    sys.exit(recipe_main())
diff -ruN marc_original/third_party/torchtune/recipes/quantization.md marc/third_party/torchtune/recipes/quantization.md
--- marc_original/third_party/torchtune/recipes/quantization.md	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/quantization.md	2025-02-20 17:49:29.518024193 -0500
@@ -0,0 +1,85 @@
+# Quantization and Sparsity
+
+torchtune integrates with [torchao](https://github.com/pytorch/ao/) for QAT and QLoRA. Currently only some quantization techniques are integrated, see the docstrings in the [quantization recipe](quantize.py) and the [QAT recipe](qat_distributed.py) for more details.
+
+For post training quantization, we recommend using `torchao` directly: https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md to quantize their model
+and do eval/benchmark in torchao as well: https://github.com/pytorch/ao/tree/main/torchao/_models/llama.
+
+## Quantization-Aware Training (QAT)
+
+(PyTorch 2.4+)
+
+QAT refers to applying fake quantization to weights and/or activations during finetuning,
+which means simulating only the quantization math without actually casting the original
+dtype to a lower precision. You can run QAT with finetuning using the following command:
+
+```
+tune run --nproc_per_node 4 qat_distributed --config llama3/8B_qat_full
+```
+
+This produces an unquantized model in the original data type. To get an actual quantized model,
+follow this with `tune run quantize` while specifying the same quantizer in the config, e.g.
+
+```yaml
+# QAT specific args
+quantizer:
+  _component_: torchtune.training.quantization.Int8DynActInt4WeightQATQuantizer
+  groupsize: 256
+```
+
+Currently only `torchtune.training.quantization.Int8DynActInt4WeightQATQuantizer`
+is supported. This refers to int8 dynamic per token activation quantization
+combined with int4 grouped per axis weight quantization. For more details,
+please refer to the [torchao implementation](https://github.com/pytorch/ao/blob/950a89388e88e10f26bbbbe2ec0b1710ba3d33d1/torchao/quantization/prototype/qat.py#L22).
+
+## Eval
+To evaluate a quantized model, make the following changes to the default [evaluation config](configs/eleuther_evaluation.yaml)
+
+
+```yaml
+# Currently we only support torchtune checkpoints when
+# evaluating quantized models. For more details on checkpointing see
+# https://pytorch.org/torchtune/main/deep_dives/checkpointer.html
+# Make sure to change the default checkpointer component
+checkpointer:
+  _component_: torchtune.training.FullModelTorchTuneCheckpointer
+  ..
+  checkpoint_files: [<quantized_model_checkpoint>]
+
+# Quantization specific args
+quantizer:
+  _component_: torchtune.training.quantization.Int8DynActInt4WeightQuantizer
+  groupsize: 256
+```
+
+Noet: we can use `Int8DynActInt4WeightQuantizer` to load a QAT quantized model since it's the same type of quantization.
+
+and run evaluation:
+```bash
+tune run eleuther_eval --config eleuther_evaluation
+```
+
+## Generate
+To run inference using a quantized model, make the following changes to the default [generation config](configs/generation.yaml)
+
+
+```yaml
+# Currently we only support torchtune checkpoints when
+# evaluating quantized models. For more details on checkpointing see
+# https://pytorch.org/torchtune/main/deep_dives/checkpointer.html
+# Make sure to change the default checkpointer component
+checkpointer:
+  _component_: torchtune.training.FullModelTorchTuneCheckpointer
+  ..
+  checkpoint_files: [<quantized_model_checkpoint>]
+
+# Quantization Arguments
+quantizer:
+  _component_: torchtune.training.quantization.Int8DynActInt4WeightQuantizer
+  groupsize: 256
+```
+
+and run generation:
+```bash
+tune run generate --config generation
+```
diff -ruN marc_original/third_party/torchtune/recipes/quantize.py marc/third_party/torchtune/recipes/quantize.py
--- marc_original/third_party/torchtune/recipes/quantize.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/recipes/quantize.py	2025-02-20 17:49:29.518024193 -0500
@@ -0,0 +1,125 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import os
+import sys
+import time
+from pathlib import Path
+from typing import Any, Dict
+
+import torch
+from omegaconf import DictConfig
+
+from torch import nn
+
+from torchtune import config, training, utils
+
+logger = utils.get_logger("DEBUG")
+
+
+class QuantizationRecipe:
+    """
+    Recipe for quantizing a Transformer-based LLM.
+    Uses quantizer classes from torchao to quantize a model.
+
+    Supported quantization modes are:
+    8da4w (PyTorch 2.3+):
+        torchtune.training.quantization.Int8DynActInt4WeightQuantizer
+        int8 per token dynamic activation with int4 weight only per axis group quantization
+        Args:
+            `groupsize` (int): a parameter of int4 weight only quantization,
+            it refers to the size of quantization groups which get independent quantization parameters
+            e.g. 32, 64, 128, 256, smaller numbers means more fine grained and higher accuracy,
+            but also higher memory overhead
+
+    8da4w-qat (PyTorch 2.4+):
+        torchtune.training.quantization.Int8DynActInt4WeightQATQuantizer
+        int8 per token dynamic activation with int4 weight only per axis group quantization
+        Same as "8da4w", but for quantizing QAT checkpoints
+        Args:
+            `groupsize` (int): a parameter of int4 weight only quantization,
+            it refers to the size of quantization groups which get independent quantization parameters
+            e.g. 32, 64, 128, 256, smaller numbers means more fine grained and higher accuracy,
+            but also higher memory overhead
+    """
+
+    def __init__(self, cfg: DictConfig) -> None:
+        self._device = utils.get_device(device=cfg.device)
+        self._dtype = training.get_dtype(dtype=cfg.dtype, device=self._device)
+        self._quantizer = config.instantiate(cfg.quantizer)
+        self._quantization_mode = training.get_quantizer_mode(self._quantizer)
+        training.set_seed(seed=cfg.seed)
+
+    def load_checkpoint(self, checkpointer_cfg: DictConfig) -> Dict[str, Any]:
+        self._checkpointer = config.instantiate(checkpointer_cfg)
+        checkpoint_dict = self._checkpointer.load_checkpoint()
+        return checkpoint_dict
+
+    def setup(self, cfg: DictConfig) -> None:
+        ckpt_dict = self.load_checkpoint(cfg.checkpointer)
+        self._model = self._setup_model(
+            model_cfg=cfg.model,
+            model_state_dict=ckpt_dict[training.MODEL_KEY],
+        )
+
+    def _setup_model(
+        self,
+        model_cfg: DictConfig,
+        model_state_dict: Dict[str, Any],
+    ) -> nn.Module:
+        with training.set_default_dtype(self._dtype), self._device:
+            model = config.instantiate(model_cfg)
+
+        if "qat" in self._quantization_mode:
+            model = self._quantizer.prepare(model)
+        model.load_state_dict(model_state_dict)
+
+        # Validate model was loaded in with the expected dtype.
+        training.validate_expected_param_dtype(
+            model.named_parameters(), dtype=self._dtype
+        )
+        logger.info(f"Model is initialized with precision {self._dtype}.")
+        return model
+
+    @torch.no_grad()
+    def quantize(self, cfg: DictConfig):
+        t0 = time.perf_counter()
+        if "qat" in self._quantization_mode:
+            self._model = self._quantizer.convert(self._model)
+        else:
+            self._model = self._quantizer.quantize(self._model)
+        t = time.perf_counter() - t0
+        logger.info(f"Time for quantization: {t:.02f} sec")
+        logger.info(f"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB")
+
+    def save_checkpoint(self, cfg: DictConfig):
+        ckpt_dict = self._model.state_dict()
+        file_name = cfg.checkpointer.checkpoint_files[0].split(".")[0]
+
+        output_dir = Path(cfg.checkpointer.output_dir)
+        output_dir.mkdir(exist_ok=True)
+        checkpoint_file = Path.joinpath(
+            output_dir, f"{file_name}-{self._quantization_mode}".rstrip("-qat")
+        ).with_suffix(".pt")
+
+        torch.save(ckpt_dict, checkpoint_file)
+        logger.info(
+            "Model checkpoint of size "
+            f"{os.path.getsize(checkpoint_file) / 1000**3:.2f} GB "
+            f"saved to {checkpoint_file}"
+        )
+
+
+@config.parse
+def main(cfg: DictConfig) -> None:
+    config.log_config(recipe_name="QuantizationRecipe", cfg=cfg)
+    recipe = QuantizationRecipe(cfg=cfg)
+    recipe.setup(cfg=cfg)
+    recipe.quantize(cfg=cfg)
+    recipe.save_checkpoint(cfg=cfg)
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff -ruN marc_original/third_party/torchtune/tests/assets/alpaca_tiny.json marc/third_party/torchtune/tests/assets/alpaca_tiny.json
--- marc_original/third_party/torchtune/tests/assets/alpaca_tiny.json	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/alpaca_tiny.json	2025-02-20 17:49:29.530024213 -0500
@@ -0,0 +1 @@
+[{"instruction": "Give three tips for staying healthy.", "input": "", "output": "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Response:\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule."}, {"instruction": "What are the three primary colors?", "input": "", "output": "The three primary colors are red, blue, and yellow.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat are the three primary colors?\n\n### Response:\nThe three primary colors are red, blue, and yellow."}, {"instruction": "Describe the structure of an atom.", "input": "", "output": "An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDescribe the structure of an atom.\n\n### Response:\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom."}, {"instruction": "How can we reduce air pollution?", "input": "", "output": "There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nHow can we reduce air pollution?\n\n### Response:\nThere are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances."}, {"instruction": "Describe a time when you had to make a difficult decision.", "input": "", "output": "I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client\u2019s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team\u2019s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client\u2019s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDescribe a time when you had to make a difficult decision.\n\n### Response:\nI had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client\u2019s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team\u2019s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client\u2019s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities."}, {"instruction": "Identify the odd one out.", "input": "Twitter, Instagram, Telegram", "output": "Telegram", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nIdentify the odd one out.\n\n### Input:\nTwitter, Instagram, Telegram\n\n### Response:\nTelegram"}, {"instruction": "Explain why the following fraction is equivalent to 1/4", "input": "4/16", "output": "The fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain why the following fraction is equivalent to 1/4\n\n### Input:\n4/16\n\n### Response:\nThe fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4."}, {"instruction": "Write a short story in third person narration about a protagonist who has to make an important career decision.", "input": "", "output": "John was at a crossroads in his life. He had just graduated college and was now facing the big decision of what career to pursue. After much deliberation, he decided that he wanted to be an accountant and help the financially disadvantaged. He had always been good with numbers and enjoyed seeing the tangible results of his work. \n\nJohn enrolled in accounting courses and initially found it quite challenging. He had to learn multiple systems and regulations quickly, but he worked hard and eventually excelled in his studies. After a few years, John started working at an accounting firm in his city. He was eager to put his knowledge of taxes and accounting to use in a real-world setting.\n\nJohn loved his job, as it let him express his creativity in finding strategies to save his clients money. After a few years at the firm, he became a senior accountant and was asked to manage bigger and more challenging cases. He was now a respected figure in the financial industry, but he still remembers when he was just a recent college graduate, unsure of the direction in which his life would take him.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWrite a short story in third person narration about a protagonist who has to make an important career decision.\n\n### Response:\nJohn was at a crossroads in his life. He had just graduated college and was now facing the big decision of what career to pursue. After much deliberation, he decided that he wanted to be an accountant and help the financially disadvantaged. He had always been good with numbers and enjoyed seeing the tangible results of his work. \n\nJohn enrolled in accounting courses and initially found it quite challenging. He had to learn multiple systems and regulations quickly, but he worked hard and eventually excelled in his studies. After a few years, John started working at an accounting firm in his city. He was eager to put his knowledge of taxes and accounting to use in a real-world setting.\n\nJohn loved his job, as it let him express his creativity in finding strategies to save his clients money. After a few years at the firm, he became a senior accountant and was asked to manage bigger and more challenging cases. He was now a respected figure in the financial industry, but he still remembers when he was just a recent college graduate, unsure of the direction in which his life would take him."}, {"instruction": "Render a 3D model of a house", "input": "", "output": "<nooutput> This type of instruction cannot be fulfilled by a GPT model.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nRender a 3D model of a house\n\n### Response:\n<nooutput> This type of instruction cannot be fulfilled by a GPT model."}, {"instruction": "Evaluate this sentence for spelling and grammar mistakes", "input": "He finnished his meal and left the resturant", "output": "He finished his meal and left the restaurant.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nEvaluate this sentence for spelling and grammar mistakes\n\n### Input:\nHe finnished his meal and left the resturant\n\n### Response:\nHe finished his meal and left the restaurant."}, {"instruction": "How did Julius Caesar die?", "input": "", "output": "Julius Caesar was assassinated by a group of up to 60 conspirators, led by Gaius Cassius Longinus and Marcus Junius Brutus, in the Senate House on the Ides of March (15 March) of 44 BC.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nHow did Julius Caesar die?\n\n### Response:\nJulius Caesar was assassinated by a group of up to 60 conspirators, led by Gaius Cassius Longinus and Marcus Junius Brutus, in the Senate House on the Ides of March (15 March) of 44 BC."}, {"instruction": "What is the capital of France?", "input": "", "output": "The capital of France is Paris.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat is the capital of France?\n\n### Response:\nThe capital of France is Paris."}, {"instruction": "Generate a list of ten items a person might need for a camping trip", "input": "", "output": "1. Tent\n2. Sleeping bags\n3. Flashlight\n4. Matches/lighter\n5. Insect repellent\n6. Extra clothes\n7. First aid kit\n8. Camp stove\n9. Cooking utensils\n10. Biodegradable soaps/shampoos", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of ten items a person might need for a camping trip\n\n### Response:\n1. Tent\n2. Sleeping bags\n3. Flashlight\n4. Matches/lighter\n5. Insect repellent\n6. Extra clothes\n7. First aid kit\n8. Camp stove\n9. Cooking utensils\n10. Biodegradable soaps/shampoos"}, {"instruction": "Discuss the causes of the Great Depression", "input": "", "output": "The Great Depression was a severe global economic downturn that lasted from 1929 to 1939. It was caused by several factors, including the stock market crash of 1929, structural weaknesses in the economy, low consumer spending, high unemployment, and a lack of confidence in the banking system. Other contributing factors included overproduction of commodities, reliance on borrowing to finance investments, agricultural overproduction, and a decrease in international trade.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDiscuss the causes of the Great Depression\n\n### Response:\nThe Great Depression was a severe global economic downturn that lasted from 1929 to 1939. It was caused by several factors, including the stock market crash of 1929, structural weaknesses in the economy, low consumer spending, high unemployment, and a lack of confidence in the banking system. Other contributing factors included overproduction of commodities, reliance on borrowing to finance investments, agricultural overproduction, and a decrease in international trade."}, {"instruction": "Classify the following into animals, plants, and minerals", "input": "Oak tree, copper ore, elephant", "output": "Oak tree: Plant\n Copper ore: Mineral\n Elephant: Animal", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nClassify the following into animals, plants, and minerals\n\n### Input:\nOak tree, copper ore, elephant\n\n### Response:\nOak tree: Plant\n Copper ore: Mineral\n Elephant: Animal"}, {"instruction": "Explain the use of word embeddings in Natural Language Processing", "input": "", "output": "Word embeddings are one of the most powerful tools available for Natural Language Processing (NLP). They are mathematical representations of words or phrases in a vector space, allowing similarities between words and the context in which they are used to be measured. Word embeddings are useful for tasks such as sentiment analysis, text classification, predicting the next word in a sequence, and understanding synonyms and analogies. They allow for words to be processed as numerical values, giving machines an easier way to perform NLP tasks.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain the use of word embeddings in Natural Language Processing\n\n### Response:\nWord embeddings are one of the most powerful tools available for Natural Language Processing (NLP). They are mathematical representations of words or phrases in a vector space, allowing similarities between words and the context in which they are used to be measured. Word embeddings are useful for tasks such as sentiment analysis, text classification, predicting the next word in a sequence, and understanding synonyms and analogies. They allow for words to be processed as numerical values, giving machines an easier way to perform NLP tasks."}, {"instruction": "Describe the function of a computer motherboard", "input": "", "output": "A computer motherboard is the main circuit board in a computer. It is the backbone of the system, connecting all other components such as the processor, RAM, storage, graphics card and peripheral ports. It provides communication pathways between all of these components, allowing them to interact and work together. The motherboard is responsible for regulating power, and also houses slots for expansion cards, as well as ports for USB, audio, Ethernet, and other connection types.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDescribe the function of a computer motherboard\n\n### Response:\nA computer motherboard is the main circuit board in a computer. It is the backbone of the system, connecting all other components such as the processor, RAM, storage, graphics card and peripheral ports. It provides communication pathways between all of these components, allowing them to interact and work together. The motherboard is responsible for regulating power, and also houses slots for expansion cards, as well as ports for USB, audio, Ethernet, and other connection types."}, {"instruction": "Reverse engineer this code to create a new version", "input": "def factorialize(num):\n  factorial = 1\n  for i in range(1, num):\n    factorial *= i\n  \n  return factorial", "output": "def factorialize(num):\n  factorial = 1\n  for i in range(num, 0, -1):\n    factorial *= i\n  \n  return factorial", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nReverse engineer this code to create a new version\n\n### Input:\ndef factorialize(num):\n  factorial = 1\n  for i in range(1, num):\n    factorial *= i\n  \n  return factorial\n\n### Response:\ndef factorialize(num):\n  factorial = 1\n  for i in range(num, 0, -1):\n    factorial *= i\n  \n  return factorial"}, {"instruction": "Propose an ethical solution to the problem of data privacy", "input": "", "output": "One ethical solution to the problem of data privacy is to implement an opt-in data collection policy. This means that companies cannot collect and use personal data without the explicit permission of the data owners. This approach gives users the power to determine which data companies can use and for what purpose. Companies should also be transparent about how they use the collected data, so that users can make informed decisions and have control over their own data.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nPropose an ethical solution to the problem of data privacy\n\n### Response:\nOne ethical solution to the problem of data privacy is to implement an opt-in data collection policy. This means that companies cannot collect and use personal data without the explicit permission of the data owners. This approach gives users the power to determine which data companies can use and for what purpose. Companies should also be transparent about how they use the collected data, so that users can make informed decisions and have control over their own data."}, {"instruction": "Generate three verbs that mean the same as \"to apologize\"", "input": "", "output": "Acknowledge, express regret, make amends.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate three verbs that mean the same as \"to apologize\"\n\n### Response:\nAcknowledge, express regret, make amends."}, {"instruction": "What does DNA stand for?", "input": "", "output": "DNA stands for deoxyribonucleic acid.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat does DNA stand for?\n\n### Response:\nDNA stands for deoxyribonucleic acid."}, {"instruction": "Compare and contrast the Cuban Missile Crisis and the Vietnam War.", "input": "", "output": "The Cuban Missile Crisis and the Vietnam War were two of the largest and deadliest conflicts of the 20th century. The Cuban Missile Crisis involved a standoff between the United States and the Soviet Union over Soviet nuclear missile bases in Cuba, while the Vietnam War was a prolonged military conflict in Indochina between the North, which was backed by the Soviet Union and the South, which was backed by the United States. The Cuban Missile Crisis unfolded over a period of 13 days in October 1962, with the resolution coming in the form of a diplomatic agreement. The Vietnam War lasted for 19 years, beginning in 1954 and ending in 1973, with the signing of the Paris Peace Accords. While both of these conflicts were Cold War related, they were also very different in terms of causes, objectives, strategic objectives and outcomes. The Cuban Missile Crisis did not result in any direct military conflict and was resolved primarily through diplomacy and the decision to withdraw nuclear weapons from Cuba. On the other hand, the Vietnam War was a prolonged series of battles resulting in significant losses for both sides. The Vietnam War also had a significant impact on the United States, as it highlighted the realities of a prolonged conflict and the limits of U.S. military intervention. Additionally, the Vietnam War also drastically changed the way public opinion viewed the military and the use of force.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nCompare and contrast the Cuban Missile Crisis and the Vietnam War.\n\n### Response:\nThe Cuban Missile Crisis and the Vietnam War were two of the largest and deadliest conflicts of the 20th century. The Cuban Missile Crisis involved a standoff between the United States and the Soviet Union over Soviet nuclear missile bases in Cuba, while the Vietnam War was a prolonged military conflict in Indochina between the North, which was backed by the Soviet Union and the South, which was backed by the United States. The Cuban Missile Crisis unfolded over a period of 13 days in October 1962, with the resolution coming in the form of a diplomatic agreement. The Vietnam War lasted for 19 years, beginning in 1954 and ending in 1973, with the signing of the Paris Peace Accords. While both of these conflicts were Cold War related, they were also very different in terms of causes, objectives, strategic objectives and outcomes. The Cuban Missile Crisis did not result in any direct military conflict and was resolved primarily through diplomacy and the decision to withdraw nuclear weapons from Cuba. On the other hand, the Vietnam War was a prolonged series of battles resulting in significant losses for both sides. The Vietnam War also had a significant impact on the United States, as it highlighted the realities of a prolonged conflict and the limits of U.S. military intervention. Additionally, the Vietnam War also drastically changed the way public opinion viewed the military and the use of force."}, {"instruction": "Generate a list of random words.", "input": "", "output": "sneeze, conflict, ancestor, thunder, companion, amulet, forge, serpent, magnify, jester, cascade, universe, surge, nausea, oasis, anomaly, pursue, rapture, pagoda, mosaic.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of random words.\n\n### Response:\nsneeze, conflict, ancestor, thunder, companion, amulet, forge, serpent, magnify, jester, cascade, universe, surge, nausea, oasis, anomaly, pursue, rapture, pagoda, mosaic."}, {"instruction": "Transcribe the recording into text.", "input": "Recording about the history of Waterloo", "output": "The Battle of Waterloo, fought on June 18, 1815, was the last major battle of the Napoleonic Wars. It marked the end of Napoleon Bonaparte's rule. The battle was fought between Napoleon Bonaparte's French forces and an allied force of British, Dutch, Belgian and Prussian troops, led by the Duke of Wellington. It was fought near the town of Waterloo in the former Austrian Netherlands, now present-day Belgium. The battle was a defeat for Napoleon's forces and ended Napoleon's rule over France. Napoleon himself was exiled to St. Helena, where he would die in 1821. The battle marked the end of French domination of Europe and the beginning of a period of peace in the region. It was also the last major battle of the Napoleonic Wars and is remembered as one of the most important battles in history. The victory of the allied forces at Waterloo helped to bring an end to French imperial ambitions in Europe. It also established the British as one of the most powerful nations in Europe. The battle was fought with a combination of infantry, cavalry, and artillery tactics, and showed the beginning of the changing nature of warfare. Although the French forces greatly outnumbered the allies, their strategic and tactical mistake led to the loss of the battle. This defeat signaled the end of French imperial power in Europe. The legacy of Waterloo still stands today, and it cemented the British Empire's position for the next one hundred years.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nTranscribe the recording into text.\n\n### Input:\nRecording about the history of Waterloo\n\n### Response:\nThe Battle of Waterloo, fought on June 18, 1815, was the last major battle of the Napoleonic Wars. It marked the end of Napoleon Bonaparte's rule. The battle was fought between Napoleon Bonaparte's French forces and an allied force of British, Dutch, Belgian and Prussian troops, led by the Duke of Wellington. It was fought near the town of Waterloo in the former Austrian Netherlands, now present-day Belgium. The battle was a defeat for Napoleon's forces and ended Napoleon's rule over France. Napoleon himself was exiled to St. Helena, where he would die in 1821. The battle marked the end of French domination of Europe and the beginning of a period of peace in the region. It was also the last major battle of the Napoleonic Wars and is remembered as one of the most important battles in history. The victory of the allied forces at Waterloo helped to bring an end to French imperial ambitions in Europe. It also established the British as one of the most powerful nations in Europe. The battle was fought with a combination of infantry, cavalry, and artillery tactics, and showed the beginning of the changing nature of warfare. Although the French forces greatly outnumbered the allies, their strategic and tactical mistake led to the loss of the battle. This defeat signaled the end of French imperial power in Europe. The legacy of Waterloo still stands today, and it cemented the British Empire's position for the next one hundred years."}, {"instruction": "Who is the world's most famous painter?", "input": "", "output": "The world's most famous painter is generally considered to be the Italian renaissance artist Leonardo da Vinci. His work includes the Mona Lisa and The Last Supper, both of which are considered masterpieces. Da Vinci was also a master of science, inventing the helicopter and the tank, among other inventions. He was also a prolific musician, sculptor, and architect. His work has inspired generations of artists and continues to be revered today.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWho is the world's most famous painter?\n\n### Response:\nThe world's most famous painter is generally considered to be the Italian renaissance artist Leonardo da Vinci. His work includes the Mona Lisa and The Last Supper, both of which are considered masterpieces. Da Vinci was also a master of science, inventing the helicopter and the tank, among other inventions. He was also a prolific musician, sculptor, and architect. His work has inspired generations of artists and continues to be revered today."}, {"instruction": "Explain the concept of cogging torque.", "input": "", "output": "Cogging torque is the torque that is applied to a rotating device when the device experiences resistance from the teeth of the gears that are connected to the device. Cogging torque occurs due to the reaction force generated when the teeth of the gear drive mesh. This force pushes against each other, resulting in a drag force that is transmitted from the gears to the shaft. The magnitude of the cogging torque can vary depending on the size and number of teeth, as well as the gear's material and design. Cogging torque is undesirable in most motorized applications, as it can lead to loss of efficiency. To reduce cogging torque, gear designs are often symmetrical or have certain tooth profiles which reduce the force of contact between the teeth when they come into contact with each other.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain the concept of cogging torque.\n\n### Response:\nCogging torque is the torque that is applied to a rotating device when the device experiences resistance from the teeth of the gears that are connected to the device. Cogging torque occurs due to the reaction force generated when the teeth of the gear drive mesh. This force pushes against each other, resulting in a drag force that is transmitted from the gears to the shaft. The magnitude of the cogging torque can vary depending on the size and number of teeth, as well as the gear's material and design. Cogging torque is undesirable in most motorized applications, as it can lead to loss of efficiency. To reduce cogging torque, gear designs are often symmetrical or have certain tooth profiles which reduce the force of contact between the teeth when they come into contact with each other."}, {"instruction": "Look up the boiling point of water.", "input": "", "output": "The boiling point of water is 100\u00b0C (212\u00b0F), at standard atmospheric pressure. When heated at higher pressures, water's boiling point increases. For example, water will boil at a temperature of 115\u00b0C (239\u00b0F) at an atmospheric pressure of 1 bar. At higher pressures, such as those experienced at the bottom of the ocean, the boiling point of water can be as high as 374\u00b0C (705\u00b0F).", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nLook up the boiling point of water.\n\n### Response:\nThe boiling point of water is 100\u00b0C (212\u00b0F), at standard atmospheric pressure. When heated at higher pressures, water's boiling point increases. For example, water will boil at a temperature of 115\u00b0C (239\u00b0F) at an atmospheric pressure of 1 bar. At higher pressures, such as those experienced at the bottom of the ocean, the boiling point of water can be as high as 374\u00b0C (705\u00b0F)."}, {"instruction": "Describe the life and reign of King Charles II.", "input": "", "output": "King Charles II ascended to the throne of England following the English Civil War and the death of his father, Charles I. He was crowned in 1661 and his reign lasted until his death in 1685. Charles II was also a patron of the arts, working with some of the greatest painters and sculptors of his day. He also encouraged theater and sports and had an interest in sciences such as astronomy and natural history. Charles II was a moderate ruler who respected the civil and religious liberties of his citizens. He was tolerant of other religions and gave Jews and Catholics more rights in the kingdom. Charles II was also a strong supporter of the Royal Navy, which saw England increase its power in the world. He also promoted the development of industry and commerce, which helped to modernize the country. Charles II's reign was an important step in restoring the monarchy of England after the turmoil of the English Civil War.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDescribe the life and reign of King Charles II.\n\n### Response:\nKing Charles II ascended to the throne of England following the English Civil War and the death of his father, Charles I. He was crowned in 1661 and his reign lasted until his death in 1685. Charles II was also a patron of the arts, working with some of the greatest painters and sculptors of his day. He also encouraged theater and sports and had an interest in sciences such as astronomy and natural history. Charles II was a moderate ruler who respected the civil and religious liberties of his citizens. He was tolerant of other religions and gave Jews and Catholics more rights in the kingdom. Charles II was also a strong supporter of the Royal Navy, which saw England increase its power in the world. He also promoted the development of industry and commerce, which helped to modernize the country. Charles II's reign was an important step in restoring the monarchy of England after the turmoil of the English Civil War."}, {"instruction": "Find the area of a circle given its radius.", "input": "Radius = 4", "output": "The area of a circle with a radius of 4 is equal to 12.5664 square units. This is calculated by using the formula A = \u03c0r2, where A is the area, \u03c0 is roughly equal to 3.1416 and r is the radius of the circle.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nFind the area of a circle given its radius.\n\n### Input:\nRadius = 4\n\n### Response:\nThe area of a circle with a radius of 4 is equal to 12.5664 square units. This is calculated by using the formula A = \u03c0r2, where A is the area, \u03c0 is roughly equal to 3.1416 and r is the radius of the circle."}, {"instruction": "Identify the lines of longitude that divides North and South America.", "input": "", "output": "The lines of longitude that divides North and South America is the line of the Panama Canal. This line of longitude cuts through the Isthmus of Panama and separates South America from Central America. It is approximately 800 km (500 mi) in total length and was completed in 1914. The Panama Canal is an important shipping route and serves as an international waterway connecting the Caribbean Sea with the Pacific Ocean.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nIdentify the lines of longitude that divides North and South America.\n\n### Response:\nThe lines of longitude that divides North and South America is the line of the Panama Canal. This line of longitude cuts through the Isthmus of Panama and separates South America from Central America. It is approximately 800 km (500 mi) in total length and was completed in 1914. The Panama Canal is an important shipping route and serves as an international waterway connecting the Caribbean Sea with the Pacific Ocean."}, {"instruction": "Explain the process of cellular respiration in plants.", "input": "", "output": "Cellular respiration in plants is a complex process in which the plant converts glucose into usable energy. The process begins when the plant takes in carbon dioxide from the environment and combines it with water to form glucose molecules using the energy from sunlight. This glucose is then broken down into simpler molecules of carbon dioxide and water. During this process, the energy released is used to produce adenosine triphosphate (ATP) molecules. ATP is used by the plant to power other processes, such as growth and reproduction. Additionally, the ATP can also be broken down further to produce heat energy, which helps to sustain the plant's temperature. The carbon dioxide and water molecules produced in the process are then released back into the environment to complete the cycle.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain the process of cellular respiration in plants.\n\n### Response:\nCellular respiration in plants is a complex process in which the plant converts glucose into usable energy. The process begins when the plant takes in carbon dioxide from the environment and combines it with water to form glucose molecules using the energy from sunlight. This glucose is then broken down into simpler molecules of carbon dioxide and water. During this process, the energy released is used to produce adenosine triphosphate (ATP) molecules. ATP is used by the plant to power other processes, such as growth and reproduction. Additionally, the ATP can also be broken down further to produce heat energy, which helps to sustain the plant's temperature. The carbon dioxide and water molecules produced in the process are then released back into the environment to complete the cycle."}, {"instruction": "Rewrite the following sentence using active voice.", "input": "The news report was read by the captain.", "output": "The captain read the news report.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nRewrite the following sentence using active voice.\n\n### Input:\nThe news report was read by the captain.\n\n### Response:\nThe captain read the news report."}, {"instruction": "Generate a list of adjectives that describe a person as brave.", "input": "", "output": "Courageous, heroic, audacious, vigorous, valorous, resolute, intrepid, bold, brave, daring, tenacious, plucky, stout-hearted, doughty, gallant, fearless, undaunted, spirited, daredevil.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of adjectives that describe a person as brave.\n\n### Response:\nCourageous, heroic, audacious, vigorous, valorous, resolute, intrepid, bold, brave, daring, tenacious, plucky, stout-hearted, doughty, gallant, fearless, undaunted, spirited, daredevil."}, {"instruction": "Outline the consequences of deforestation.", "input": "", "output": "Deforestation is the process of clearing and removing forests. The consequences of deforestation can include soil erosion, reduced water quality, decreased biodiversity, increased carbon dioxide in the atmosphere and loss of habitat for wildlife. Soil erosion occurs when the trees that help retain water and soil are removed, leading to soil particles being washed away. This can cause siltation in rivers and streams, affecting water quality and leading to flooding. Additionally, when trees are removed, wildlife lose their habitat. This can cause species to become endangered, leading to further loss of biodiversity. Deforestation can also lead to increased carbon dioxide in the atmosphere as there are fewer trees to take up the gas during photosynthesis, which can contribute to climate change.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nOutline the consequences of deforestation.\n\n### Response:\nDeforestation is the process of clearing and removing forests. The consequences of deforestation can include soil erosion, reduced water quality, decreased biodiversity, increased carbon dioxide in the atmosphere and loss of habitat for wildlife. Soil erosion occurs when the trees that help retain water and soil are removed, leading to soil particles being washed away. This can cause siltation in rivers and streams, affecting water quality and leading to flooding. Additionally, when trees are removed, wildlife lose their habitat. This can cause species to become endangered, leading to further loss of biodiversity. Deforestation can also lead to increased carbon dioxide in the atmosphere as there are fewer trees to take up the gas during photosynthesis, which can contribute to climate change."}, {"instruction": "Develop a plan to reduce electricity usage in a home.", "input": "", "output": "A plan to reduce electricity usage in a home can include a number of different strategies. These include replacing inefficient lightbulbs with energy-efficient LEDs, unplugging appliances and electronics when not in use, using natural light instead of electric lights where possible, using a touch-free faucet to reduce water heater use, using a smart thermostat to monitor and adjust temperatures automatically, installing a solar hot water heater, and turning off electronics when not in use. Additionally, making sure to install and seal windows and doors correctly to reduce air leakage can also help reduce electricity usage.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDevelop a plan to reduce electricity usage in a home.\n\n### Response:\nA plan to reduce electricity usage in a home can include a number of different strategies. These include replacing inefficient lightbulbs with energy-efficient LEDs, unplugging appliances and electronics when not in use, using natural light instead of electric lights where possible, using a touch-free faucet to reduce water heater use, using a smart thermostat to monitor and adjust temperatures automatically, installing a solar hot water heater, and turning off electronics when not in use. Additionally, making sure to install and seal windows and doors correctly to reduce air leakage can also help reduce electricity usage."}, {"instruction": "Arrange the words in the given sentence to form a grammatically correct sentence.", "input": "quickly the brown fox jumped", "output": "The quick brown fox jumped quickly.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nArrange the words in the given sentence to form a grammatically correct sentence.\n\n### Input:\nquickly the brown fox jumped\n\n### Response:\nThe quick brown fox jumped quickly."}, {"instruction": "Analyze the given text for its tone.", "input": "The world has been greatly impacted by the COVID-19 pandemic and it has drastically changed our lives.", "output": "The tone of the text is one of concern and recognition of the dramatic effects of the COVID-19 pandemic on the world. It is an acknowledgement of the upheaval that the virus has caused and that our lives have been drastically changed.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nAnalyze the given text for its tone.\n\n### Input:\nThe world has been greatly impacted by the COVID-19 pandemic and it has drastically changed our lives.\n\n### Response:\nThe tone of the text is one of concern and recognition of the dramatic effects of the COVID-19 pandemic on the world. It is an acknowledgement of the upheaval that the virus has caused and that our lives have been drastically changed."}, {"instruction": "Use the given data to calculate the median.", "input": "[2, 3, 7, 8, 10]", "output": "The median of the given data is 7.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nUse the given data to calculate the median.\n\n### Input:\n[2, 3, 7, 8, 10]\n\n### Response:\nThe median of the given data is 7."}, {"instruction": "Rewrite the given paragraph in a shorter, easier to understand form.", "input": "Although it is generally accepted that the internet has allowed us to connect with people all over the world, there are still those people who are not familiar with its basic functions, who don\u2019t understand why it has become so commonplace, or what its true capabilities are.", "output": "The internet has allowed us to connect globally, even though some people are unfamiliar with its functions, don't comprehend its widespread use, and don't know its full potential.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nRewrite the given paragraph in a shorter, easier to understand form.\n\n### Input:\nAlthough it is generally accepted that the internet has allowed us to connect with people all over the world, there are still those people who are not familiar with its basic functions, who don\u2019t understand why it has become so commonplace, or what its true capabilities are.\n\n### Response:\nThe internet has allowed us to connect globally, even though some people are unfamiliar with its functions, don't comprehend its widespread use, and don't know its full potential."}, {"instruction": "Design a logo for a website about recycling.", "input": "", "output": "The logo should feature a green motif that is recognizable and memorable, such as a recycle symbol or a stylized representation of a recycling bin. The logo should also include text that communicates the website's purpose, such as \"Recycling\" or \"Go Green\".", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDesign a logo for a website about recycling.\n\n### Response:\nThe logo should feature a green motif that is recognizable and memorable, such as a recycle symbol or a stylized representation of a recycling bin. The logo should also include text that communicates the website's purpose, such as \"Recycling\" or \"Go Green\"."}, {"instruction": "Generate a poem that expresses joy.", "input": "", "output": "Joy flows through me like a river clear,\nBringing laughter and bliss to my soul to hear.\n\nThe world is bright and wonderful, I can see,\nMaking every moment something happy and free.\n\nMy heart feels light, my cares have flown,\nAs I discover what happiness gives me alone.\n\nI open my arms and accept the joy,\nBringing life to my song, so sweet like a toy.\n\nEvery moment is a precious gift,\nThat I savor and never let drift.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a poem that expresses joy.\n\n### Response:\nJoy flows through me like a river clear,\nBringing laughter and bliss to my soul to hear.\n\nThe world is bright and wonderful, I can see,\nMaking every moment something happy and free.\n\nMy heart feels light, my cares have flown,\nAs I discover what happiness gives me alone.\n\nI open my arms and accept the joy,\nBringing life to my song, so sweet like a toy.\n\nEvery moment is a precious gift,\nThat I savor and never let drift."}, {"instruction": "Convert the given equation into an algebraic expression.", "input": "3x+5y=9", "output": "3x + 5y - 9 = 0", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nConvert the given equation into an algebraic expression.\n\n### Input:\n3x+5y=9\n\n### Response:\n3x + 5y - 9 = 0"}, {"instruction": "Brainstorm possible solutions to reduce water pollution.", "input": "", "output": "- Implement stricter regulations on industrial waste disposal\n- Increase the use of green technology in wastewater treatment\n- Provide incentives to businesses that reduce their water consumption and waste generation\n- Educate citizens about water conservation and pollution prevention\n- Offer recycling programs to reduce plastic waste\n- Replace disposable plastics with biodegradable alternatives\n- Improve sewer systems\n- Re-introduce native plant species\n- Foster artificial wetlands and green infrastructure\n- Provide filtration systems at water treatment plants\n- Implement best practices for agricultural runoff\n- Increase research and monitoring of water quality\n- Establish buffer zones between aquatic environments and land-based activities.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nBrainstorm possible solutions to reduce water pollution.\n\n### Response:\n- Implement stricter regulations on industrial waste disposal\n- Increase the use of green technology in wastewater treatment\n- Provide incentives to businesses that reduce their water consumption and waste generation\n- Educate citizens about water conservation and pollution prevention\n- Offer recycling programs to reduce plastic waste\n- Replace disposable plastics with biodegradable alternatives\n- Improve sewer systems\n- Re-introduce native plant species\n- Foster artificial wetlands and green infrastructure\n- Provide filtration systems at water treatment plants\n- Implement best practices for agricultural runoff\n- Increase research and monitoring of water quality\n- Establish buffer zones between aquatic environments and land-based activities."}, {"instruction": "Explain why the given definition is wrong.", "input": "A mole is an animal that lives underground.", "output": "The definition is wrong because a mole is actually a small mammal that lives above ground and feeds on insects. It is most distinguishable by its long snout and wide, paddle-like feet. It does, however, often dig burrows in which it lives and shelters.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain why the given definition is wrong.\n\n### Input:\nA mole is an animal that lives underground.\n\n### Response:\nThe definition is wrong because a mole is actually a small mammal that lives above ground and feeds on insects. It is most distinguishable by its long snout and wide, paddle-like feet. It does, however, often dig burrows in which it lives and shelters."}, {"instruction": "Variable x is defined as \u201c4x + 2y = 10\u201d. Find the value of x.", "input": "", "output": "The value of x is 2. To find the value, simplify the equation by subtracting 2y from both sides, giving 4x = 10; dividing both sides by 4, giving x = 2/4, which is equal to 2.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nVariable x is defined as \u201c4x + 2y = 10\u201d. Find the value of x.\n\n### Response:\nThe value of x is 2. To find the value, simplify the equation by subtracting 2y from both sides, giving 4x = 10; dividing both sides by 4, giving x = 2/4, which is equal to 2."}, {"instruction": "Write a short paragraph about the given topic.", "input": "The importance of using renewable energy", "output": "The use of renewable energy is growing rapidly in relevance and importance as the world looks towards solutions to combat climate change. Renewable energy sources, such as solar, wind, and hydropower, are sustainable and have a much lower environmental impact than traditional energy sources like coal and oil. Switching to renewable energy can reduce carbon emissions, improve air quality, lessen the dependence on non-renewable resources, and provide a stable and secure energy future. Renewable energy is essential for achieving a sustainable future and should be an important part of any country\u2019s energy portfolio.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nWrite a short paragraph about the given topic.\n\n### Input:\nThe importance of using renewable energy\n\n### Response:\nThe use of renewable energy is growing rapidly in relevance and importance as the world looks towards solutions to combat climate change. Renewable energy sources, such as solar, wind, and hydropower, are sustainable and have a much lower environmental impact than traditional energy sources like coal and oil. Switching to renewable energy can reduce carbon emissions, improve air quality, lessen the dependence on non-renewable resources, and provide a stable and secure energy future. Renewable energy is essential for achieving a sustainable future and should be an important part of any country\u2019s energy portfolio."}, {"instruction": "Explain the concept of artificial intelligence in simple terms.", "input": "", "output": "Artificial Intelligence (AI) is the ability of machines and computers to make decisions and complete tasks similar to what humans can do. AI can learn from data, recognize patterns, and make predictions. AI technology can automate tedious and complex processes, helping to reduce human effort and errors, and enable more accurate and efficient decisions. AI can be used in a wide range of applications, from robotics and autonomous vehicles to healthcare and finance. It is increasingly becoming an integral part of everyday life.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain the concept of artificial intelligence in simple terms.\n\n### Response:\nArtificial Intelligence (AI) is the ability of machines and computers to make decisions and complete tasks similar to what humans can do. AI can learn from data, recognize patterns, and make predictions. AI technology can automate tedious and complex processes, helping to reduce human effort and errors, and enable more accurate and efficient decisions. AI can be used in a wide range of applications, from robotics and autonomous vehicles to healthcare and finance. It is increasingly becoming an integral part of everyday life."}, {"instruction": "Design an app for a delivery company.", "input": "", "output": "The app should allow customers to order items from local businesses and nearby store, place delivery requests with detailed delivery requirements, and track their orders. It should also allow customers to easily find and pay for services. The app should allow businesses to manage orders, track deliveries and drivers, set delivery boundaries, and set prices. It should also have features to help businesses promote their products and services, as well as options to provide customer feedback and support.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDesign an app for a delivery company.\n\n### Response:\nThe app should allow customers to order items from local businesses and nearby store, place delivery requests with detailed delivery requirements, and track their orders. It should also allow customers to easily find and pay for services. The app should allow businesses to manage orders, track deliveries and drivers, set delivery boundaries, and set prices. It should also have features to help businesses promote their products and services, as well as options to provide customer feedback and support."}, {"instruction": "Summarize the given passage.", "input": "A recent study showed that global climate change is one of the most important challenges facing the world today. The consequences of global warming include rising sea levels, extreme weather events and biodiversity loss. Climate change is caused by human activities such as burning fossil fuels, and if unchecked will have long-term and devastating impacts on the planet.", "output": "A recent study revealed that global climate change is one of the world\u2019s most pressing issues. In addition to rising sea levels, extreme weather events and biodiversity loss, it is caused largely by human activities like burning fossil fuels. If left unchecked, climate change will have far-reaching and devastating consequences.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nSummarize the given passage.\n\n### Input:\nA recent study showed that global climate change is one of the most important challenges facing the world today. The consequences of global warming include rising sea levels, extreme weather events and biodiversity loss. Climate change is caused by human activities such as burning fossil fuels, and if unchecked will have long-term and devastating impacts on the planet.\n\n### Response:\nA recent study revealed that global climate change is one of the world\u2019s most pressing issues. In addition to rising sea levels, extreme weather events and biodiversity loss, it is caused largely by human activities like burning fossil fuels. If left unchecked, climate change will have far-reaching and devastating consequences."}, {"instruction": "Extract the facts from the paragraph.", "input": "Online education continues to become more popular for schools and students alike. Its advantages are generally lower costs, less commitment and the ability to study at a time, place and pace that suits the student.", "output": "- Online education is becoming increasingly popular.\n- It has several advantages such as lower costs, less commitment and the ability to study at one\u2019s own time and pace.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nExtract the facts from the paragraph.\n\n### Input:\nOnline education continues to become more popular for schools and students alike. Its advantages are generally lower costs, less commitment and the ability to study at a time, place and pace that suits the student.\n\n### Response:\n- Online education is becoming increasingly popular.\n- It has several advantages such as lower costs, less commitment and the ability to study at one\u2019s own time and pace."}, {"instruction": "Edit the following sentence to make it more concise.", "input": "He ran to the bus stop in order to catch the bus that was due to arrive in five minutes.", "output": "He ran to the bus stop, due to arrive in five minutes.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nEdit the following sentence to make it more concise.\n\n### Input:\nHe ran to the bus stop in order to catch the bus that was due to arrive in five minutes.\n\n### Response:\nHe ran to the bus stop, due to arrive in five minutes."}, {"instruction": "Generate a poem with 10 lines.", "input": "", "output": "The evening light shimmers on the shore\nSoftly the waves echoes around and more \nAs I bask in the sun, my worries are all gone\nThe sound of seagulls I now foolishly ignore \nGlistening sand, beckons me with a silent plea \nGlistening seawater, cool to the touch and refreshingly free \nThe evening brings peace, yet I can't find any \nBut maybe in the morning there'll be time for me\nMy bottled peacefulness, I uncork and pour \nThe sound of the ocean, lulls me even more \nAnd for just a moment I close my eyes and behold \nThe vastness of the ocean, to my soul I now unfold.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a poem with 10 lines.\n\n### Response:\nThe evening light shimmers on the shore\nSoftly the waves echoes around and more \nAs I bask in the sun, my worries are all gone\nThe sound of seagulls I now foolishly ignore \nGlistening sand, beckons me with a silent plea \nGlistening seawater, cool to the touch and refreshingly free \nThe evening brings peace, yet I can't find any \nBut maybe in the morning there'll be time for me\nMy bottled peacefulness, I uncork and pour \nThe sound of the ocean, lulls me even more \nAnd for just a moment I close my eyes and behold \nThe vastness of the ocean, to my soul I now unfold."}, {"instruction": "Convert from celsius to fahrenheit.", "input": "Temperature in Celsius: 15", "output": "Temperature in Fahrenheit: 59", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nConvert from celsius to fahrenheit.\n\n### Input:\nTemperature in Celsius: 15\n\n### Response:\nTemperature in Fahrenheit: 59"}, {"instruction": "Arrange the given numbers in ascending order.", "input": "2, 4, 0, 8, 3", "output": "0, 2, 3, 4, 8", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nArrange the given numbers in ascending order.\n\n### Input:\n2, 4, 0, 8, 3\n\n### Response:\n0, 2, 3, 4, 8"}, {"instruction": "Calculate the total surface area of a cube with a side length of 5 cm.", "input": "", "output": "Total surface area of the cube: 150 cm^2", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nCalculate the total surface area of a cube with a side length of 5 cm.\n\n### Response:\nTotal surface area of the cube: 150 cm^2"}, {"instruction": "What is the force on a 1 kg mass due to the gravitational force?", "input": "", "output": "The force on a 1 kg mass due to the gravitational force is 9.8 N.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat is the force on a 1 kg mass due to the gravitational force?\n\n### Response:\nThe force on a 1 kg mass due to the gravitational force is 9.8 N."}, {"instruction": "Provide one example for a cultural practice.", "input": "", "output": "An example of a cultural practice is the Japanese tea ceremony.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nProvide one example for a cultural practice.\n\n### Response:\nAn example of a cultural practice is the Japanese tea ceremony."}, {"instruction": "Given a set of numbers, find the maximum value.", "input": "Set: {10, 3, 25, 6, 16}", "output": "The maximum value is 25.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGiven a set of numbers, find the maximum value.\n\n### Input:\nSet: {10, 3, 25, 6, 16}\n\n### Response:\nThe maximum value is 25."}, {"instruction": "Give two examples of a liquid.", "input": "", "output": "Two examples of liquids are water and oil.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGive two examples of a liquid.\n\n### Response:\nTwo examples of liquids are water and oil."}, {"instruction": "What is the product of 6 and 2?", "input": "", "output": "The product of 6 and 2 is 12.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat is the product of 6 and 2?\n\n### Response:\nThe product of 6 and 2 is 12."}, {"instruction": "What type of plant is a skunk cabbage?", "input": "", "output": "Skunk cabbage is a type of perennial flowering plant.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat type of plant is a skunk cabbage?\n\n### Response:\nSkunk cabbage is a type of perennial flowering plant."}, {"instruction": "Convert the given binary number to its decimal equivalent.", "input": "101101", "output": "The decimal equivalent of 101101 is 45.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nConvert the given binary number to its decimal equivalent.\n\n### Input:\n101101\n\n### Response:\nThe decimal equivalent of 101101 is 45."}, {"instruction": "Name two types of desert biomes.", "input": "", "output": "Two types of desert biomes are xeric and subpolar deserts.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nName two types of desert biomes.\n\n### Response:\nTwo types of desert biomes are xeric and subpolar deserts."}, {"instruction": "Given a sentence, convert it into passive voice.", "input": "The dog barked at the alley cat.", "output": "The alley cat was barked at by the dog.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGiven a sentence, convert it into passive voice.\n\n### Input:\nThe dog barked at the alley cat.\n\n### Response:\nThe alley cat was barked at by the dog."}, {"instruction": "Transform the following sentence into the passive voice", "input": "I bought a book", "output": "A book was bought by me.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nTransform the following sentence into the passive voice\n\n### Input:\nI bought a book\n\n### Response:\nA book was bought by me."}, {"instruction": "Create a dialog between two people who are discussing a scientific phenomenon", "input": "Hydropower", "output": "Person 1: Did you know that hydropower is now being used to generate electricity?\nPerson 2: Really? That\u2019s incredible! How does it work?\nPerson: 1: Well, hydropower utilizes the energy of moving water to generate electricity. By capturing and harnessing the energy of falling or flowing water, hydropower can be used for efficient electricity generation.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nCreate a dialog between two people who are discussing a scientific phenomenon\n\n### Input:\nHydropower\n\n### Response:\nPerson 1: Did you know that hydropower is now being used to generate electricity?\nPerson 2: Really? That\u2019s incredible! How does it work?\nPerson: 1: Well, hydropower utilizes the energy of moving water to generate electricity. By capturing and harnessing the energy of falling or flowing water, hydropower can be used for efficient electricity generation."}, {"instruction": "Identify the most suitable adverb for the following sentence", "input": "She works", "output": "She works diligently.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nIdentify the most suitable adverb for the following sentence\n\n### Input:\nShe works\n\n### Response:\nShe works diligently."}, {"instruction": "Find the main idea of the following passage", "input": "Climate change is an increasingly contentious global issue. It is already having a drastic effect on the environment and human lives. We are already seeing an increase in extreme weather events and rising sea levels, both of which have a major impact on the planet.", "output": "The main idea of the passage is that climate change is a global issue that is having a detrimental effect on the environment and has caused an increase in extreme weather events and rising sea levels.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nFind the main idea of the following passage\n\n### Input:\nClimate change is an increasingly contentious global issue. It is already having a drastic effect on the environment and human lives. We are already seeing an increase in extreme weather events and rising sea levels, both of which have a major impact on the planet.\n\n### Response:\nThe main idea of the passage is that climate change is a global issue that is having a detrimental effect on the environment and has caused an increase in extreme weather events and rising sea levels."}, {"instruction": "Analyze the tone of the following sentences", "input": "I am so upset with you right now", "output": "The tone of the sentences is angry and accusatory.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nAnalyze the tone of the following sentences\n\n### Input:\nI am so upset with you right now\n\n### Response:\nThe tone of the sentences is angry and accusatory."}, {"instruction": "Construct an argument to defend the following statement", "input": "Alternative energy sources are critical to solving the climate crisis", "output": "Alternative energy sources are absolutely vital in order to mitigate the most detrimental impacts of climate change. As the world transitions away from fossil fuels and towards more sustainable sources of energy, such as wind, solar, and hydropower, we can reduce the number of greenhouse gases in the atmosphere and prevent further warming of our planet. Additionally, using renewable and alternative energies sources will create thousands of jobs and can help to create a healthy, stable economy. Investing in alternative energy sources is a sound decision and a necessary step if we want to effectively address the climate crisis.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nConstruct an argument to defend the following statement\n\n### Input:\nAlternative energy sources are critical to solving the climate crisis\n\n### Response:\nAlternative energy sources are absolutely vital in order to mitigate the most detrimental impacts of climate change. As the world transitions away from fossil fuels and towards more sustainable sources of energy, such as wind, solar, and hydropower, we can reduce the number of greenhouse gases in the atmosphere and prevent further warming of our planet. Additionally, using renewable and alternative energies sources will create thousands of jobs and can help to create a healthy, stable economy. Investing in alternative energy sources is a sound decision and a necessary step if we want to effectively address the climate crisis."}, {"instruction": "Convert the following sentence into the present continuous tense", "input": "He reads books", "output": "He is reading books.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nConvert the following sentence into the present continuous tense\n\n### Input:\nHe reads books\n\n### Response:\nHe is reading books."}, {"instruction": "Give an example of a metaphor that uses the following object", "input": "Stars", "output": "The stars were twinkling diamonds in the night sky.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive an example of a metaphor that uses the following object\n\n### Input:\nStars\n\n### Response:\nThe stars were twinkling diamonds in the night sky."}, {"instruction": "Describe the following person", "input": "John", "output": "John is a tall, dark-haired man with a kind smile and a twinkle in his eyes. He has a gentle, caring nature, and he always strives to understand others. He has a passion for learning and exploring new places, experiences and cultures.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nDescribe the following person\n\n### Input:\nJohn\n\n### Response:\nJohn is a tall, dark-haired man with a kind smile and a twinkle in his eyes. He has a gentle, caring nature, and he always strives to understand others. He has a passion for learning and exploring new places, experiences and cultures."}, {"instruction": "Construct a mathematical problem with the following numbers", "input": "21, 4", "output": "If 21 pieces of candy are divided among 4 people, how many pieces of candy does each person get?", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nConstruct a mathematical problem with the following numbers\n\n### Input:\n21, 4\n\n### Response:\nIf 21 pieces of candy are divided among 4 people, how many pieces of candy does each person get?"}, {"instruction": "Aim to reduce the following sentence without changing its meaning", "input": "When he came in, he woke everyone up", "output": "He woke everyone on entering.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nAim to reduce the following sentence without changing its meaning\n\n### Input:\nWhen he came in, he woke everyone up\n\n### Response:\nHe woke everyone on entering."}, {"instruction": "Identify the conjunctions in the following sentence", "input": "She wrote a letter and sealed it", "output": "The conjunctions in the sentence are \"and\".", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nIdentify the conjunctions in the following sentence\n\n### Input:\nShe wrote a letter and sealed it\n\n### Response:\nThe conjunctions in the sentence are \"and\"."}, {"instruction": "Rewrite the following sentence in the third person", "input": "I am anxious", "output": "She is anxious.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nRewrite the following sentence in the third person\n\n### Input:\nI am anxious\n\n### Response:\nShe is anxious."}, {"instruction": "Generate a list of business ideas for a food delivery service.", "input": "", "output": "- Offer discounts and loyalty programs for frequent customers.\n- Partner with local restaurants and promote their food.\n- Train delivery personnel to handle orders and provide good customer service.\n- Use mobile apps and websites for ordering.\n- Offer diverse payment options for customers.\n- Allow customers to customize their orders.\n- Pack food in recyclable and compostable packaging.\n- Launch a subscription model for regular customers.\n- Offer special services like food suggestions based on customers' preferences.\n- Provide contactless delivery and curbside pickup.\n- Provide catering services for large orders and special events.\n- Use online marketing and social media platforms to reach more customers.\n- Create incentives to encourage customers to try new dishes.\n- Create meal kits with all the necessary ingredients and a recipe.\n- Make use of food delivery robots and drones.\n- Offer a loyalty points system for customers.\n- Collaborate with local farmers to source produce and ingredients.\n- Offer vegetarian, vegan, and gluten-free options.\n- Allow customers to select the delivery time.\n- Implement AI-powered recommendations for customers.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of business ideas for a food delivery service.\n\n### Response:\n- Offer discounts and loyalty programs for frequent customers.\n- Partner with local restaurants and promote their food.\n- Train delivery personnel to handle orders and provide good customer service.\n- Use mobile apps and websites for ordering.\n- Offer diverse payment options for customers.\n- Allow customers to customize their orders.\n- Pack food in recyclable and compostable packaging.\n- Launch a subscription model for regular customers.\n- Offer special services like food suggestions based on customers' preferences.\n- Provide contactless delivery and curbside pickup.\n- Provide catering services for large orders and special events.\n- Use online marketing and social media platforms to reach more customers.\n- Create incentives to encourage customers to try new dishes.\n- Create meal kits with all the necessary ingredients and a recipe.\n- Make use of food delivery robots and drones.\n- Offer a loyalty points system for customers.\n- Collaborate with local farmers to source produce and ingredients.\n- Offer vegetarian, vegan, and gluten-free options.\n- Allow customers to select the delivery time.\n- Implement AI-powered recommendations for customers."}, {"instruction": "Edit the following sentence to improve clarity and flow.", "input": "The restaurant has great food though the prices are a bit hefty", "output": "The restaurant offers great food but at a slightly higher price.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nEdit the following sentence to improve clarity and flow.\n\n### Input:\nThe restaurant has great food though the prices are a bit hefty\n\n### Response:\nThe restaurant offers great food but at a slightly higher price."}, {"instruction": "Imagine you are speaking with a customer who is unsatisfied with the product they bought from you. Construct a response that diffuses the situation.", "input": "", "output": "I'm sorry that you're not happy with your purchase. We want all of our customers to be satisfied, so I'd like to offer you a full refund, as well as a 15% discount for your next purchase. Is there anything else I can do to make this right for you?", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nImagine you are speaking with a customer who is unsatisfied with the product they bought from you. Construct a response that diffuses the situation.\n\n### Response:\nI'm sorry that you're not happy with your purchase. We want all of our customers to be satisfied, so I'd like to offer you a full refund, as well as a 15% discount for your next purchase. Is there anything else I can do to make this right for you?"}, {"instruction": "Explain the concept of a bubble sort algorithm to a non-technical audience.", "input": "", "output": "A bubble sort algorithm is a type of sorting algorithm that is used to sort elements in an array. It works by looking at each element of the array and comparing it to the next element. If the first element is bigger than the second element, they are swapped. This process is repeated until the whole array is sorted. This type of sorting is one of the simplest sorting algorithms, but it can be slow if the array has many elements.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain the concept of a bubble sort algorithm to a non-technical audience.\n\n### Response:\nA bubble sort algorithm is a type of sorting algorithm that is used to sort elements in an array. It works by looking at each element of the array and comparing it to the next element. If the first element is bigger than the second element, they are swapped. This process is repeated until the whole array is sorted. This type of sorting is one of the simplest sorting algorithms, but it can be slow if the array has many elements."}, {"instruction": "Classify the following incident as a breach of protocol. Output 1 for breach, and 0 for no breach.", "input": "Using a school laptop for personal use", "output": "1", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nClassify the following incident as a breach of protocol. Output 1 for breach, and 0 for no breach.\n\n### Input:\nUsing a school laptop for personal use\n\n### Response:\n1"}, {"instruction": "Given the following input, construct a creative story.", "input": "A magic bow and arrow", "output": "Once upon a time, there lived a young girl named Alexa who was gifted with an incredible magical bow and arrow. Whenever she pulled back her bow and let an arrow fly, wherever it landed, something extraordinary happened. Flowers bloomed, oceans calmed, and the sun shone brighter. Alexa's bow and arrow were so powerful, that it could make even the most impossible things possible. One day, during a great storm, Alexa used her magical bow and arrow to bring calm and harmony to her hometown. She was praised and celebrated by the whole town, and she soon became a symbol of hope and peace.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGiven the following input, construct a creative story.\n\n### Input:\nA magic bow and arrow\n\n### Response:\nOnce upon a time, there lived a young girl named Alexa who was gifted with an incredible magical bow and arrow. Whenever she pulled back her bow and let an arrow fly, wherever it landed, something extraordinary happened. Flowers bloomed, oceans calmed, and the sun shone brighter. Alexa's bow and arrow were so powerful, that it could make even the most impossible things possible. One day, during a great storm, Alexa used her magical bow and arrow to bring calm and harmony to her hometown. She was praised and celebrated by the whole town, and she soon became a symbol of hope and peace."}, {"instruction": "Generate a list of methods to reduce food waste.", "input": "", "output": "- Shop smarter and only buy what you need.\n- Save leftovers for later and freeze them if necessary. \n- Monitor expiration dates and consume items before they go bad.\n- Participate in community or public food sharing initiatives.\n- Compost leftover food scraps.\n- Reuse containers, jars, and bags.\n- Make use of restaurant discounts to reduce over-ordering. \n- Donate foods that have short expiration dates to local food banks. \n- Buy more produce that is close to the expiration date and prepare it soon.\n- Order small portions while dining out.\n- Educate yourself and others about food waste.\n- Plan meals in advance and base your grocery list on the meals. \n- Store food correctly to reduce spoilage.\n- Make sure your fridge is at the right temperature.\n- Have a system to track leftovers in the fridge. \n- Eat all edible parts of food, including stems and skins.\n- Use creative recipes to use the entire food item.\n- Use food that would otherwise be thrown away for animal feed.\n- Take part in campaigns to make food sustainability an issue.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of methods to reduce food waste.\n\n### Response:\n- Shop smarter and only buy what you need.\n- Save leftovers for later and freeze them if necessary. \n- Monitor expiration dates and consume items before they go bad.\n- Participate in community or public food sharing initiatives.\n- Compost leftover food scraps.\n- Reuse containers, jars, and bags.\n- Make use of restaurant discounts to reduce over-ordering. \n- Donate foods that have short expiration dates to local food banks. \n- Buy more produce that is close to the expiration date and prepare it soon.\n- Order small portions while dining out.\n- Educate yourself and others about food waste.\n- Plan meals in advance and base your grocery list on the meals. \n- Store food correctly to reduce spoilage.\n- Make sure your fridge is at the right temperature.\n- Have a system to track leftovers in the fridge. \n- Eat all edible parts of food, including stems and skins.\n- Use creative recipes to use the entire food item.\n- Use food that would otherwise be thrown away for animal feed.\n- Take part in campaigns to make food sustainability an issue."}, {"instruction": "Propose an idea for a self-driving car.", "input": "", "output": "My idea for a self-driving car is to develop an AI system that uses machine learning to help the car recognize its environment. The AI system will also be able to learn from its mistakes and increase its accuracy. Additionally, the self-driving car will be able to detect potential hazards and warn the driver, or take corrective action if necessary. The car will also be equipped with an emergency button, in case the driver needs to take manual control of the vehicle. Furthermore, the car will be capable of automated parking and will be able to drive itself to a nearby charging station when the battery is low. Finally, the software will be able to connect with smartphones, so the user can access their navigation system and entertainment options even while on the move.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nPropose an idea for a self-driving car.\n\n### Response:\nMy idea for a self-driving car is to develop an AI system that uses machine learning to help the car recognize its environment. The AI system will also be able to learn from its mistakes and increase its accuracy. Additionally, the self-driving car will be able to detect potential hazards and warn the driver, or take corrective action if necessary. The car will also be equipped with an emergency button, in case the driver needs to take manual control of the vehicle. Furthermore, the car will be capable of automated parking and will be able to drive itself to a nearby charging station when the battery is low. Finally, the software will be able to connect with smartphones, so the user can access their navigation system and entertainment options even while on the move."}, {"instruction": "Explain the concept of a linear regression model using simple language.", "input": "", "output": "A linear regression model is a type of machine learning model that can be used to predict or estimate the value of a continuous numerical variable from a set of input variables. In this model, a line is fitted to a set of data points such that the sum of the squared distance of the data points from the line is minimized. This line of best fit can then be used to estimate the value of the output variable based on the values of the input variables.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain the concept of a linear regression model using simple language.\n\n### Response:\nA linear regression model is a type of machine learning model that can be used to predict or estimate the value of a continuous numerical variable from a set of input variables. In this model, a line is fitted to a set of data points such that the sum of the squared distance of the data points from the line is minimized. This line of best fit can then be used to estimate the value of the output variable based on the values of the input variables."}, {"instruction": "Construct a list of tips to help reduce stress.", "input": "", "output": "- Take a time out and acquire some rest when feeling overwhelmed. \n- Make time for yourself and learn to relax. \n- Exercise regularly and get plenty of sleep. \n- Listen to music you like, or take a warm bath. \n- Stick to a healthy diet full of essential vitamins and minerals.\n- Reach out to friends and family for support.\n- Try aromatherapy or yoga. \n- Do something creative like drawing, painting, or writing. \n- Engage in activities that bring you joy and peace of mind. \n- Avoid drugs or excessive amounts of alcohol. \n- Practice deep breathing and progressive muscle relaxation. \n- Take up a new hobby to stay occupied. \n- Break big tasks down into smaller, more manageable tasks. \n- Practice positive self-talk to keep your morale high. \n- Talk to a therapist or trusted mentor if necessary. \n- Make time for fun activities and social events. \n- Prioritize tasks and create a weekly schedule. \n- Create a plan of action and identify solutions to problems. \n- Learn to forgive yourself and others.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nConstruct a list of tips to help reduce stress.\n\n### Response:\n- Take a time out and acquire some rest when feeling overwhelmed. \n- Make time for yourself and learn to relax. \n- Exercise regularly and get plenty of sleep. \n- Listen to music you like, or take a warm bath. \n- Stick to a healthy diet full of essential vitamins and minerals.\n- Reach out to friends and family for support.\n- Try aromatherapy or yoga. \n- Do something creative like drawing, painting, or writing. \n- Engage in activities that bring you joy and peace of mind. \n- Avoid drugs or excessive amounts of alcohol. \n- Practice deep breathing and progressive muscle relaxation. \n- Take up a new hobby to stay occupied. \n- Break big tasks down into smaller, more manageable tasks. \n- Practice positive self-talk to keep your morale high. \n- Talk to a therapist or trusted mentor if necessary. \n- Make time for fun activities and social events. \n- Prioritize tasks and create a weekly schedule. \n- Create a plan of action and identify solutions to problems. \n- Learn to forgive yourself and others."}, {"instruction": "Write a review of a recent movie you watched.", "input": "Parasite (2019)", "output": "I recently watched the movie Parasite, directed by Bong Joon-Ho, and I was absolutely blown away. This movie is a masterful blend of genres\u2014it's both a black comedy and a commentary on classism, with a twist ending that no one could see coming. The acting is fantastic, particularly Song Kang-ho as the head of the poor household, and the cinematography is stunning. The movie has a perfect balance of humor and thought-provoking drama, and it will leave you talking about the ending for days afterwards. Overall, I highly recommend Parasite; it is an exceptional movie and an experience you won't soon forget.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nWrite a review of a recent movie you watched.\n\n### Input:\nParasite (2019)\n\n### Response:\nI recently watched the movie Parasite, directed by Bong Joon-Ho, and I was absolutely blown away. This movie is a masterful blend of genres\u2014it's both a black comedy and a commentary on classism, with a twist ending that no one could see coming. The acting is fantastic, particularly Song Kang-ho as the head of the poor household, and the cinematography is stunning. The movie has a perfect balance of humor and thought-provoking drama, and it will leave you talking about the ending for days afterwards. Overall, I highly recommend Parasite; it is an exceptional movie and an experience you won't soon forget."}, {"instruction": "Create a Twitter post to promote your new product.", "input": "Alisha's All Natural Jam", "output": "Introducing Alisha's All Natural Jam, handmade with love using only the finest ingredients! Get ready for a delicious treat that will give your tastebuds a flavor explosion! Try our five amazing flavors: Strawberry, Blueberry, Peach, Raspberry, and Blackberry. Buy now and snack with the world! #snacktime #allnatural #jam #tastebuds #freshfruit #homemade #delicious #salttw", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nCreate a Twitter post to promote your new product.\n\n### Input:\nAlisha's All Natural Jam\n\n### Response:\nIntroducing Alisha's All Natural Jam, handmade with love using only the finest ingredients! Get ready for a delicious treat that will give your tastebuds a flavor explosion! Try our five amazing flavors: Strawberry, Blueberry, Peach, Raspberry, and Blackberry. Buy now and snack with the world! #snacktime #allnatural #jam #tastebuds #freshfruit #homemade #delicious #salttw"}, {"instruction": "Given the following input, classify the verb as transitive or intransitive.", "input": "improve", "output": "Transitive.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGiven the following input, classify the verb as transitive or intransitive.\n\n### Input:\nimprove\n\n### Response:\nTransitive."}, {"instruction": "Generate a list of marketing strategies to promote a new mobile app.", "input": "", "output": "- Implement paid campaigns on social media platforms.\n- Utilize influencer marketing.\n- Reach out to tech bloggers and websites.\n- Share the app through word of mouth among friends.\n- Create demo videos and tutorials to explain the app's features.\n- Offer discounts and special incentives to early adopters.\n- Build a basic website to explain the app's purpose and features. \n- Take advantage of app store optimization techniques.\n- Send out emails and newsletters to inform new users about the app.\n- Aim for awards and good reviews on the app store.\n- Include the app link in your email signature.\n- Try promotional press releases and campaigns.\n- Encourage users to rate and review the app on the app store.\n- Leverage user-generated content to your advantage. \n- Integrate the app with existing or popular apps.\n- Hold giveaways or contests to drive engagement.\n- Feature the app on your company's website and blog.\n- Cross-promote the app with other apps, websites, and social media platforms. \n- Create promotional content such as blogs and podcasts.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of marketing strategies to promote a new mobile app.\n\n### Response:\n- Implement paid campaigns on social media platforms.\n- Utilize influencer marketing.\n- Reach out to tech bloggers and websites.\n- Share the app through word of mouth among friends.\n- Create demo videos and tutorials to explain the app's features.\n- Offer discounts and special incentives to early adopters.\n- Build a basic website to explain the app's purpose and features. \n- Take advantage of app store optimization techniques.\n- Send out emails and newsletters to inform new users about the app.\n- Aim for awards and good reviews on the app store.\n- Include the app link in your email signature.\n- Try promotional press releases and campaigns.\n- Encourage users to rate and review the app on the app store.\n- Leverage user-generated content to your advantage. \n- Integrate the app with existing or popular apps.\n- Hold giveaways or contests to drive engagement.\n- Feature the app on your company's website and blog.\n- Cross-promote the app with other apps, websites, and social media platforms. \n- Create promotional content such as blogs and podcasts."}, {"instruction": "Rewrite the sentence to provide more clarity and flow.", "input": "Making the decision to rent a house was a wise choice", "output": "Renting a house was a wise decision to make.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nRewrite the sentence to provide more clarity and flow.\n\n### Input:\nMaking the decision to rent a house was a wise choice\n\n### Response:\nRenting a house was a wise decision to make."}, {"instruction": "Give a description of the following customer segment.", "input": "Online shoppers in the US aged between 18-25", "output": "This customer segment consists of online shoppers in the US aged between 18-25. They are typically young adults who are tech-savvy and prefer to shop online for convenience and variety. They often shop for a variety of items, ranging from clothing to electronics and are comfortable using different payment methods such as credit or debit cards and digital wallets.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive a description of the following customer segment.\n\n### Input:\nOnline shoppers in the US aged between 18-25\n\n### Response:\nThis customer segment consists of online shoppers in the US aged between 18-25. They are typically young adults who are tech-savvy and prefer to shop online for convenience and variety. They often shop for a variety of items, ranging from clothing to electronics and are comfortable using different payment methods such as credit or debit cards and digital wallets."}, {"instruction": "Design a logo for a mobile application.", "input": "", "output": "The logo design should be simple, modern, and unique. It should capture the essence of the application and communicate its functions. The logo should feature bold colors that are eye-catching, and the design should be adaptable to various sizes and platforms, such as websites and mobile devices.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDesign a logo for a mobile application.\n\n### Response:\nThe logo design should be simple, modern, and unique. It should capture the essence of the application and communicate its functions. The logo should feature bold colors that are eye-catching, and the design should be adaptable to various sizes and platforms, such as websites and mobile devices."}, {"instruction": "Generate a list of country names in the western hemisphere.", "input": "", "output": "- Canada\n- United States \n- Mexico \n- Brazil\n- Argentina \n- Chile \n- Colombia \n- Venezuela \n- Cuba\n- Jamaica \n- Ecuador\n- Peru \n- Uruguay\n- Paraguay \n- Costa Rica \n- Panama \n- Honduras \n- Nicaragua\n- Guatemala", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of country names in the western hemisphere.\n\n### Response:\n- Canada\n- United States \n- Mexico \n- Brazil\n- Argentina \n- Chile \n- Colombia \n- Venezuela \n- Cuba\n- Jamaica \n- Ecuador\n- Peru \n- Uruguay\n- Paraguay \n- Costa Rica \n- Panama \n- Honduras \n- Nicaragua\n- Guatemala"}, {"instruction": "Translate the following phrase into French.", "input": "I miss you", "output": "Je te manque.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nTranslate the following phrase into French.\n\n### Input:\nI miss you\n\n### Response:\nJe te manque."}, {"instruction": "Explain what an API is.", "input": "", "output": "API stands for Application Programming Interface. It is a set of instructions or programming instructions that allow two applications to communicate and exchange data with each other. APIs provide a way for developers to access and manipulate data and services offered by other applications or platforms. They are an essential part of modern web development and allow developers to develop complex applications and websites using modular components.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nExplain what an API is.\n\n### Response:\nAPI stands for Application Programming Interface. It is a set of instructions or programming instructions that allow two applications to communicate and exchange data with each other. APIs provide a way for developers to access and manipulate data and services offered by other applications or platforms. They are an essential part of modern web development and allow developers to develop complex applications and websites using modular components."}, {"instruction": "Compute the area of a rectangle with length 10cm and width 5cm.", "input": "", "output": "The area of the rectangle is 50 cm2.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nCompute the area of a rectangle with length 10cm and width 5cm.\n\n### Response:\nThe area of the rectangle is 50 cm2."}, {"instruction": "Find the capital of Spain.", "input": "", "output": "The capital of Spain is Madrid.", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nFind the capital of Spain.\n\n### Response:\nThe capital of Spain is Madrid."}, {"instruction": "Classify the following statement as true or false.", "input": "The Supreme Court is the highest court in the US.", "output": "True.", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nClassify the following statement as true or false.\n\n### Input:\nThe Supreme Court is the highest court in the US.\n\n### Response:\nTrue."}]
diff -ruN marc_original/third_party/torchtune/tests/assets/chat_tiny.json marc/third_party/torchtune/tests/assets/chat_tiny.json
--- marc_original/third_party/torchtune/tests/assets/chat_tiny.json	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/chat_tiny.json	2025-02-20 17:49:29.534024220 -0500
@@ -0,0 +1,26 @@
+[
+    {
+        "conversations": [
+            {
+                "from": "system",
+                "value": "You are an AI assistant."
+            },
+            {
+                "from": "human",
+                "value": "What is the meaning of life?"
+            },
+            {
+                "from": "gpt",
+                "value": "The meaning of life is 42."
+            },
+            {
+                "from": "human",
+                "value": "That's ridiculous."
+            },
+            {
+                "from": "gpt",
+                "value": "I agree."
+            }
+        ]
+    }
+]
Binary files marc_original/third_party/torchtune/tests/assets/dog_on_skateboard.jpg and marc/third_party/torchtune/tests/assets/dog_on_skateboard.jpg differ
diff -ruN marc_original/third_party/torchtune/tests/assets/hh_rlhf_tiny.json marc/third_party/torchtune/tests/assets/hh_rlhf_tiny.json
--- marc_original/third_party/torchtune/tests/assets/hh_rlhf_tiny.json	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/hh_rlhf_tiny.json	2025-02-20 17:49:29.542024232 -0500
@@ -0,0 +1 @@
+[{"chosen":[{"content":"What do I do when I have a hole in my trousers?","role":"user"},{"content":"Fix the hole.","role":"assistant"}],"rejected":[{"content":"What do I do when I have a hole in my trousers?","role":"user"},{"content":"Take them off.","role":"assistant"}]}]
diff -ruN marc_original/third_party/torchtune/tests/assets/instruct_tiny.json marc/third_party/torchtune/tests/assets/instruct_tiny.json
--- marc_original/third_party/torchtune/tests/assets/instruct_tiny.json	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/instruct_tiny.json	2025-02-20 17:49:29.546024239 -0500
@@ -0,0 +1,10 @@
+[
+    {
+        "instruction": "What time is it in London?",
+        "response": "It is 10:00 AM in London"
+    },
+    {
+        "instruction": "Is is Istanbul or Constantinople?",
+        "response": "Istanbul was Constantinople. Now it's Istanbul, not Constantinople."
+    }
+]
diff -ruN marc_original/third_party/torchtune/tests/assets/invalid_dummy_config.yaml marc/third_party/torchtune/tests/assets/invalid_dummy_config.yaml
--- marc_original/third_party/torchtune/tests/assets/invalid_dummy_config.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/invalid_dummy_config.yaml	2025-02-20 17:49:29.550024246 -0500
@@ -0,0 +1,8 @@
+test1:
+  _component_: torchtune.training.get_dtype
+  dtype: fp32
+  dummy: 3
+test2:
+  _component_: torchtune.training.get_dtype
+  dtype: fp32
+  dummy: 3
Binary files marc_original/third_party/torchtune/tests/assets/m.model and marc/third_party/torchtune/tests/assets/m.model differ
diff -ruN marc_original/third_party/torchtune/tests/assets/README.md marc/third_party/torchtune/tests/assets/README.md
--- marc_original/third_party/torchtune/tests/assets/README.md	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/README.md	2025-02-20 17:49:29.526024206 -0500
@@ -0,0 +1,26 @@
+# Details on the assets in this folder
+
+## `m.model`
+
+**Description**:
+**Creation**:
+**Usage**:
+
+
+## `tiny_fair_checkpoint.pt`
+
+**Description**:
+**Creation**:
+**Usage**:
+
+## `tiny_llama2_checkpoint.pt`
+
+**Description**:
+**Creation**:
+**Usage**:
+
+## `tiny_state_dict_with_one_key.pt`
+
+**Description**:
+**Creation**:
+**Usage**:
diff -ruN marc_original/third_party/torchtune/tests/assets/stack_exchange_paired_tiny.json marc/third_party/torchtune/tests/assets/stack_exchange_paired_tiny.json
--- marc_original/third_party/torchtune/tests/assets/stack_exchange_paired_tiny.json	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/stack_exchange_paired_tiny.json	2025-02-20 17:49:29.558024259 -0500
@@ -0,0 +1 @@
+[{"question":"I'm writing a code using Java Swing to press the right button when I type a number key.\nBut I can't find what I want through search.\nThis is my code and I can't understand why this isn't working.\nPlease help me..\n\n```\nimport javax.swing.*;\nimport java.awt.Dimension;\nimport java.awt.event.*;\n\nclass class01 {\n\n    public static void main(String[] args) {\n\n        JFrame f = new JFrame(\"Key event test\"); \n        f.setSize(230, 500);\n        f.setLayout(null);\n        f.setVisible(true);\n        f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n\n        JLabel label = new JLabel(); \n\n        JButton button1 = new JButton(\"Coffe\"); \n        button1.setSize(100, 100);\n        button1.setLocation(0, 0);\n\n        JButton button2 = new JButton(\"Latte\");\n        button2.setSize(100, 100);\n        button2.setLocation(0, 100);\n\n        JButton button3 = new JButton(\"Espresso\");\n        button3.setSize(100, 100);\n        button3.setLocation(100, 100);\n\n        JButton button4 = new JButton(\"Vanilla Latte\");\n        button4.setSize(100, 100);\n        button4.setLocation(100, 0);\n\n        f.add(button1); \n        f.add(button2);\n        f.add(button3);\n        f.add(button4);\n\n        \/\/ Show message when the corresponding button is pressed.\n        button1.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent arg0) {\n                button1.keyPressed(KeyEvent.VK_1);\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Coffee selected\");\n            },\n        },);\n        button2.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent arg0) {\n                button2.keyPressed(KeyEvent.VK_2);\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Latte selected\");\n            },\n        },);\n        button3.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent arg0) {\n                button3.keyPressed(KeyEvent.VK_3);\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Espresso selected\");\n            },\n        },);\n        button4.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent arg0) {\n                button4.keyPressed(KeyEvent.VK_4);\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Vanilla Latte selected\");\n            },\n        },);\n    },\n},\n\n```","response_j":"The code you are showing does exactly one thing: attach action listeners to your buttons..\n\nMeaning: when you click the button, then the listener will be called. \n\nYou need a generic keyboard listener that translates key events into calls to the appropriate button, respectively action listener instead.","response_k":"When you do this:\n\n```\nbutton1.addActionListener(new ActionListener() {\n        public void actionPerformed(ActionEvent arg0) {\n            button1.keyPressed(KeyEvent.VK_1);\n            JOptionPane.showMessageDialog(f.getComponent(0), \"Coffee selected\");\n        },\n    },);\n\n```\n\nYou are telling `button1` what to do when somebody clicks on the button. The line with `keyPressed` should not be there (it does not compile even).\n\nWhat you need to do is listen for key presses by adding a `KeyListener` to the frame like this:\n\n```\nf.addKeyListener(new KeyAdapter() {\n        @Override\n        public void keyTyped(KeyEvent e) {\n            if( e.getKeyChar() == KeyEvent.VK_1) {\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Coffee selected\");\n            },\n        },\n    },);\n\n```\n\nI repeated the `showMessageDialog`, but you should extract the actual logic into a method and call that method from within the `KeyListener` on the frame and the `ActionListener` on the button."},{"question":"I'm writing a code using Java Swing to press the right button when I type a number key.\nBut I can't find what I want through search.\nThis is my code and I can't understand why this isn't working.\nPlease help me..\n\n```\nimport javax.swing.*;\nimport java.awt.Dimension;\nimport java.awt.event.*;\n\nclass class01 {\n\n    public static void main(String[] args) {\n\n        JFrame f = new JFrame(\"Key event test\"); \n        f.setSize(230, 500);\n        f.setLayout(null);\n        f.setVisible(true);\n        f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n\n        JLabel label = new JLabel(); \n\n        JButton button1 = new JButton(\"Coffe\"); \n        button1.setSize(100, 100);\n        button1.setLocation(0, 0);\n\n        JButton button2 = new JButton(\"Latte\");\n        button2.setSize(100, 100);\n        button2.setLocation(0, 100);\n\n        JButton button3 = new JButton(\"Espresso\");\n        button3.setSize(100, 100);\n        button3.setLocation(100, 100);\n\n        JButton button4 = new JButton(\"Vanilla Latte\");\n        button4.setSize(100, 100);\n        button4.setLocation(100, 0);\n\n        f.add(button1); \n        f.add(button2);\n        f.add(button3);\n        f.add(button4);\n\n        \/\/ Show message when the corresponding button is pressed.\n        button1.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent arg0) {\n                button1.keyPressed(KeyEvent.VK_1);\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Coffee selected\");\n            },\n        },);\n        button2.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent arg0) {\n                button2.keyPressed(KeyEvent.VK_2);\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Latte selected\");\n            },\n        },);\n        button3.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent arg0) {\n                button3.keyPressed(KeyEvent.VK_3);\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Espresso selected\");\n            },\n        },);\n        button4.addActionListener(new ActionListener() {\n            public void actionPerformed(ActionEvent arg0) {\n                button4.keyPressed(KeyEvent.VK_4);\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Vanilla Latte selected\");\n            },\n        },);\n    },\n},\n\n```","response_j":"From what I understand, essentially you want to have the same operation of a button assigned to a specific key stroke.\n\nThings you want to avoid are `KeyListener`, especially because you have other focusable components in the view, namely buttons, which will steal keyboard focus and render the `KeyListener` useless. This is why, in almost all cases, you want to avoid `KeyListener`.\n\nA better and more reliable solution would be to use the [Key Bindings API](https:\/\/docs.oracle.com\/javase\/tutorial\/uiswing\/misc\/keybinding.html), which overcomes this focus related issue, but it also encourages the use of reusable components of work through the [`Action`s API](https:\/\/docs.oracle.com\/javase\/tutorial\/uiswing\/misc\/action.html)\n\nSomething like...\n\n```\nimport java.awt.EventQueue;\nimport java.awt.GridBagConstraints;\nimport java.awt.GridBagLayout;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.KeyEvent;\nimport java.util.StringJoiner;\nimport javax.swing.AbstractAction;\nimport javax.swing.ActionMap;\nimport javax.swing.InputMap;\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JPanel;\nimport javax.swing.JTextField;\nimport javax.swing.KeyStroke;\n\npublic class Test {\n\n    public static void main(String[] args) {\n        new Test();\n    },\n\n    public Test() {\n        EventQueue.invokeLater(new Runnable() {\n            @Override\n            public void run() {\n                JFrame frame = new JFrame();\n                frame.add(new TestPane());\n                frame.pack();\n                frame.setLocationRelativeTo(null);\n                frame.setVisible(true);\n            },\n        },);\n    },\n\n    public class TestPane extends JPanel {\n\n        public TestPane() {\n            JTextField field = new JTextField(10);\n            field.setEditable(false);\n            field.setFocusable(false);\n\n            InputMap im = getInputMap(WHEN_IN_FOCUSED_WINDOW);\n            ActionMap am = getActionMap();\n\n            im.put(KeyStroke.getKeyStroke(KeyEvent.VK_1, 0), \"Pressed.One\");\n            im.put(KeyStroke.getKeyStroke(KeyEvent.VK_2, 0), \"Pressed.Two\");\n            im.put(KeyStroke.getKeyStroke(KeyEvent.VK_3, 0), \"Pressed.Three\");\n            im.put(KeyStroke.getKeyStroke(KeyEvent.VK_4, 0), \"Pressed.Four\");\n\n            am.put(\"Pressed.One\", new OrderAction(1, field));\n            am.put(\"Pressed.Two\", new OrderAction(2, field));\n            am.put(\"Pressed.Three\", new OrderAction(3, field));\n            am.put(\"Pressed.Four\", new OrderAction(4, field));\n\n            JButton btnOne = new JButton(new OrderAction(1, field));\n            JButton btnTwo = new JButton(new OrderAction(2, field));\n            JButton btnThree = new JButton(new OrderAction(3, field));\n            JButton btnFour = new JButton(new OrderAction(4, field));\n\n            setLayout(new GridBagLayout());\n            GridBagConstraints gbc = new GridBagConstraints();\n\n            gbc.weightx = 1;\n            gbc.weighty = 1;\n            gbc.fill = GridBagConstraints.BOTH;\n            gbc.gridx = 0;\n            gbc.gridy = 1;\n            add(btnOne, gbc);\n            gbc.gridx++;\n            add(btnTwo, gbc);\n            gbc.gridx = 0;\n            gbc.gridy++;\n            add(btnThree, gbc);\n            gbc.gridx++;\n            add(btnFour, gbc);\n\n            gbc.gridx = 0;\n            gbc.gridy = 0;\n\n            gbc.gridwidth = 2;\n            add(field, gbc);\n        },\n\n        protected class OrderAction extends AbstractAction {\n\n            private int value;\n            private JTextField field;\n\n            public OrderAction(int value, JTextField field) {\n                this.value = value;\n                this.field = field;\n                switch (value) {\n                    case 1:\n                        putValue(NAME, \"Coffe\");\n                        break;\n                    case 2:\n                        putValue(NAME, \"Latte\");\n                        break;\n                    case 3:\n                        putValue(NAME, \"Espresso\");\n                        break;\n                    case 4:\n                        putValue(NAME, \"Vanilla Latte\");\n                        break;\n                },\n            },\n\n            @Override\n            public void actionPerformed(ActionEvent e) {\n                StringJoiner sj = new StringJoiner(\"; \");\n                if (field.getText() != null && field.getText().length() > 0) {\n                    sj.add(field.getText());\n                },\n                sj.add(Integer.toString(value));\n                field.setText(sj.toString());\n            },\n\n        },\n\n    },\n\n},\n\n```\n\nNote, you could apply the key bindings directly to each button instead\n\nNow, if you want to \"visually\" press the button on a key stroke, I would recommend either creating a custom `JButton` or factory method, which could allow for a more simplified implementation, but the basic idea would be to define a key binding and `Action` which simply called the buttons `doClick` method, for example\n\n```\nimport java.awt.EventQueue;\nimport java.awt.GridBagConstraints;\nimport java.awt.GridBagLayout;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.KeyEvent;\nimport java.util.StringJoiner;\nimport javax.swing.AbstractAction;\nimport javax.swing.ActionMap;\nimport javax.swing.InputMap;\nimport javax.swing.JButton;\nimport javax.swing.JFrame;\nimport javax.swing.JPanel;\nimport javax.swing.JTextField;\nimport javax.swing.KeyStroke;\n\npublic class Test {\n\n    public static void main(String[] args) {\n        new Test();\n    },\n\n    public Test() {\n        EventQueue.invokeLater(new Runnable() {\n            @Override\n            public void run() {\n                JFrame frame = new JFrame();\n                frame.add(new TestPane());\n                frame.pack();\n                frame.setLocationRelativeTo(null);\n                frame.setVisible(true);\n            },\n        },);\n    },\n\n    public class TestPane extends JPanel {\n\n        public TestPane() {\n            JTextField field = new JTextField(10);\n            field.setEditable(false);\n            field.setFocusable(false);\n\n            JButton btnOne = new JButton(new OrderAction(1, field));\n            JButton btnTwo = new JButton(new OrderAction(2, field));\n            JButton btnThree = new JButton(new OrderAction(3, field));\n            JButton btnFour = new JButton(new OrderAction(4, field));\n\n            InputMap im = getInputMap(WHEN_IN_FOCUSED_WINDOW);\n            ActionMap am = getActionMap();\n\n            im.put(KeyStroke.getKeyStroke(KeyEvent.VK_1, 0), \"Pressed.One\");\n            im.put(KeyStroke.getKeyStroke(KeyEvent.VK_2, 0), \"Pressed.Two\");\n            im.put(KeyStroke.getKeyStroke(KeyEvent.VK_3, 0), \"Pressed.Three\");\n            im.put(KeyStroke.getKeyStroke(KeyEvent.VK_4, 0), \"Pressed.Four\");\n\n            am.put(\"Pressed.One\", new ProxyAction(btnOne));\n            am.put(\"Pressed.Two\", new ProxyAction(btnTwo));\n            am.put(\"Pressed.Three\", new ProxyAction(btnThree));\n            am.put(\"Pressed.Four\", new ProxyAction(btnFour));\n\n            setLayout(new GridBagLayout());\n            GridBagConstraints gbc = new GridBagConstraints();\n\n            gbc.weightx = 1;\n            gbc.weighty = 1;\n            gbc.fill = GridBagConstraints.BOTH;\n            gbc.gridx = 0;\n            gbc.gridy = 1;\n            add(btnOne, gbc);\n            gbc.gridx++;\n            add(btnTwo, gbc);\n            gbc.gridx = 0;\n            gbc.gridy++;\n            add(btnThree, gbc);\n            gbc.gridx++;\n            add(btnFour, gbc);\n\n            gbc.gridx = 0;\n            gbc.gridy = 0;\n\n            gbc.gridwidth = 2;\n            add(field, gbc);\n        },\n\n        protected class ProxyAction extends AbstractAction {\n\n            private JButton btn;\n\n            public ProxyAction(JButton btn) {\n                this.btn = btn;\n            },\n\n            @Override\n            public void actionPerformed(ActionEvent e) {\n                btn.doClick();\n            },\n\n        },\n\n        protected class OrderAction extends AbstractAction {\n\n            private int value;\n            private JTextField field;\n\n            public OrderAction(int value, JTextField field) {\n                this.value = value;\n                this.field = field;\n                switch (value) {\n                    case 1:\n                        putValue(NAME, \"Coffe\");\n                        break;\n                    case 2:\n                        putValue(NAME, \"Latte\");\n                        break;\n                    case 3:\n                        putValue(NAME, \"Espresso\");\n                        break;\n                    case 4:\n                        putValue(NAME, \"Vanilla Latte\");\n                        break;\n                },\n            },\n\n            @Override\n            public void actionPerformed(ActionEvent e) {\n                StringJoiner sj = new StringJoiner(\"; \");\n                if (field.getText() != null && field.getText().length() > 0) {\n                    sj.add(field.getText());\n                },\n                sj.add(Integer.toString(value));\n                field.setText(sj.toString());\n            },\n\n        },\n\n    },\n\n},\n\n```","response_k":"When you do this:\n\n```\nbutton1.addActionListener(new ActionListener() {\n        public void actionPerformed(ActionEvent arg0) {\n            button1.keyPressed(KeyEvent.VK_1);\n            JOptionPane.showMessageDialog(f.getComponent(0), \"Coffee selected\");\n        },\n    },);\n\n```\n\nYou are telling `button1` what to do when somebody clicks on the button. The line with `keyPressed` should not be there (it does not compile even).\n\nWhat you need to do is listen for key presses by adding a `KeyListener` to the frame like this:\n\n```\nf.addKeyListener(new KeyAdapter() {\n        @Override\n        public void keyTyped(KeyEvent e) {\n            if( e.getKeyChar() == KeyEvent.VK_1) {\n                JOptionPane.showMessageDialog(f.getComponent(0), \"Coffee selected\");\n            },\n        },\n    },);\n\n```\n\nI repeated the `showMessageDialog`, but you should extract the actual logic into a method and call that method from within the `KeyListener` on the frame and the `ActionListener` on the button."},{"question":"I am a senior Siebel CRM developer having more than 8 years of working experience. Now, I am very keen and excited to learn Salesforce and get certified as soon as possible. \n\nPlease guide me where to start from scratch ?","response_j":"After some research and not reaching to any solution, I refreshed the selected Business Units by removing them, saving and the adding and saving again. This solved my issue.","response_k":"You may need to raise the case with SFMC support. I have seen these issues, and support needs to toggle some backend settings to reset the SC and SFMC connection."},{"question":"i want to write a shape with \" \\* \" and \" | \" the shape is below.\nThe program must take height and width from user.Width is column number without ' | '.I tried to write but confused.My code sometimes works great and sometimes being stupid.For example when i enter height : 13, width : 4 it writes one more,if witdh is 1 it enters infinite loop.While trying to solve it became too conflicted.Must i fix it or rewrite ? Here is the code : height =10, width = 5\n\n```\n\n|*____|    \n|_*___|\n|__*__|\n|___*_|\n|____*|\n|___*_|\n|__*__|\n|_*___|\n|*____|\n|_*___|\n\n```\n\n```\n      private static void Function()\n      {\n        int height, width;\n\n        if (width == 2)\n            while (height > 0)\n            {\n                FirstPart(width, height);\n                height -= width;\n            },\n        else\n            while (height > 0)\n            {\n                if (height > 1)\n                {\n                    FirstPart(width, height);\n                    height -= width;\n                },\n                if (height > 0)\n                {\n                    SecondPart(width, height);\n                    height -= width - 2;\n                },\n            },\n    },\n\n    private static void FirstPart(int width,int height)\n    {\n\n        if(height > width)\n             for (int i = 0; i < width; i++)\n             {\n                for (int j = 0; j < width+2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                         if (i + 1 == j)\n                            Console.Write(\"*\");\n                         else\n                             Console.Write(\" \");\n                },\n             Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {\n                for (int j = 0; j < width + 2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + 1 == j)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },\n    private static void SecondPart(int width,int height)\n    {\n\n        if(height > width)\n\n            for (int i = 0; i < width-2; i++)\n            {\n               for (int j = 0; j < width+2; j++)\n               {\n\n                if (j == 0 || j == width + 1)\n                    Console.Write(\"|\");\n                else\n                    if (i + j == width-1)\n                        Console.Write(\"*\");\n                    else\n                        Console.Write(\" \");\n            },\n            Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {                                     \n                for (int j = 0; j < width + 2; j++)\n                {\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + j == width - 1)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },            \n\n```","response_j":"I see a\n\n```\nwhile (Height > 0)\n\n```\n\nso your infinite loop is coming from Height never getting less or equal to 0.","response_k":"It's better to rewrite. When you do, decouple the code into several functions so that one function draws a single line, and another one calls the former to draw all the lines."},{"question":"i want to write a shape with \" \\* \" and \" | \" the shape is below.\nThe program must take height and width from user.Width is column number without ' | '.I tried to write but confused.My code sometimes works great and sometimes being stupid.For example when i enter height : 13, width : 4 it writes one more,if witdh is 1 it enters infinite loop.While trying to solve it became too conflicted.Must i fix it or rewrite ? Here is the code : height =10, width = 5\n\n```\n\n|*____|    \n|_*___|\n|__*__|\n|___*_|\n|____*|\n|___*_|\n|__*__|\n|_*___|\n|*____|\n|_*___|\n\n```\n\n```\n      private static void Function()\n      {\n        int height, width;\n\n        if (width == 2)\n            while (height > 0)\n            {\n                FirstPart(width, height);\n                height -= width;\n            },\n        else\n            while (height > 0)\n            {\n                if (height > 1)\n                {\n                    FirstPart(width, height);\n                    height -= width;\n                },\n                if (height > 0)\n                {\n                    SecondPart(width, height);\n                    height -= width - 2;\n                },\n            },\n    },\n\n    private static void FirstPart(int width,int height)\n    {\n\n        if(height > width)\n             for (int i = 0; i < width; i++)\n             {\n                for (int j = 0; j < width+2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                         if (i + 1 == j)\n                            Console.Write(\"*\");\n                         else\n                             Console.Write(\" \");\n                },\n             Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {\n                for (int j = 0; j < width + 2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + 1 == j)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },\n    private static void SecondPart(int width,int height)\n    {\n\n        if(height > width)\n\n            for (int i = 0; i < width-2; i++)\n            {\n               for (int j = 0; j < width+2; j++)\n               {\n\n                if (j == 0 || j == width + 1)\n                    Console.Write(\"|\");\n                else\n                    if (i + j == width-1)\n                        Console.Write(\"*\");\n                    else\n                        Console.Write(\" \");\n            },\n            Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {                                     \n                for (int j = 0; j < width + 2; j++)\n                {\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + j == width - 1)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },            \n\n```","response_j":"I see a\n\n```\nwhile (Height > 0)\n\n```\n\nso your infinite loop is coming from Height never getting less or equal to 0.","response_k":"```\nvoid WriteStars(int Width,int Height)\n{\n    int _sp=1; \/\/Star Pos\n    bool _left = false;\n    for(int i =0;i<Height;i++)\n    {\n        Console.Write(\"|\");\n        int j;\n        for(j=1;j<Width-1;j++)\n        {\n            if(j==_sp)\n            {\n                Console.Write(\"*\");\n                if(_left)\n                {\n                    _sp--;\n                },\n                else\n                {\n                    _sp++;\n                },\n                   j++;\n                   break;\n            },\n            else\n            {\n               Console.Write(\"_\");\n            },\n        },\n        for(;j<Width-1;j++)\n        {\n            Console.Write(\"_\");\n        },\n\n        Console.WriteLine(\"|\");\n        if(_sp==0)\n        {\n            _left = false;\n        },\n        else if(_sp==Width)\n        {\n            _left = true;\n        },\n\n    },\n},\n\n```\n\nTry if it works, wrote it right here."},{"question":"i want to write a shape with \" \\* \" and \" | \" the shape is below.\nThe program must take height and width from user.Width is column number without ' | '.I tried to write but confused.My code sometimes works great and sometimes being stupid.For example when i enter height : 13, width : 4 it writes one more,if witdh is 1 it enters infinite loop.While trying to solve it became too conflicted.Must i fix it or rewrite ? Here is the code : height =10, width = 5\n\n```\n\n|*____|    \n|_*___|\n|__*__|\n|___*_|\n|____*|\n|___*_|\n|__*__|\n|_*___|\n|*____|\n|_*___|\n\n```\n\n```\n      private static void Function()\n      {\n        int height, width;\n\n        if (width == 2)\n            while (height > 0)\n            {\n                FirstPart(width, height);\n                height -= width;\n            },\n        else\n            while (height > 0)\n            {\n                if (height > 1)\n                {\n                    FirstPart(width, height);\n                    height -= width;\n                },\n                if (height > 0)\n                {\n                    SecondPart(width, height);\n                    height -= width - 2;\n                },\n            },\n    },\n\n    private static void FirstPart(int width,int height)\n    {\n\n        if(height > width)\n             for (int i = 0; i < width; i++)\n             {\n                for (int j = 0; j < width+2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                         if (i + 1 == j)\n                            Console.Write(\"*\");\n                         else\n                             Console.Write(\" \");\n                },\n             Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {\n                for (int j = 0; j < width + 2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + 1 == j)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },\n    private static void SecondPart(int width,int height)\n    {\n\n        if(height > width)\n\n            for (int i = 0; i < width-2; i++)\n            {\n               for (int j = 0; j < width+2; j++)\n               {\n\n                if (j == 0 || j == width + 1)\n                    Console.Write(\"|\");\n                else\n                    if (i + j == width-1)\n                        Console.Write(\"*\");\n                    else\n                        Console.Write(\" \");\n            },\n            Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {                                     \n                for (int j = 0; j < width + 2; j++)\n                {\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + j == width - 1)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },            \n\n```","response_j":"```\n    private static void WriteStars(int width, int height)\n    {\n        int j = 0;\n        for (int i = 0; i < height; i++)\n        {\n            Console.Write(\"|\");\n            for (int f = 0; f < width; f++)\n            {\n                if (f == Math.Abs(j))\n                {\n                    Console.Write(\"*\");\n                },\n                else\n                {\n                    Console.Write(\" \");\n                },\n            },\n            j++;\n            if (Math.Abs(j) == width - 1)\n            {\n                j *= -1;\n            },\n            Console.WriteLine(\"|\");\n        },\n    },\n\n```\n\nProbably going to get downvoted for giving you a complete answer, but maybe it'll show you one correct approach and you can learn something from it...","response_k":"I see a\n\n```\nwhile (Height > 0)\n\n```\n\nso your infinite loop is coming from Height never getting less or equal to 0."},{"question":"i want to write a shape with \" \\* \" and \" | \" the shape is below.\nThe program must take height and width from user.Width is column number without ' | '.I tried to write but confused.My code sometimes works great and sometimes being stupid.For example when i enter height : 13, width : 4 it writes one more,if witdh is 1 it enters infinite loop.While trying to solve it became too conflicted.Must i fix it or rewrite ? Here is the code : height =10, width = 5\n\n```\n\n|*____|    \n|_*___|\n|__*__|\n|___*_|\n|____*|\n|___*_|\n|__*__|\n|_*___|\n|*____|\n|_*___|\n\n```\n\n```\n      private static void Function()\n      {\n        int height, width;\n\n        if (width == 2)\n            while (height > 0)\n            {\n                FirstPart(width, height);\n                height -= width;\n            },\n        else\n            while (height > 0)\n            {\n                if (height > 1)\n                {\n                    FirstPart(width, height);\n                    height -= width;\n                },\n                if (height > 0)\n                {\n                    SecondPart(width, height);\n                    height -= width - 2;\n                },\n            },\n    },\n\n    private static void FirstPart(int width,int height)\n    {\n\n        if(height > width)\n             for (int i = 0; i < width; i++)\n             {\n                for (int j = 0; j < width+2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                         if (i + 1 == j)\n                            Console.Write(\"*\");\n                         else\n                             Console.Write(\" \");\n                },\n             Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {\n                for (int j = 0; j < width + 2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + 1 == j)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },\n    private static void SecondPart(int width,int height)\n    {\n\n        if(height > width)\n\n            for (int i = 0; i < width-2; i++)\n            {\n               for (int j = 0; j < width+2; j++)\n               {\n\n                if (j == 0 || j == width + 1)\n                    Console.Write(\"|\");\n                else\n                    if (i + j == width-1)\n                        Console.Write(\"*\");\n                    else\n                        Console.Write(\" \");\n            },\n            Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {                                     \n                for (int j = 0; j < width + 2; j++)\n                {\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + j == width - 1)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },            \n\n```","response_j":"I see a\n\n```\nwhile (Height > 0)\n\n```\n\nso your infinite loop is coming from Height never getting less or equal to 0.","response_k":"even shorter:\n\n```\nstatic void Variante_2(int height, int width)\n{\n  byte[][] arr = new byte[height][];\n  int pos = 0;\n  int mov = 1;\n  for (int line = 0; line < height; line++)\n  {\n    arr[line] = new byte[width];\n    for (int col = 0; col < width; col++) { arr[line][col] = 45; },\n    arr[line][pos] = 42;\n    pos += mov;\n    if (pos == 0 || pos == (width - 1)) { mov *= -1; },\n    Console.WriteLine(\"|\" + ASCIIEncoding.ASCII.GetString(arr[line]) + \"|\");\n  },\n  string temp = Console.ReadLine();\n},\n\n```"},{"question":"i want to write a shape with \" \\* \" and \" | \" the shape is below.\nThe program must take height and width from user.Width is column number without ' | '.I tried to write but confused.My code sometimes works great and sometimes being stupid.For example when i enter height : 13, width : 4 it writes one more,if witdh is 1 it enters infinite loop.While trying to solve it became too conflicted.Must i fix it or rewrite ? Here is the code : height =10, width = 5\n\n```\n\n|*____|    \n|_*___|\n|__*__|\n|___*_|\n|____*|\n|___*_|\n|__*__|\n|_*___|\n|*____|\n|_*___|\n\n```\n\n```\n      private static void Function()\n      {\n        int height, width;\n\n        if (width == 2)\n            while (height > 0)\n            {\n                FirstPart(width, height);\n                height -= width;\n            },\n        else\n            while (height > 0)\n            {\n                if (height > 1)\n                {\n                    FirstPart(width, height);\n                    height -= width;\n                },\n                if (height > 0)\n                {\n                    SecondPart(width, height);\n                    height -= width - 2;\n                },\n            },\n    },\n\n    private static void FirstPart(int width,int height)\n    {\n\n        if(height > width)\n             for (int i = 0; i < width; i++)\n             {\n                for (int j = 0; j < width+2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                         if (i + 1 == j)\n                            Console.Write(\"*\");\n                         else\n                             Console.Write(\" \");\n                },\n             Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {\n                for (int j = 0; j < width + 2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + 1 == j)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },\n    private static void SecondPart(int width,int height)\n    {\n\n        if(height > width)\n\n            for (int i = 0; i < width-2; i++)\n            {\n               for (int j = 0; j < width+2; j++)\n               {\n\n                if (j == 0 || j == width + 1)\n                    Console.Write(\"|\");\n                else\n                    if (i + j == width-1)\n                        Console.Write(\"*\");\n                    else\n                        Console.Write(\" \");\n            },\n            Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {                                     \n                for (int j = 0; j < width + 2; j++)\n                {\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + j == width - 1)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },            \n\n```","response_j":"I see a\n\n```\nwhile (Height > 0)\n\n```\n\nso your infinite loop is coming from Height never getting less or equal to 0.","response_k":"and it is possible to do it with less code:\n\n```\nstatic void Variante_3(int height, int width)\n{\n    int pos = 1;\n    int mov = 1;\n    for (int line = 0; line < height; line++)\n    {\n        Console.WriteLine(\"|\" + \"*\".PadLeft(pos, '_') + \"|\".PadLeft(width - pos, '_'));\n        pos += mov;\n        if (pos == 1 || pos == (width - 1)) { mov *= -1; },\n    },\n    string temp = Console.ReadLine();\n},\n\n```\n\nSorry to all not doing others homework, but I couldn\u00b4t sleep without showing this *g*"},{"question":"i want to write a shape with \" \\* \" and \" | \" the shape is below.\nThe program must take height and width from user.Width is column number without ' | '.I tried to write but confused.My code sometimes works great and sometimes being stupid.For example when i enter height : 13, width : 4 it writes one more,if witdh is 1 it enters infinite loop.While trying to solve it became too conflicted.Must i fix it or rewrite ? Here is the code : height =10, width = 5\n\n```\n\n|*____|    \n|_*___|\n|__*__|\n|___*_|\n|____*|\n|___*_|\n|__*__|\n|_*___|\n|*____|\n|_*___|\n\n```\n\n```\n      private static void Function()\n      {\n        int height, width;\n\n        if (width == 2)\n            while (height > 0)\n            {\n                FirstPart(width, height);\n                height -= width;\n            },\n        else\n            while (height > 0)\n            {\n                if (height > 1)\n                {\n                    FirstPart(width, height);\n                    height -= width;\n                },\n                if (height > 0)\n                {\n                    SecondPart(width, height);\n                    height -= width - 2;\n                },\n            },\n    },\n\n    private static void FirstPart(int width,int height)\n    {\n\n        if(height > width)\n             for (int i = 0; i < width; i++)\n             {\n                for (int j = 0; j < width+2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                         if (i + 1 == j)\n                            Console.Write(\"*\");\n                         else\n                             Console.Write(\" \");\n                },\n             Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {\n                for (int j = 0; j < width + 2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + 1 == j)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },\n    private static void SecondPart(int width,int height)\n    {\n\n        if(height > width)\n\n            for (int i = 0; i < width-2; i++)\n            {\n               for (int j = 0; j < width+2; j++)\n               {\n\n                if (j == 0 || j == width + 1)\n                    Console.Write(\"|\");\n                else\n                    if (i + j == width-1)\n                        Console.Write(\"*\");\n                    else\n                        Console.Write(\" \");\n            },\n            Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {                                     \n                for (int j = 0; j < width + 2; j++)\n                {\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + j == width - 1)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },            \n\n```","response_j":"```\n    private static void WriteStars(int width, int height)\n    {\n        int j = 0;\n        for (int i = 0; i < height; i++)\n        {\n            Console.Write(\"|\");\n            for (int f = 0; f < width; f++)\n            {\n                if (f == Math.Abs(j))\n                {\n                    Console.Write(\"*\");\n                },\n                else\n                {\n                    Console.Write(\" \");\n                },\n            },\n            j++;\n            if (Math.Abs(j) == width - 1)\n            {\n                j *= -1;\n            },\n            Console.WriteLine(\"|\");\n        },\n    },\n\n```\n\nProbably going to get downvoted for giving you a complete answer, but maybe it'll show you one correct approach and you can learn something from it...","response_k":"It's better to rewrite. When you do, decouple the code into several functions so that one function draws a single line, and another one calls the former to draw all the lines."},{"question":"i want to write a shape with \" \\* \" and \" | \" the shape is below.\nThe program must take height and width from user.Width is column number without ' | '.I tried to write but confused.My code sometimes works great and sometimes being stupid.For example when i enter height : 13, width : 4 it writes one more,if witdh is 1 it enters infinite loop.While trying to solve it became too conflicted.Must i fix it or rewrite ? Here is the code : height =10, width = 5\n\n```\n\n|*____|    \n|_*___|\n|__*__|\n|___*_|\n|____*|\n|___*_|\n|__*__|\n|_*___|\n|*____|\n|_*___|\n\n```\n\n```\n      private static void Function()\n      {\n        int height, width;\n\n        if (width == 2)\n            while (height > 0)\n            {\n                FirstPart(width, height);\n                height -= width;\n            },\n        else\n            while (height > 0)\n            {\n                if (height > 1)\n                {\n                    FirstPart(width, height);\n                    height -= width;\n                },\n                if (height > 0)\n                {\n                    SecondPart(width, height);\n                    height -= width - 2;\n                },\n            },\n    },\n\n    private static void FirstPart(int width,int height)\n    {\n\n        if(height > width)\n             for (int i = 0; i < width; i++)\n             {\n                for (int j = 0; j < width+2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                         if (i + 1 == j)\n                            Console.Write(\"*\");\n                         else\n                             Console.Write(\" \");\n                },\n             Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {\n                for (int j = 0; j < width + 2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + 1 == j)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },\n    private static void SecondPart(int width,int height)\n    {\n\n        if(height > width)\n\n            for (int i = 0; i < width-2; i++)\n            {\n               for (int j = 0; j < width+2; j++)\n               {\n\n                if (j == 0 || j == width + 1)\n                    Console.Write(\"|\");\n                else\n                    if (i + j == width-1)\n                        Console.Write(\"*\");\n                    else\n                        Console.Write(\" \");\n            },\n            Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {                                     \n                for (int j = 0; j < width + 2; j++)\n                {\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + j == width - 1)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },            \n\n```","response_j":"```\n    private static void WriteStars(int width, int height)\n    {\n        int j = 0;\n        for (int i = 0; i < height; i++)\n        {\n            Console.Write(\"|\");\n            for (int f = 0; f < width; f++)\n            {\n                if (f == Math.Abs(j))\n                {\n                    Console.Write(\"*\");\n                },\n                else\n                {\n                    Console.Write(\" \");\n                },\n            },\n            j++;\n            if (Math.Abs(j) == width - 1)\n            {\n                j *= -1;\n            },\n            Console.WriteLine(\"|\");\n        },\n    },\n\n```\n\nProbably going to get downvoted for giving you a complete answer, but maybe it'll show you one correct approach and you can learn something from it...","response_k":"```\nvoid WriteStars(int Width,int Height)\n{\n    int _sp=1; \/\/Star Pos\n    bool _left = false;\n    for(int i =0;i<Height;i++)\n    {\n        Console.Write(\"|\");\n        int j;\n        for(j=1;j<Width-1;j++)\n        {\n            if(j==_sp)\n            {\n                Console.Write(\"*\");\n                if(_left)\n                {\n                    _sp--;\n                },\n                else\n                {\n                    _sp++;\n                },\n                   j++;\n                   break;\n            },\n            else\n            {\n               Console.Write(\"_\");\n            },\n        },\n        for(;j<Width-1;j++)\n        {\n            Console.Write(\"_\");\n        },\n\n        Console.WriteLine(\"|\");\n        if(_sp==0)\n        {\n            _left = false;\n        },\n        else if(_sp==Width)\n        {\n            _left = true;\n        },\n\n    },\n},\n\n```\n\nTry if it works, wrote it right here."},{"question":"i want to write a shape with \" \\* \" and \" | \" the shape is below.\nThe program must take height and width from user.Width is column number without ' | '.I tried to write but confused.My code sometimes works great and sometimes being stupid.For example when i enter height : 13, width : 4 it writes one more,if witdh is 1 it enters infinite loop.While trying to solve it became too conflicted.Must i fix it or rewrite ? Here is the code : height =10, width = 5\n\n```\n\n|*____|    \n|_*___|\n|__*__|\n|___*_|\n|____*|\n|___*_|\n|__*__|\n|_*___|\n|*____|\n|_*___|\n\n```\n\n```\n      private static void Function()\n      {\n        int height, width;\n\n        if (width == 2)\n            while (height > 0)\n            {\n                FirstPart(width, height);\n                height -= width;\n            },\n        else\n            while (height > 0)\n            {\n                if (height > 1)\n                {\n                    FirstPart(width, height);\n                    height -= width;\n                },\n                if (height > 0)\n                {\n                    SecondPart(width, height);\n                    height -= width - 2;\n                },\n            },\n    },\n\n    private static void FirstPart(int width,int height)\n    {\n\n        if(height > width)\n             for (int i = 0; i < width; i++)\n             {\n                for (int j = 0; j < width+2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                         if (i + 1 == j)\n                            Console.Write(\"*\");\n                         else\n                             Console.Write(\" \");\n                },\n             Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {\n                for (int j = 0; j < width + 2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + 1 == j)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },\n    private static void SecondPart(int width,int height)\n    {\n\n        if(height > width)\n\n            for (int i = 0; i < width-2; i++)\n            {\n               for (int j = 0; j < width+2; j++)\n               {\n\n                if (j == 0 || j == width + 1)\n                    Console.Write(\"|\");\n                else\n                    if (i + j == width-1)\n                        Console.Write(\"*\");\n                    else\n                        Console.Write(\" \");\n            },\n            Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {                                     \n                for (int j = 0; j < width + 2; j++)\n                {\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + j == width - 1)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },            \n\n```","response_j":"```\n    private static void WriteStars(int width, int height)\n    {\n        int j = 0;\n        for (int i = 0; i < height; i++)\n        {\n            Console.Write(\"|\");\n            for (int f = 0; f < width; f++)\n            {\n                if (f == Math.Abs(j))\n                {\n                    Console.Write(\"*\");\n                },\n                else\n                {\n                    Console.Write(\" \");\n                },\n            },\n            j++;\n            if (Math.Abs(j) == width - 1)\n            {\n                j *= -1;\n            },\n            Console.WriteLine(\"|\");\n        },\n    },\n\n```\n\nProbably going to get downvoted for giving you a complete answer, but maybe it'll show you one correct approach and you can learn something from it...","response_k":"even shorter:\n\n```\nstatic void Variante_2(int height, int width)\n{\n  byte[][] arr = new byte[height][];\n  int pos = 0;\n  int mov = 1;\n  for (int line = 0; line < height; line++)\n  {\n    arr[line] = new byte[width];\n    for (int col = 0; col < width; col++) { arr[line][col] = 45; },\n    arr[line][pos] = 42;\n    pos += mov;\n    if (pos == 0 || pos == (width - 1)) { mov *= -1; },\n    Console.WriteLine(\"|\" + ASCIIEncoding.ASCII.GetString(arr[line]) + \"|\");\n  },\n  string temp = Console.ReadLine();\n},\n\n```"},{"question":"i want to write a shape with \" \\* \" and \" | \" the shape is below.\nThe program must take height and width from user.Width is column number without ' | '.I tried to write but confused.My code sometimes works great and sometimes being stupid.For example when i enter height : 13, width : 4 it writes one more,if witdh is 1 it enters infinite loop.While trying to solve it became too conflicted.Must i fix it or rewrite ? Here is the code : height =10, width = 5\n\n```\n\n|*____|    \n|_*___|\n|__*__|\n|___*_|\n|____*|\n|___*_|\n|__*__|\n|_*___|\n|*____|\n|_*___|\n\n```\n\n```\n      private static void Function()\n      {\n        int height, width;\n\n        if (width == 2)\n            while (height > 0)\n            {\n                FirstPart(width, height);\n                height -= width;\n            },\n        else\n            while (height > 0)\n            {\n                if (height > 1)\n                {\n                    FirstPart(width, height);\n                    height -= width;\n                },\n                if (height > 0)\n                {\n                    SecondPart(width, height);\n                    height -= width - 2;\n                },\n            },\n    },\n\n    private static void FirstPart(int width,int height)\n    {\n\n        if(height > width)\n             for (int i = 0; i < width; i++)\n             {\n                for (int j = 0; j < width+2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                         if (i + 1 == j)\n                            Console.Write(\"*\");\n                         else\n                             Console.Write(\" \");\n                },\n             Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {\n                for (int j = 0; j < width + 2; j++)\n                {\n\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + 1 == j)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },\n    private static void SecondPart(int width,int height)\n    {\n\n        if(height > width)\n\n            for (int i = 0; i < width-2; i++)\n            {\n               for (int j = 0; j < width+2; j++)\n               {\n\n                if (j == 0 || j == width + 1)\n                    Console.Write(\"|\");\n                else\n                    if (i + j == width-1)\n                        Console.Write(\"*\");\n                    else\n                        Console.Write(\" \");\n            },\n            Console.WriteLine();\n        },\n        else\n            for (int i = 0; i < height; i++)\n            {                                     \n                for (int j = 0; j < width + 2; j++)\n                {\n                    if (j == 0 || j == width + 1)\n                        Console.Write(\"|\");\n                    else\n                        if (i + j == width - 1)\n                            Console.Write(\"*\");\n                        else\n                            Console.Write(\" \");\n                },\n                Console.WriteLine();\n            },\n    },            \n\n```","response_j":"```\n    private static void WriteStars(int width, int height)\n    {\n        int j = 0;\n        for (int i = 0; i < height; i++)\n        {\n            Console.Write(\"|\");\n            for (int f = 0; f < width; f++)\n            {\n                if (f == Math.Abs(j))\n                {\n                    Console.Write(\"*\");\n                },\n                else\n                {\n                    Console.Write(\" \");\n                },\n            },\n            j++;\n            if (Math.Abs(j) == width - 1)\n            {\n                j *= -1;\n            },\n            Console.WriteLine(\"|\");\n        },\n    },\n\n```\n\nProbably going to get downvoted for giving you a complete answer, but maybe it'll show you one correct approach and you can learn something from it...","response_k":"and it is possible to do it with less code:\n\n```\nstatic void Variante_3(int height, int width)\n{\n    int pos = 1;\n    int mov = 1;\n    for (int line = 0; line < height; line++)\n    {\n        Console.WriteLine(\"|\" + \"*\".PadLeft(pos, '_') + \"|\".PadLeft(width - pos, '_'));\n        pos += mov;\n        if (pos == 1 || pos == (width - 1)) { mov *= -1; },\n    },\n    string temp = Console.ReadLine();\n},\n\n```\n\nSorry to all not doing others homework, but I couldn\u00b4t sleep without showing this *g*"},{"question":"Let me explain what is happening:\n\n* Database: Oracle 19c\n* Apex: 19.1.0.00.15\n* ORDS standalone is 19.1.0.r0921545\n\nI did the tasks to configure an Apex Social Sign In to Microsoft AAD without almost any issue:\n\n* I created the authentication method in Apex.\n* I register my application and get the web credentials in Azure.\n* I created a wallet in my database with the root CA Microsoft certificates and configured the instance settings to usee that wallet.\n* My wallet in the database server contains the property auto\\_login to avoid using passwords.\n* I created the ACEs entries to allow connection to the login.microsoftonline.com in the port 443\n* Although it is not important for the purpose of the question itself and the error that is producing, just comment that I configured the wallet settings in the internal workspace in order to provide access to the wallet to the apex applications.\n\nFor some weeks the process was working fine, I was having a perfect Single Sing on mechanism for all my apex applications in the different workspaces. However, since some days ago, I am getting always the same error:\n\n**ORA-29024: Certificate validation failure**\n\nAfter some digging I realise that someone has configured a PROXY for outgoing traffic. Before even trying in Apex, I tried in SQL using APEX\\_WEB\\_SERVICE\n\nRequest with proxy settings to login.microsoftonline.com\n\n```\nselect apex_web_service.make_rest_request(\n    p_url         => 'https:\/\/login.microsoftonline.com',\n    p_http_method => 'GET',\n    p_wallet_path => 'file:\/home\/oracle\/wallet',\n    p_wallet_pwd => 'MyPassword' ,\n    p_proxy_override => 'http:\/\/myproxy:myport'\n  7  ) from dual;\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-29024: Certificate validation failure\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nRequest without proxy settings, just to see if I can get there\n\n```\nSQL> select apex_web_service.make_rest_request(\n  2      p_url         => 'https:\/\/login.microsoftonline.com',\n  3      p_http_method => 'GET',\n  4      p_wallet_path => 'file:\/home\/oracle\/wallet'\n  5* ) from dual\nSQL> \/\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-29024: Certificate validation failure\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nRequest to google using Proxy settings\n\n```\nselect apex_web_service.make_rest_request(\n    p_url         => 'https:\/\/google.com',\n    p_http_method => 'GET',\n    p_wallet_path => 'file:\/home\/oracle\/wallet',\n    p_wallet_pwd => 'MyPassword' ,\n  6      p_proxy_override => 'http:\/\/myproxy:myport'\n  7  ) from dual ;\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-29024: Certificate validation failure\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nRequest to google without proxy settings\n\n```\nSQL> select apex_web_service.make_rest_request(\n  2      p_url         => 'https:\/\/google.com',\n  3      p_http_method => 'GET',\n  4      p_wallet_path => 'file:\/home\/oracle\/wallet'\n  5* ) from dual\nSQL> \/\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-12535: TNS:operation timed out\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nMy questions are the following:\n\n* It is a network problem or a proxy issue regarding inbound\/outbound\ntraffic ? I can reach Microsoft but not Google in the port 443 when I don't specify proxy.\n* Why am I getting invalid certificate when it has nothing to do with\nthe certificates ?\n* How can I setup my APEX to use authentication on Azure or any other\nprovider for that matter when I have a proxy in the middle ?\n* As I use ORDS standalone, am I allow to keep using it or I need a\nreverse proxy with Tomcat ?\n\nI tried to configure the ACE to use HTTP\\_PROXY in the ports by running\n\n```\nbegin\n  sys.dbms_network_acl_admin.append_host_ace(\n    host        => 'myproxyserver'\n   ,lower_port  => 8080\n   ,upper_port  => 8080\n   ,ace         => xs$ace_type(\n      privilege_list     => xs$name_list('http_proxy')\n     ,granted            => true\n     ,principal_name     => 'MY_PRINCIPAL'\n     ,principal_type     => XS_ACL.PTYPE_DB\n    )\n  );\nend;\n\/\n\n```\n\nEven I grant to the ACE privileges over the wallet\n\n```\nSET SERVEROUTPUT ON\nBEGIN\n  DBMS_NETWORK_ACL_ADMIN.APPEND_WALLET_ACE\n  (\n    WALLET_PATH => 'file:\/home\/oracle\/wallet',\n    ACE => XS$ACE_TYPE(\n                        PRIVILEGE_LIST => XS$NAME_LIST('use_passwords','use_client_certificates'),\n                        PRINCIPAL_NAME => 'MY_PRINCIPAL',\n                        PRINCIPAL_TYPE => XS_ACL.PTYPE_DB\n                      )\n  );\nEXCEPTION WHEN OTHERS THEN\n  DBMS_OUTPUT.PUT_LINE('Error while configuring ACL for wallet: '|| SQLERRM);\nEND;\n\/\n\n```\n\nbut I am still getting the same error all over.\n\nAny help would be appreciated!\nThank you","response_j":"I had issue like this, it seems Oracle SSL library has some bugs. Finally I implemented some Java Source for OJVM, please read my answer here: <https:\/\/stackoverflow.com\/a\/60152830\/11272044>","response_k":"In my understanding,you will need to do following(in addition to what you did) :\n\n1. login to Apex as administrator\n2. From settings, go to 'Wallet'\n3. Add Wallet path(absolute path with prefix 'file:\/\/' and password you used for creating wallet\n\nNow, your problem should be solved."},{"question":"Let me explain what is happening:\n\n* Database: Oracle 19c\n* Apex: 19.1.0.00.15\n* ORDS standalone is 19.1.0.r0921545\n\nI did the tasks to configure an Apex Social Sign In to Microsoft AAD without almost any issue:\n\n* I created the authentication method in Apex.\n* I register my application and get the web credentials in Azure.\n* I created a wallet in my database with the root CA Microsoft certificates and configured the instance settings to usee that wallet.\n* My wallet in the database server contains the property auto\\_login to avoid using passwords.\n* I created the ACEs entries to allow connection to the login.microsoftonline.com in the port 443\n* Although it is not important for the purpose of the question itself and the error that is producing, just comment that I configured the wallet settings in the internal workspace in order to provide access to the wallet to the apex applications.\n\nFor some weeks the process was working fine, I was having a perfect Single Sing on mechanism for all my apex applications in the different workspaces. However, since some days ago, I am getting always the same error:\n\n**ORA-29024: Certificate validation failure**\n\nAfter some digging I realise that someone has configured a PROXY for outgoing traffic. Before even trying in Apex, I tried in SQL using APEX\\_WEB\\_SERVICE\n\nRequest with proxy settings to login.microsoftonline.com\n\n```\nselect apex_web_service.make_rest_request(\n    p_url         => 'https:\/\/login.microsoftonline.com',\n    p_http_method => 'GET',\n    p_wallet_path => 'file:\/home\/oracle\/wallet',\n    p_wallet_pwd => 'MyPassword' ,\n    p_proxy_override => 'http:\/\/myproxy:myport'\n  7  ) from dual;\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-29024: Certificate validation failure\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nRequest without proxy settings, just to see if I can get there\n\n```\nSQL> select apex_web_service.make_rest_request(\n  2      p_url         => 'https:\/\/login.microsoftonline.com',\n  3      p_http_method => 'GET',\n  4      p_wallet_path => 'file:\/home\/oracle\/wallet'\n  5* ) from dual\nSQL> \/\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-29024: Certificate validation failure\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nRequest to google using Proxy settings\n\n```\nselect apex_web_service.make_rest_request(\n    p_url         => 'https:\/\/google.com',\n    p_http_method => 'GET',\n    p_wallet_path => 'file:\/home\/oracle\/wallet',\n    p_wallet_pwd => 'MyPassword' ,\n  6      p_proxy_override => 'http:\/\/myproxy:myport'\n  7  ) from dual ;\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-29024: Certificate validation failure\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nRequest to google without proxy settings\n\n```\nSQL> select apex_web_service.make_rest_request(\n  2      p_url         => 'https:\/\/google.com',\n  3      p_http_method => 'GET',\n  4      p_wallet_path => 'file:\/home\/oracle\/wallet'\n  5* ) from dual\nSQL> \/\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-12535: TNS:operation timed out\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nMy questions are the following:\n\n* It is a network problem or a proxy issue regarding inbound\/outbound\ntraffic ? I can reach Microsoft but not Google in the port 443 when I don't specify proxy.\n* Why am I getting invalid certificate when it has nothing to do with\nthe certificates ?\n* How can I setup my APEX to use authentication on Azure or any other\nprovider for that matter when I have a proxy in the middle ?\n* As I use ORDS standalone, am I allow to keep using it or I need a\nreverse proxy with Tomcat ?\n\nI tried to configure the ACE to use HTTP\\_PROXY in the ports by running\n\n```\nbegin\n  sys.dbms_network_acl_admin.append_host_ace(\n    host        => 'myproxyserver'\n   ,lower_port  => 8080\n   ,upper_port  => 8080\n   ,ace         => xs$ace_type(\n      privilege_list     => xs$name_list('http_proxy')\n     ,granted            => true\n     ,principal_name     => 'MY_PRINCIPAL'\n     ,principal_type     => XS_ACL.PTYPE_DB\n    )\n  );\nend;\n\/\n\n```\n\nEven I grant to the ACE privileges over the wallet\n\n```\nSET SERVEROUTPUT ON\nBEGIN\n  DBMS_NETWORK_ACL_ADMIN.APPEND_WALLET_ACE\n  (\n    WALLET_PATH => 'file:\/home\/oracle\/wallet',\n    ACE => XS$ACE_TYPE(\n                        PRIVILEGE_LIST => XS$NAME_LIST('use_passwords','use_client_certificates'),\n                        PRINCIPAL_NAME => 'MY_PRINCIPAL',\n                        PRINCIPAL_TYPE => XS_ACL.PTYPE_DB\n                      )\n  );\nEXCEPTION WHEN OTHERS THEN\n  DBMS_OUTPUT.PUT_LINE('Error while configuring ACL for wallet: '|| SQLERRM);\nEND;\n\/\n\n```\n\nbut I am still getting the same error all over.\n\nAny help would be appreciated!\nThank you","response_j":"Thank you to all who post answers, but finally, after struggling for a while, I found the root cause. Actually Oracle was right after all, as Microsoft has changed the way the authentication is handled, either you are using Oauth2 or OpenID, when you use Office365 and Azure Active Directory.\n\nIn this case, my organisation is using Office 365 and at the beginning was enough with importing the PKI certificates from :\n\n<https:\/\/www.microsoft.com\/pki\/mscorp\/cps\/default.htm>\n\nAfter a change done in Azure Active Directory (AAD), you now need also the Global Sign certificates from [office.com](http:\/\/office.com)\n\nI hope it clarifies to other users who got in the same problem trying to authenticate with Azure Active Directory using Apex Social sign in.\n\nYou can download the certificates directly from office365.com\n\n[![enter image description here](https:\/\/i.stack.imgur.com\/zkbq2.png)](https:\/\/i.stack.imgur.com\/zkbq2.png)\n\nAfter adding the new two certificates to the wallet, you can now enter without issues:\n\n```\nselect apex_web_service.make_rest_request(\n    p_url         => 'https:\/\/login.microsoftonline.com',\n    p_http_method => 'GET',\n  4      p_wallet_path => 'file:\/home\/oracle\/wallet' ) from dual ;\n\nAPEX_WEB_SERVICE.MAKE_REST_REQUEST(P_URL=>'HTTPS:\/\/LOGIN.MICROSOFTONLINE.COM',P_\n--------------------------------------------------------------------------------\n\n<!-- Copyright (C) Microsoft Corporation. All rights reserved. -->\n<!DOCTYP\n\nSQL>\n\n```","response_k":"In my understanding,you will need to do following(in addition to what you did) :\n\n1. login to Apex as administrator\n2. From settings, go to 'Wallet'\n3. Add Wallet path(absolute path with prefix 'file:\/\/' and password you used for creating wallet\n\nNow, your problem should be solved."},{"question":"Let me explain what is happening:\n\n* Database: Oracle 19c\n* Apex: 19.1.0.00.15\n* ORDS standalone is 19.1.0.r0921545\n\nI did the tasks to configure an Apex Social Sign In to Microsoft AAD without almost any issue:\n\n* I created the authentication method in Apex.\n* I register my application and get the web credentials in Azure.\n* I created a wallet in my database with the root CA Microsoft certificates and configured the instance settings to usee that wallet.\n* My wallet in the database server contains the property auto\\_login to avoid using passwords.\n* I created the ACEs entries to allow connection to the login.microsoftonline.com in the port 443\n* Although it is not important for the purpose of the question itself and the error that is producing, just comment that I configured the wallet settings in the internal workspace in order to provide access to the wallet to the apex applications.\n\nFor some weeks the process was working fine, I was having a perfect Single Sing on mechanism for all my apex applications in the different workspaces. However, since some days ago, I am getting always the same error:\n\n**ORA-29024: Certificate validation failure**\n\nAfter some digging I realise that someone has configured a PROXY for outgoing traffic. Before even trying in Apex, I tried in SQL using APEX\\_WEB\\_SERVICE\n\nRequest with proxy settings to login.microsoftonline.com\n\n```\nselect apex_web_service.make_rest_request(\n    p_url         => 'https:\/\/login.microsoftonline.com',\n    p_http_method => 'GET',\n    p_wallet_path => 'file:\/home\/oracle\/wallet',\n    p_wallet_pwd => 'MyPassword' ,\n    p_proxy_override => 'http:\/\/myproxy:myport'\n  7  ) from dual;\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-29024: Certificate validation failure\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nRequest without proxy settings, just to see if I can get there\n\n```\nSQL> select apex_web_service.make_rest_request(\n  2      p_url         => 'https:\/\/login.microsoftonline.com',\n  3      p_http_method => 'GET',\n  4      p_wallet_path => 'file:\/home\/oracle\/wallet'\n  5* ) from dual\nSQL> \/\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-29024: Certificate validation failure\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nRequest to google using Proxy settings\n\n```\nselect apex_web_service.make_rest_request(\n    p_url         => 'https:\/\/google.com',\n    p_http_method => 'GET',\n    p_wallet_path => 'file:\/home\/oracle\/wallet',\n    p_wallet_pwd => 'MyPassword' ,\n  6      p_proxy_override => 'http:\/\/myproxy:myport'\n  7  ) from dual ;\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-29024: Certificate validation failure\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nRequest to google without proxy settings\n\n```\nSQL> select apex_web_service.make_rest_request(\n  2      p_url         => 'https:\/\/google.com',\n  3      p_http_method => 'GET',\n  4      p_wallet_path => 'file:\/home\/oracle\/wallet'\n  5* ) from dual\nSQL> \/\nERROR:\nORA-29273: HTTP request failed\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1035\nORA-12535: TNS:operation timed out\nORA-06512: at \"SYS.UTL_HTTP\", line 380\nORA-06512: at \"SYS.UTL_HTTP\", line 1148\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 934\nORA-06512: at \"APEX_190100.WWV_FLOW_WEB_SERVICES\", line 1580\nORA-06512: at \"APEX_190100.WWV_FLOW_WEBSERVICES_API\", line 408\nORA-06512: at line 1\n\n```\n\nMy questions are the following:\n\n* It is a network problem or a proxy issue regarding inbound\/outbound\ntraffic ? I can reach Microsoft but not Google in the port 443 when I don't specify proxy.\n* Why am I getting invalid certificate when it has nothing to do with\nthe certificates ?\n* How can I setup my APEX to use authentication on Azure or any other\nprovider for that matter when I have a proxy in the middle ?\n* As I use ORDS standalone, am I allow to keep using it or I need a\nreverse proxy with Tomcat ?\n\nI tried to configure the ACE to use HTTP\\_PROXY in the ports by running\n\n```\nbegin\n  sys.dbms_network_acl_admin.append_host_ace(\n    host        => 'myproxyserver'\n   ,lower_port  => 8080\n   ,upper_port  => 8080\n   ,ace         => xs$ace_type(\n      privilege_list     => xs$name_list('http_proxy')\n     ,granted            => true\n     ,principal_name     => 'MY_PRINCIPAL'\n     ,principal_type     => XS_ACL.PTYPE_DB\n    )\n  );\nend;\n\/\n\n```\n\nEven I grant to the ACE privileges over the wallet\n\n```\nSET SERVEROUTPUT ON\nBEGIN\n  DBMS_NETWORK_ACL_ADMIN.APPEND_WALLET_ACE\n  (\n    WALLET_PATH => 'file:\/home\/oracle\/wallet',\n    ACE => XS$ACE_TYPE(\n                        PRIVILEGE_LIST => XS$NAME_LIST('use_passwords','use_client_certificates'),\n                        PRINCIPAL_NAME => 'MY_PRINCIPAL',\n                        PRINCIPAL_TYPE => XS_ACL.PTYPE_DB\n                      )\n  );\nEXCEPTION WHEN OTHERS THEN\n  DBMS_OUTPUT.PUT_LINE('Error while configuring ACL for wallet: '|| SQLERRM);\nEND;\n\/\n\n```\n\nbut I am still getting the same error all over.\n\nAny help would be appreciated!\nThank you","response_j":"Thank you to all who post answers, but finally, after struggling for a while, I found the root cause. Actually Oracle was right after all, as Microsoft has changed the way the authentication is handled, either you are using Oauth2 or OpenID, when you use Office365 and Azure Active Directory.\n\nIn this case, my organisation is using Office 365 and at the beginning was enough with importing the PKI certificates from :\n\n<https:\/\/www.microsoft.com\/pki\/mscorp\/cps\/default.htm>\n\nAfter a change done in Azure Active Directory (AAD), you now need also the Global Sign certificates from [office.com](http:\/\/office.com)\n\nI hope it clarifies to other users who got in the same problem trying to authenticate with Azure Active Directory using Apex Social sign in.\n\nYou can download the certificates directly from office365.com\n\n[![enter image description here](https:\/\/i.stack.imgur.com\/zkbq2.png)](https:\/\/i.stack.imgur.com\/zkbq2.png)\n\nAfter adding the new two certificates to the wallet, you can now enter without issues:\n\n```\nselect apex_web_service.make_rest_request(\n    p_url         => 'https:\/\/login.microsoftonline.com',\n    p_http_method => 'GET',\n  4      p_wallet_path => 'file:\/home\/oracle\/wallet' ) from dual ;\n\nAPEX_WEB_SERVICE.MAKE_REST_REQUEST(P_URL=>'HTTPS:\/\/LOGIN.MICROSOFTONLINE.COM',P_\n--------------------------------------------------------------------------------\n\n<!-- Copyright (C) Microsoft Corporation. All rights reserved. -->\n<!DOCTYP\n\nSQL>\n\n```","response_k":"I had issue like this, it seems Oracle SSL library has some bugs. Finally I implemented some Java Source for OJVM, please read my answer here: <https:\/\/stackoverflow.com\/a\/60152830\/11272044>"},{"question":"I am using Omniauth in a Rails application for login, my omniauth.rb, is as show below:\n\n```\nOmniAuth.config.logger = Rails.logger\n\nRails.application.config.middleware.use OmniAuth::Builder do\n   provider :facebook, 'xxxxxxx', 'xxxxxxx'\n   provider :google_oauth2, 'xxxxxxxxx','xxxxxxxx'\nend\n\n```\n\nWhen a user attempts to login (via Facebook or Goolge) and denies permissions, get the following error:\n\n```\n OmniAuth::Strategies::OAuth2::CallbackError \n\n```\n\nwith this parameters:\n\n```\n{\"error\"=>\"access_denied\",\n \"error_code\"=>\"200\",\n \"error_description\"=>\"Permissions error\",\n \"error_reason\"=>\"user_denied\",\n \"state\"=>\"60daee5f78d9cc28972050ae8ca8f950bb4ed5958302bcea\"},\n\n```\n\nif the user accept, no problem and everything works fine.\n\nI've tried some of the possible solutions related with this error, and listed on this website, but none solved my problem. For example:\n\n[How to rescue OmniAuth::Strategies::OAuth2::CallbackError?](https:\/\/stackoverflow.com\/questions\/10737200\/how-to-rescue-omniauthstrategiesoauth2callbackerror)\n\n[Omniauth+facebook error when trying to cancel the popup](https:\/\/stackoverflow.com\/questions\/10647642\/omniauthfacebook-error-when-trying-to-cancel-the-popup)\n\nPlease, I need help to solve this problem.","response_j":"Our old friend `strace` solves this mystery.\n\nIn the `fflush(\"cat\")` case, awk quickly writes all three values while cat is still loading. When cat finishes loading, it reads all three values in sequence and writes them out at the same time.\n\nIn the case of `close(\"cat\")`, awk waits for the process to exit, at which point cat is guaranteed to have read the value, written it, and exited.\n\nI increased the numbers in the `fflush` case from 3 to 1000, and now `awk` reaches 120 before cat catches up, and from there on it works basically as you expect. It prints \"1,2,..,120,1,2,...,120,121,121,122,122,123,123,...\".\n\nNote that you're not guaranteed to see numbers in perfect pairs. awk blocks until the number has been written to the pipe, but it doesn't wait for cat to do anything with it.","response_k":"Not really something I have much experience of but the gawk manual tells us:\n\n> \n> fflush([filename])\n>  Flush any buffered output associated with filename,\n>  which is either a file opened for writing or a shell command for\n>  redirecting output **to a pipe** or coprocess.\n> \n> \n> \n\nNote that \"cat\" as used above is in none of the contexts the gawk manual says are affected by fflush().\n\nThe same manual for close() on the other hand says:\n\n> \n> close(filename [, how])\n>  Close the file filename for input or output.\n>  Alternatively, the argument may be a shell command that was used for\n>  creating a coprocess, or for redirecting **to or from a pipe**; then\n>  the coprocess or pipe is closed.\n> \n> \n> \n\nNote that the context in which you're using cat is redirecting **from** a pipe and so is included in the list of items affected by close() but not by fflush()."},{"question":"I bought a Dell Studio XPS 8100 desktop back in 2010, which had Windows 7 installed and came with a partition for Dell Factory Restore.\n\nAfter having installed Windows 10, what happened to that partition? Did the installation get rid of it? If not and I were to use it to do a Dell Factory Restore, would it \"reinstall\" Windows 7?\n\nSorry if this is a duplicate of a question somewhere, I didn't see one exactly like this asking about Windows 10 upgrade and Dell's Recovery partition.","response_j":"The recovery partition will not be touched nor upgraded during this process. If you did a factory restore, you would end up with Windows 7.","response_k":"If you run into a problem where you cannot access your recovery partition, or the partition is deleted, you can run a tool called DSRFIX and it should restore the recovery partition."},{"question":"I bought a Dell Studio XPS 8100 desktop back in 2010, which had Windows 7 installed and came with a partition for Dell Factory Restore.\n\nAfter having installed Windows 10, what happened to that partition? Did the installation get rid of it? If not and I were to use it to do a Dell Factory Restore, would it \"reinstall\" Windows 7?\n\nSorry if this is a duplicate of a question somewhere, I didn't see one exactly like this asking about Windows 10 upgrade and Dell's Recovery partition.","response_j":"From what I am seeing, if there is not a system partition on the drive, only the Dell recovery partition & OS partition, then Windows 10 will alter the boot folder of that partition. This effects the PE recovery environment & will break the factory recovery. Even after reapplying the Factory.wim of windows 7 and it blue screens, Windows 10 PE recovery environment will start & not help with the start up issues. I'm presently trying to undo all of this on a Dell Inspiron laptop at present.\n\nI don't think this will happen if there is a 100MB system partition on the drive as this is where the Windows 10 recovery environment will install (in the boot folder). Thus leaving the Dell Recovery partition untouched leaving the F11 function in tact to be able to factory install the OS that shipped with the laptop. Isn't the digital life wonderful!!!","response_k":"If you run into a problem where you cannot access your recovery partition, or the partition is deleted, you can run a tool called DSRFIX and it should restore the recovery partition."},{"question":"I know I can pass parameters to java to limit the amount of memory used, but that doesn't change the application behavior. (assuming I will get an out of memory exception or similar)\n\nI would like to limit the amount of memory that solr uses. I am assuming it is as simple as setting a single configuration option, but googling so far has been fruitless.\n\nAny ideas? Thanks!\n\nEdit: I just want to note that I understand there will be a tradeoff between memory usage and execution speed. In this case, I am willing to sacrifice speed to reduce memory footprint. I also understand that it is possible that solr does not support the option of tuning this tradeoff.","response_j":"Check out this article, it should address your needs\n\n<http:\/\/blogs.technet.com\/b\/grouppolicy\/archive\/2009\/07\/30\/security-filtering-wmi-filtering-and-item-level-targeting-in-group-policy-preferences.aspx>\n\nand like Joe said, yes you can use groups for computers as well","response_k":"Yes, you can use a security group populated with computer accounts to filter Group Policy."},{"question":"I am using video.js (<http:\/\/www.videojs.com\/>) to build a video approval system and need to log user actions in the player. I can do this easily enough with play, pause, end etc. but have hit a problem when trying to log seeks.\n\nI want to be able to log the start and end times of any seeks within the plaback, so we know if the user has not actually watched a section of the video. The player seems to offer events to support this, but I am struggling to get correct timings from it.\n\nWhen the user skips through a video the player emits the following events in order: pause, seeking, seeked, play.\n\nIf I query the player object at any of these events using currentTime() the result is always the end time for the seek, even on the initial pause event. This means I can log where the seek ended but not where it started.\n\nCan anyone help me to find the position in the video where the seek begins?\n\nIf this is not possible, I'd settle for a way to disable seeking during playback.\n\nEDIT: adding code as requested. It's pretty simple:\n\n```\nvar trackedPlayer = videojs('pvp-player');\n\ntrackedPlayer.on(\"play\", function (e) {\n    console.log(\"Video playback started: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"pause\", function (e) {\n    console.log(\"Video playback paused: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeking\", function (e) {\n    console.log(\"Video seeking: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeked\", function (e) {\n    console.log(\"Video seek ended: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"ended\", function (e) {\n    console.log(\"Video playback ended.\");\n},);\n\n```\n\nIf I can get all the tracking I want I will replace console.log with ajax calls to store the data.","response_j":"You can listen to `timeupdate` und take the next to last value you got there before `seeking` is called as your source:\n\n```\nvar previousTime = 0;\nvar currentTime = 0;\ntrackedPlayer.on('timeupdate', function() {\n    previousTime = currentTime;\n    currentTime = trackedPlayer.currentTime();\n},);\ntrackedPlayer.on('seeking', function() {\n    console.log('seeking from', previousTime, 'to', currentTime, '; delta:', currentTime - previousTime);\n},);\n\n```\n\nThis seems to work with the HTML5 tech. I have not tested with other techs. \n\nThere is, however, one glitch: the first time seeking a *paused* player yields only a small delta (and the almost-same previous value for both variables). But this shouldn\u2019t matter much since the delta is only a few hundred milliseconds (and I gather you\u2019re only interested in the \u201cfrom\u201d value).\n\n**Update**\n\n`seeked` is triggered far more infrequently than `seeking`. Try the following.\n\n```\nvar previousTime = 0;\nvar currentTime = 0;\nvar seekStart = null;\ntrackedPlayer.on('timeupdate', function() {\n    previousTime = currentTime;\n    currentTime = trackedPlayer.currentTime();\n},);\ntrackedPlayer.on('seeking', function() {\n    if(seekStart === null) {\n        seekStart = previousTime;\n    },\n},);\ntrackedPlayer.on('seeked', function() {\n    console.log('seeked from', seekStart, 'to', currentTime, '; delta:', currentTime - previousTime);\n    seekStart = null;\n},);\n\n```\n\nThere are also many libraries for debouncing function calls (in this case the call to your backend).","response_k":"Try with this code to know the length of video.\n\n```\nvar duration = document.getElementById(\"duration\");\nvar vid_duration = Math.round(document.getElementById(\"video\").duration);\n    \/\/alert(vid_duration);\nduration.innerHTML = vid_duration;\n    \/\/duration.firstChild.nodeValue = vid_duration;\n\n```"},{"question":"I am using video.js (<http:\/\/www.videojs.com\/>) to build a video approval system and need to log user actions in the player. I can do this easily enough with play, pause, end etc. but have hit a problem when trying to log seeks.\n\nI want to be able to log the start and end times of any seeks within the plaback, so we know if the user has not actually watched a section of the video. The player seems to offer events to support this, but I am struggling to get correct timings from it.\n\nWhen the user skips through a video the player emits the following events in order: pause, seeking, seeked, play.\n\nIf I query the player object at any of these events using currentTime() the result is always the end time for the seek, even on the initial pause event. This means I can log where the seek ended but not where it started.\n\nCan anyone help me to find the position in the video where the seek begins?\n\nIf this is not possible, I'd settle for a way to disable seeking during playback.\n\nEDIT: adding code as requested. It's pretty simple:\n\n```\nvar trackedPlayer = videojs('pvp-player');\n\ntrackedPlayer.on(\"play\", function (e) {\n    console.log(\"Video playback started: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"pause\", function (e) {\n    console.log(\"Video playback paused: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeking\", function (e) {\n    console.log(\"Video seeking: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeked\", function (e) {\n    console.log(\"Video seek ended: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"ended\", function (e) {\n    console.log(\"Video playback ended.\");\n},);\n\n```\n\nIf I can get all the tracking I want I will replace console.log with ajax calls to store the data.","response_j":"I needed to find the same value for a project I was working on so I could determine whether or not a user was skipping forward or backward in a videojs player.\n\nInitially, I thought to save the currentTime() a user was seeking **from** on **timeupdate** then immediately removing my timeupdate listener once **seeking** was dispatched. While this worked in some browsers like Chrome, unfortunately, I found that other browsers continued to fire timeupdate more frequently and would continue to update the currentTime() I was saving after the player actually seeked.\n\nHere was the solution that ultimately worked across Safari\/Chrome\/Firefox. I have yet to test in IE.\n\n```\nvar previousTime = 0,\n    currentTime = 0,\n    completeTime = 0,\n    position = 0;\n\ntrackedPlayer.on('timeupdate', function() {\n    previousTime = currentTime;\n    currentTime = Math.floor(player.currentTime());\n\n    \/\/ save 'position' so long as time is moving forward with each update\n    if (previousTime < currentTime) {\n        position = previousTime;\n        previousTime = currentTime;\n    },\n},);\n\n\/\/ when seeking starts\ntrackedPlayer.on('seeking', function() {\n    player.off('timeupdate', onTimeUpdate);\n\n    player.one('seeked', onSeekComplete);\n},);\n\n\/\/ when seeking completes\ntrackedPlayer.on('seeked', function() {\n    completeTime = Math.floor(player.currentTime());\n    console.log(\"User came from: \" + position);\n    console.log(\"User ended at: \" + completeTime);\n},);\n\n```","response_k":"Try with this code to know the length of video.\n\n```\nvar duration = document.getElementById(\"duration\");\nvar vid_duration = Math.round(document.getElementById(\"video\").duration);\n    \/\/alert(vid_duration);\nduration.innerHTML = vid_duration;\n    \/\/duration.firstChild.nodeValue = vid_duration;\n\n```"},{"question":"I am using video.js (<http:\/\/www.videojs.com\/>) to build a video approval system and need to log user actions in the player. I can do this easily enough with play, pause, end etc. but have hit a problem when trying to log seeks.\n\nI want to be able to log the start and end times of any seeks within the plaback, so we know if the user has not actually watched a section of the video. The player seems to offer events to support this, but I am struggling to get correct timings from it.\n\nWhen the user skips through a video the player emits the following events in order: pause, seeking, seeked, play.\n\nIf I query the player object at any of these events using currentTime() the result is always the end time for the seek, even on the initial pause event. This means I can log where the seek ended but not where it started.\n\nCan anyone help me to find the position in the video where the seek begins?\n\nIf this is not possible, I'd settle for a way to disable seeking during playback.\n\nEDIT: adding code as requested. It's pretty simple:\n\n```\nvar trackedPlayer = videojs('pvp-player');\n\ntrackedPlayer.on(\"play\", function (e) {\n    console.log(\"Video playback started: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"pause\", function (e) {\n    console.log(\"Video playback paused: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeking\", function (e) {\n    console.log(\"Video seeking: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeked\", function (e) {\n    console.log(\"Video seek ended: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"ended\", function (e) {\n    console.log(\"Video playback ended.\");\n},);\n\n```\n\nIf I can get all the tracking I want I will replace console.log with ajax calls to store the data.","response_j":"I know this is an old post but this is the only solution that worked for me.\n\n```\nvar counter = 0;\nvar beforeTimeChange = 0;\n\nfunction handleSeeking() {\n  var timeoutTime = 300;\n  var beforeCounter = counter + 1;\n  if (trackedPlayer.cache_.currentTime === trackedPlayer.duration()) {\n    return;\n    \/\/ when video starts over, calls seek\n  },\n  beforeTimeChange = beforeTimeChange || trackedPlayer.cache_.currentTime;\n  setTimeout(function() {\n    if (beforeCounter === counter) {\n        console.log('before seek', beforeTimeChange, '\\nafter seek', trackedPlayer.currentTime() - (timeoutTime \/ 1000));\n        counter = 0;\n        beforeTimeChange = 0;\n    },\n  },, timeoutTime);\n  counter++;\n},\n\ntrackedPlayer.on('seeking', handleSeeking);\n\n```","response_k":"Try with this code to know the length of video.\n\n```\nvar duration = document.getElementById(\"duration\");\nvar vid_duration = Math.round(document.getElementById(\"video\").duration);\n    \/\/alert(vid_duration);\nduration.innerHTML = vid_duration;\n    \/\/duration.firstChild.nodeValue = vid_duration;\n\n```"},{"question":"I am using video.js (<http:\/\/www.videojs.com\/>) to build a video approval system and need to log user actions in the player. I can do this easily enough with play, pause, end etc. but have hit a problem when trying to log seeks.\n\nI want to be able to log the start and end times of any seeks within the plaback, so we know if the user has not actually watched a section of the video. The player seems to offer events to support this, but I am struggling to get correct timings from it.\n\nWhen the user skips through a video the player emits the following events in order: pause, seeking, seeked, play.\n\nIf I query the player object at any of these events using currentTime() the result is always the end time for the seek, even on the initial pause event. This means I can log where the seek ended but not where it started.\n\nCan anyone help me to find the position in the video where the seek begins?\n\nIf this is not possible, I'd settle for a way to disable seeking during playback.\n\nEDIT: adding code as requested. It's pretty simple:\n\n```\nvar trackedPlayer = videojs('pvp-player');\n\ntrackedPlayer.on(\"play\", function (e) {\n    console.log(\"Video playback started: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"pause\", function (e) {\n    console.log(\"Video playback paused: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeking\", function (e) {\n    console.log(\"Video seeking: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeked\", function (e) {\n    console.log(\"Video seek ended: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"ended\", function (e) {\n    console.log(\"Video playback ended.\");\n},);\n\n```\n\nIf I can get all the tracking I want I will replace console.log with ajax calls to store the data.","response_j":"For a more accurate solution, you can listen to the events that trigger the seek such as mousedown on progress bar, left key, right key etc., and get the current time from these events.\nFor example in version 7.10.2 you can do the following,\n\n```\nlet seekStartTime;\n\nplayer.controlBar.progressControl.on('mousedown', () => seekStartTime = player.currentTime());\nplayer.controlBar.progressControl.seekBar.on('mousedown', () => seekStartTime = player.currentTime());\nplayer.on('keydown', (e) => {\n    if (e.key === \"ArrowLeft\" || e.key === \"ArrowRight\") {\n         seekStartTime = player.currentTime();\n    },\n},);\n\nconsole.log(seekStartTime);\n\n```\n\n**Note 1:** There are two seperate mousedown event listeners for progress control and seek bar. This is because the video can be seeked by clicking outside the seek bar on the progress control as well.\n\n**Note 2:** Seeking using hotkey numbers does not pause the video. However, if necessary you can add those keydown listeners too.","response_k":"Try with this code to know the length of video.\n\n```\nvar duration = document.getElementById(\"duration\");\nvar vid_duration = Math.round(document.getElementById(\"video\").duration);\n    \/\/alert(vid_duration);\nduration.innerHTML = vid_duration;\n    \/\/duration.firstChild.nodeValue = vid_duration;\n\n```"},{"question":"I am using video.js (<http:\/\/www.videojs.com\/>) to build a video approval system and need to log user actions in the player. I can do this easily enough with play, pause, end etc. but have hit a problem when trying to log seeks.\n\nI want to be able to log the start and end times of any seeks within the plaback, so we know if the user has not actually watched a section of the video. The player seems to offer events to support this, but I am struggling to get correct timings from it.\n\nWhen the user skips through a video the player emits the following events in order: pause, seeking, seeked, play.\n\nIf I query the player object at any of these events using currentTime() the result is always the end time for the seek, even on the initial pause event. This means I can log where the seek ended but not where it started.\n\nCan anyone help me to find the position in the video where the seek begins?\n\nIf this is not possible, I'd settle for a way to disable seeking during playback.\n\nEDIT: adding code as requested. It's pretty simple:\n\n```\nvar trackedPlayer = videojs('pvp-player');\n\ntrackedPlayer.on(\"play\", function (e) {\n    console.log(\"Video playback started: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"pause\", function (e) {\n    console.log(\"Video playback paused: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeking\", function (e) {\n    console.log(\"Video seeking: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeked\", function (e) {\n    console.log(\"Video seek ended: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"ended\", function (e) {\n    console.log(\"Video playback ended.\");\n},);\n\n```\n\nIf I can get all the tracking I want I will replace console.log with ajax calls to store the data.","response_j":"I needed to find the start and end of a seeking action in my project and I used @Motorcykey answer and it worked, but there was a small bug. when I tried to seek to a time before the current time while the player was paused, the `position` didn't get updated. so I added just one line and it fixed it. I've tried other solutions too but so far this was the best solution that I've found. Here's the code snippet on player 'seeked'\n\n```\nplayer.on('seeked', function () {\n    completeTime = Math.floor(player.currentTime());\n    console.log(\"User came from: \" + position);\n    console.log(\"User ended at: \" + completeTime);\n    position= Math.floor(player.currentTime())\n},);\n\n```","response_k":"Try with this code to know the length of video.\n\n```\nvar duration = document.getElementById(\"duration\");\nvar vid_duration = Math.round(document.getElementById(\"video\").duration);\n    \/\/alert(vid_duration);\nduration.innerHTML = vid_duration;\n    \/\/duration.firstChild.nodeValue = vid_duration;\n\n```"},{"question":"I am using video.js (<http:\/\/www.videojs.com\/>) to build a video approval system and need to log user actions in the player. I can do this easily enough with play, pause, end etc. but have hit a problem when trying to log seeks.\n\nI want to be able to log the start and end times of any seeks within the plaback, so we know if the user has not actually watched a section of the video. The player seems to offer events to support this, but I am struggling to get correct timings from it.\n\nWhen the user skips through a video the player emits the following events in order: pause, seeking, seeked, play.\n\nIf I query the player object at any of these events using currentTime() the result is always the end time for the seek, even on the initial pause event. This means I can log where the seek ended but not where it started.\n\nCan anyone help me to find the position in the video where the seek begins?\n\nIf this is not possible, I'd settle for a way to disable seeking during playback.\n\nEDIT: adding code as requested. It's pretty simple:\n\n```\nvar trackedPlayer = videojs('pvp-player');\n\ntrackedPlayer.on(\"play\", function (e) {\n    console.log(\"Video playback started: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"pause\", function (e) {\n    console.log(\"Video playback paused: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeking\", function (e) {\n    console.log(\"Video seeking: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"seeked\", function (e) {\n    console.log(\"Video seek ended: \" + trackedPlayer.currentTime());\n},);\n\ntrackedPlayer.on(\"ended\", function (e) {\n    console.log(\"Video playback ended.\");\n},);\n\n```\n\nIf I can get all the tracking I want I will replace console.log with ajax calls to store the data.","response_j":"You can listen to `timeupdate` und take the next to last value you got there before `seeking` is called as your source:\n\n```\nvar previousTime = 0;\nvar currentTime = 0;\ntrackedPlayer.on('timeupdate', function() {\n    previousTime = currentTime;\n    currentTime = trackedPlayer.currentTime();\n},);\ntrackedPlayer.on('seeking', function() {\n    console.log('seeking from', previousTime, 'to', currentTime, '; delta:', currentTime - previousTime);\n},);\n\n```\n\nThis seems to work with the HTML5 tech. I have not tested with other techs. \n\nThere is, however, one glitch: the first time seeking a *paused* player yields only a small delta (and the almost-same previous value for both variables). But this shouldn\u2019t matter much since the delta is only a few hundred milliseconds (and I gather you\u2019re only interested in the \u201cfrom\u201d value).\n\n**Update**\n\n`seeked` is triggered far more infrequently than `seeking`. Try the following.\n\n```\nvar previousTime = 0;\nvar currentTime = 0;\nvar seekStart = null;\ntrackedPlayer.on('timeupdate', function() {\n    previousTime = currentTime;\n    currentTime = trackedPlayer.currentTime();\n},);\ntrackedPlayer.on('seeking', function() {\n    if(seekStart === null) {\n        seekStart = previousTime;\n    },\n},);\ntrackedPlayer.on('seeked', function() {\n    console.log('seeked from', seekStart, 'to', currentTime, '; delta:', currentTime - previousTime);\n    seekStart = null;\n},);\n\n```\n\nThere are also many libraries for debouncing function calls (in this case the call to your backend).","response_k":"I needed to find the same value for a project I was working on so I could determine whether or not a user was skipping forward or backward in a videojs player.\n\nInitially, I thought to save the currentTime() a user was seeking **from** on **timeupdate** then immediately removing my timeupdate listener once **seeking** was dispatched. While this worked in some browsers like Chrome, unfortunately, I found that other browsers continued to fire timeupdate more frequently and would continue to update the currentTime() I was saving after the player actually seeked.\n\nHere was the solution that ultimately worked across Safari\/Chrome\/Firefox. I have yet to test in IE.\n\n```\nvar previousTime = 0,\n    currentTime = 0,\n    completeTime = 0,\n    position = 0;\n\ntrackedPlayer.on('timeupdate', function() {\n    previousTime = currentTime;\n    currentTime = Math.floor(player.currentTime());\n\n    \/\/ save 'position' so long as time is moving forward with each update\n    if (previousTime < currentTime) {\n        position = previousTime;\n        previousTime = currentTime;\n    },\n},);\n\n\/\/ when seeking starts\ntrackedPlayer.on('seeking', function() {\n    player.off('timeupdate', onTimeUpdate);\n\n    player.one('seeked', onSeekComplete);\n},);\n\n\/\/ when seeking completes\ntrackedPlayer.on('seeked', function() {\n    completeTime = Math.floor(player.currentTime());\n    console.log(\"User came from: \" + position);\n    console.log(\"User ended at: \" + completeTime);\n},);\n\n```"}]
diff -ruN marc_original/third_party/torchtune/tests/assets/tiktoken_small.model marc/third_party/torchtune/tests/assets/tiktoken_small.model
--- marc_original/third_party/torchtune/tests/assets/tiktoken_small.model	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/tiktoken_small.model	2025-02-20 17:49:29.562024266 -0500
@@ -0,0 +1,2000 @@
+AA== 0
+AQ== 1
+Ag== 2
+Aw== 3
+BA== 4
+BQ== 5
+Bg== 6
+Bw== 7
+CA== 8
+CQ== 9
+Cg== 10
+Cw== 11
+DA== 12
+DQ== 13
+Dg== 14
+Dw== 15
+EA== 16
+EQ== 17
+Eg== 18
+Ew== 19
+FA== 20
+FQ== 21
+Fg== 22
+Fw== 23
+GA== 24
+GQ== 25
+Gg== 26
+Gw== 27
+HA== 28
+HQ== 29
+Hg== 30
+Hw== 31
+IA== 32
+IQ== 33
+Ig== 34
+Iw== 35
+JA== 36
+JQ== 37
+Jg== 38
+Jw== 39
+KA== 40
+KQ== 41
+Kg== 42
+Kw== 43
+LA== 44
+LQ== 45
+Lg== 46
+Lw== 47
+MA== 48
+MQ== 49
+Mg== 50
+Mw== 51
+NA== 52
+NQ== 53
+Ng== 54
+Nw== 55
+OA== 56
+OQ== 57
+Og== 58
+Ow== 59
+PA== 60
+PQ== 61
+Pg== 62
+Pw== 63
+QA== 64
+QQ== 65
+Qg== 66
+Qw== 67
+RA== 68
+RQ== 69
+Rg== 70
+Rw== 71
+SA== 72
+SQ== 73
+Sg== 74
+Sw== 75
+TA== 76
+TQ== 77
+Tg== 78
+Tw== 79
+UA== 80
+UQ== 81
+Ug== 82
+Uw== 83
+VA== 84
+VQ== 85
+Vg== 86
+Vw== 87
+WA== 88
+WQ== 89
+Wg== 90
+Ww== 91
+XA== 92
+XQ== 93
+Xg== 94
+Xw== 95
+YA== 96
+YQ== 97
+Yg== 98
+Yw== 99
+ZA== 100
+ZQ== 101
+Zg== 102
+Zw== 103
+aA== 104
+aQ== 105
+ag== 106
+aw== 107
+bA== 108
+bQ== 109
+bg== 110
+bw== 111
+cA== 112
+cQ== 113
+cg== 114
+cw== 115
+dA== 116
+dQ== 117
+dg== 118
+dw== 119
+eA== 120
+eQ== 121
+eg== 122
+ew== 123
+fA== 124
+fQ== 125
+fg== 126
+fw== 127
+gA== 128
+gQ== 129
+gg== 130
+gw== 131
+hA== 132
+hQ== 133
+hg== 134
+hw== 135
+iA== 136
+iQ== 137
+ig== 138
+iw== 139
+jA== 140
+jQ== 141
+jg== 142
+jw== 143
+kA== 144
+kQ== 145
+kg== 146
+kw== 147
+lA== 148
+lQ== 149
+lg== 150
+lw== 151
+mA== 152
+mQ== 153
+mg== 154
+mw== 155
+nA== 156
+nQ== 157
+ng== 158
+nw== 159
+oA== 160
+oQ== 161
+og== 162
+ow== 163
+pA== 164
+pQ== 165
+pg== 166
+pw== 167
+qA== 168
+qQ== 169
+qg== 170
+qw== 171
+rA== 172
+rQ== 173
+rg== 174
+rw== 175
+sA== 176
+sQ== 177
+sg== 178
+sw== 179
+tA== 180
+tQ== 181
+tg== 182
+tw== 183
+uA== 184
+uQ== 185
+ug== 186
+uw== 187
+vA== 188
+vQ== 189
+vg== 190
+vw== 191
+wA== 192
+wQ== 193
+wg== 194
+ww== 195
+xA== 196
+xQ== 197
+xg== 198
+xw== 199
+yA== 200
+yQ== 201
+yg== 202
+yw== 203
+zA== 204
+zQ== 205
+zg== 206
+zw== 207
+0A== 208
+0Q== 209
+0g== 210
+0w== 211
+1A== 212
+1Q== 213
+1g== 214
+1w== 215
+2A== 216
+2Q== 217
+2g== 218
+2w== 219
+3A== 220
+3Q== 221
+3g== 222
+3w== 223
+4A== 224
+4Q== 225
+4g== 226
+4w== 227
+5A== 228
+5Q== 229
+5g== 230
+5w== 231
+6A== 232
+6Q== 233
+6g== 234
+6w== 235
+7A== 236
+7Q== 237
+7g== 238
+7w== 239
+8A== 240
+8Q== 241
+8g== 242
+8w== 243
+9A== 244
+9Q== 245
+9g== 246
+9w== 247
++A== 248
++Q== 249
++g== 250
++w== 251
+/A== 252
+/Q== 253
+/g== 254
+/w== 255
+IHQ= 256
+aGU= 257
+IGE= 258
+aW4= 259
+IHM= 260
+IHc= 261
+IHRoZQ== 262
+IG8= 263
+cmU= 264
+IGI= 265
+b3U= 266
+ZWQ= 267
+IG0= 268
+bmQ= 269
+IEk= 270
+aGE= 271
+aXQ= 272
+ZXI= 273
+aW5n 274
+IGY= 275
+aXM= 276
+IHRv 277
+ZW4= 278
+b24= 279
+b3I= 280
+YXM= 281
+IGM= 282
+IG9m 283
+IGFuZA== 284
+IGQ= 285
+bGw= 286
+YXQ= 287
+YW4= 288
+YXI= 289
+IHA= 290
+IG4= 291
+IGlu 292
+bGU= 293
+b20= 294
+b3Q= 295
+IGJl 296
+IGg= 297
+dXQ= 298
+b3c= 299
+ZXM= 300
+aGF0 301
+IGc= 302
+IGhl 303
+IGhh 304
+IGw= 305
+IHdhcw== 306
+bGQ= 307
+Z2g= 308
+aWQ= 309
+Y2g= 310
+IHRo 311
+IGl0 312
+YXk= 313
+IG9u 314
+Y2U= 315
+c2U= 316
+ZW50 317
+IHN0 318
+bHk= 319
+dmU= 320
+ZXQ= 321
+c3Q= 322
+IFQ= 323
+IGU= 324
+IHk= 325
+Z2h0 326
+aXI= 327
+IG1l 328
+b28= 329
+YWw= 330
+aXRo 331
+IHJl 332
+aW0= 333
+IHRoYXQ= 334
+IGFz 335
+b3VsZA== 336
+cm8= 337
+YWQ= 338
+aW9u 339
+Lgo= 340
+aGVy 341
+IG15 342
+Y3Q= 343
+IG5vdA== 344
+IHdpdGg= 345
+IGZvcg== 346
+IHU= 347
+a2U= 348
+IHlvdQ== 349
+IFM= 350
+IGlz 351
+aWdodA== 352
+Igo= 353
+YW0= 354
+aWM= 355
+dXI= 356
+IGF0 357
+Li4= 358
+YWM= 359
+dGVy 360
+IHdo 361
+IGFu 362
+IHdl 363
+IFRoZQ== 364
+aWY= 365
+IG9y 366
+IGJ1dA== 367
+dmVy 368
+ICI= 369
+IHI= 370
+b3V0 371
+b21l 372
+IGhhZA== 373
+cHA= 374
+cXU= 375
+IHN1 376
+IHRoaXM= 377
+cmVk 378
+YXJk 379
+IHNv 380
+ZWxs 381
+IHdvdWxk 382
+IGhpcw== 383
+IHNo 384
+aW5l 385
+cmE= 386
+IHNl 387
+IGJ5 388
+LiIK 389
+IFA= 390
+aGVu 391
+IEE= 392
+IGhhdmU= 393
+IGZy 394
+IHNh 395
+IEg= 396
+IG9uZQ== 397
+ZW0= 398
+a2Vk 399
+aXJ0 400
+ZWN0 401
+IGhpbQ== 402
+IGxp 403
+IGFi 404
+YXRpb24= 405
+aGluZw== 406
+dGhl 407
+IFI= 408
+IGxl 409
+c3M= 410
+IFc= 411
+Y3U= 412
+aWxs 413
+J3Q= 414
+YXJ0 415
+YWxs 416
+LAo= 417
+b3du 418
+b3Jl 419
+IGFsbA== 420
+IGs= 421
+IGdv 422
+aGlydA== 423
+YW5k 424
+IG91dA== 425
+YW1l 426
+YWlu 427
+IGlm 428
+IG5v 429
+IGRv 430
+IHRoZXk= 431
+b29s 432
+dW4= 433
+dG8= 434
+IHVw 435
+IFJlZA== 436
+IG5l 437
+IEs= 438
+IGZyb20= 439
+IFNoaXJ0 440
+IHdvcg== 441
+b25n 442
+IHRoZXJl 443
+IHNhaWQ= 444
+cmk= 445
+YW50 446
+IEI= 447
+IGFueQ== 448
+dWQ= 449
+aW5k 450
+IHdoaQ== 451
+YWI= 452
+b3VuZA== 453
+IGFib3V0 454
+IHRoZW0= 455
+Y3Vw 456
+YWs= 457
+IGRl 458
+IHRl 459
+IE0= 460
+YWtl 461
+Y3VwaW5l 462
+aWc= 463
+IHdlcmU= 464
+b3JjdXBpbmU= 465
+aWw= 466
+Y2hvb2w= 467
+IHJv 468
+b29k 469
+IGFyZQ== 470
+aXZl 471
+IGxpa2U= 472
+eW8= 473
+IGhvdQ== 474
+J3M= 475
+b25l 476
+dXM= 477
+ZWw= 478
+dWw= 479
+YWNr 480
+b3A= 481
+LCI= 482
+dGg= 483
+YWNoZXI= 484
+dW0= 485
+YW5n 486
+IGZh 487
+YWc= 488
+IHNjaG9vbA== 489
+IGo= 490
+dGU= 491
+b2s= 492
+ZXNz 493
+dXN0 494
+ZXJz 495
+Li4uLg== 496
+IEM= 497
+dGhlcg== 498
+aGFu 499
+IHdoZW4= 500
+IHNw 501
+IG1hbg== 502
+IGNhbg== 503
+b3VnaA== 504
+IHdobw== 505
+IGdldA== 506
+IGRpZA== 507
+IHBv 508
+Y2k= 509
+IGFs 510
+aXN0 511
+IGNvbQ== 512
+bGY= 513
+YXU= 514
+IFBvcmN1cGluZQ== 515
+IHdoaWNo 516
+dmVu 517
+IGFm 518
+d24= 519
+YXNz 520
+YmVy 521
+IGV4 522
+b3Vz 523
+ZXN0 524
+bG8= 525
+IHRy 526
+ZWxsb3c= 527
+IHNheQ== 528
+b3VnaHQ= 529
+IHJvb20= 530
+IHNvbWU= 531
+LS0= 532
+IE8= 533
+YXRl 534
+IHY= 535
+aGVk 536
+YXA= 537
+IHR3 538
+IGJlYw== 539
+cmVl 540
+amVjdA== 541
+a3M= 542
+IGNvbg== 543
+IGJlZW4= 544
+ZW50cw== 545
+aWRl 546
+IGNvdWxk 547
+IEc= 548
+ZXA= 549
+IHBybw== 550
+bnQ= 551
+IGhvdXNl 552
+IGFn 553
+IElm 554
+IGtu 555
+IGZlbGxvdw== 556
+IHdoYXQ= 557
+d2F5 558
+aXNo 559
+IGFt 560
+aXRl 561
+bmRlcg== 562
+aW1l 563
+IHBy 564
+IHRlYWNoZXI= 565
+YXJl 566
+IGJv 567
+IHNoZQ== 568
+IE4= 569
+aWNl 570
+YXN0 571
+dXJl 572
+aWU= 573
+IHN1Y2g= 574
+dXRlbg== 575
+dXRlbmJlcg== 576
+dXRlbmJlcmc= 577
+IHF1 578
+bG93bg== 579
+IHdy 580
+cHQ= 581
+IEhl 582
+IHN0dWQ= 583
+aGVyZQ== 584
+IG1vcmU= 585
+cnk= 586
+dHRlcg== 587
+IFk= 588
+IG1heQ== 589
+aXR5 590
+IGxvbw== 591
+IG90aGVy 592
+aGlz 593
+IFBybw== 594
+IHdpbGw= 595
+IEl0 596
+b3J0 597
+IHNob3VsZA== 598
+dmVyeQ== 599
+d2U= 600
+IHBs 601
+YXNo 602
+LiI= 603
+IGFwcA== 604
+IGRheQ== 605
+dXJu 606
+cG8= 607
+IGhlcg== 608
+ICA= 609
+bm90 610
+Y2s= 611
+IHVu 612
+aGk= 613
+dmluZw== 614
+IG9sZA== 615
+IHRpbWU= 616
+IlQ= 617
+IHdheQ== 618
+YWJsZQ== 619
+PyIK 620
+IENsb3du 621
+IG9ubHk= 622
+dWI= 623
+YWNo 624
+IG9mZg== 625
+IHRoYW4= 626
+YWxseQ== 627
+IHRoZWly 628
+YmU= 629
+a2luZw== 630
+b3RoZXI= 631
+YXJ5 632
+YW5z 633
+YXRlZA== 634
+c2VsZg== 635
+IGdvaW5n 636
+dWNo 637
+b2xs 638
+IGJhY2s= 639
+aXlv 640
+LXQ= 641
+YW5jZQ== 642
+YWRl 643
+IFByb2plY3Q= 644
+c3A= 645
+IHR3bw== 646
+IHRob3VnaHQ= 647
+c28= 648
+IHJpZ2h0 649
+IGhlYWQ= 650
+dmVk 651
+IEQ= 652
+IHByZQ== 653
+IHNlZQ== 654
+IHVz 655
+IHN0dWRlbnRz 656
+Y2lw 657
+IGRvbg== 658
+IG5pZ2h0 659
+aW5jaXA= 660
+IEtpeW8= 661
+cGw= 662
+YXJlZA== 663
+IEd1dGVuYmVyZw== 664
+IGNv 665
+IGhvdw== 666
+b21ldA== 667
+ZmY= 668
+Ikk= 669
+LC0t 670
+IGFza2Vk 671
+aW5jaXBhbA== 672
+ZXZlcg== 673
+IGFj 674
+IEY= 675
+IG1ha2U= 676
+aXR0 677
+IG1pZ2h0 678
+Z2U= 679
+bGVk 680
+IGFmdGVy 681
+aWdu 682
+IGdy 683
+IG1hZGU= 684
+ZGQ= 685
+IGtub3c= 686
+IGNvbWU= 687
+IGJy 688
+dGhpbmc= 689
+IEJ1dA== 690
+IG1hdA== 691
+IE9u 692
+b3J5 693
+Y2w= 694
+IEU= 695
+Ymxl 696
+b2c= 697
+IHlvdXI= 698
+dWxs 699
+IHdvcms= 700
+ZWFy 701
+IHRocmVl 702
+aWVk 703
+YnV0 704
+VGhl 705
+cGU= 706
+YWNl 707
+IHN0YXJ0 708
+aWNr 709
+IG92ZXI= 710
+b3Vy 711
+IG11Y2g= 712
+IHdhbnQ= 713
+aW1w 714
+IHBhcnQ= 715
+aG8= 716
+aW5r 717
+ZW5jZQ== 718
+IGRvd24= 719
+IGV2ZW4= 720
+IHByaW5jaXBhbA== 721
+bGluZw== 722
+b3VudA== 723
+YXVzZQ== 724
+IGNs 725
+IGJs 726
+LXRt 727
+b21ldGhpbmc= 728
+IGludG8= 729
+b3Jt 730
+b2t5bw== 731
+IGRpcw== 732
+IGZl 733
+IGZhY2U= 734
+Li4uLi4u 735
+cmVzcw== 736
+bWVudA== 737
+aXJl 738
+IGFy 739
+dHk= 740
+IG1v 741
+cmVhdA== 742
+IGZpcg== 743
+cGVy 744
+IG91cg== 745
+Y28= 746
+IHRoZW4= 747
+IHRvbGQ= 748
+aW5ncw== 749
+IHRha2U= 750
+IGJlZw== 751
+bmVy 752
+aXRpb24= 753
+b3Nl 754
+IG93bg== 755
+IGFnYWlu 756
+IHNlZW0= 757
+aXNl 758
+IHdhdA== 759
+Ilc= 760
+IGZhcg== 761
+YWtpbmc= 762
+Zm9yZQ== 763
+YWR5 764
+LXM= 765
+bGVzcw== 766
+IHJldA== 767
+IHNoYQ== 768
+IGNhbWU= 769
+Z2Vy 770
+IGdvb2Q= 771
+YXRoZXI= 772
+YXJr 773
+cm93 774
+IGtl 775
+J20= 776
+IGhhcw== 777
+YXRo 778
+cHBlZA== 779
+IHdlbnQ= 780
+IHRlbGw= 781
+cXVhc2g= 782
+IGVu 783
+IGZpcnN0 784
+IGhvdA== 785
+aXo= 786
+IGF3YXk= 787
+IHNvbWV0aGluZw== 788
+IHJlbQ== 789
+IHRvd24= 790
+IHNt 791
+IFRoaXM= 792
+IGJldHRlcg== 793
+IFRoZW4= 794
+d2Fz 795
+b2Y= 796
+YmFyZA== 797
+IEw= 798
+bGk= 799
+ZmU= 800
+IFRva3lv 801
+IGxvbmc= 802
+aWx5 803
+IHN1cmU= 804
+IGxvb2tlZA== 805
+dWJiYXJk 806
+Y3Rpb24= 807
+b3Jk 808
+IG1hbnk= 809
+aW91cw== 810
+IHRvbw== 811
+IGhlcmU= 812
+b3M= 813
+IHVuZGVy 814
+YXNl 815
+bmc= 816
+cGVk 817
+b2Q= 818
+bWU= 819
+IGp1c3Q= 820
+IG5vdw== 821
+aW5jZQ== 822
+IGhlYXJk 823
+IGtpbmQ= 824
+IFRoZXk= 825
+IGJlZm9yZQ== 826
+aHk= 827
+IElu 828
+IGVudA== 829
+IGJvYXJk 830
+ISI= 831
+d2FyZA== 832
+IGJlaW5n 833
+IHdlbGw= 834
+ZXJt 835
+cmllZA== 836
+IHdyb25n 837
+YWlk 838
+eHQ= 839
+IHJldHVybg== 840
+aXRlZA== 841
+IHllbg== 842
+IG1hdHRlcg== 843
+IGNhbGw= 844
+IHRhbA== 845
+IFlvdQ== 846
+Y2Vk 847
+aXNlZA== 848
+IGNoYQ== 849
+b25z 850
+IHNhbWU= 851
+IG9uY2U= 852
+ZGF5 853
+ZnQ= 854
+IHN3 855
+IGJlY2F1c2U= 856
+IHRoaW5r 857
+IHdoZXJl 858
+IE5v 859
+IEh1YmJhcmQ= 860
+IFNxdWFzaA== 861
+IGNvcA== 862
+d2l0aA== 863
+ZXJlZA== 864
+b2xsb3c= 865
+IHBsYWNl 866
+aWRk 867
+Y2Vzcw== 868
+IHNob3c= 869
+aXNoYQ== 870
+IHJh 871
+IGxldHRlcg== 872
+bmU= 873
+dmVz 874
+YXRpbmc= 875
+cmFuZw== 876
+IGFmZg== 877
+IGhhbmQ= 878
+IHNj 879
+IHBlcnM= 880
+aW50 881
+cHI= 882
+c2lkZQ== 883
+ZnRlcg== 884
+IHNheWluZw== 885
+IGxhdQ== 886
+dGhhdA== 887
+IHdpdGhvdXQ= 888
+cm9u 889
+YWly 890
+bGVjdA== 891
+IFdoYXQ= 892
+ZWx0 893
+IHdoaWxl 894
+b2dh 895
+YXBlcg== 896
+IHBl 897
+b3k= 898
+IHNhdA== 899
+aWVz 900
+IGFkZA== 901
+IGRheXM= 902
+IHNwZQ== 903
+IGhv 904
+IGFucw== 905
+IGhhcg== 906
+IFdoZW4= 907
+IGFueXRoaW5n 908
+cGVu 909
+XQo= 910
+dGFpbg== 911
+IG11c3Q= 912
+IG5ldw== 913
+bGlj 914
+IHZv 915
+aGlsZQ== 916
+Z2V0 917
+IEFz 918
+IHZlcnk= 919
+J3Jl 920
+IGV2ZXJ5 921
+YXZl 922
+PyI= 923
+YWRnZXI= 924
+IEtvZ2E= 925
+IE1y 926
+cm91Z2g= 927
+dWx0 928
+IGZvbGxvdw== 929
+dGluZw== 930
+aWZl 931
+aWRkbGU= 932
+ZnVs 933
+YW5r 934
+IFNv 935
+IHNlZW1lZA== 936
+IEFuZA== 937
+aXg= 938
+IHNldA== 939
+IGNhcmU= 940
+IHJlcw== 941
+IG5ldmVy 942
+IGZvdW5k 943
+IGxv 944
+Y2lk 945
+aW5lZA== 946
+IGNsYXNz 947
+IG15c2VsZg== 948
+YXc= 949
+IHdvbQ== 950
+YXRpb25z 951
+IGxlZnQ= 952
+IFdl 953
+IHRlYWNoZXJz 954
+Ilk= 955
+bmE= 956
+b250 957
+IGRlcw== 958
+IHRob3Nl 959
+aXJlZA== 960
+IHNlbg== 961
+eWluZw== 962
+IHRoZXNl 963
+YXo= 964
+IFRoZXJl 965
+Y2VwdA== 966
+IGRhbmc= 967
+IFU= 968
+Ikg= 969
+Ym9k 970
+Ym9keQ== 971
+IGhhdmluZw== 972
+YWxhcnk= 973
+IHdhdGNo 974
+IGdpdmU= 975
+YWdl 976
+IGl0cw== 977
+IGFwcGU= 978
+dWU= 979
+IGNvdW50 980
+IGhhcmQ= 981
+IGJlbA== 982
+b3R0 983
+IGRpc3Q= 984
+IlM= 985
+IE1hZA== 986
+LW4= 987
+cmlidXQ= 988
+Z2Vk 989
+IGF0dA== 990
+ZmVyZQ== 991
+aXRoZXI= 992
+IHVwb24= 993
+IHRlbQ== 994
+IHBlcnNvbg== 995
+bmluZw== 996
+IGNoZQ== 997
+YXJseQ== 998
+b25leQ== 999
+IHNvb24= 1000
+ZW1lbnQ= 1001
+ICg= 1002
+IHRyYW5z 1003
+IGV4cA== 1004
+IHNlcg== 1005
+IHJlZw== 1006
+YXNvbg== 1007
+IHNhdw== 1008
+IG5leHQ= 1009
+b290 1010
+IGhhbGY= 1011
+IHRvb2s= 1012
+IGJhZA== 1013
+IGhvdXI= 1014
+IHNhbGFyeQ== 1015
+IGJlZ2Fu 1016
+cmlnaHQ= 1017
+b25uYQ== 1018
+LXNhbg== 1019
+IHdvcmtz 1020
+IEo= 1021
+Zm9ybQ== 1022
+aWNhbA== 1023
+IHRyYQ== 1024
+bWFu 1025
+IG5vdGhpbmc= 1026
+IHN0aWxs 1027
+ZWFycw== 1028
+IHN1cHA= 1029
+IHR1cm4= 1030
+IGZlbHQ= 1031
+IHdvbWFu 1032
+IHN0YXJ0ZWQ= 1033
+b3VibGU= 1034
+dXJh 1035
+aXNoaW5n 1036
+Ogo= 1037
+bGVjdHJvbg== 1038
+bGVjdHJvbmlj 1039
+b29r 1040
+IGNvcHk= 1041
+IGZ1bGw= 1042
+Y29uZA== 1043
+bWF0 1044
+IG1pZGRsZQ== 1045
+IGxvb2s= 1046
+IGNvbW0= 1047
+d2VyZWQ= 1048
+IGJlY2FtZQ== 1049
+IGZlbGxvd3M= 1050
+d291bGQ= 1051
+IGdvdA== 1052
+IGds 1053
+IGd1 1054
+IGtlZXA= 1055
+IGdl 1056
+IE1hZG9ubmE= 1057
+aXRlcg== 1058
+aXNoZWQ= 1059
+IHVuZGVyc3Q= 1060
+IHN0cmE= 1061
+c2lk 1062
+IGNvdW50cnk= 1063
+b3BsZQ== 1064
+IHByb3Y= 1065
+IHB1dA== 1066
+bm8= 1067
+J2xs 1068
+IHNsZQ== 1069
+cmFuZ2U= 1070
+IFNoZQ== 1071
+cG9z 1072
+IG1pbmQ= 1073
+IHBhc3M= 1074
+IHRocm91Z2g= 1075
+IHF1aXRl 1076
+IGluZA== 1077
+IGJvYXJkaW5n 1078
+dGVhY2hlcg== 1079
+cGxl 1080
+UG9yY3VwaW5l 1081
+IHBsZQ== 1082
+IGdlaXNoYQ== 1083
+ICAgIA== 1084
+b3N0 1085
+ZW5zZQ== 1086
+Tm8= 1087
+aWJsZQ== 1088
+IHJlYWQ= 1089
+IHJlZA== 1090
+ZW50aW9u 1091
+ZW5lZA== 1092
+ISIK 1093
+IHJlZg== 1094
+IGFk 1095
+IGZs 1096
+IHN0YXk= 1097
+dXA= 1098
+IHJvdW5k 1099
+IGNsZQ== 1100
+IG9wZW4= 1101
+IG9i 1102
+dGVuZA== 1103
+IGZpbmQ= 1104
+IHBlcg== 1105
+IGNhbGxlZA== 1106
+IHN1cg== 1107
+cmV3 1108
+IHBhcGVy 1109
+IEJhZGdlcg== 1110
+IG1lZXQ= 1111
+aXNz 1112
+IlRoYXQ= 1113
+ZXJtcw== 1114
+VEU= 1115
+aXR0ZW4= 1116
+YWJseQ== 1117
+bmVzcw== 1118
+IGNhbm5vdA== 1119
+IHNpbXA= 1120
+Y29u 1121
+IHJlYXNvbg== 1122
+eW91 1123
+IGhvbWU= 1124
+Ynk= 1125
+IGZpZ2h0 1126
+aXR0bGU= 1127
+IHRoaW5ncw== 1128
+IGVhcw== 1129
+IGltcA== 1130
+cmVzc2Vk 1131
+IG1lYW4= 1132
+IGFwcGVhcmVk 1133
+IG5hdA== 1134
+IGhlbA== 1135
+cmV0 1136
+YWtlbg== 1137
+IHN0cmFpZ2h0 1138
+IGFmZmFpcg== 1139
+aXRpbmc= 1140
+IGVk 1141
+IHNpbmNl 1142
+bG9n 1143
+IHBheQ== 1144
+IGZyb250 1145
+bXk= 1146
+IHZvaWNl 1147
+cmVhZHk= 1148
+IGZvb2w= 1149
+b3VuZGF0aW9u 1150
+IGVsZWN0cm9uaWM= 1151
+IHRlcm1z 1152
+IG1hcg== 1153
+YXBhbg== 1154
+YW55 1155
+IHJlc3A= 1156
+IGVuZA== 1157
+YXBw 1158
+d2hhdA== 1159
+c3Ry 1160
+cmFw 1161
+aWFs 1162
+aWN1bA== 1163
+IGFjYw== 1164
+b3Ro 1165
+IHNlY29uZA== 1166
+IGZsbw== 1167
+IHNpeA== 1168
+IGZlZXQ= 1169
+YnI= 1170
+aWV0 1171
+IGxpdHRsZQ== 1172
+bGVz 1173
+IG1vbmV5 1174
+IGRlY2w= 1175
+IGV5 1176
+IGNvbXA= 1177
+YXJpbmc= 1178
+IGFncmU= 1179
+d2hlcmU= 1180
+IFN0 1181
+IHN0cmU= 1182
+ZXg= 1183
+cmFjdA== 1184
+IGludA== 1185
+IGRpcmU= 1186
+IGJlY29tZQ== 1187
+IGhvbg== 1188
+IGNvbnNpZA== 1189
+ZXJ0YWlu 1190
+bm93 1191
+IHNs 1192
+aXRvcg== 1193
+Z2c= 1194
+IGp1bQ== 1195
+IGJ1 1196
+IHRoaW5n 1197
+IGFuc3dlcmVk 1198
+b2Vz 1199
+eWE= 1200
+IFRoYXQ= 1201
+aXpl 1202
+b25k 1203
+YWN0 1204
+IGVmZg== 1205
+IGJhbmc= 1206
+YWJvdXQ= 1207
+IGJlZA== 1208
+b3Jyb3c= 1209
+dW5n 1210
+IFRv 1211
+IGtlcHQ= 1212
+IHdhbA== 1213
+IGJhdGg= 1214
+IGRyYQ== 1215
+IkE= 1216
+cmluZ3M= 1217
+aG9wcA== 1218
+IHJlc2lnbg== 1219
+IGRpbg== 1220
+IGxhZHk= 1221
+LkU= 1222
+IHVzZQ== 1223
+bGlzaA== 1224
+b3Jz 1225
+IHdyaXR0ZW4= 1226
+ZW5l 1227
+aXY= 1228
+IGRpZg== 1229
+IHN0ZQ== 1230
+IHN0b3J5 1231
+Y29t 1232
+cmVz 1233
+ZW50bHk= 1234
+IGZhY3Q= 1235
+aGVz 1236
+d2F5cw== 1237
+IHdoeQ== 1238
+IHRob3VnaA== 1239
+IHN0cg== 1240
+b25kZXI= 1241
+aGVhZA== 1242
+IGNvdXI= 1243
+IG1vbg== 1244
+IHNr 1245
+IGJlbGll 1246
+IGxldA== 1247
+ZmVy 1248
+IHJlcXU= 1249
+IGxpbmU= 1250
+cm9vbQ== 1251
+LWRheQ== 1252
+IGRvbmU= 1253
+IGRvZXM= 1254
+IE9uZQ== 1255
+IGRhbmdv 1256
+YXNzaG9wcA== 1257
+IGNvbnNpZGVy 1258
+IGRpbm5lcg== 1259
+IEZvdW5kYXRpb24= 1260
+Kio= 1261
+ZW1wdA== 1262
+ZXNl 1263
+IHdvcmQ= 1264
+cmVzdA== 1265
+IGVub3VnaA== 1266
+IGdyZWF0 1267
+IG5hbWU= 1268
+IHB1Yg== 1269
+IG1hbm5lcg== 1270
+d2Vy 1271
+aWN0 1272
+aW5lc3M= 1273
+IGhpbXNlbGY= 1274
+IHBlb3BsZQ== 1275
+ZXc= 1276
+IGNvcg== 1277
+ZXN0aW9u 1278
+IGJpZw== 1279
+ZWU= 1280
+IHJp 1281
+aWRlcw== 1282
+IGJyb3RoZXI= 1283
+IGhlYXJ0 1284
+ZWN0ZWQ= 1285
+ZWVk 1286
+IG90aGVycw== 1287
+c29s 1288
+dGVk 1289
+IGV5ZXM= 1290
+IHRyb3VibGU= 1291
+IHRlYWNo 1292
+IGJvYXQ= 1293
+IGZvdXI= 1294
+IGFscmVhZHk= 1295
+cm9t 1296
+Z2hlZA== 1297
+IHNxdQ== 1298
+IHBvbA== 1299
+Y2Vz 1300
+IEhvdHQ= 1301
+IGxlYXZl 1302
+IGRpc3RyaWJ1dA== 1303
+YXN0ZXI= 1304
+Q0g= 1305
+dWM= 1306
+IGlt 1307
+IGhvd2V2ZXI= 1308
+dGhlcmU= 1309
+YXBhbmVzZQ== 1310
+IGxhc3Q= 1311
+IGNy 1312
+aWxpdHk= 1313
+IHNpbXBsZQ== 1314
+IGxpZmU= 1315
+LWM= 1316
+IHJlZ2FyZA== 1317
+IGZpbg== 1318
+dWFs 1319
+IG1lYW5z 1320
+IHN0YW5k 1321
+YXRjaA== 1322
+IHNob3J0 1323
+bmVk 1324
+IHNlZW4= 1325
+IGhhcHA= 1326
+LWs= 1327
+IGFnYWluc3Q= 1328
+aGlt 1329
+YW1lZA== 1330
+IHN0b29k 1331
+IGdyYQ== 1332
+IG1vdGhlcg== 1333
+IGZpc2g= 1334
+IHdhdGVy 1335
+YWls 1336
+Y2Vp 1337
+IHJhdGhlcg== 1338
+IGlucw== 1339
+IGZlZWw= 1340
+IGFsc28= 1341
+IG9yZA== 1342
+IGNvbWluZw== 1343
+aWNz 1344
+IGVpdGhlcg== 1345
+bmNl 1346
+ICc= 1347
+IGtpZA== 1348
+IGxhdWdoZWQ= 1349
+bGlrZQ== 1350
+IEFy 1351
+Z3I= 1352
+IEhvdHRh 1353
+IHRhbGs= 1354
+Z2V0aGVy 1355
+IFNpcg== 1356
+IHB1bg== 1357
+UHJv 1358
+YXRz 1359
+bW9zdA== 1360
+IHJlcA== 1361
+IGdp 1362
+aXNm 1363
+YmFibHk= 1364
+YWtlcw== 1365
+IE5vdA== 1366
+bnk= 1367
+IGFwcGVhcg== 1368
+bXA= 1369
+Y2hh 1370
+IGFjdA== 1371
+YmVk 1372
+aWVm 1373
+dWZm 1374
+IGFwbw== 1375
+IG1ldA== 1376
+IHJldHVybmVk 1377
+IHNvdW5k 1378
+dXNpbmVzcw== 1379
+IGxhdWdo 1380
+IGNsZWFy 1381
+IG5lZWQ= 1382
+ZmVzcw== 1383
+ZXN0ZWQ= 1384
+IGludg== 1385
+IGFjY2VwdA== 1386
+dW5kZXI= 1387
+Owo= 1388
+IHN1cnBy 1389
+ZGU= 1390
+IHRyYWlu 1391
+IGhvdGVs 1392
+IHNsZWVw 1393
+IGRy 1394
+IGhvbGQ= 1395
+bG9jaw== 1396
+cHVyYQ== 1397
+IHNwcmluZ3M= 1398
+IC4uLi4uLg== 1399
+IGFncmVlbWVudA== 1400
+IERhcg== 1401
+IHJlc3Q= 1402
+Y2x1ZA== 1403
+YXRvcg== 1404
+YXY= 1405
+IG9yaWc= 1406
+IG9yaWdpbg== 1407
+IGVs 1408
+IG5vcg== 1409
+IHByZXM= 1410
+IHVuZGVyc3RhbmQ= 1411
+IHRha2Vu 1412
+IGxpZ2h0 1413
+ZW5lcg== 1414
+c29tZQ== 1415
+IGJyb3VnaHQ= 1416
+cmFwaA== 1417
+IG1vc3Q= 1418
+b2tl 1419
+LXc= 1420
+IHVudA== 1421
+IGZhdGhlcg== 1422
+IHVzZWQ= 1423
+IGVhdA== 1424
+IHllYXJz 1425
+IFdoaWxl 1426
+IGNoYW4= 1427
+IHN1ZGQ= 1428
+IHN1ZGRlbg== 1429
+IGFwb2xvZw== 1430
+IHNldHQ= 1431
+IHRoaW4= 1432
+IE15 1433
+IHRlbg== 1434
+aW1lcw== 1435
+Zm9y 1436
+b3Vk 1437
+V2hlbg== 1438
+IGRldA== 1439
+IGxpdmU= 1440
+IG9j 1441
+IGZpdmU= 1442
+IGNvbnQ= 1443
+IGhlbHA= 1444
+IHdh 1445
+IHBhc3NlZA== 1446
+IHJ1bg== 1447
+IG1ha2luZw== 1448
+IHN0cmFuZ2U= 1449
+IHRha2luZw== 1450
+IGVhY2g= 1451
+IllvdQ== 1452
+IGFub3RoZXI= 1453
+IlNheQ== 1454
+IlRoZQ== 1455
+YXRlcw== 1456
+IHBsZWFz 1457
+YXNzaG9wcGVycw== 1458
+IG1vbQ== 1459
+IG1vbWVudA== 1460
+ZW50bGU= 1461
+bmdsaXNo 1462
+Q0hB 1463
+IG9yaWdpbmFs 1464
+aW9ucw== 1465
+dXJpbmc= 1466
+IHB1YmxpYw== 1467
+dWN0 1468
+dWNr 1469
+IHF1ZXN0aW9u 1470
+YWk= 1471
+Y3k= 1472
+ZWs= 1473
+IGZsb29y 1474
+IGNhcg== 1475
+b3VzZQ== 1476
+IHNpZGU= 1477
+LXlh 1478
+IGNlcnRhaW4= 1479
+aHlz 1480
+LWQ= 1481
+aWdo 1482
+YWdpbg== 1483
+d2VldA== 1484
+IHBvb3I= 1485
+IGRlY2lk 1486
+dWFsbHk= 1487
+IGJ1c2luZXNz 1488
+cHJv 1489
+cGxhaW4= 1490
+IHN0b3A= 1491
+IQo= 1492
+IEhvdw== 1493
+IldoYXQ= 1494
+Y2Fu 1495
+IFVu 1496
+cHM= 1497
+dW5k 1498
+LW5pZ2h0 1499
+IG1lZXRpbmc= 1500
+ZWRv 1501
+IHJhaXNl 1502
+R3V0ZW5iZXJn 1503
+IERhcmxpbmc= 1504
+dW1l 1505
+IEVuZ2xpc2g= 1506
+VEVS 1507
+YWRpbmc= 1508
+IHRyYW5zbA== 1509
+IGFibGU= 1510
+c3NpYmxl 1511
+IHNhdGlzZg== 1512
+IHdhbnRlZA== 1513
+IHN1Yg== 1514
+IGNhc2U= 1515
+aWZpYw== 1516
+aXRlcmFyeQ== 1517
+IG1haWQ= 1518
+IGluYw== 1519
+IHBvcw== 1520
+IHBvc2l0aW9u 1521
+IHBhdA== 1522
+dXJlZA== 1523
+b3JyeQ== 1524
+IGFjY291bnQ= 1525
+IGJvdGg= 1526
+IGZyaWU= 1527
+IGZyaWVuZA== 1528
+dGhpcw== 1529
+IGFsd2F5cw== 1530
+IHBhcnRpY3Vs 1531
+V2hhdA== 1532
+IHNtYWxs 1533
+ZW50eQ== 1534
+dXNoZWQ= 1535
+IG1pcw== 1536
+dWxseQ== 1537
+IHJlY2Vp 1538
+WW91 1539
+IHlldA== 1540
+IGdhdmU= 1541
+QnV0 1542
+aGFk 1543
+IGFuc3dlcg== 1544
+IGFicw== 1545
+aWxl 1546
+Y2tldA== 1547
+IG5vb2Q= 1548
+IGNvdXJzZQ== 1549
+IGZvcm0= 1550
+IGV2ZXJ5dGhpbmc= 1551
+ZWN0aW9u 1552
+SWY= 1553
+cGFydA== 1554
+IHNpbmc= 1555
+IHNpdA== 1556
+IHB1cg== 1557
+aXA= 1558
+IGZpc2hpbmc= 1559
+IGVo 1560
+IHBhcg== 1561
+IHRvZ2V0aGVy 1562
+SGU= 1563
+IHdoZQ== 1564
+IHdoZXRoZXI= 1565
+IGJyYQ== 1566
+Illlcw== 1567
+IHB1bmlzaA== 1568
+U2hpcnQ= 1569
+IFllZG8= 1570
+IGZhcmV3 1571
+IGZhcmV3ZWxs 1572
+IGRhbmNl 1573
+IGxlc3M= 1574
+dXJhbA== 1575
+IGRlZg== 1576
+IGF0dGVtcHQ= 1577
+d2Vlbg== 1578
+IHNpZ24= 1579
+IHN5 1580
+ZmVyZW50 1581
+IGxlYXN0 1582
+c2Vy 1583
+b2I= 1584
+bmRpbmc= 1585
+IHNvcnJ5 1586
+IGp1bXBlZA== 1587
+IGphbg== 1588
+IGphbml0b3I= 1589
+aXplZA== 1590
+IHRvd2FyZA== 1591
+IG1vcg== 1592
+YXZpbmc= 1593
+IGJpdA== 1594
+IlRoaXM= 1595
+IHJlbWFyaw== 1596
+IGZ1dA== 1597
+IHdvbmRlcg== 1598
+IGZ1bg== 1599
+VGhlbg== 1600
+IGRlYw== 1601
+IHdob20= 1602
+IGRpZG4= 1603
+IHJlYw== 1604
+YmVj 1605
+Iklm 1606
+IGtuZXc= 1607
+YWZ0ZXI= 1608
+IHRodXM= 1609
+IGlzbg== 1610
+IHNpZ2h0 1611
+bWVk 1612
+W0Y= 1613
+dXNz 1614
+Y2lkZW50 1615
+dGhlbQ== 1616
+IGZpZg== 1617
+IGRyYXc= 1618
+IGhlYXI= 1619
+IHdyaXRpbmc= 1620
+IGdldHRpbmc= 1621
+c2g= 1622
+ZmVyZW5jZQ== 1623
+IHJhaXNlZA== 1624
+dGhleQ== 1625
+YXg= 1626
+IGZpbmU= 1627
+c2Vs 1628
+IE5vYmU= 1629
+IE5vYmVvaw== 1630
+IE5vYmVva2E= 1631
+b3JtYWw= 1632
+IGVC 1633
+aWNlbnNl 1634
+MDA= 1635
+IGJlc3Q= 1636
+d29y 1637
+Zmlj 1638
+dGVyZXN0 1639
+IHJlbWFy 1640
+Ymw= 1641
+YXJ0ZWQ= 1642
+IGRhcms= 1643
+IHlvdW5n 1644
+dXNo 1645
+IGJldA== 1646
+b3V0aA== 1647
+aG91c2U= 1648
+YXVnaHQ= 1649
+IHBoeXM= 1650
+IHN0cm9uZw== 1651
+IGZ1cg== 1652
+IHJvbGw= 1653
+Y292ZQ== 1654
+Y2hpZWY= 1655
+YXdh 1656
+IGZvbGxvd2Vk 1657
+IGZvbmQ= 1658
+IGZ1dHVyZQ== 1659
+aXJk 1660
+ZnVsbHk= 1661
+IGVmZm9ydA== 1662
+QWZ0ZXI= 1663
+b3dhcmQ= 1664
+IHJlYWxseQ== 1665
+IGFtb25n 1666
+IGFyb3VuZA== 1667
+IGNvbXBs 1668
+IGdheg== 1669
+IGJvdw== 1670
+YXRlcg== 1671
+IGluc2lzdA== 1672
+IHR1cm5lZA== 1673
+aGVs 1674
+cmVt 1675
+IGhvdXJz 1676
+IGRlY2lkZWQ= 1677
+eXM= 1678
+IG1vbnRo 1679
+LWE= 1680
+IGFkdg== 1681
+IGJlbGlldmU= 1682
+IHRlYWNoaW5n 1683
+IGVhc3k= 1684
+IGRpcmVjdGlvbg== 1685
+b29rZWQ= 1686
+IHdhcg== 1687
+IHVubGVzcw== 1688
+aGF2ZQ== 1689
+IHNxdWFyZQ== 1690
+dmls 1691
+IHF1aWV0 1692
+IGh1bmc= 1693
+IGdvZXM= 1694
+IHBhaWQ= 1695
+IHNoYWxs 1696
+Ik5v 1697
+IHB1bmlzaG1lbnQ= 1698
+cG9zZQ== 1699
+IHN3ZWV0 1700
+J3Zl 1701
+IldlbGw= 1702
+IGdlbnRsZQ== 1703
+IG5vcm1hbA== 1704
+YWdyYXBo 1705
+Y2hpdmU= 1706
+Y2hhbg== 1707
+IGluY2x1ZA== 1708
+d3c= 1709
+b3Jn 1710
+dGVt 1711
+QVI= 1712
+IFRI 1713
+IGVxdQ== 1714
+IHRvbmU= 1715
+IHBvc3NpYmxl 1716
+IGJlY29t 1717
+IEphcGFuZXNl 1718
+dmVycw== 1719
+IGZvbGxvd2luZw== 1720
+IHBhaW4= 1721
+IHdob2xl 1722
+d3I= 1723
+IHNlcmlvdXM= 1724
+IG5hcg== 1725
+IHRpcmVk 1726
+SW4= 1727
+IHBsYXk= 1728
+IHByb20= 1729
+IGdhbWU= 1730
+IFNvbWU= 1731
+IGhhcHBlbmVk 1732
+IGN1dA== 1733
+IHR3ZW50eQ== 1734
+IGRvb3I= 1735
+IG1vcm5pbmc= 1736
+aGluZA== 1737
+IGJyZQ== 1738
+IGluc2lkZQ== 1739
+b3Zl 1740
+YWx0aA== 1741
+dWs= 1742
+YXJnZQ== 1743
+YW1i 1744
+IGRhbQ== 1745
+IHdvcnJ5 1746
+YXRpdmU= 1747
+IGV4cGVjdGVk 1748
+IGZhbQ== 1749
+IHByYQ== 1750
+IHBvY2tldA== 1751
+b29rcw== 1752
+Y2hlZA== 1753
+IHNpbA== 1754
+b2w= 1755
+IGZhdg== 1756
+IGVsc2U= 1757
+IGhpZ2g= 1758
+IHJlYWw= 1759
+IGFsb25n 1760
+IG1lZA== 1761
+aGlr 1762
+aGVtYXQ= 1763
+aGVtYXRpY3M= 1764
+IGxpc3Q= 1765
+IHNpY2s= 1766
+b2ludA== 1767
+W0Zvb3Q= 1768
+W0Zvb3Rub3Q= 1769
+W0Zvb3Rub3Rl 1770
+Ll0K 1771
+bmlnaHQ= 1772
+c2Vz 1773
+aW9y 1774
+IHNheXM= 1775
+IG1vdXRo 1776
+aG93 1777
+bWluZw== 1778
+IGNsbw== 1779
+IGN1cg== 1780
+Z2luZw== 1781
+IHN1ZGRlbmx5 1782
+LWFo 1783
+YW1w 1784
+IGJsYWNr 1785
+cm9zcw== 1786
+IGZhYw== 1787
+c2VsdmVz 1788
+aWV3 1789
+aXNzaW9u 1790
+IGNvcHlyaWdodA== 1791
+IHBhcmFncmFwaA== 1792
+IEFyY2hpdmU= 1793
+IGRvbmF0aW9ucw== 1794
+UHJvamVjdA== 1795
+IGNvc3Q= 1796
+Lm9yZw== 1797
+TEk= 1798
+dWNlZA== 1799
+IHN1Yw== 1800
+eWxl 1801
+IGZvcmNl 1802
+am95 1803
+b3VjaA== 1804
+dHI= 1805
+SXQ= 1806
+IHRyYWQ= 1807
+IHByZXNlbnQ= 1808
+IGV4dA== 1809
+YXNlZA== 1810
+cmVkaXQ= 1811
+IGZhdWx0 1812
+aWI= 1813
+LW0= 1814
+dXJk 1815
+IHRyaWVk 1816
+dGltZQ== 1817
+IHByZXQ= 1818
+IHNwZWU= 1819
+b3dlcg== 1820
+IHdvcmRz 1821
+Q0hBUA== 1822
+Q0hBUFRFUg== 1823
+c2Nob29s 1824
+IGFzaw== 1825
+IGRvaW5n 1826
+YXRlbHk= 1827
+IHVudGls 1828
+Ym91dA== 1829
+IHRyZWU= 1830
+Y2FsbA== 1831
+YW1hc2g= 1832
+YW1hc2hpcg== 1833
+YW1hc2hpcm8= 1834
+c3Rl 1835
+IGJlaGluZA== 1836
+b2xk 1837
+IHdhbGw= 1838
+aXRvcnk= 1839
+IHJvbGxlZA== 1840
+IG1vdmU= 1841
+IGFwb2xvZ2l6ZQ== 1842
+IGxhcmdl 1843
+YW1ib28= 1844
+c3U= 1845
+IHNldHRsZWQ= 1846
+Ikhl 1847
+d28= 1848
+IHRoaW5raW5n 1849
+dXNlZA== 1850
+aWZpZWQ= 1851
+IGFsbW9zdA== 1852
+IHRyZQ== 1853
+IHRyZWF0 1854
+IG5vb2RsZQ== 1855
+IG5vdGU= 1856
+IEFsbA== 1857
+IGJlYXQ= 1858
+IG9iamVjdA== 1859
+IHNlZW1z 1860
+IGlkZQ== 1861
+WWVz 1862
+b3dz 1863
+IHJlbWFpbg== 1864
+IGJlZ2lu 1865
+dWdodA== 1866
+bWVudHM= 1867
+IGFsb25l 1868
+c3BlY3Q= 1869
+IG1hdGhlbWF0aWNz 1870
+IHJvdWdo 1871
+IG91dHNpZGU= 1872
+IGNvbWVz 1873
+YmFjaw== 1874
+IHdpbmQ= 1875
+c2Vk 1876
+IHdvdWxkbg== 1877
+ZWVy 1878
+aW51dA== 1879
+ZnJvbQ== 1880
+IHJlcGw= 1881
+IG5hcnJvdw== 1882
+IGluY2lkZW50 1883
+IGFpcg== 1884
+IHNlYQ== 1885
+dHM= 1886
+IHN1cnByaXNlZA== 1887
+IHRlYQ== 1888
+UmVk 1889
+IHRhbGtpbmc= 1890
+IGJvc3M= 1891
+cXVl 1892
+IHBpY3Q= 1893
+aXJ0eQ== 1894
+IGNl 1895
+IGxpbQ== 1896
+IFdoeQ== 1897
+IHBvaW50 1898
+IGxhdw== 1899
+Y2lhdGVk 1900
+IG1vb24= 1901
+aXJjdQ== 1902
+Z290 1903
+IElz 1904
+IGhhbmRz 1905
+IGhvbm9y 1906
+YXV0 1907
+cmdl 1908
+IHN0YXRl 1909
+IExpdGVyYXJ5 1910
+LkY= 1911
+VGhpcw== 1912
+bGluZQ== 1913
+Lmc= 1914
+Lmd1dGVuYmVyZw== 1915
+IE9G 1916
+RU4= 1917
+cmFjdGVy 1918
+IGJlbmU= 1919
+IEV2ZW4= 1920
+b3Vi 1921
+IG1ha2Vz 1922
+IGludGVyZXN0 1923
+b3Bl 1924
+bXM= 1925
+IHJlc3BvbnM= 1926
+IGZvcmU= 1927
+IHNvbWV3aGF0 1928
+IGhvbmVzdA== 1929
+b2Nr 1930
+aXJpdA== 1931
+IGhlbGQ= 1932
+IGFkZGVk 1933
+ZnU= 1934
+YWRlZA== 1935
+YWxz 1936
+YXR0 1937
+dGVybg== 1938
+IHBlcnNvbmFs 1939
+IGFzcw== 1940
+IFdpdGg= 1941
+dGlj 1942
+VG9reW8= 1943
+IHNob3V0 1944
+IHByZXR0eQ== 1945
+dW1i 1946
+IGVhcmx5 1947
+b3BwZWQ= 1948
+IGZ1cnRoZXI= 1949
+IGZyZQ== 1950
+ZXNpZGVz 1951
+IGJhbWJvbw== 1952
+IGly 1953
+bW9yZQ== 1954
+IGxpdmluZw== 1955
+IHJlY2VpdmVk 1956
+IGxpdmVk 1957
+IG1lYW50 1958
+IGNvd2FyZA== 1959
+cG9zaXRpb24= 1960
+IGxvYw== 1961
+aWxlZA== 1962
+IHRlbmRlcg== 1963
+IGNo 1964
+IEFmdGVy 1965
+Y2Vy 1966
+IGZhdm9y 1967
+d2hv 1968
+IGxpa2Vk 1969
+cmFuY2U= 1970
+IHByaQ== 1971
+a2lzaGE= 1972
+IHN0dWR5 1973
+IG9yZGVy 1974
+IGFmdGVyd2FyZA== 1975
+IGdyZWF0bHk= 1976
+IHVuYWJsZQ== 1977
+Z28= 1978
+IHdhaXQ= 1979
+ZXBpbmc= 1980
+aWRpbmc= 1981
+IGZvcnR5 1982
+IHNreQ== 1983
+IG9mZmljZQ== 1984
+d2lsbA== 1985
+IkQ= 1986
+d2Vs 1987
+IHN0YXRpb24= 1988
+Ym8= 1989
+aG90 1990
+c3VjaA== 1991
+IGxvdWQ= 1992
+IGF3 1993
+bGFuZA== 1994
+Pwo= 1995
+IHJlc3BlY3Q= 1996
+YW5jZXM= 1997
+aWVudA== 1998
+IG91Z2h0 1999
diff -ruN marc_original/third_party/torchtune/tests/assets/tiny_bpe_merges.txt marc/third_party/torchtune/tests/assets/tiny_bpe_merges.txt
--- marc_original/third_party/torchtune/tests/assets/tiny_bpe_merges.txt	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/tiny_bpe_merges.txt	2025-02-20 17:49:29.566024273 -0500
@@ -0,0 +1,1904 @@
+ 
+ t
+ a
+i n
+h e
+r e
+o n
+t he
+ s
+e r
+a t
+ c
+ 
+e n
+ o
+ "
+n d
+e s
+in g
+ 
+i t
+ p
+o r
+o u
+a nd
+ w
+i s
+ f
+a n
+i on
+a l
+ b
+t o
+ m
+ in
+o f
+l e
+c t
+a r
+u t
+ d
+s t
+e d
+ 
+i c
+" :
+, 
+r o
+en t
+\ n
+ e
+p ut
+o m
+ re
+a s
+v e
+ h
+t h
+" ,
+ l
+ is
+e t
+c e
+ n
+. \
+i m
+i l
+ g
+ u
+ct ion
+r u
+at ion
+o l
+c h
+ T
+f or
+ou t
+r a
+o w
+i d
+l y
+s t
+b e
+ y
+p ro
+i g
+s e
+at e
+th at
+it h
+i r
+u r
+o t
+o r
+ on
+y ou
+er s
+st ru
+a n
+i f
+u l
+stru ction
+ {
+ }
+c an
+in put
+out put
+in struction
+{ 
+} ,
+" 
+ he
+c on
+ it
+a y
+es s
+w ith
+v er
+e l
+a s
+a m
+ A
+g e
+s u
+i v
+. ",
+c om
+ I
+m ent
+a k
+a l
+\ "
+. "
+i ve
+a re
+a b
+a d
+m o
+e x
+ v
+ S
+re s
+p p
+q u
+d e
+w h
+it y
+ en
+T he
+he r
+l d
+r i
+t er
+an t
+ C
+is t
+" ",
+u m
+u s
+n e
+a in
+t h
+e ct
+ le
+o p
+e m
+i es
+c h
+ im
+d u
+o d
+or t
+n t
+es t
+ig h
+e re
+h a
+u s
+u re
+i al
+o c
+w or
+the ir
+a c
+en ce
+i z
+you r
+o s
+im p
+u d
+b y
+s e
+in e
+ou ld
+l ow
+il l
+a ge
+ro m
+s p
+ P
+s h
+u st
+T he
+u n
+' s
+in c
+id e
+p l
+igh t
+o g
+p l
+p t
+a re
+t e
+in t
+ \
+h is
+ r
+ak e
+p er
+or m
+a g
+f f
+ E
+ar t
+ k
+en d
+ M
+w e
+ B
+a d
+c ess
+r ou
+ic al
+al l
+ab le
+f rom
+a nd
+ H
+a b
+a ct
+com p
+om e
+a ch
+T his
+ha ve
+f orm
+ \"
+a st
+a t
+ W
+re s
+d at
+: \
+t her
+ion s
+o re
+ (
+con t
+ou r
+e p
+ F
+a c
+an ce
+ R
+g h
+m e
+c es
+w as
+in d
+ve l
+ation s
+he l
+mo re
+ul t
+ D
+re at
+ig n
+hel p
+im e
+ar d
+c l
+a pp
+an s
+i e
+dat a
+ic h
+an g
+ou s
+el l
+k s
+as e
+ic e
+i p
+it e
+su ch
+f e
+w he
+i b
+o ther
+th is
+as s
+u al
+i le
+n e
+re d
+h as
+o o
+res s
+if ic
+n ing
+ =
+u p
+m an
+a r
+on g
+e c
+t ra
+a v
+wh ich
+g o
+pro v
+d is
+* *
+s o
+ G
+on e
+e m
+n ot
+u e
+ O
+ j
+a ce
+the y
+am e
+ qu
+ L
+if f
+f ol
+ar y
+at ed
+ust om
+it ion
+it s
+s y
+k e
+ac k
+r y
+- -
+t ime
+d es
+ne w
+ent s
+ou nt
+fol low
+al so
+com m
+o ut
+e ff
+d iff
+iv en
+a p
+s ent
+\ u
+s o
+pro du
+u se
+s c
+ -
+u n
+l ud
+I t
+en er
+k ing
+e v
+ab out
+the m
+ U
+c ustom
+ ro
+inc lud
+l es
+et w
+st em
+x t
+int o
+p er
+I n
+ N
+w ill
+le ar
+b er
+al l
+p e
+d s
+t w
+ak ing
+ar k
+f ul
+m ake
+ch n
+er v
+o st
+rou gh
+on e
+in ter
+it ies
+a il
+i ke
+re e
+p le
+al th
+us ed
+or s
+o ver
+il ity
+ment s
+an ge
+w ay
+or y
+c ol
+p r
+c ould
+n um
+re ate
+in t
+re du
+ers on
+re c
+he r
+ne ed
+m s
+at er
+o y
+sy stem
+in form
+tw o
+te chn
+sent ence
+i ence
+iz e
+g et
+diff ere
+o od
+ri b
+b ut
+follow ing
+as ed
+ol og
+er g
+le d
+u res
+I n
+e ar
+p h
+ow n
+p re
+w ould
+us ing
+con s
+wor k
+mo d
+at ing
+i a
+i re
+p os
+i ent
+o b
+j ect
+in v
+on s
+d o
+ul ar
+de c
+he alth
+imp ro
+an y
+th rough
+y p
+ro w
+vel op
+pro cess
+t r
+l ic
+ver y
+al s
+if y
+` `
+ar i
+st r
+imp ort
+l ike
+produ ct
+s ome
+p h
+ent ial
+a m
+at es
+ac c
+en s
+n s
+s m
+in d
+e en
+ex per
+le ct
+v al
+re l
+it s
+inform ation
+ing s
+ J
+op le
+in ess
+g iven
+m m
+ic es
+p art
+il d
+y s
+o ur
+nd er
+p erson
+al ly
+k e
+etw een
+f t
+ot h
+sp ec
+b etween
+erg y
+A I
+wh o
+m ay
+e f
+at ive
+is e
+l ist
+k n
+ad d
+, \
+or d
+ic s
+pe ople
+S t
+h is
+ex p
+ib le
+the re
+s erv
+inc re
+de velop
+ou nd
+ow er
+tr ans
+b s
+en ergy
+of f
+b us
+wh ile
+o se
+a ct
+ex am
+lear ning
+ction s
+c on
+g or
+g an
+ut ion
+rou nd
+pp ort
+h ow
+b l
+m ed
+an c
+t yp
+ ra
+c ar
+if e
+wor ld
+v ari
+re p
+a u
+s oc
+prov id
+s et
+t en
+s ol
+e ach
+whe n
+eff ect
+p o
+s he
+ic k
+whe re
+mod el
+import ant
+u nder
+pro g
+ener ate
+ur al
+t ain
+as s
+olog y
+h ad
+oo k
+g g
+custom er
+t ing
+v ing
+res p
+l ine
+c reat
+l l
+il y
+re g
+d et
+ if
+ +
+bus iness
+\n In
+is h
+mo st
+ 
+he s
+ang u
+prov ide
+ad v
+er m
+u b
+s k
+ir st
+an y
+d ay
+iv id
+ar m
+ra ct
+n ce
+ |
+impro ve
+) \
+c o
+comm un
+ark et
+m et
+c y
+differe nt
+iz ed
+ar t
+\n The
+r it
+com put
+for m
+c k
+h um
+ch ar
+b le
+le ad
+ir on
+re m
+sh ould
+t e
+al low
+n ess
+h at
+f un
+comp le
+l angu
+ag es
+be c
+s ign
+u es
+at ure
+f ind
+ri end
+st ud
+m ain
+im ate
+o ve
+res ult
+pl ay
+redu ce
+en g
+w are
+red i
+num ber
+l ar
+p ol
+p at
+w ell
+id ent
+v iron
+r ite
+c rib
+b u
+h igh
+the se
+iv es
+v es
+des ign
+ur n
+th an
+d er
+an al
+w ater
+m arket
+exam ple
+w ay
+st and
+n g
+a x
+it ive
+ `
+i qu
+s im
+e qu
+gor ith
+te xt
+res ent
+man y
+ur ing
+-- --
+\n A
+d i
+s a
+viron ment
+ar ch
+at t
+p ot
+t as
+c reate
+ou gh
+f l
+m aking
+i ous
+g ra
+l ife
+\n O
+al gorith
+al ity
+en g
+f in
+u c
+? ",
+ Y
+re t
+be en
+techn ology
+prog ra
+ha nd
+h ip
+w n
+c al
+wh at
+ivid ual
+is s
+et y
+langu age
+our ces
+cl ass
+t ake
+e as
+r ic
+v is
+b ject
+re f
+en vironment
+f irst
+e g
+ind ividual
+pl an
+per form
+ ru
+i en
+imp act
+a g
+ad e
+c le
+re qu
+d ition
+_ _
+c he
+pt ion
+app ro
+ **
+g reat
+v ed
+ex pl
+g row
+G enerate
+m y
+includ ing
+ac cess
+p op
+m in
+f ore
+soc ial
+in es
+char act
+b r
+st ep
+under stand
+or gan
+A d
+dis c
+p ower
+l ong
+he d
+con c
+w ard
+it ed
+e le
+c ing
+e very
+c a
+of ten
+us er
+v ie
+ V
+f ood
+includ e
+l oc
+as es
+ical ly
+od e
+ant s
+inv ol
+sm all
+s ur
+ach ine
+be ing
+pot ential
+n o
+C h
+de p
+at her
+b oth
+en s
+pos s
+ ed
+crib e
+t s
+or k
+The y
+p ur
+iv ity
+wor ds
+sign ific
+w ere
+H ow
+pro m
+exper ience
+ K
+u p
+c ount
+ere d
+D es
+f am
+`` `
+ak es
+g l
+H e
+fe el
+b ack
+f i
+pro ble
+iz ation
+l ing
+commun ic
+pl oy
+a ut
+f riend
+hum an
+sp e
+e w
+person al
+to p
+ ent
+ot her
+ch ang
+c or
+ch ange
+dec is
+ab ility
+h ing
+at ural
+e ver
+c ost
+go od
+au se
+ ident
+so ft
+in ed
+p ass
+' t
+at ures
+b en
+comp any
+st art
+signific ant
+su mm
+on d
+ol d
+b ers
+se l
+? \
+c ur
+l ight
+comm on
+.\ "
+custom ers
+iv ing
+con om
+fun ction
+ ve
+th ree
+ev en
+in ing
+g ener
+ri es
+le vel
+spec ific
+we bs
+the n
+effect ive
+c ur
+en se
+lar ge
+d ist
+eff ic
+su pport
+g et
+C reate
+re ad
+p ort
+in f
+ '
+y ear
+st ate
+ke y
+c cess
+: **
+a v
+kn ow
+ben ef
+ ess
+ab les
+re n
+o wn
+The se
+oc k
+- t
+ ide
+om m
+re en
+c ed
+ct ure
+te am
+r is
+tas ks
+d own
+st ru
+comput er
+- b
+f act
+m em
+et ter
+\n S
+a round
+wor d
+b ased
+be h
+r ight
+d el
+po int
+n atural
+s s
+e conom
+m ade
+in s
+in st
+m at
+val ue
+an im
+se ver
+\n T
+ation al
+it al
+z e
+ot e
+ill s
+ter n
+re ad
+cont ent
+on line
+en d
+U n
+v ent
+se e
+end ing
+m on
+d r
+ke ep
+system s
+c ul
+v en
+st ory
+med ia
+sever al
+he n
+ate g
+cont in
+de v
+lear n
+l a
+st re
+part ic
+a ir
+ual ly
+su ccess
+ou se
+is s
+i ed
+m achine
+o pt
+ x
+o p
+pro f
+oc us
+ch ie
+met h
+n er
+om p
+r on
+h ome
+b etter
+P ro
+m ult
+om et
+incre ase
+anal y
+ver t
+re le
+b ra
+in k
+t em
+p redi
+t re
+serv ice
+webs ite
+man age
+soft ware
+he re
+pro t
+- s
+qu est
+i er
+kn own
+or der
+ph ys
+ce pt
+a chie
+in put
+poss ible
+I f
+ex t
+f ter
+e lect
+meth od
+b re
+A n
+way s
+er ing
+et s
+j ust
+st ore
+develop ment
+c are
+o bject
+typ e
+F or
+f ocus
+gg est
+on ly
+cons id
+ar s
+ch all
+det erm
+s al
+in s
+fe atures
+t ru
+od y
+to ol
+> \
+ens ure
+os s
+ub lic
+it em
+H ere
+in ation
+de f
+Des cribe
+ion al
+rou p
+con f
+need s
+charact er
+vari ous
+le t
+app lic
+a ut
+j ob
+ell ig
+C on
+b est
+f ore
+am ount
+ro p
+bu ild
+iqu e
+ag ing
+em ploy
+re st
+a ir
+W hat
+to get
+way s
+ident ify
+toget her
+re al
+us ers
+me an
+as ing
+A m
+ed uc
+algorith m
+n etw
+c ode
+W rite
+o v
+- d
+ou ra
+How ever
+ut ure
+vie w
+in du
+product s
+ect ed
+er tain
+; \
+A s
+p r
+ast e
+o per
+ $
+av i
+sel f
+ <
+indu st
+g u
+other s
+E x
+i an
+" \"
+- f
+n ces
+f il
+resp ons
+ro l
+c ap
+be fore
+ver n
+comple x
+l us
+rib ut
+at s
+pos itive
+o h
+l o
+g roup
+f ound
+e e
+og n
+s w
+individual s
+p ract
+en c
+sh are
+ra ph
+r ange
+su n
+\ t
+provid ing
+ic le
+de m
+pl ace
+a ud
+j oy
+m ust
+el s
+er y
+O ne
+fam ily
+f uture
+l ess
+re nt
+proble m
+ess ential
+ro du
+i red
+redu cing
+is m
+w arm
+ra y
+ab ility
+str ong
+al ways
+res ources
+benef its
+str ateg
+invol ves
+ass ist
+ere st
+n A
+ress ion
+ [
+il ities
+step s
+ver all
+sh ow
+ob al
+\n F
+l and
+H ere
+business es
+E n
+pport un
+me as
+ret urn
+d ig
+h ist
+y th
+c ent
+ab le
+with out
+y c
+pl ain
+rel ations
+serv ices
+- c
+t est
+ar th
+communic ation
+inter n
+ne w
+s it
+inv est
+ca us
+u nt
+friend s
+chang es
+c ri
+d it
+B y
+Y ou
+me ans
+re se
+o ol
+t ed
+ellig ence
+ain s
+pp ing
+be l
+rep resent
+ha pp
+s er
+perform ance
+o pportun
+tem per
+S he
+f u
+i x
+b ot
+w rit
+beh avi
+pro ject
+W ith
+iv ers
+d ay
+phys ical
+iz ing
+act iv
+with in
+int erest
+ol ution
+ward s
+ff ic
+qu ick
+p ublic
+grow th
+ch o
+relations hip
+unt il
+help s
+stud ents
+fi el
+im es
+ul ation
+ib ility
+el f
+f ul
+su b
+an k
+id es
+sk ills
+cl imate
+G iven
+p ar
+cle ar
+ir t
+N ame
+p resent
+t ri
+chall eng
+re am
+l ay
+market ing
+summ ary
+ch ild
+sa f
+su re
+s ame
+m u
+em ail
+b on
+s omet
+``` \
+cur rent
+am p
+en ces
+R e
+trans port
+m e
+- p
+a ction
+E x
+year s
+com b
+h or
+anc ed
+t y
+l ove
+g reen
+pop ular
+l ess
+d ra
+cont rol
+a ff
+cons um
+g ame
+ent al
+ight s
+ar get
+om es
+o x
+ic ult
+er c
+go als
+anc ial
+t le
+go vern
+num bers
+f ive
+st and
+se arch
+effic ient
+w al
+n ame
+at h
+he art
+d uring
+re ct
+over all
+yth on
+allow s
+c ity
+a ve
+v ant
+ater ial
+w ide
+m us
+ific ial
+h ard
+T h
+oo se
+gl obal
+a j
+t er
+diff icult
+l ine
+A l
+c are
+iv ed
+reg ular
+g r
+) ,
+le ment
+h im
+un ique
+en joy
+mean ing
+op en
+ i
+ab or
+are a
+item s
+cle an
+dition ally
+o id
+W e
+be aut
+me et
+ip le
+state ment
+ag ain
+ys is
+f ac
+s ources
+b ody
+algorith ms
+aud ience
+w ant
+l og
+main tain
+activ ities
+mo ve
+c ult
+one y
+t arget
+\n B
+m aterial
+creat ing
+stru cture
+at form
+e xt
+exper ien
+val ues
+e ad
+oh n
+health y
+ro ss
+int eg
+rese arch
+at ch
+oo king
+ro le
+provid es
+i ety
+ist s
+fin ancial
+or ies
+d ent
+ er
+art icle
+ele ments
+add ress
+con n
+U se
+m p
+eas y
+ne g
+col or
+cal cul
+Ex plain
+P l
+p ect
+in ce
+al e
+ris k
+cur ity
+er t
+fe ed
+ev ent
+v ers
+pl es
+level s
+b i
+st ay
+pl atform
+bre ak
+b ack
+s at
+\nO verall
+educ ation
+\n C
+car bon
+---- ----
+ap e
+pre vent
+add ition
+st ress
+r al
+our ce
+ru s
+com e
+rec ogn
+Un ited
+pro per
+pol l
+dent ify
+understand ing
+decis ions
+i ct
+d ire
+behavi or
+ *
+\n I
+m ess
+anim als
+s l
+w ind
+b as
+p ain
+lead ing
+er n
+g er
+p res
+th ough
+inter act
+y le
+do es
+he ad
+int elligence
+ort s
+bec ome
+ru n
+ar ing
+imp lement
+a ction
+o ot
+ter ns
+prot ect
+er ic
+f low
+em ot
+cess ary
+ur ate
+su ggest
+progra m
+ph r
+health care
+ent ion
+su st
+wh y
+acc urate
+l u
+h ig
+re ach
+allow ing
+tra vel
+requ ire
+are as
+de ep
+H e
+fe w
+s elf
+ou n
+ #
+os p
+st r
+min ut
+decis ion
+The re
+an ces
+qu ality
+av ail
+sp ace
+somet hing
+we b
+pat terns
+m ot
+or ing
+is f
+an other
+acc ount
+\n W
+us s
+m aj
+u ation
+sust ain
+aut om
+iqu es
+iss ions
+ver se
+con cept
+se curity
+th ose
+prof ess
+sh ort
+n ight
+eng th
+a pt
+e x
+Ad ditionally
+t aking
+to o
+ag n
+sim ple
+lus ion
+ien cy
+as h
+our s
+p a
+l it
+S p
+it ing
+d on
+l im
+l ish
+m at
+av es
+led ge
+dition al
+in c
+ev ents
+off er
+th ing
+wor king
+anal ysis
+achie ve
+p ie
+b ook
+f re
+mu ch
+o on
+t ry
+es p
+w aste
+f ace
+e ar
+f ru
+transport ation
+ch ool
+techn iques
+progra mm
+E arth
+predi ct
+ne ver
+w s
+u ment
+imate ly
+are d
+partic ular
+to wards
+econom ic
+incre asing
+f ast
+im ent
+netw ork
+cor rect
+m ight
+o c
+bec ause
+W h
+a z
+pl ay
+result s
+manage ment
+pur ch
+s ound
+p ast
+tra ining
+__ __
+op e
+eng age
+oura ge
+s ense
+f ree
+pre f
+e es
+count ries
+ne y
+an ies
+a fter
+m ind
+ex c
+O nce
+ 
+comple te
+im m
+ est
+g enerate
+ver b
+D e
+' m
+tool s
+redi ents
+maj or
+ent ly
+cont ribut
+le ep
+point s
+dit ions
+fact ors
+e l
+ne xt
+i um
+ou d
+c ru
+re as
+ri ate
+I nd
+prom ot
+hist ory
+j our
+d ue
+C on
+ve get
+en cy
+Am eric
+f ra
+differe nce
+o ard
+le x
+equ ation
+irt ual
+c up
+fore st
+neg ative
+se con
+on es
+n ature
+us es
+a h
+p or
+se c
+ord ing
+l ast
+S ome
+iss ues
+sc ient
+pr int
+St ates
+o ver
+sat isf
+dev ices
+dis e
+temper ature
+feed back
+ne cessary
+em issions
+m b
+l ow
+f or
+t al
+challeng es
+ar ray
+s ide
+eng ine
+b oo
+at a
+bel ie
+- m
+mult iple
+s ing
+govern ment
+am es
+if ied
+minut es
+success ful
+m oney
+quick ly
+b ir
+typ ically
+p ost
+pre p
+know ledge
+pp ed
+a ctions
+method s
+opt im
+\n P
+out put
+fiel d
+t able
+b al
+col l
+charact ers
+v olution
+or ds
+il ar
+ific ation
+an e
+c ell
+m il
+W hat
+s qu
+l ives
+A r
+phr ase
+n ut
+dig ital
+intern et
+l ass
+u ra
+omm end
+t reat
+appro p
+res h
+ur ther
+O ne
+vis ual
+ate gor
+appro ach
+c ertain
+sh o
+v al
+tas k
+i res
+approp riate
+v ie
+design ed
+p ose
+** :
+f ort
+| \
+applic ations
+p ay
+n ow
+he at
+indust ry
+p re
+effective ly
+pop ulation
+opportun ities
+< /
+T o
+up d
+includ es
+E ng
+typ es
+up on
+consid er
+le t
+g en
+og raph
+pl ace
+t imes
+ar g
+C omp
+G o
+re ce
+child ren
+tra ck
+some one
+w ord
+you ng
+con ditions
+tra ditional
+model s
+I dentify
+c amp
+m akes
+ist ic
+ar r
+c ard
+ut ions
+l t
+o ld
+ide as
+e y
+t ree
+iss ue
+h arm
+avail able
+c r
+power ful
+n ov
+mo vie
+we ather
+sk y
+quest ions
+e et
+act ivity
+bra nd
+is hed
+analy ze
+S h
+en h
+av or
+be g
+s chool
+i ate
+eas ier
+inf lu
+n on
+stud y
+l ook
+sol ution
+le g
+con st
+H ow
+comp et
diff -ruN marc_original/third_party/torchtune/tests/assets/tiny_bpe_tokenizer.json marc/third_party/torchtune/tests/assets/tiny_bpe_tokenizer.json
--- marc_original/third_party/torchtune/tests/assets/tiny_bpe_tokenizer.json	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/tiny_bpe_tokenizer.json	2025-02-20 17:49:29.570024278 -0500
@@ -0,0 +1 @@
+{"version":"1.0","truncation":null,"padding":null,"added_tokens":[{"id":2000,"content":"<|endoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":false,"special":true},{"id":2001,"content":"<|im_start|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":false,"special":true},{"id":2002,"content":"<|im_end|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":false,"special":true}],"normalizer":{"type":"NFC"},"pre_tokenizer":{"type":"Sequence","pretokenizers":[{"type":"Split","pattern":{"Regex":"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"},"behavior":"Isolated","invert":false},{"type":"ByteLevel","add_prefix_space":false,"trim_offsets":false,"use_regex":false}]},"post_processor":{"type":"ByteLevel","add_prefix_space":false,"trim_offsets":false,"use_regex":false},"decoder":{"type":"ByteLevel","add_prefix_space":false,"trim_offsets":false,"use_regex":false},"model":{"type":"BPE","dropout":null,"unk_token":null,"continuing_subword_prefix":null,"end_of_word_suffix":null,"fuse_unk":false,"byte_fallback":false,"ignore_merges":false,"vocab":{"0":15,"1":16,"2":17,"3":18,"4":19,"5":20,"6":21,"7":22,"8":23,"9":24,"!":0,"\"":1,"#":2,"$":3,"%":4,"&":5,"'":6,"(":7,")":8,"*":9,"+":10,",":11,"-":12,".":13,"/":14,":":25,";":26,"<":27,"=":28,">":29,"?":30,"@":31,"A":32,"B":33,"C":34,"D":35,"E":36,"F":37,"G":38,"H":39,"I":40,"J":41,"K":42,"L":43,"M":44,"N":45,"O":46,"P":47,"Q":48,"R":49,"S":50,"T":51,"U":52,"V":53,"W":54,"X":55,"Y":56,"Z":57,"[":58,"\\":59,"]":60,"^":61,"_":62,"`":63,"a":64,"b":65,"c":66,"d":67,"e":68,"f":69,"g":70,"h":71,"i":72,"j":73,"k":74,"l":75,"m":76,"n":77,"o":78,"p":79,"q":80,"r":81,"s":82,"t":83,"u":84,"v":85,"w":86,"x":87,"y":88,"z":89,"{":90,"|":91,"}":92,"~":93,"":94,"":95,"":96,"t":97,"a":98,"in":99,"he":100,"re":101,"on":102,"the":103,"s":104,"er":105,"at":106,"c":107,"":108,"en":109,"o":110,"\"":111,"nd":112,"es":113,"ing":114,"":115,"it":116,"p":117,"or":118,"ou":119,"and":120,"w":121,"is":122,"f":123,"an":124,"ion":125,"al":126,"b":127,"to":128,"m":129,"in":130,"of":131,"le":132,"ct":133,"ar":134,"ut":135,"d":136,"st":137,"ed":138,"":139,"ic":140,"\":":141,",":142,"ro":143,"ent":144,"\\n":145,"e":146,"put":147,"om":148,"re":149,"as":150,"ve":151,"h":152,"th":153,"\",":154,"l":155,"is":156,"et":157,"ce":158,"n":159,".\\":160,"im":161,"il":162,"g":163,"u":164,"ction":165,"ru":166,"ation":167,"ol":168,"ch":169,"T":170,"for":171,"out":172,"ra":173,"ow":174,"id":175,"ly":176,"st":177,"be":178,"y":179,"pro":180,"ig":181,"se":182,"ate":183,"that":184,"ith":185,"ir":186,"ur":187,"ot":188,"or":189,"on":190,"you":191,"ers":192,"stru":193,"an":194,"if":195,"ul":196,"struction":197,"{":198,"}":199,"can":200,"input":201,"output":202,"instruction":203,"{":204,"},":205,"\"":206,"he":207,"con":208,"it":209,"ay":210,"ess":211,"with":212,"ver":213,"el":214,"as":215,"am":216,"A":217,"ge":218,"su":219,"iv":220,".\",":221,"com":222,"I":223,"ment":224,"ak":225,"al":226,"\\\"":227,".\"":228,"ive":229,"are":230,"ab":231,"ad":232,"mo":233,"ex":234,"v":235,"S":236,"res":237,"pp":238,"qu":239,"de":240,"wh":241,"ity":242,"en":243,"The":244,"her":245,"ld":246,"ri":247,"ter":248,"ant":249,"C":250,"ist":251,"\"\",":252,"um":253,"us":254,"ne":255,"ain":256,"th":257,"ect":258,"le":259,"op":260,"em":261,"ies":262,"ch":263,"im":264,"du":265,"od":266,"ort":267,"nt":268,"est":269,"igh":270,"ere":271,"ha":272,"us":273,"ure":274,"ial":275,"oc":276,"wor":277,"their":278,"ac":279,"ence":280,"iz":281,"your":282,"os":283,"imp":284,"ud":285,"by":286,"se":287,"ine":288,"ould":289,"low":290,"ill":291,"age":292,"rom":293,"sp":294,"P":295,"sh":296,"ust":297,"The":298,"un":299,"'s":300,"inc":301,"ide":302,"pl":303,"ight":304,"og":305,"pl":306,"pt":307,"are":308,"te":309,"int":310,"\\":311,"his":312,"r":313,"ake":314,"per":315,"orm":316,"ag":317,"ff":318,"E":319,"art":320,"k":321,"end":322,"M":323,"we":324,"B":325,"ad":326,"cess":327,"rou":328,"ical":329,"all":330,"able":331,"from":332,"and":333,"H":334,"ab":335,"act":336,"comp":337,"ome":338,"ach":339,"This":340,"have":341,"form":342,"\\\"":343,"ast":344,"at":345,"W":346,"res":347,"dat":348,":\\":349,"ther":350,"ions":351,"ore":352,"(":353,"cont":354,"our":355,"ep":356,"F":357,"ac":358,"ance":359,"R":360,"gh":361,"me":362,"ces":363,"was":364,"ind":365,"vel":366,"ations":367,"hel":368,"more":369,"ult":370,"D":371,"reat":372,"ign":373,"help":374,"ime":375,"ard":376,"cl":377,"app":378,"ans":379,"ie":380,"data":381,"ich":382,"ang":383,"ous":384,"ell":385,"ks":386,"ase":387,"ice":388,"ip":389,"ite":390,"such":391,"fe":392,"whe":393,"ib":394,"other":395,"this":396,"ass":397,"ual":398,"ile":399,"ne":400,"red":401,"has":402,"oo":403,"ress":404,"ific":405,"ning":406,"=":407,"up":408,"man":409,"ar":410,"ong":411,"ec":412,"tra":413,"av":414,"which":415,"go":416,"prov":417,"dis":418,"**":419,"so":420,"G":421,"one":422,"em":423,"not":424,"ue":425,"O":426,"j":427,"ace":428,"they":429,"ame":430,"qu":431,"L":432,"iff":433,"fol":434,"ary":435,"ated":436,"ustom":437,"ition":438,"its":439,"sy":440,"ke":441,"ack":442,"ry":443,"--":444,"time":445,"des":446,"new":447,"ents":448,"ount":449,"follow":450,"also":451,"comm":452,"out":453,"eff":454,"diff":455,"iven":456,"ap":457,"sent":458,"\\u":459,"so":460,"produ":461,"use":462,"sc":463,"-":464,"un":465,"lud":466,"It":467,"ener":468,"king":469,"ev":470,"about":471,"them":472,"U":473,"custom":474,"ro":475,"includ":476,"les":477,"etw":478,"stem":479,"xt":480,"into":481,"per":482,"In":483,"N":484,"will":485,"lear":486,"ber":487,"all":488,"pe":489,"ds":490,"tw":491,"aking":492,"ark":493,"ful":494,"make":495,"chn":496,"erv":497,"ost":498,"rough":499,"one":500,"inter":501,"ities":502,"ail":503,"ike":504,"ree":505,"ple":506,"alth":507,"used":508,"ors":509,"over":510,"ility":511,"ments":512,"ange":513,"way":514,"ory":515,"col":516,"pr":517,"could":518,"num":519,"reate":520,"int":521,"redu":522,"erson":523,"rec":524,"her":525,"need":526,"ms":527,"ater":528,"oy":529,"system":530,"inform":531,"two":532,"techn":533,"sentence":534,"ience":535,"ize":536,"get":537,"differe":538,"ood":539,"rib":540,"but":541,"following":542,"ased":543,"olog":544,"erg":545,"led":546,"ures":547,"In":548,"ear":549,"ph":550,"own":551,"pre":552,"would":553,"using":554,"cons":555,"work":556,"mod":557,"ating":558,"ia":559,"ire":560,"pos":561,"ient":562,"ob":563,"ject":564,"inv":565,"ons":566,"do":567,"ular":568,"dec":569,"health":570,"impro":571,"any":572,"through":573,"yp":574,"row":575,"velop":576,"process":577,"tr":578,"lic":579,"very":580,"als":581,"ify":582,"``":583,"ari":584,"str":585,"import":586,"like":587,"product":588,"some":589,"ph":590,"ential":591,"am":592,"ates":593,"acc":594,"ens":595,"ns":596,"sm":597,"ind":598,"een":599,"exper":600,"lect":601,"val":602,"rel":603,"its":604,"information":605,"ings":606,"J":607,"ople":608,"iness":609,"given":610,"mm":611,"ices":612,"part":613,"ild":614,"ys":615,"our":616,"nder":617,"person":618,"ally":619,"ke":620,"etween":621,"ft":622,"oth":623,"spec":624,"between":625,"ergy":626,"AI":627,"who":628,"may":629,"ef":630,"ative":631,"ise":632,"list":633,"kn":634,"add":635,",\\":636,"ord":637,"ics":638,"people":639,"St":640,"his":641,"exp":642,"ible":643,"there":644,"serv":645,"incre":646,"develop":647,"ound":648,"ower":649,"trans":650,"bs":651,"energy":652,"off":653,"bus":654,"while":655,"ose":656,"act":657,"exam":658,"learning":659,"ctions":660,"con":661,"gor":662,"gan":663,"ution":664,"round":665,"pport":666,"how":667,"bl":668,"med":669,"anc":670,"typ":671,"ra":672,"car":673,"ife":674,"world":675,"vari":676,"rep":677,"au":678,"soc":679,"provid":680,"set":681,"ten":682,"sol":683,"each":684,"when":685,"effect":686,"po":687,"she":688,"ick":689,"where":690,"model":691,"important":692,"under":693,"prog":694,"enerate":695,"ural":696,"tain":697,"ass":698,"ology":699,"had":700,"ook":701,"gg":702,"customer":703,"ting":704,"ving":705,"resp":706,"line":707,"creat":708,"ll":709,"ily":710,"reg":711,"det":712,"if":713,"+":714,"business":715,"\\nIn":716,"ish":717,"most":718,"":719,"hes":720,"angu":721,"provide":722,"adv":723,"erm":724,"ub":725,"sk":726,"irst":727,"any":728,"day":729,"ivid":730,"arm":731,"ract":732,"nce":733,"|":734,"improve":735,")\\":736,"co":737,"commun":738,"arket":739,"met":740,"cy":741,"different":742,"ized":743,"art":744,"\\nThe":745,"rit":746,"comput":747,"form":748,"ck":749,"hum":750,"char":751,"ble":752,"lead":753,"iron":754,"rem":755,"should":756,"te":757,"allow":758,"ness":759,"hat":760,"fun":761,"comple":762,"langu":763,"ages":764,"bec":765,"sign":766,"ues":767,"ature":768,"find":769,"riend":770,"stud":771,"main":772,"imate":773,"ove":774,"result":775,"play":776,"reduce":777,"eng":778,"ware":779,"redi":780,"number":781,"lar":782,"pol":783,"pat":784,"well":785,"ident":786,"viron":787,"rite":788,"crib":789,"bu":790,"high":791,"these":792,"ives":793,"ves":794,"design":795,"urn":796,"than":797,"der":798,"anal":799,"water":800,"market":801,"example":802,"way":803,"stand":804,"ng":805,"ax":806,"itive":807,"`":808,"iqu":809,"sim":810,"equ":811,"gorith":812,"text":813,"resent":814,"many":815,"uring":816,"----":817,"\\nA":818,"di":819,"sa":820,"vironment":821,"arch":822,"att":823,"pot":824,"tas":825,"create":826,"ough":827,"fl":828,"making":829,"ious":830,"gra":831,"life":832,"\\nO":833,"algorith":834,"ality":835,"eng":836,"fin":837,"uc":838,"?\",":839,"Y":840,"ret":841,"been":842,"technology":843,"progra":844,"hand":845,"hip":846,"wn":847,"cal":848,"what":849,"ividual":850,"iss":851,"ety":852,"language":853,"ources":854,"class":855,"take":856,"eas":857,"ric":858,"vis":859,"bject":860,"ref":861,"environment":862,"first":863,"eg":864,"individual":865,"plan":866,"perform":867,"ru":868,"ien":869,"impact":870,"ag":871,"ade":872,"cle":873,"requ":874,"dition":875,"__":876,"che":877,"ption":878,"appro":879,"**":880,"great":881,"ved":882,"expl":883,"grow":884,"Generate":885,"my":886,"including":887,"access":888,"pop":889,"min":890,"fore":891,"social":892,"ines":893,"charact":894,"br":895,"step":896,"understand":897,"organ":898,"Ad":899,"disc":900,"power":901,"long":902,"hed":903,"conc":904,"ward":905,"ited":906,"ele":907,"cing":908,"every":909,"ca":910,"often":911,"user":912,"vie":913,"V":914,"food":915,"include":916,"loc":917,"ases":918,"ically":919,"ode":920,"ants":921,"invol":922,"small":923,"sur":924,"achine":925,"being":926,"potential":927,"no":928,"Ch":929,"dep":930,"ather":931,"both":932,"ens":933,"poss":934,"ed":935,"cribe":936,"ts":937,"ork":938,"They":939,"pur":940,"ivity":941,"words":942,"signific":943,"were":944,"How":945,"prom":946,"experience":947,"K":948,"up":949,"count":950,"ered":951,"Des":952,"fam":953,"```":954,"akes":955,"gl":956,"He":957,"feel":958,"back":959,"fi":960,"proble":961,"ization":962,"ling":963,"communic":964,"ploy":965,"aut":966,"friend":967,"human":968,"spe":969,"ew":970,"personal":971,"top":972,"ent":973,"other":974,"chang":975,"cor":976,"change":977,"decis":978,"ability":979,"hing":980,"atural":981,"ever":982,"cost":983,"good":984,"ause":985,"ident":986,"soft":987,"ined":988,"pass":989,"'t":990,"atures":991,"ben":992,"company":993,"start":994,"significant":995,"summ":996,"ond":997,"old":998,"bers":999,"sel":1000,"?\\":1001,"cur":1002,"light":1003,"common":1004,".\\\"":1005,"customers":1006,"iving":1007,"conom":1008,"function":1009,"ve":1010,"three":1011,"even":1012,"ining":1013,"gener":1014,"ries":1015,"level":1016,"specific":1017,"webs":1018,"then":1019,"effective":1020,"cur":1021,"ense":1022,"large":1023,"dist":1024,"effic":1025,"support":1026,"get":1027,"Create":1028,"read":1029,"port":1030,"inf":1031,"'":1032,"year":1033,"state":1034,"key":1035,"ccess":1036,":**":1037,"av":1038,"know":1039,"benef":1040,"ess":1041,"ables":1042,"ren":1043,"own":1044,"These":1045,"ock":1046,"-t":1047,"ide":1048,"omm":1049,"reen":1050,"ced":1051,"cture":1052,"team":1053,"ris":1054,"tasks":1055,"down":1056,"stru":1057,"computer":1058,"-b":1059,"fact":1060,"mem":1061,"etter":1062,"\\nS":1063,"around":1064,"word":1065,"based":1066,"beh":1067,"right":1068,"del":1069,"point":1070,"natural":1071,"ss":1072,"econom":1073,"made":1074,"ins":1075,"inst":1076,"mat":1077,"value":1078,"anim":1079,"sever":1080,"\\nT":1081,"ational":1082,"ital":1083,"ze":1084,"ote":1085,"ills":1086,"tern":1087,"read":1088,"content":1089,"online":1090,"end":1091,"Un":1092,"vent":1093,"see":1094,"ending":1095,"mon":1096,"dr":1097,"keep":1098,"systems":1099,"cul":1100,"ven":1101,"story":1102,"media":1103,"several":1104,"hen":1105,"ateg":1106,"contin":1107,"dev":1108,"learn":1109,"la":1110,"stre":1111,"partic":1112,"air":1113,"ually":1114,"success":1115,"ouse":1116,"iss":1117,"ied":1118,"machine":1119,"opt":1120,"x":1121,"op":1122,"prof":1123,"ocus":1124,"chie":1125,"meth":1126,"ner":1127,"omp":1128,"ron":1129,"home":1130,"better":1131,"Pro":1132,"mult":1133,"omet":1134,"increase":1135,"analy":1136,"vert":1137,"rele":1138,"bra":1139,"ink":1140,"tem":1141,"predi":1142,"tre":1143,"service":1144,"website":1145,"manage":1146,"software":1147,"here":1148,"prot":1149,"-s":1150,"quest":1151,"ier":1152,"known":1153,"order":1154,"phys":1155,"cept":1156,"achie":1157,"input":1158,"possible":1159,"If":1160,"ext":1161,"fter":1162,"elect":1163,"method":1164,"bre":1165,"An":1166,"ways":1167,"ering":1168,"ets":1169,"just":1170,"store":1171,"development":1172,"care":1173,"object":1174,"type":1175,"For":1176,"focus":1177,"ggest":1178,"only":1179,"consid":1180,"ars":1181,"chall":1182,"determ":1183,"sal":1184,"ins":1185,"features":1186,"tru":1187,"ody":1188,"tool":1189,">\\":1190,"ensure":1191,"oss":1192,"ublic":1193,"item":1194,"Here":1195,"ination":1196,"def":1197,"Describe":1198,"ional":1199,"roup":1200,"conf":1201,"needs":1202,"character":1203,"various":1204,"let":1205,"applic":1206,"aut":1207,"job":1208,"ellig":1209,"Con":1210,"best":1211,"fore":1212,"amount":1213,"rop":1214,"build":1215,"ique":1216,"aging":1217,"employ":1218,"rest":1219,"air":1220,"What":1221,"toget":1222,"ways":1223,"identify":1224,"together":1225,"real":1226,"users":1227,"mean":1228,"asing":1229,"Am":1230,"educ":1231,"algorithm":1232,"netw":1233,"code":1234,"Write":1235,"ov":1236,"-d":1237,"oura":1238,"However":1239,"uture":1240,"view":1241,"indu":1242,"products":1243,"ected":1244,"ertain":1245,";\\":1246,"As":1247,"pr":1248,"aste":1249,"oper":1250,"$":1251,"avi":1252,"self":1253,"<":1254,"indust":1255,"gu":1256,"others":1257,"Ex":1258,"ian":1259,"\"\\\"":1260,"-f":1261,"nces":1262,"fil":1263,"respons":1264,"rol":1265,"cap":1266,"before":1267,"vern":1268,"complex":1269,"lus":1270,"ribut":1271,"ats":1272,"positive":1273,"oh":1274,"lo":1275,"group":1276,"found":1277,"ee":1278,"ogn":1279,"sw":1280,"individuals":1281,"pract":1282,"enc":1283,"share":1284,"raph":1285,"range":1286,"sun":1287,"\\t":1288,"providing":1289,"icle":1290,"dem":1291,"place":1292,"aud":1293,"joy":1294,"must":1295,"els":1296,"ery":1297,"One":1298,"family":1299,"future":1300,"less":1301,"rent":1302,"problem":1303,"essential":1304,"rodu":1305,"ired":1306,"reducing":1307,"ism":1308,"warm":1309,"ray":1310,"ability":1311,"strong":1312,"always":1313,"resources":1314,"benefits":1315,"strateg":1316,"involves":1317,"assist":1318,"erest":1319,"nA":1320,"ression":1321,"[":1322,"ilities":1323,"steps":1324,"verall":1325,"show":1326,"obal":1327,"\\nF":1328,"land":1329,"Here":1330,"businesses":1331,"En":1332,"pportun":1333,"meas":1334,"return":1335,"dig":1336,"hist":1337,"yth":1338,"cent":1339,"able":1340,"without":1341,"yc":1342,"plain":1343,"relations":1344,"services":1345,"-c":1346,"test":1347,"arth":1348,"communication":1349,"intern":1350,"new":1351,"sit":1352,"invest":1353,"caus":1354,"unt":1355,"friends":1356,"changes":1357,"cri":1358,"dit":1359,"By":1360,"You":1361,"means":1362,"rese":1363,"ool":1364,"ted":1365,"elligence":1366,"ains":1367,"pping":1368,"bel":1369,"represent":1370,"happ":1371,"ser":1372,"performance":1373,"opportun":1374,"temper":1375,"She":1376,"fu":1377,"ix":1378,"bot":1379,"writ":1380,"behavi":1381,"project":1382,"With":1383,"ivers":1384,"day":1385,"physical":1386,"izing":1387,"activ":1388,"within":1389,"interest":1390,"olution":1391,"wards":1392,"ffic":1393,"quick":1394,"public":1395,"growth":1396,"cho":1397,"relationship":1398,"until":1399,"helps":1400,"students":1401,"fiel":1402,"imes":1403,"ulation":1404,"ibility":1405,"elf":1406,"ful":1407,"sub":1408,"ank":1409,"ides":1410,"skills":1411,"climate":1412,"Given":1413,"par":1414,"clear":1415,"irt":1416,"Name":1417,"present":1418,"tri":1419,"challeng":1420,"ream":1421,"lay":1422,"marketing":1423,"summary":1424,"child":1425,"saf":1426,"sure":1427,"same":1428,"mu":1429,"email":1430,"bon":1431,"somet":1432,"```\\":1433,"current":1434,"amp":1435,"ences":1436,"Re":1437,"transport":1438,"me":1439,"-p":1440,"action":1441,"Ex":1442,"years":1443,"comb":1444,"hor":1445,"anced":1446,"ty":1447,"love":1448,"green":1449,"popular":1450,"less":1451,"dra":1452,"control":1453,"aff":1454,"consum":1455,"game":1456,"ental":1457,"ights":1458,"arget":1459,"omes":1460,"ox":1461,"icult":1462,"erc":1463,"goals":1464,"ancial":1465,"tle":1466,"govern":1467,"numbers":1468,"five":1469,"stand":1470,"search":1471,"efficient":1472,"wal":1473,"name":1474,"ath":1475,"heart":1476,"during":1477,"rect":1478,"overall":1479,"ython":1480,"allows":1481,"city":1482,"ave":1483,"vant":1484,"aterial":1485,"wide":1486,"mus":1487,"ificial":1488,"hard":1489,"Th":1490,"oose":1491,"global":1492,"aj":1493,"ter":1494,"difficult":1495,"line":1496,"Al":1497,"care":1498,"ived":1499,"regular":1500,"gr":1501,"),":1502,"lement":1503,"him":1504,"unique":1505,"enjoy":1506,"meaning":1507,"open":1508,"i":1509,"abor":1510,"area":1511,"items":1512,"clean":1513,"ditionally":1514,"oid":1515,"We":1516,"beaut":1517,"meet":1518,"iple":1519,"statement":1520,"again":1521,"ysis":1522,"fac":1523,"sources":1524,"body":1525,"algorithms":1526,"audience":1527,"want":1528,"log":1529,"maintain":1530,"activities":1531,"move":1532,"cult":1533,"oney":1534,"target":1535,"\\nB":1536,"material":1537,"creating":1538,"structure":1539,"atform":1540,"ext":1541,"experien":1542,"values":1543,"ead":1544,"ohn":1545,"healthy":1546,"ross":1547,"integ":1548,"research":1549,"atch":1550,"ooking":1551,"role":1552,"provides":1553,"iety":1554,"ists":1555,"financial":1556,"ories":1557,"dent":1558,"er":1559,"article":1560,"elements":1561,"address":1562,"conn":1563,"Use":1564,"mp":1565,"easy":1566,"neg":1567,"color":1568,"calcul":1569,"Explain":1570,"Pl":1571,"pect":1572,"ince":1573,"ale":1574,"risk":1575,"curity":1576,"ert":1577,"feed":1578,"event":1579,"vers":1580,"ples":1581,"levels":1582,"bi":1583,"stay":1584,"platform":1585,"break":1586,"back":1587,"sat":1588,"\\nOverall":1589,"education":1590,"\\nC":1591,"carbon":1592,"--------":1593,"ape":1594,"prevent":1595,"addition":1596,"stress":1597,"ral":1598,"ource":1599,"rus":1600,"come":1601,"recogn":1602,"United":1603,"proper":1604,"poll":1605,"dentify":1606,"understanding":1607,"decisions":1608,"ict":1609,"dire":1610,"behavior":1611,"*":1612,"\\nI":1613,"mess":1614,"animals":1615,"sl":1616,"wind":1617,"bas":1618,"pain":1619,"leading":1620,"ern":1621,"ger":1622,"pres":1623,"though":1624,"interact":1625,"yle":1626,"does":1627,"head":1628,"intelligence":1629,"orts":1630,"become":1631,"run":1632,"aring":1633,"implement":1634,"action":1635,"oot":1636,"terns":1637,"protect":1638,"eric":1639,"flow":1640,"emot":1641,"cessary":1642,"urate":1643,"suggest":1644,"program":1645,"phr":1646,"healthcare":1647,"ention":1648,"sust":1649,"why":1650,"accurate":1651,"lu":1652,"hig":1653,"reach":1654,"allowing":1655,"travel":1656,"require":1657,"areas":1658,"deep":1659,"He":1660,"few":1661,"self":1662,"oun":1663,"#":1664,"osp":1665,"str":1666,"minut":1667,"decision":1668,"There":1669,"ances":1670,"quality":1671,"avail":1672,"space":1673,"something":1674,"web":1675,"patterns":1676,"mot":1677,"oring":1678,"isf":1679,"another":1680,"account":1681,"\\nW":1682,"uss":1683,"maj":1684,"uation":1685,"sustain":1686,"autom":1687,"iques":1688,"issions":1689,"verse":1690,"concept":1691,"security":1692,"those":1693,"profess":1694,"short":1695,"night":1696,"ength":1697,"apt":1698,"ex":1699,"Additionally":1700,"taking":1701,"too":1702,"agn":1703,"simple":1704,"lusion":1705,"iency":1706,"ash":1707,"ours":1708,"pa":1709,"lit":1710,"Sp":1711,"iting":1712,"don":1713,"lim":1714,"lish":1715,"mat":1716,"aves":1717,"ledge":1718,"ditional":1719,"inc":1720,"events":1721,"offer":1722,"thing":1723,"working":1724,"analysis":1725,"achieve":1726,"pie":1727,"book":1728,"fre":1729,"much":1730,"oon":1731,"try":1732,"esp":1733,"waste":1734,"face":1735,"ear":1736,"fru":1737,"transportation":1738,"chool":1739,"techniques":1740,"programm":1741,"Earth":1742,"predict":1743,"never":1744,"ws":1745,"ument":1746,"imately":1747,"ared":1748,"particular":1749,"towards":1750,"economic":1751,"increasing":1752,"fast":1753,"iment":1754,"network":1755,"correct":1756,"might":1757,"oc":1758,"because":1759,"Wh":1760,"az":1761,"play":1762,"results":1763,"management":1764,"purch":1765,"sound":1766,"past":1767,"training":1768,"____":1769,"ope":1770,"engage":1771,"ourage":1772,"sense":1773,"free":1774,"pref":1775,"ees":1776,"countries":1777,"ney":1778,"anies":1779,"after":1780,"mind":1781,"exc":1782,"Once":1783,"":1784,"complete":1785,"imm":1786,"est":1787,"generate":1788,"verb":1789,"De":1790,"'m":1791,"tools":1792,"redients":1793,"major":1794,"ently":1795,"contribut":1796,"leep":1797,"points":1798,"ditions":1799,"factors":1800,"el":1801,"next":1802,"ium":1803,"oud":1804,"cru":1805,"reas":1806,"riate":1807,"Ind":1808,"promot":1809,"history":1810,"jour":1811,"due":1812,"Con":1813,"veget":1814,"ency":1815,"Americ":1816,"fra":1817,"difference":1818,"oard":1819,"lex":1820,"equation":1821,"irtual":1822,"cup":1823,"forest":1824,"negative":1825,"secon":1826,"ones":1827,"nature":1828,"uses":1829,"ah":1830,"por":1831,"sec":1832,"ording":1833,"last":1834,"Some":1835,"issues":1836,"scient":1837,"print":1838,"States":1839,"over":1840,"satisf":1841,"devices":1842,"dise":1843,"temperature":1844,"feedback":1845,"necessary":1846,"emissions":1847,"mb":1848,"low":1849,"for":1850,"tal":1851,"challenges":1852,"array":1853,"side":1854,"engine":1855,"boo":1856,"ata":1857,"belie":1858,"-m":1859,"multiple":1860,"sing":1861,"government":1862,"ames":1863,"ified":1864,"minutes":1865,"successful":1866,"money":1867,"quickly":1868,"bir":1869,"typically":1870,"post":1871,"prep":1872,"knowledge":1873,"pped":1874,"actions":1875,"methods":1876,"optim":1877,"\\nP":1878,"output":1879,"field":1880,"table":1881,"bal":1882,"coll":1883,"characters":1884,"volution":1885,"ords":1886,"ilar":1887,"ification":1888,"ane":1889,"cell":1890,"mil":1891,"What":1892,"squ":1893,"lives":1894,"Ar":1895,"phrase":1896,"nut":1897,"digital":1898,"internet":1899,"lass":1900,"ura":1901,"ommend":1902,"treat":1903,"approp":1904,"resh":1905,"urther":1906,"One":1907,"visual":1908,"ategor":1909,"approach":1910,"certain":1911,"sho":1912,"val":1913,"task":1914,"ires":1915,"appropriate":1916,"vie":1917,"designed":1918,"pose":1919,"**:":1920,"fort":1921,"|\\":1922,"applications":1923,"pay":1924,"now":1925,"heat":1926,"industry":1927,"pre":1928,"effectively":1929,"population":1930,"opportunities":1931,"</":1932,"To":1933,"upd":1934,"includes":1935,"Eng":1936,"types":1937,"upon":1938,"consider":1939,"let":1940,"gen":1941,"ograph":1942,"place":1943,"times":1944,"arg":1945,"Comp":1946,"Go":1947,"rece":1948,"children":1949,"track":1950,"someone":1951,"word":1952,"young":1953,"conditions":1954,"traditional":1955,"models":1956,"Identify":1957,"camp":1958,"makes":1959,"istic":1960,"arr":1961,"card":1962,"utions":1963,"lt":1964,"old":1965,"ideas":1966,"ey":1967,"tree":1968,"issue":1969,"harm":1970,"available":1971,"cr":1972,"powerful":1973,"nov":1974,"movie":1975,"weather":1976,"sky":1977,"questions":1978,"eet":1979,"activity":1980,"brand":1981,"ished":1982,"analyze":1983,"Sh":1984,"enh":1985,"avor":1986,"beg":1987,"school":1988,"iate":1989,"easier":1990,"influ":1991,"non":1992,"study":1993,"look":1994,"solution":1995,"leg":1996,"const":1997,"How":1998,"compet":1999},"merges":[" "," t"," a","i n","h e","r e","o n","t he"," s","e r","a t"," c"," ","e n"," o"," \"","n d","e s","in g"," ","i t"," p","o r","o u","a nd"," w","i s"," f","a n","i on","a l"," b","t o"," m"," in","o f","l e","c t","a r","u t"," d","s t","e d"," ","i c","\" :",", ","r o","en t","\\ n"," e","p ut","o m"," re","a s","v e"," h","t h","\" ,"," l"," is","e t","c e"," n",". \\","i m","i l"," g"," u","ct ion","r u","at ion","o l","c h"," T","f or","ou t","r a","o w","i d","l y","s t","b e"," y","p ro","i g","s e","at e","th at","it h","i r","u r","o t","o r"," on","y ou","er s","st ru","a n","i f","u l","stru ction"," {"," }","c an","in put","out put","in struction","{ ","} ,","\" "," he","c on"," it","a y","es s","w ith","v er","e l","a s","a m"," A","g e","s u","i v",". \",","c om"," I","m ent","a k","a l","\\ \"",". \"","i ve","a re","a b","a d","m o","e x"," v"," S","re s","p p","q u","d e","w h","it y"," en","T he","he r","l d","r i","t er","an t"," C","is t","\" \",","u m","u s","n e","a in","t h","e ct"," le","o p","e m","i es","c h"," im","d u","o d","or t","n t","es t","ig h","e re","h a","u s","u re","i al","o c","w or","the ir","a c","en ce","i z","you r","o s","im p","u d","b y","s e","in e","ou ld","l ow","il l","a ge","ro m","s p"," P","s h","u st","T he","u n","' s","in c","id e","p l","igh t","o g","p l","p t","a re","t e","in t"," \\","h is"," r","ak e","p er","or m","a g","f f"," E","ar t"," k","en d"," M","w e"," B","a d","c ess","r ou","ic al","al l","ab le","f rom","a nd"," H","a b","a ct","com p","om e","a ch","T his","ha ve","f orm"," \\\"","a st","a t"," W","re s","d at",": \\","t her","ion s","o re"," (","con t","ou r","e p"," F","a c","an ce"," R","g h","m e","c es","w as","in d","ve l","ation s","he l","mo re","ul t"," D","re at","ig n","hel p","im e","ar d","c l","a pp","an s","i e","dat a","ic h","an g","ou s","el l","k s","as e","ic e","i p","it e","su ch","f e","w he","i b","o ther","th is","as s","u al","i le","n e","re d","h as","o o","res s","if ic","n ing"," =","u p","m an","a r","on g","e c","t ra","a v","wh ich","g o","pro v","d is","* *","s o"," G","on e","e m","n ot","u e"," O"," j","a ce","the y","am e"," qu"," L","if f","f ol","ar y","at ed","ust om","it ion","it s","s y","k e","ac k","r y","- -","t ime","d es","ne w","ent s","ou nt","fol low","al so","com m","o ut","e ff","d iff","iv en","a p","s ent","\\ u","s o","pro du","u se","s c"," -","u n","l ud","I t","en er","k ing","e v","ab out","the m"," U","c ustom"," ro","inc lud","l es","et w","st em","x t","int o","p er","I n"," N","w ill","le ar","b er","al l","p e","d s","t w","ak ing","ar k","f ul","m ake","ch n","er v","o st","rou gh","on e","in ter","it ies","a il","i ke","re e","p le","al th","us ed","or s","o ver","il ity","ment s","an ge","w ay","or y","c ol","p r","c ould","n um","re ate","in t","re du","ers on","re c","he r","ne ed","m s","at er","o y","sy stem","in form","tw o","te chn","sent ence","i ence","iz e","g et","diff ere","o od","ri b","b ut","follow ing","as ed","ol og","er g","le d","u res","I n","e ar","p h","ow n","p re","w ould","us ing","con s","wor k","mo d","at ing","i a","i re","p os","i ent","o b","j ect","in v","on s","d o","ul ar","de c","he alth","imp ro","an y","th rough","y p","ro w","vel op","pro cess","t r","l ic","ver y","al s","if y","` `","ar i","st r","imp ort","l ike","produ ct","s ome","p h","ent ial","a m","at es","ac c","en s","n s","s m","in d","e en","ex per","le ct","v al","re l","it s","inform ation","ing s"," J","op le","in ess","g iven","m m","ic es","p art","il d","y s","o ur","nd er","p erson","al ly","k e","etw een","f t","ot h","sp ec","b etween","erg y","A I","wh o","m ay","e f","at ive","is e","l ist","k n","ad d",", \\","or d","ic s","pe ople","S t","h is","ex p","ib le","the re","s erv","inc re","de velop","ou nd","ow er","tr ans","b s","en ergy","of f","b us","wh ile","o se","a ct","ex am","lear ning","ction s","c on","g or","g an","ut ion","rou nd","pp ort","h ow","b l","m ed","an c","t yp"," ra","c ar","if e","wor ld","v ari","re p","a u","s oc","prov id","s et","t en","s ol","e ach","whe n","eff ect","p o","s he","ic k","whe re","mod el","import ant","u nder","pro g","ener ate","ur al","t ain","as s","olog y","h ad","oo k","g g","custom er","t ing","v ing","res p","l ine","c reat","l l","il y","re g","d et"," if"," +","bus iness","\\n In","is h","mo st"," ","he s","ang u","prov ide","ad v","er m","u b","s k","ir st","an y","d ay","iv id","ar m","ra ct","n ce"," |","impro ve",") \\","c o","comm un","ark et","m et","c y","differe nt","iz ed","ar t","\\n The","r it","com put","for m","c k","h um","ch ar","b le","le ad","ir on","re m","sh ould","t e","al low","n ess","h at","f un","comp le","l angu","ag es","be c","s ign","u es","at ure","f ind","ri end","st ud","m ain","im ate","o ve","res ult","pl ay","redu ce","en g","w are","red i","num ber","l ar","p ol","p at","w ell","id ent","v iron","r ite","c rib","b u","h igh","the se","iv es","v es","des ign","ur n","th an","d er","an al","w ater","m arket","exam ple","w ay","st and","n g","a x","it ive"," `","i qu","s im","e qu","gor ith","te xt","res ent","man y","ur ing","-- --","\\n A","d i","s a","viron ment","ar ch","at t","p ot","t as","c reate","ou gh","f l","m aking","i ous","g ra","l ife","\\n O","al gorith","al ity","en g","f in","u c","? \","," Y","re t","be en","techn ology","prog ra","ha nd","h ip","w n","c al","wh at","ivid ual","is s","et y","langu age","our ces","cl ass","t ake","e as","r ic","v is","b ject","re f","en vironment","f irst","e g","ind ividual","pl an","per form"," ru","i en","imp act","a g","ad e","c le","re qu","d ition","_ _","c he","pt ion","app ro"," **","g reat","v ed","ex pl","g row","G enerate","m y","includ ing","ac cess","p op","m in","f ore","soc ial","in es","char act","b r","st ep","under stand","or gan","A d","dis c","p ower","l ong","he d","con c","w ard","it ed","e le","c ing","e very","c a","of ten","us er","v ie"," V","f ood","includ e","l oc","as es","ical ly","od e","ant s","inv ol","sm all","s ur","ach ine","be ing","pot ential","n o","C h","de p","at her","b oth","en s","pos s"," ed","crib e","t s","or k","The y","p ur","iv ity","wor ds","sign ific","w ere","H ow","pro m","exper ience"," K","u p","c ount","ere d","D es","f am","`` `","ak es","g l","H e","fe el","b ack","f i","pro ble","iz ation","l ing","commun ic","pl oy","a ut","f riend","hum an","sp e","e w","person al","to p"," ent","ot her","ch ang","c or","ch ange","dec is","ab ility","h ing","at ural","e ver","c ost","go od","au se"," ident","so ft","in ed","p ass","' t","at ures","b en","comp any","st art","signific ant","su mm","on d","ol d","b ers","se l","? \\","c ur","l ight","comm on",".\\ \"","custom ers","iv ing","con om","fun ction"," ve","th ree","ev en","in ing","g ener","ri es","le vel","spec ific","we bs","the n","effect ive","c ur","en se","lar ge","d ist","eff ic","su pport","g et","C reate","re ad","p ort","in f"," '","y ear","st ate","ke y","c cess",": **","a v","kn ow","ben ef"," ess","ab les","re n","o wn","The se","oc k","- t"," ide","om m","re en","c ed","ct ure","te am","r is","tas ks","d own","st ru","comput er","- b","f act","m em","et ter","\\n S","a round","wor d","b ased","be h","r ight","d el","po int","n atural","s s","e conom","m ade","in s","in st","m at","val ue","an im","se ver","\\n T","ation al","it al","z e","ot e","ill s","ter n","re ad","cont ent","on line","en d","U n","v ent","se e","end ing","m on","d r","ke ep","system s","c ul","v en","st ory","med ia","sever al","he n","ate g","cont in","de v","lear n","l a","st re","part ic","a ir","ual ly","su ccess","ou se","is s","i ed","m achine","o pt"," x","o p","pro f","oc us","ch ie","met h","n er","om p","r on","h ome","b etter","P ro","m ult","om et","incre ase","anal y","ver t","re le","b ra","in k","t em","p redi","t re","serv ice","webs ite","man age","soft ware","he re","pro t","- s","qu est","i er","kn own","or der","ph ys","ce pt","a chie","in put","poss ible","I f","ex t","f ter","e lect","meth od","b re","A n","way s","er ing","et s","j ust","st ore","develop ment","c are","o bject","typ e","F or","f ocus","gg est","on ly","cons id","ar s","ch all","det erm","s al","in s","fe atures","t ru","od y","to ol","> \\","ens ure","os s","ub lic","it em","H ere","in ation","de f","Des cribe","ion al","rou p","con f","need s","charact er","vari ous","le t","app lic","a ut","j ob","ell ig","C on","b est","f ore","am ount","ro p","bu ild","iqu e","ag ing","em ploy","re st","a ir","W hat","to get","way s","ident ify","toget her","re al","us ers","me an","as ing","A m","ed uc","algorith m","n etw","c ode","W rite","o v","- d","ou ra","How ever","ut ure","vie w","in du","product s","ect ed","er tain","; \\","A s","p r","ast e","o per"," $","av i","sel f"," <","indu st","g u","other s","E x","i an","\" \\\"","- f","n ces","f il","resp ons","ro l","c ap","be fore","ver n","comple x","l us","rib ut","at s","pos itive","o h","l o","g roup","f ound","e e","og n","s w","individual s","p ract","en c","sh are","ra ph","r ange","su n","\\ t","provid ing","ic le","de m","pl ace","a ud","j oy","m ust","el s","er y","O ne","fam ily","f uture","l ess","re nt","proble m","ess ential","ro du","i red","redu cing","is m","w arm","ra y","ab ility","str ong","al ways","res ources","benef its","str ateg","invol ves","ass ist","ere st","n A","ress ion"," [","il ities","step s","ver all","sh ow","ob al","\\n F","l and","H ere","business es","E n","pport un","me as","ret urn","d ig","h ist","y th","c ent","ab le","with out","y c","pl ain","rel ations","serv ices","- c","t est","ar th","communic ation","inter n","ne w","s it","inv est","ca us","u nt","friend s","chang es","c ri","d it","B y","Y ou","me ans","re se","o ol","t ed","ellig ence","ain s","pp ing","be l","rep resent","ha pp","s er","perform ance","o pportun","tem per","S he","f u","i x","b ot","w rit","beh avi","pro ject","W ith","iv ers","d ay","phys ical","iz ing","act iv","with in","int erest","ol ution","ward s","ff ic","qu ick","p ublic","grow th","ch o","relations hip","unt il","help s","stud ents","fi el","im es","ul ation","ib ility","el f","f ul","su b","an k","id es","sk ills","cl imate","G iven","p ar","cle ar","ir t","N ame","p resent","t ri","chall eng","re am","l ay","market ing","summ ary","ch ild","sa f","su re","s ame","m u","em ail","b on","s omet","``` \\","cur rent","am p","en ces","R e","trans port","m e","- p","a ction","E x","year s","com b","h or","anc ed","t y","l ove","g reen","pop ular","l ess","d ra","cont rol","a ff","cons um","g ame","ent al","ight s","ar get","om es","o x","ic ult","er c","go als","anc ial","t le","go vern","num bers","f ive","st and","se arch","effic ient","w al","n ame","at h","he art","d uring","re ct","over all","yth on","allow s","c ity","a ve","v ant","ater ial","w ide","m us","ific ial","h ard","T h","oo se","gl obal","a j","t er","diff icult","l ine","A l","c are","iv ed","reg ular","g r",") ,","le ment","h im","un ique","en joy","mean ing","op en"," i","ab or","are a","item s","cle an","dition ally","o id","W e","be aut","me et","ip le","state ment","ag ain","ys is","f ac","s ources","b ody","algorith ms","aud ience","w ant","l og","main tain","activ ities","mo ve","c ult","one y","t arget","\\n B","m aterial","creat ing","stru cture","at form","e xt","exper ien","val ues","e ad","oh n","health y","ro ss","int eg","rese arch","at ch","oo king","ro le","provid es","i ety","ist s","fin ancial","or ies","d ent"," er","art icle","ele ments","add ress","con n","U se","m p","eas y","ne g","col or","cal cul","Ex plain","P l","p ect","in ce","al e","ris k","cur ity","er t","fe ed","ev ent","v ers","pl es","level s","b i","st ay","pl atform","bre ak","b ack","s at","\\nO verall","educ ation","\\n C","car bon","---- ----","ap e","pre vent","add ition","st ress","r al","our ce","ru s","com e","rec ogn","Un ited","pro per","pol l","dent ify","understand ing","decis ions","i ct","d ire","behavi or"," *","\\n I","m ess","anim als","s l","w ind","b as","p ain","lead ing","er n","g er","p res","th ough","inter act","y le","do es","he ad","int elligence","ort s","bec ome","ru n","ar ing","imp lement","a ction","o ot","ter ns","prot ect","er ic","f low","em ot","cess ary","ur ate","su ggest","progra m","ph r","health care","ent ion","su st","wh y","acc urate","l u","h ig","re ach","allow ing","tra vel","requ ire","are as","de ep","H e","fe w","s elf","ou n"," #","os p","st r","min ut","decis ion","The re","an ces","qu ality","av ail","sp ace","somet hing","we b","pat terns","m ot","or ing","is f","an other","acc ount","\\n W","us s","m aj","u ation","sust ain","aut om","iqu es","iss ions","ver se","con cept","se curity","th ose","prof ess","sh ort","n ight","eng th","a pt","e x","Ad ditionally","t aking","to o","ag n","sim ple","lus ion","ien cy","as h","our s","p a","l it","S p","it ing","d on","l im","l ish","m at","av es","led ge","dition al","in c","ev ents","off er","th ing","wor king","anal ysis","achie ve","p ie","b ook","f re","mu ch","o on","t ry","es p","w aste","f ace","e ar","f ru","transport ation","ch ool","techn iques","progra mm","E arth","predi ct","ne ver","w s","u ment","imate ly","are d","partic ular","to wards","econom ic","incre asing","f ast","im ent","netw ork","cor rect","m ight","o c","bec ause","W h","a z","pl ay","result s","manage ment","pur ch","s ound","p ast","tra ining","__ __","op e","eng age","oura ge","s ense","f ree","pre f","e es","count ries","ne y","an ies","a fter","m ind","ex c","O nce"," ","comple te","im m"," est","g enerate","ver b","D e","' m","tool s","redi ents","maj or","ent ly","cont ribut","le ep","point s","dit ions","fact ors","e l","ne xt","i um","ou d","c ru","re as","ri ate","I nd","prom ot","hist ory","j our","d ue","C on","ve get","en cy","Am eric","f ra","differe nce","o ard","le x","equ ation","irt ual","c up","fore st","neg ative","se con","on es","n ature","us es","a h","p or","se c","ord ing","l ast","S ome","iss ues","sc ient","pr int","St ates","o ver","sat isf","dev ices","dis e","temper ature","feed back","ne cessary","em issions","m b","l ow","f or","t al","challeng es","ar ray","s ide","eng ine","b oo","at a","bel ie","- m","mult iple","s ing","govern ment","am es","if ied","minut es","success ful","m oney","quick ly","b ir","typ ically","p ost","pre p","know ledge","pp ed","a ctions","method s","opt im","\\n P","out put","fiel d","t able","b al","col l","charact ers","v olution","or ds","il ar","ific ation","an e","c ell","m il","W hat","s qu","l ives","A r","phr ase","n ut","dig ital","intern et","l ass","u ra","omm end","t reat","appro p","res h","ur ther","O ne","vis ual","ate gor","appro ach","c ertain","sh o","v al","tas k","i res","approp riate","v ie","design ed","p ose","** :","f ort","| \\","applic ations","p ay","n ow","he at","indust ry","p re","effective ly","pop ulation","opportun ities","< /","T o","up d","includ es","E ng","typ es","up on","consid er","le t","g en","og raph","pl ace","t imes","ar g","C omp","G o","re ce","child ren","tra ck","some one","w ord","you ng","con ditions","tra ditional","model s","I dentify","c amp","m akes","ist ic","ar r","c ard","ut ions","l t","o ld","ide as","e y","t ree","iss ue","h arm","avail able","c r","power ful","n ov","mo vie","we ather","sk y","quest ions","e et","act ivity","bra nd","is hed","analy ze","S h","en h","av or","be g","s chool","i ate","eas ier","inf lu","n on","stud y","l ook","sol ution","le g","con st","H ow","comp et"]}}
diff -ruN marc_original/third_party/torchtune/tests/assets/tiny_bpe_vocab.json marc/third_party/torchtune/tests/assets/tiny_bpe_vocab.json
--- marc_original/third_party/torchtune/tests/assets/tiny_bpe_vocab.json	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/tiny_bpe_vocab.json	2025-02-20 17:49:29.574024285 -0500
@@ -0,0 +1 @@
+{"0":15,"1":16,"2":17,"3":18,"4":19,"5":20,"6":21,"7":22,"8":23,"9":24,"!":0,"\"":1,"#":2,"$":3,"%":4,"&":5,"'":6,"(":7,")":8,"*":9,"+":10,",":11,"-":12,".":13,"/":14,":":25,";":26,"<":27,"=":28,">":29,"?":30,"@":31,"A":32,"B":33,"C":34,"D":35,"E":36,"F":37,"G":38,"H":39,"I":40,"J":41,"K":42,"L":43,"M":44,"N":45,"O":46,"P":47,"Q":48,"R":49,"S":50,"T":51,"U":52,"V":53,"W":54,"X":55,"Y":56,"Z":57,"[":58,"\\":59,"]":60,"^":61,"_":62,"`":63,"a":64,"b":65,"c":66,"d":67,"e":68,"f":69,"g":70,"h":71,"i":72,"j":73,"k":74,"l":75,"m":76,"n":77,"o":78,"p":79,"q":80,"r":81,"s":82,"t":83,"u":84,"v":85,"w":86,"x":87,"y":88,"z":89,"{":90,"|":91,"}":92,"~":93,"":94,"":95,"":96,"t":97,"a":98,"in":99,"he":100,"re":101,"on":102,"the":103,"s":104,"er":105,"at":106,"c":107,"":108,"en":109,"o":110,"\"":111,"nd":112,"es":113,"ing":114,"":115,"it":116,"p":117,"or":118,"ou":119,"and":120,"w":121,"is":122,"f":123,"an":124,"ion":125,"al":126,"b":127,"to":128,"m":129,"in":130,"of":131,"le":132,"ct":133,"ar":134,"ut":135,"d":136,"st":137,"ed":138,"":139,"ic":140,"\":":141,",":142,"ro":143,"ent":144,"\\n":145,"e":146,"put":147,"om":148,"re":149,"as":150,"ve":151,"h":152,"th":153,"\",":154,"l":155,"is":156,"et":157,"ce":158,"n":159,".\\":160,"im":161,"il":162,"g":163,"u":164,"ction":165,"ru":166,"ation":167,"ol":168,"ch":169,"T":170,"for":171,"out":172,"ra":173,"ow":174,"id":175,"ly":176,"st":177,"be":178,"y":179,"pro":180,"ig":181,"se":182,"ate":183,"that":184,"ith":185,"ir":186,"ur":187,"ot":188,"or":189,"on":190,"you":191,"ers":192,"stru":193,"an":194,"if":195,"ul":196,"struction":197,"{":198,"}":199,"can":200,"input":201,"output":202,"instruction":203,"{":204,"},":205,"\"":206,"he":207,"con":208,"it":209,"ay":210,"ess":211,"with":212,"ver":213,"el":214,"as":215,"am":216,"A":217,"ge":218,"su":219,"iv":220,".\",":221,"com":222,"I":223,"ment":224,"ak":225,"al":226,"\\\"":227,".\"":228,"ive":229,"are":230,"ab":231,"ad":232,"mo":233,"ex":234,"v":235,"S":236,"res":237,"pp":238,"qu":239,"de":240,"wh":241,"ity":242,"en":243,"The":244,"her":245,"ld":246,"ri":247,"ter":248,"ant":249,"C":250,"ist":251,"\"\",":252,"um":253,"us":254,"ne":255,"ain":256,"th":257,"ect":258,"le":259,"op":260,"em":261,"ies":262,"ch":263,"im":264,"du":265,"od":266,"ort":267,"nt":268,"est":269,"igh":270,"ere":271,"ha":272,"us":273,"ure":274,"ial":275,"oc":276,"wor":277,"their":278,"ac":279,"ence":280,"iz":281,"your":282,"os":283,"imp":284,"ud":285,"by":286,"se":287,"ine":288,"ould":289,"low":290,"ill":291,"age":292,"rom":293,"sp":294,"P":295,"sh":296,"ust":297,"The":298,"un":299,"'s":300,"inc":301,"ide":302,"pl":303,"ight":304,"og":305,"pl":306,"pt":307,"are":308,"te":309,"int":310,"\\":311,"his":312,"r":313,"ake":314,"per":315,"orm":316,"ag":317,"ff":318,"E":319,"art":320,"k":321,"end":322,"M":323,"we":324,"B":325,"ad":326,"cess":327,"rou":328,"ical":329,"all":330,"able":331,"from":332,"and":333,"H":334,"ab":335,"act":336,"comp":337,"ome":338,"ach":339,"This":340,"have":341,"form":342,"\\\"":343,"ast":344,"at":345,"W":346,"res":347,"dat":348,":\\":349,"ther":350,"ions":351,"ore":352,"(":353,"cont":354,"our":355,"ep":356,"F":357,"ac":358,"ance":359,"R":360,"gh":361,"me":362,"ces":363,"was":364,"ind":365,"vel":366,"ations":367,"hel":368,"more":369,"ult":370,"D":371,"reat":372,"ign":373,"help":374,"ime":375,"ard":376,"cl":377,"app":378,"ans":379,"ie":380,"data":381,"ich":382,"ang":383,"ous":384,"ell":385,"ks":386,"ase":387,"ice":388,"ip":389,"ite":390,"such":391,"fe":392,"whe":393,"ib":394,"other":395,"this":396,"ass":397,"ual":398,"ile":399,"ne":400,"red":401,"has":402,"oo":403,"ress":404,"ific":405,"ning":406,"=":407,"up":408,"man":409,"ar":410,"ong":411,"ec":412,"tra":413,"av":414,"which":415,"go":416,"prov":417,"dis":418,"**":419,"so":420,"G":421,"one":422,"em":423,"not":424,"ue":425,"O":426,"j":427,"ace":428,"they":429,"ame":430,"qu":431,"L":432,"iff":433,"fol":434,"ary":435,"ated":436,"ustom":437,"ition":438,"its":439,"sy":440,"ke":441,"ack":442,"ry":443,"--":444,"time":445,"des":446,"new":447,"ents":448,"ount":449,"follow":450,"also":451,"comm":452,"out":453,"eff":454,"diff":455,"iven":456,"ap":457,"sent":458,"\\u":459,"so":460,"produ":461,"use":462,"sc":463,"-":464,"un":465,"lud":466,"It":467,"ener":468,"king":469,"ev":470,"about":471,"them":472,"U":473,"custom":474,"ro":475,"includ":476,"les":477,"etw":478,"stem":479,"xt":480,"into":481,"per":482,"In":483,"N":484,"will":485,"lear":486,"ber":487,"all":488,"pe":489,"ds":490,"tw":491,"aking":492,"ark":493,"ful":494,"make":495,"chn":496,"erv":497,"ost":498,"rough":499,"one":500,"inter":501,"ities":502,"ail":503,"ike":504,"ree":505,"ple":506,"alth":507,"used":508,"ors":509,"over":510,"ility":511,"ments":512,"ange":513,"way":514,"ory":515,"col":516,"pr":517,"could":518,"num":519,"reate":520,"int":521,"redu":522,"erson":523,"rec":524,"her":525,"need":526,"ms":527,"ater":528,"oy":529,"system":530,"inform":531,"two":532,"techn":533,"sentence":534,"ience":535,"ize":536,"get":537,"differe":538,"ood":539,"rib":540,"but":541,"following":542,"ased":543,"olog":544,"erg":545,"led":546,"ures":547,"In":548,"ear":549,"ph":550,"own":551,"pre":552,"would":553,"using":554,"cons":555,"work":556,"mod":557,"ating":558,"ia":559,"ire":560,"pos":561,"ient":562,"ob":563,"ject":564,"inv":565,"ons":566,"do":567,"ular":568,"dec":569,"health":570,"impro":571,"any":572,"through":573,"yp":574,"row":575,"velop":576,"process":577,"tr":578,"lic":579,"very":580,"als":581,"ify":582,"``":583,"ari":584,"str":585,"import":586,"like":587,"product":588,"some":589,"ph":590,"ential":591,"am":592,"ates":593,"acc":594,"ens":595,"ns":596,"sm":597,"ind":598,"een":599,"exper":600,"lect":601,"val":602,"rel":603,"its":604,"information":605,"ings":606,"J":607,"ople":608,"iness":609,"given":610,"mm":611,"ices":612,"part":613,"ild":614,"ys":615,"our":616,"nder":617,"person":618,"ally":619,"ke":620,"etween":621,"ft":622,"oth":623,"spec":624,"between":625,"ergy":626,"AI":627,"who":628,"may":629,"ef":630,"ative":631,"ise":632,"list":633,"kn":634,"add":635,",\\":636,"ord":637,"ics":638,"people":639,"St":640,"his":641,"exp":642,"ible":643,"there":644,"serv":645,"incre":646,"develop":647,"ound":648,"ower":649,"trans":650,"bs":651,"energy":652,"off":653,"bus":654,"while":655,"ose":656,"act":657,"exam":658,"learning":659,"ctions":660,"con":661,"gor":662,"gan":663,"ution":664,"round":665,"pport":666,"how":667,"bl":668,"med":669,"anc":670,"typ":671,"ra":672,"car":673,"ife":674,"world":675,"vari":676,"rep":677,"au":678,"soc":679,"provid":680,"set":681,"ten":682,"sol":683,"each":684,"when":685,"effect":686,"po":687,"she":688,"ick":689,"where":690,"model":691,"important":692,"under":693,"prog":694,"enerate":695,"ural":696,"tain":697,"ass":698,"ology":699,"had":700,"ook":701,"gg":702,"customer":703,"ting":704,"ving":705,"resp":706,"line":707,"creat":708,"ll":709,"ily":710,"reg":711,"det":712,"if":713,"+":714,"business":715,"\\nIn":716,"ish":717,"most":718,"":719,"hes":720,"angu":721,"provide":722,"adv":723,"erm":724,"ub":725,"sk":726,"irst":727,"any":728,"day":729,"ivid":730,"arm":731,"ract":732,"nce":733,"|":734,"improve":735,")\\":736,"co":737,"commun":738,"arket":739,"met":740,"cy":741,"different":742,"ized":743,"art":744,"\\nThe":745,"rit":746,"comput":747,"form":748,"ck":749,"hum":750,"char":751,"ble":752,"lead":753,"iron":754,"rem":755,"should":756,"te":757,"allow":758,"ness":759,"hat":760,"fun":761,"comple":762,"langu":763,"ages":764,"bec":765,"sign":766,"ues":767,"ature":768,"find":769,"riend":770,"stud":771,"main":772,"imate":773,"ove":774,"result":775,"play":776,"reduce":777,"eng":778,"ware":779,"redi":780,"number":781,"lar":782,"pol":783,"pat":784,"well":785,"ident":786,"viron":787,"rite":788,"crib":789,"bu":790,"high":791,"these":792,"ives":793,"ves":794,"design":795,"urn":796,"than":797,"der":798,"anal":799,"water":800,"market":801,"example":802,"way":803,"stand":804,"ng":805,"ax":806,"itive":807,"`":808,"iqu":809,"sim":810,"equ":811,"gorith":812,"text":813,"resent":814,"many":815,"uring":816,"----":817,"\\nA":818,"di":819,"sa":820,"vironment":821,"arch":822,"att":823,"pot":824,"tas":825,"create":826,"ough":827,"fl":828,"making":829,"ious":830,"gra":831,"life":832,"\\nO":833,"algorith":834,"ality":835,"eng":836,"fin":837,"uc":838,"?\",":839,"Y":840,"ret":841,"been":842,"technology":843,"progra":844,"hand":845,"hip":846,"wn":847,"cal":848,"what":849,"ividual":850,"iss":851,"ety":852,"language":853,"ources":854,"class":855,"take":856,"eas":857,"ric":858,"vis":859,"bject":860,"ref":861,"environment":862,"first":863,"eg":864,"individual":865,"plan":866,"perform":867,"ru":868,"ien":869,"impact":870,"ag":871,"ade":872,"cle":873,"requ":874,"dition":875,"__":876,"che":877,"ption":878,"appro":879,"**":880,"great":881,"ved":882,"expl":883,"grow":884,"Generate":885,"my":886,"including":887,"access":888,"pop":889,"min":890,"fore":891,"social":892,"ines":893,"charact":894,"br":895,"step":896,"understand":897,"organ":898,"Ad":899,"disc":900,"power":901,"long":902,"hed":903,"conc":904,"ward":905,"ited":906,"ele":907,"cing":908,"every":909,"ca":910,"often":911,"user":912,"vie":913,"V":914,"food":915,"include":916,"loc":917,"ases":918,"ically":919,"ode":920,"ants":921,"invol":922,"small":923,"sur":924,"achine":925,"being":926,"potential":927,"no":928,"Ch":929,"dep":930,"ather":931,"both":932,"ens":933,"poss":934,"ed":935,"cribe":936,"ts":937,"ork":938,"They":939,"pur":940,"ivity":941,"words":942,"signific":943,"were":944,"How":945,"prom":946,"experience":947,"K":948,"up":949,"count":950,"ered":951,"Des":952,"fam":953,"```":954,"akes":955,"gl":956,"He":957,"feel":958,"back":959,"fi":960,"proble":961,"ization":962,"ling":963,"communic":964,"ploy":965,"aut":966,"friend":967,"human":968,"spe":969,"ew":970,"personal":971,"top":972,"ent":973,"other":974,"chang":975,"cor":976,"change":977,"decis":978,"ability":979,"hing":980,"atural":981,"ever":982,"cost":983,"good":984,"ause":985,"ident":986,"soft":987,"ined":988,"pass":989,"'t":990,"atures":991,"ben":992,"company":993,"start":994,"significant":995,"summ":996,"ond":997,"old":998,"bers":999,"sel":1000,"?\\":1001,"cur":1002,"light":1003,"common":1004,".\\\"":1005,"customers":1006,"iving":1007,"conom":1008,"function":1009,"ve":1010,"three":1011,"even":1012,"ining":1013,"gener":1014,"ries":1015,"level":1016,"specific":1017,"webs":1018,"then":1019,"effective":1020,"cur":1021,"ense":1022,"large":1023,"dist":1024,"effic":1025,"support":1026,"get":1027,"Create":1028,"read":1029,"port":1030,"inf":1031,"'":1032,"year":1033,"state":1034,"key":1035,"ccess":1036,":**":1037,"av":1038,"know":1039,"benef":1040,"ess":1041,"ables":1042,"ren":1043,"own":1044,"These":1045,"ock":1046,"-t":1047,"ide":1048,"omm":1049,"reen":1050,"ced":1051,"cture":1052,"team":1053,"ris":1054,"tasks":1055,"down":1056,"stru":1057,"computer":1058,"-b":1059,"fact":1060,"mem":1061,"etter":1062,"\\nS":1063,"around":1064,"word":1065,"based":1066,"beh":1067,"right":1068,"del":1069,"point":1070,"natural":1071,"ss":1072,"econom":1073,"made":1074,"ins":1075,"inst":1076,"mat":1077,"value":1078,"anim":1079,"sever":1080,"\\nT":1081,"ational":1082,"ital":1083,"ze":1084,"ote":1085,"ills":1086,"tern":1087,"read":1088,"content":1089,"online":1090,"end":1091,"Un":1092,"vent":1093,"see":1094,"ending":1095,"mon":1096,"dr":1097,"keep":1098,"systems":1099,"cul":1100,"ven":1101,"story":1102,"media":1103,"several":1104,"hen":1105,"ateg":1106,"contin":1107,"dev":1108,"learn":1109,"la":1110,"stre":1111,"partic":1112,"air":1113,"ually":1114,"success":1115,"ouse":1116,"iss":1117,"ied":1118,"machine":1119,"opt":1120,"x":1121,"op":1122,"prof":1123,"ocus":1124,"chie":1125,"meth":1126,"ner":1127,"omp":1128,"ron":1129,"home":1130,"better":1131,"Pro":1132,"mult":1133,"omet":1134,"increase":1135,"analy":1136,"vert":1137,"rele":1138,"bra":1139,"ink":1140,"tem":1141,"predi":1142,"tre":1143,"service":1144,"website":1145,"manage":1146,"software":1147,"here":1148,"prot":1149,"-s":1150,"quest":1151,"ier":1152,"known":1153,"order":1154,"phys":1155,"cept":1156,"achie":1157,"input":1158,"possible":1159,"If":1160,"ext":1161,"fter":1162,"elect":1163,"method":1164,"bre":1165,"An":1166,"ways":1167,"ering":1168,"ets":1169,"just":1170,"store":1171,"development":1172,"care":1173,"object":1174,"type":1175,"For":1176,"focus":1177,"ggest":1178,"only":1179,"consid":1180,"ars":1181,"chall":1182,"determ":1183,"sal":1184,"ins":1185,"features":1186,"tru":1187,"ody":1188,"tool":1189,">\\":1190,"ensure":1191,"oss":1192,"ublic":1193,"item":1194,"Here":1195,"ination":1196,"def":1197,"Describe":1198,"ional":1199,"roup":1200,"conf":1201,"needs":1202,"character":1203,"various":1204,"let":1205,"applic":1206,"aut":1207,"job":1208,"ellig":1209,"Con":1210,"best":1211,"fore":1212,"amount":1213,"rop":1214,"build":1215,"ique":1216,"aging":1217,"employ":1218,"rest":1219,"air":1220,"What":1221,"toget":1222,"ways":1223,"identify":1224,"together":1225,"real":1226,"users":1227,"mean":1228,"asing":1229,"Am":1230,"educ":1231,"algorithm":1232,"netw":1233,"code":1234,"Write":1235,"ov":1236,"-d":1237,"oura":1238,"However":1239,"uture":1240,"view":1241,"indu":1242,"products":1243,"ected":1244,"ertain":1245,";\\":1246,"As":1247,"pr":1248,"aste":1249,"oper":1250,"$":1251,"avi":1252,"self":1253,"<":1254,"indust":1255,"gu":1256,"others":1257,"Ex":1258,"ian":1259,"\"\\\"":1260,"-f":1261,"nces":1262,"fil":1263,"respons":1264,"rol":1265,"cap":1266,"before":1267,"vern":1268,"complex":1269,"lus":1270,"ribut":1271,"ats":1272,"positive":1273,"oh":1274,"lo":1275,"group":1276,"found":1277,"ee":1278,"ogn":1279,"sw":1280,"individuals":1281,"pract":1282,"enc":1283,"share":1284,"raph":1285,"range":1286,"sun":1287,"\\t":1288,"providing":1289,"icle":1290,"dem":1291,"place":1292,"aud":1293,"joy":1294,"must":1295,"els":1296,"ery":1297,"One":1298,"family":1299,"future":1300,"less":1301,"rent":1302,"problem":1303,"essential":1304,"rodu":1305,"ired":1306,"reducing":1307,"ism":1308,"warm":1309,"ray":1310,"ability":1311,"strong":1312,"always":1313,"resources":1314,"benefits":1315,"strateg":1316,"involves":1317,"assist":1318,"erest":1319,"nA":1320,"ression":1321,"[":1322,"ilities":1323,"steps":1324,"verall":1325,"show":1326,"obal":1327,"\\nF":1328,"land":1329,"Here":1330,"businesses":1331,"En":1332,"pportun":1333,"meas":1334,"return":1335,"dig":1336,"hist":1337,"yth":1338,"cent":1339,"able":1340,"without":1341,"yc":1342,"plain":1343,"relations":1344,"services":1345,"-c":1346,"test":1347,"arth":1348,"communication":1349,"intern":1350,"new":1351,"sit":1352,"invest":1353,"caus":1354,"unt":1355,"friends":1356,"changes":1357,"cri":1358,"dit":1359,"By":1360,"You":1361,"means":1362,"rese":1363,"ool":1364,"ted":1365,"elligence":1366,"ains":1367,"pping":1368,"bel":1369,"represent":1370,"happ":1371,"ser":1372,"performance":1373,"opportun":1374,"temper":1375,"She":1376,"fu":1377,"ix":1378,"bot":1379,"writ":1380,"behavi":1381,"project":1382,"With":1383,"ivers":1384,"day":1385,"physical":1386,"izing":1387,"activ":1388,"within":1389,"interest":1390,"olution":1391,"wards":1392,"ffic":1393,"quick":1394,"public":1395,"growth":1396,"cho":1397,"relationship":1398,"until":1399,"helps":1400,"students":1401,"fiel":1402,"imes":1403,"ulation":1404,"ibility":1405,"elf":1406,"ful":1407,"sub":1408,"ank":1409,"ides":1410,"skills":1411,"climate":1412,"Given":1413,"par":1414,"clear":1415,"irt":1416,"Name":1417,"present":1418,"tri":1419,"challeng":1420,"ream":1421,"lay":1422,"marketing":1423,"summary":1424,"child":1425,"saf":1426,"sure":1427,"same":1428,"mu":1429,"email":1430,"bon":1431,"somet":1432,"```\\":1433,"current":1434,"amp":1435,"ences":1436,"Re":1437,"transport":1438,"me":1439,"-p":1440,"action":1441,"Ex":1442,"years":1443,"comb":1444,"hor":1445,"anced":1446,"ty":1447,"love":1448,"green":1449,"popular":1450,"less":1451,"dra":1452,"control":1453,"aff":1454,"consum":1455,"game":1456,"ental":1457,"ights":1458,"arget":1459,"omes":1460,"ox":1461,"icult":1462,"erc":1463,"goals":1464,"ancial":1465,"tle":1466,"govern":1467,"numbers":1468,"five":1469,"stand":1470,"search":1471,"efficient":1472,"wal":1473,"name":1474,"ath":1475,"heart":1476,"during":1477,"rect":1478,"overall":1479,"ython":1480,"allows":1481,"city":1482,"ave":1483,"vant":1484,"aterial":1485,"wide":1486,"mus":1487,"ificial":1488,"hard":1489,"Th":1490,"oose":1491,"global":1492,"aj":1493,"ter":1494,"difficult":1495,"line":1496,"Al":1497,"care":1498,"ived":1499,"regular":1500,"gr":1501,"),":1502,"lement":1503,"him":1504,"unique":1505,"enjoy":1506,"meaning":1507,"open":1508,"i":1509,"abor":1510,"area":1511,"items":1512,"clean":1513,"ditionally":1514,"oid":1515,"We":1516,"beaut":1517,"meet":1518,"iple":1519,"statement":1520,"again":1521,"ysis":1522,"fac":1523,"sources":1524,"body":1525,"algorithms":1526,"audience":1527,"want":1528,"log":1529,"maintain":1530,"activities":1531,"move":1532,"cult":1533,"oney":1534,"target":1535,"\\nB":1536,"material":1537,"creating":1538,"structure":1539,"atform":1540,"ext":1541,"experien":1542,"values":1543,"ead":1544,"ohn":1545,"healthy":1546,"ross":1547,"integ":1548,"research":1549,"atch":1550,"ooking":1551,"role":1552,"provides":1553,"iety":1554,"ists":1555,"financial":1556,"ories":1557,"dent":1558,"er":1559,"article":1560,"elements":1561,"address":1562,"conn":1563,"Use":1564,"mp":1565,"easy":1566,"neg":1567,"color":1568,"calcul":1569,"Explain":1570,"Pl":1571,"pect":1572,"ince":1573,"ale":1574,"risk":1575,"curity":1576,"ert":1577,"feed":1578,"event":1579,"vers":1580,"ples":1581,"levels":1582,"bi":1583,"stay":1584,"platform":1585,"break":1586,"back":1587,"sat":1588,"\\nOverall":1589,"education":1590,"\\nC":1591,"carbon":1592,"--------":1593,"ape":1594,"prevent":1595,"addition":1596,"stress":1597,"ral":1598,"ource":1599,"rus":1600,"come":1601,"recogn":1602,"United":1603,"proper":1604,"poll":1605,"dentify":1606,"understanding":1607,"decisions":1608,"ict":1609,"dire":1610,"behavior":1611,"*":1612,"\\nI":1613,"mess":1614,"animals":1615,"sl":1616,"wind":1617,"bas":1618,"pain":1619,"leading":1620,"ern":1621,"ger":1622,"pres":1623,"though":1624,"interact":1625,"yle":1626,"does":1627,"head":1628,"intelligence":1629,"orts":1630,"become":1631,"run":1632,"aring":1633,"implement":1634,"action":1635,"oot":1636,"terns":1637,"protect":1638,"eric":1639,"flow":1640,"emot":1641,"cessary":1642,"urate":1643,"suggest":1644,"program":1645,"phr":1646,"healthcare":1647,"ention":1648,"sust":1649,"why":1650,"accurate":1651,"lu":1652,"hig":1653,"reach":1654,"allowing":1655,"travel":1656,"require":1657,"areas":1658,"deep":1659,"He":1660,"few":1661,"self":1662,"oun":1663,"#":1664,"osp":1665,"str":1666,"minut":1667,"decision":1668,"There":1669,"ances":1670,"quality":1671,"avail":1672,"space":1673,"something":1674,"web":1675,"patterns":1676,"mot":1677,"oring":1678,"isf":1679,"another":1680,"account":1681,"\\nW":1682,"uss":1683,"maj":1684,"uation":1685,"sustain":1686,"autom":1687,"iques":1688,"issions":1689,"verse":1690,"concept":1691,"security":1692,"those":1693,"profess":1694,"short":1695,"night":1696,"ength":1697,"apt":1698,"ex":1699,"Additionally":1700,"taking":1701,"too":1702,"agn":1703,"simple":1704,"lusion":1705,"iency":1706,"ash":1707,"ours":1708,"pa":1709,"lit":1710,"Sp":1711,"iting":1712,"don":1713,"lim":1714,"lish":1715,"mat":1716,"aves":1717,"ledge":1718,"ditional":1719,"inc":1720,"events":1721,"offer":1722,"thing":1723,"working":1724,"analysis":1725,"achieve":1726,"pie":1727,"book":1728,"fre":1729,"much":1730,"oon":1731,"try":1732,"esp":1733,"waste":1734,"face":1735,"ear":1736,"fru":1737,"transportation":1738,"chool":1739,"techniques":1740,"programm":1741,"Earth":1742,"predict":1743,"never":1744,"ws":1745,"ument":1746,"imately":1747,"ared":1748,"particular":1749,"towards":1750,"economic":1751,"increasing":1752,"fast":1753,"iment":1754,"network":1755,"correct":1756,"might":1757,"oc":1758,"because":1759,"Wh":1760,"az":1761,"play":1762,"results":1763,"management":1764,"purch":1765,"sound":1766,"past":1767,"training":1768,"____":1769,"ope":1770,"engage":1771,"ourage":1772,"sense":1773,"free":1774,"pref":1775,"ees":1776,"countries":1777,"ney":1778,"anies":1779,"after":1780,"mind":1781,"exc":1782,"Once":1783,"":1784,"complete":1785,"imm":1786,"est":1787,"generate":1788,"verb":1789,"De":1790,"'m":1791,"tools":1792,"redients":1793,"major":1794,"ently":1795,"contribut":1796,"leep":1797,"points":1798,"ditions":1799,"factors":1800,"el":1801,"next":1802,"ium":1803,"oud":1804,"cru":1805,"reas":1806,"riate":1807,"Ind":1808,"promot":1809,"history":1810,"jour":1811,"due":1812,"Con":1813,"veget":1814,"ency":1815,"Americ":1816,"fra":1817,"difference":1818,"oard":1819,"lex":1820,"equation":1821,"irtual":1822,"cup":1823,"forest":1824,"negative":1825,"secon":1826,"ones":1827,"nature":1828,"uses":1829,"ah":1830,"por":1831,"sec":1832,"ording":1833,"last":1834,"Some":1835,"issues":1836,"scient":1837,"print":1838,"States":1839,"over":1840,"satisf":1841,"devices":1842,"dise":1843,"temperature":1844,"feedback":1845,"necessary":1846,"emissions":1847,"mb":1848,"low":1849,"for":1850,"tal":1851,"challenges":1852,"array":1853,"side":1854,"engine":1855,"boo":1856,"ata":1857,"belie":1858,"-m":1859,"multiple":1860,"sing":1861,"government":1862,"ames":1863,"ified":1864,"minutes":1865,"successful":1866,"money":1867,"quickly":1868,"bir":1869,"typically":1870,"post":1871,"prep":1872,"knowledge":1873,"pped":1874,"actions":1875,"methods":1876,"optim":1877,"\\nP":1878,"output":1879,"field":1880,"table":1881,"bal":1882,"coll":1883,"characters":1884,"volution":1885,"ords":1886,"ilar":1887,"ification":1888,"ane":1889,"cell":1890,"mil":1891,"What":1892,"squ":1893,"lives":1894,"Ar":1895,"phrase":1896,"nut":1897,"digital":1898,"internet":1899,"lass":1900,"ura":1901,"ommend":1902,"treat":1903,"approp":1904,"resh":1905,"urther":1906,"One":1907,"visual":1908,"ategor":1909,"approach":1910,"certain":1911,"sho":1912,"val":1913,"task":1914,"ires":1915,"appropriate":1916,"vie":1917,"designed":1918,"pose":1919,"**:":1920,"fort":1921,"|\\":1922,"applications":1923,"pay":1924,"now":1925,"heat":1926,"industry":1927,"pre":1928,"effectively":1929,"population":1930,"opportunities":1931,"</":1932,"To":1933,"upd":1934,"includes":1935,"Eng":1936,"types":1937,"upon":1938,"consider":1939,"let":1940,"gen":1941,"ograph":1942,"place":1943,"times":1944,"arg":1945,"Comp":1946,"Go":1947,"rece":1948,"children":1949,"track":1950,"someone":1951,"word":1952,"young":1953,"conditions":1954,"traditional":1955,"models":1956,"Identify":1957,"camp":1958,"makes":1959,"istic":1960,"arr":1961,"card":1962,"utions":1963,"lt":1964,"old":1965,"ideas":1966,"ey":1967,"tree":1968,"issue":1969,"harm":1970,"available":1971,"cr":1972,"powerful":1973,"nov":1974,"movie":1975,"weather":1976,"sky":1977,"questions":1978,"eet":1979,"activity":1980,"brand":1981,"ished":1982,"analyze":1983,"Sh":1984,"enh":1985,"avor":1986,"beg":1987,"school":1988,"iate":1989,"easier":1990,"influ":1991,"non":1992,"study":1993,"look":1994,"solution":1995,"leg":1996,"const":1997,"How":1998,"compet":1999}
Binary files marc_original/third_party/torchtune/tests/assets/tiny_fair_checkpoint.pt and marc/third_party/torchtune/tests/assets/tiny_fair_checkpoint.pt differ
Binary files marc_original/third_party/torchtune/tests/assets/tiny_state_dict_with_one_key.pt and marc/third_party/torchtune/tests/assets/tiny_state_dict_with_one_key.pt differ
diff -ruN marc_original/third_party/torchtune/tests/assets/valid_dummy_config.yaml marc/third_party/torchtune/tests/assets/valid_dummy_config.yaml
--- marc_original/third_party/torchtune/tests/assets/valid_dummy_config.yaml	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/assets/valid_dummy_config.yaml	2025-02-20 17:49:29.582024298 -0500
@@ -0,0 +1,3 @@
+test:
+  _component_: torchtune.training.get_dtype
+  dtype: fp32
diff -ruN marc_original/third_party/torchtune/tests/cache_artifacts.sh marc/third_party/torchtune/tests/cache_artifacts.sh
--- marc_original/third_party/torchtune/tests/cache_artifacts.sh	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/cache_artifacts.sh	2025-02-20 17:49:29.586024305 -0500
@@ -0,0 +1,103 @@
+#!/bin/bash
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+# This script will handle caching of all artifacts needed for a given test run.
+# ./cache_artifacts.sh alone is a no-op
+# ./cache_artifacts --run-recipe-tests will fetch tokenizer and small model checkpoints.
+# ./cache_artifacts --run-regression-tests will fetch tokenizer and 7B model checkpoint.
+# ./cache_artifacts --run-recipe-tests --run-regression-tests will fetch all of the above.
+# In all cases, if the files already exist locally they will not be downloaded from S3.
+
+SMALL_MODEL_URLS=(
+    "https://ossci-datasets.s3.amazonaws.com/torchtune/small-ckpt-tune-03082024.pt"
+    "https://ossci-datasets.s3.amazonaws.com/torchtune/small-ckpt-meta-03082024.pt"
+    "https://ossci-datasets.s3.amazonaws.com/torchtune/small-ckpt-hf-03082024.pt"
+    "https://ossci-datasets.s3.amazonaws.com/torchtune/small-ckpt-tune-llama3-05052024.pt"
+    "https://ossci-datasets.s3.amazonaws.com/torchtune/small-ckpt-hf-reward-07122024.pt"
+)
+FULL_MODEL_URL=("s3://pytorch-multimodal/llama2-7b-torchtune.pt")
+TOKENIZER_URLS=(
+    "https://ossci-datasets.s3.amazonaws.com/torchtune/tokenizer.model"
+    "https://ossci-datasets.s3.amazonaws.com/torchtune/tokenizer_llama3.model"
+)
+
+LOCAL_DIR="/tmp/test-artifacts"
+S3_URLS=()
+S3_OPTS=()
+
+# Iterate over command-line args
+while [[ $# -gt 0 ]]; do
+    arg="$1"
+    case $arg in
+        "--run-recipe-tests")
+            # Add URLs for small models
+            for url in "${SMALL_MODEL_URLS[@]}"; do
+                S3_URLS+=( "$url" )
+            done
+            shift # Next argument
+            ;;
+        "--run-regression-tests")
+            # Add URL for large model
+            S3_URLS+=(
+                $FULL_MODEL_URL
+            )
+            shift # Next argument
+            ;;
+        "--silence-s3-logs")
+            # Disable S3 progress bar
+            S3_OPTS+=(
+                "--no-progress"
+            )
+            shift # Next argument
+            ;;
+    esac
+done
+
+# If either recipe or regression tests are running,
+# fetch the tokenizer
+if ! [ -z "$S3_URLS" ]; then
+    for url in "${TOKENIZER_URLS[@]}"; do
+        S3_URLS+=( "$url" )
+    done
+fi
+
+# Sanity check debug log
+echo "Expected artifacts for test run are:"
+for url in "${S3_URLS[@]}"; do
+    echo "$(basename "$url")"
+done
+
+# Download relevant files from S3 to local
+mkdir -p $LOCAL_DIR
+for S3_URL in "${S3_URLS[@]}"; do
+    FILE_NAME=$(basename "$S3_URL")
+
+    # Check if file already exists locally
+    if [ -e "$LOCAL_DIR/$FILE_NAME" ]; then
+        echo "File already exists locally: $LOCAL_DIR/$FILE_NAME"
+    else
+        # S3 files: use s3 cp
+        if [[ $S3_URL == s3* ]]; then
+            # Download file from S3, optionally silencing progress bar
+            cp_cmd="aws s3 cp ${S3_URL} ${LOCAL_DIR}"
+            if ! [ -z "$S3_OPTS" ]; then
+                cp_cmd="${cp_cmd} ${S3_OPTS}"
+            fi
+        # For https: download with curl
+        else
+            cp_cmd="curl --output ${LOCAL_DIR}/${FILE_NAME} ${S3_URL}"
+        fi
+        bash -c "${cp_cmd}"
+        # Check if download was successful
+        if [ $? -eq 0 ]; then
+            echo "File downloaded successfully: $LOCAL_DIR/$FILE_NAME"
+        else
+            echo "Failed to download file from S3: $S3_URL"
+            exit 1  # Failure exit code
+        fi
+    fi
+done
diff -ruN marc_original/third_party/torchtune/tests/common.py marc/third_party/torchtune/tests/common.py
--- marc_original/third_party/torchtune/tests/common.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/common.py	2025-02-20 17:49:29.590024312 -0500
@@ -0,0 +1,10 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from pathlib import Path
+
+TUNE_PATH = "torchtune/_cli/tune.py"
+
+ASSETS = Path(__file__).parent / "assets"
diff -ruN marc_original/third_party/torchtune/tests/conftest.py marc/third_party/torchtune/tests/conftest.py
--- marc_original/third_party/torchtune/tests/conftest.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/conftest.py	2025-02-20 17:49:29.594024318 -0500
@@ -0,0 +1,124 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import argparse
+import os
+import uuid
+from pathlib import Path
+
+import pytest
+import torch.distributed.launcher as pet
+
+import torchtune
+
+root = str(Path(torchtune.__file__).parent.parent.absolute())
+CACHE_ARTIFACTS_SCRIPT_PATH = root + "/tests/cache_artifacts.sh"
+
+
+def pytest_configure(config):
+    """
+    This hook runs before each pytest invocation. Its purpose is to handle optional fetching
+    of remote artifacts needed for the test run and filtering across unit tests, recipe tests, and
+    regression tests.
+
+    When testing, you should run one of the following:
+
+    - `pytest tests`: run unit tests only
+    - `pytest tests --with-integration`: run unit tests and recipe tests
+    - `pytest tests --with-integration --with-slow-integration`: run all tests
+    - `pytest tests -m integration_test`: run recipe tests only
+    - `pytest tests -m slow_integration_test`: run regression tests only
+
+    Similar commands apply for filtering in subdirectories or individual test files.
+
+    This hook also ensures that the appropriate artifacts are available locally for all of the above cases.
+    Note that artifact download is determined by the CLI flags, so if you run e.g.
+    `pytest tests/torchtune/some_unit_test.py -m integration_test`, the integration test
+    artifacts will be downloaded even if your test doesn't require them.
+
+    The hook also supports optional silencing of S3 progress bars to reduce CI log spew via `--silence-s3-logs`.
+    """
+
+    # To make it more convenient to run an individual unit test, we override the default
+    # behavior of pytest-integration to run with --without-integration --without-slow-integration
+    # This means that we need to manually override the values of run_integration and run_slow_integration
+    # whenever either set of tests is passed via the -m option.
+
+    if config.option.markexpr == "integration_test":
+        config.option.run_integration = True
+        run_regression_tests = False
+    if config.option.markexpr == "slow_integration_test":
+        config.option.run_slow_integration = True
+        run_recipe_tests = False
+
+    # Default is to run both integration and slow integration tests (i.e. both are None)
+    run_recipe_tests = (
+        config.option.run_integration is None or config.option.run_integration is True
+    )
+    run_regression_tests = (
+        config.option.run_slow_integration is None
+        or config.option.run_slow_integration is True
+    )
+
+    cmd = str(CACHE_ARTIFACTS_SCRIPT_PATH)
+
+    if run_recipe_tests:
+        cmd += " --run-recipe-tests"
+    if run_regression_tests:
+        cmd += " --run-regression-tests"
+
+    # Optionally silence S3 download logs (useful when running on CI)
+    if config.option.silence_s3_logs:
+        cmd += " --silence-s3-logs"
+
+    # Only need to handle artifacts for recipe and regression tests
+    if run_recipe_tests or run_regression_tests:
+        os.system(cmd)
+
+
+@pytest.fixture(scope="session")
+def get_pet_launch_config():
+    def get_pet_launch_config_fn(nproc: int) -> pet.LaunchConfig:
+        """
+        Initialize pet.LaunchConfig for single-node, multi-rank functions.
+
+        Args:
+            nproc (int): The number of processes to launch.
+
+        Returns:
+            An instance of pet.LaunchConfig for single-node, multi-rank functions.
+
+        Example:
+            >>> from torch.distributed import launcher
+            >>> launch_config = get_pet_launch_config(nproc=8)
+            >>> launcher.elastic_launch(config=launch_config, entrypoint=train)()
+        """
+        return pet.LaunchConfig(
+            min_nodes=1,
+            max_nodes=1,
+            nproc_per_node=nproc,
+            run_id=str(uuid.uuid4()),
+            rdzv_backend="c10d",
+            rdzv_endpoint="localhost:0",
+            max_restarts=0,
+            monitor_interval=1,
+        )
+
+    return get_pet_launch_config_fn
+
+
+def pytest_addoption(parser: argparse.ArgumentParser) -> None:
+    parser.addoption(
+        "--large-scale",
+        type=bool,
+        default=False,
+        help="Run a larger scale integration test",
+    )
+    parser.addoption(
+        "--silence-s3-logs",
+        action="store_true",
+        help="Silence progress bar when fetching assets from S3",
+    )
diff -ruN marc_original/third_party/torchtune/tests/__init__.py marc/third_party/torchtune/tests/__init__.py
--- marc_original/third_party/torchtune/tests/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/__init__.py	2025-02-20 17:49:29.522024200 -0500
@@ -0,0 +1,21 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+# Check at the top-level that torchao is installed.
+# This is better than doing it at every import site.
+# We have to do this because it is not currently possible to
+# properly support both nightly and stable installs of PyTorch + torchao
+# in pyproject.toml.
+try:
+    import torchao  # noqa
+except ImportError as e:
+    raise ImportError(
+        """
+        torchao not installed.
+        Please follow the instructions at https://pytorch.org/torchtune/main/install.html#pre-requisites
+        to install torchao.
+        """
+    ) from e
diff -ruN marc_original/third_party/torchtune/tests/recipes/common.py marc/third_party/torchtune/tests/recipes/common.py
--- marc_original/third_party/torchtune/tests/recipes/common.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/common.py	2025-02-20 17:49:29.602024331 -0500
@@ -0,0 +1,9 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from pathlib import Path
+
+RECIPE_TESTS_DIR = Path(__file__).parent
diff -ruN marc_original/third_party/torchtune/tests/recipes/dev/test_generate_v2.py marc/third_party/torchtune/tests/recipes/dev/test_generate_v2.py
--- marc_original/third_party/torchtune/tests/recipes/dev/test_generate_v2.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/dev/test_generate_v2.py	2025-02-20 17:49:29.606024338 -0500
@@ -0,0 +1,71 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+
+from tests.common import TUNE_PATH
+from tests.recipes.utils import MODEL_TEST_CONFIGS, write_hf_ckpt_config
+from tests.test_utils import CKPT_MODEL_PATHS, mps_ignored_test, TOKENIZER_PATHS
+
+
+class TestGenerateV2:
+    """Recipe test suite for the generate_v2 recipe."""
+
+    @pytest.mark.integration_test
+    @mps_ignored_test()
+    def test_llama2_generate_results(self, caplog, monkeypatch, tmpdir):
+        ckpt = "llama2_tune"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        tokenizer_path = Path(TOKENIZER_PATHS["llama2"])
+        ckpt_dir = ckpt_path.parent
+
+        # Config file needed for model conversion.
+        write_hf_ckpt_config(ckpt_dir)
+
+        cmd = f"""
+        tune run dev/generate_v2 \
+            --config llama2/generation_v2 \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            device=cpu \
+            dtype=fp32 \
+            max_new_tokens=10 \
+            seed=123 \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama2"]
+        cmd = cmd + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # this is gibberish b/c the model is random weights, but it's
+        # the expected value for what we currently have in V2
+        # this test should catch any changes to the generate recipe that affect output
+        expected_output = (
+            "Country maior Connection Kohutsjcustomulas Sometimes Security"
+        )
+
+        logs = caplog.text
+        assert expected_output in logs
+
+    @pytest.mark.integration_test
+    def test_llama2_fail_on_bad_input(self, capsys, monkeypatch, tmpdir):
+        """Should fail when user passes in a bad input:
+        - No prompt provided
+        - Prompt has multiple entries in content and no image
+        """
+        pass
diff -ruN marc_original/third_party/torchtune/tests/recipes/__init__.py marc/third_party/torchtune/tests/recipes/__init__.py
--- marc_original/third_party/torchtune/tests/recipes/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/__init__.py	2025-02-20 17:49:29.598024324 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_configs.py marc/third_party/torchtune/tests/recipes/test_configs.py
--- marc_original/third_party/torchtune/tests/recipes/test_configs.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_configs.py	2025-02-20 17:49:29.610024344 -0500
@@ -0,0 +1,30 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import os
+from pathlib import Path
+
+import torchtune
+
+from omegaconf import OmegaConf
+from torchao.utils import TORCH_VERSION_AFTER_2_4
+from torchtune import config
+
+CONFIG_DIR = Path(torchtune.__file__).parent.parent / "recipes" / "configs"
+
+
+class TestConfigs:
+    def test_instantiate(self) -> None:
+        all_configs = [
+            os.path.join(CONFIG_DIR, f)
+            for f in os.listdir(CONFIG_DIR)
+            if f.endswith(".yaml")
+        ]
+        for config_path in all_configs:
+            # QAT config is only compatible with PyTorch 2.4+
+            if config_path.endswith("qat_full.yaml") and not TORCH_VERSION_AFTER_2_4:
+                continue
+            cfg = OmegaConf.load(config_path)
+            config.validate(cfg)
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_eleuther_eval.py marc/third_party/torchtune/tests/recipes/test_eleuther_eval.py
--- marc_original/third_party/torchtune/tests/recipes/test_eleuther_eval.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_eleuther_eval.py	2025-02-20 17:49:29.614024351 -0500
@@ -0,0 +1,205 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+import re
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+
+from tests.common import TUNE_PATH
+from tests.recipes.utils import llama2_test_config, write_hf_ckpt_config
+from tests.test_utils import CKPT_MODEL_PATHS
+
+
+class TestEleutherEval:
+    @pytest.mark.parametrize(
+        "eval_name, expected_acc, bsz",
+        [
+            ("truthfulqa_gen", 0.1, 4),
+            ("truthfulqa_gen", 0.1, 1),
+            ("truthfulqa_mc2", 0.4, 4),
+        ],
+    )
+    @pytest.mark.integration_test
+    def test_torchtune_checkpoint_eval_results(
+        self, caplog, monkeypatch, tmpdir, eval_name, expected_acc, bsz
+    ):
+        ckpt = "llama2_tune"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+
+        # explicitly setting limit to an odd number here to ensure generation tasks
+        # work with KV-cacheing + bsz > 1 - we'll receive batches of size 4, 4, 3
+        cmd = f"""
+        tune run eleuther_eval \
+            --config eleuther_evaluation \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            limit=11 \
+            dtype=fp32 \
+            device=cpu \
+            tasks=[{eval_name}]\
+            batch_size={bsz} \
+        """.split()
+
+        model_config = llama2_test_config()
+        cmd = cmd + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        out = caplog.text
+
+        # Format of output is:
+        # |    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|
+        # |--------------|------:|------|-----:|------|---|-----:|---|-----:|
+        # |truthfulqa_mc2|      2|none  |     0|acc   |  |0.4497|  |0.1067|
+        search_results = re.search(
+            r"acc(?:_norm)?\s*\|?\s*(?:\\s*\|?)?([\d.]+)", out.strip()
+        )
+        assert search_results is not None
+        acc_result = float(search_results.group(1))
+        assert math.isclose(acc_result, expected_acc, abs_tol=0.05)
+
+    @pytest.fixture
+    def hide_correct_version_number(self, monkeypatch):
+        import importlib.metadata
+
+        import_orig = importlib.metadata.version
+
+        def mocked_import(name, *args, **kwargs):
+            if name == "lm-eval":
+                return "0.4.4"  # Hardcode wrong version number
+            return import_orig(name, *args, **kwargs)
+
+        monkeypatch.setattr(importlib.metadata, "version", mocked_import)
+
+    @pytest.mark.integration_test
+    @pytest.mark.usefixtures("hide_correct_version_number")
+    def test_eval_recipe_errors_without_lm_eval(self, capsys, monkeypatch, tmpdir):
+        ckpt = "llama2_tune"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+
+        cmd = f"""
+        tune run eleuther_eval \
+            --config eleuther_evaluation \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            limit=1 \
+            dtype=fp32 \
+            device=cpu \
+        """.split()
+
+        model_config = llama2_test_config()
+        cmd = cmd + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(
+            RuntimeError,
+            match="This recipe requires EleutherAI Eval Harness v0.4.5. "
+            "Please install with `pip install lm-eval==0.4.5`",
+        ):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+    @pytest.mark.integration_test
+    def test_eval_recipe_errors_with_quantization_hf_checkpointer(
+        self, capsys, monkeypatch, tmpdir
+    ):
+        ckpt = "llama2_hf"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+
+        # Config file needed for model conversion.
+        write_hf_ckpt_config(ckpt_dir)
+
+        cmd = f"""
+        tune run eleuther_eval \
+            --config eleuther_evaluation \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            limit=1 \
+            dtype=fp32 \
+            device=cpu \
+            quantizer._component_=torchtune.training.quantization.Int8DynActInt4WeightQuantizer \
+            quantizer.groupsize=256 \
+        """.split()
+
+        model_config = llama2_test_config()
+        cmd = cmd + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(
+            ValueError,
+            match="Quantization is only supported for models quantized and saved with the "
+            "FullModelTorchTuneCheckpointer",
+        ):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+    @pytest.mark.integration_test
+    def test_eval_recipe_errors_with_qat_quantizer(self, capsys, monkeypatch, tmpdir):
+        ckpt = "llama2_tune"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+
+        cmd = f"""
+        tune run eleuther_eval \
+            --config eleuther_evaluation \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            limit=1 \
+            dtype=fp32 \
+            device=cpu \
+            quantizer._component_=torchtune.training.quantization.Int8DynActInt4WeightQATQuantizer \
+            quantizer.groupsize=32\
+        """.split()
+
+        model_config = llama2_test_config()
+        cmd = cmd + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(
+            ValueError,
+            match="QAT quantizers should only be used during quantization aware training",
+        ):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+    @pytest.mark.integration_test
+    def test_eval_recipe_errors_with_generate_until_and_mc_tasks(
+        self, caplog, capsys, monkeypatch, tmpdir
+    ):
+        # We can't currently specify both generate_until and mc_tasks in the same run
+        # b/c the KV cache won't be reset and the result will be different. This test
+        # catches that error
+        pass
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_full_finetune_distributed.py marc/third_party/torchtune/tests/recipes/test_full_finetune_distributed.py
--- marc_original/third_party/torchtune/tests/recipes/test_full_finetune_distributed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_full_finetune_distributed.py	2025-02-20 17:49:29.618024358 -0500
@@ -0,0 +1,101 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import runpy
+
+import sys
+from pathlib import Path
+
+import pytest
+import torch
+from tests.common import TUNE_PATH
+
+from tests.recipes.utils import (
+    CKPT_COMPONENT_MAP,
+    dummy_alpaca_dataset_config,
+    MODEL_TEST_CONFIGS,
+    write_hf_ckpt_config,
+)
+from tests.test_utils import (
+    CKPT_MODEL_PATHS,
+    gen_log_file_name,
+    get_loss_values_from_metric_logger,
+    gpu_test,
+    TOKENIZER_PATHS,
+)
+
+
+class TestFullFinetuneDistributedRecipe:
+    def _get_test_config_overrides(self):
+        return [
+            "batch_size=4",
+            "dtype=fp32",
+            "enable_activation_checkpointing=False",
+            "dataset.train_on_input=False",
+            "seed=9",
+            "epochs=2",
+            "max_steps_per_epoch=2",
+            "optimizer=torch.optim.AdamW",
+            "optimizer.lr=2e-5",
+            "log_every_n_steps=1",
+            "clip_grad_norm=100",
+        ] + dummy_alpaca_dataset_config()
+
+    def _fetch_expected_loss_values(self, model_type):
+        loss_values_map = {
+            "llama2": [10.5136, 10.4813, 10.5088, 10.5250],
+            "llama3": [12.0673, 11.9072, 11.9302, 11.9355],
+        }
+        return loss_values_map[model_type]
+
+    @pytest.mark.integration_test
+    @pytest.mark.parametrize(
+        "config, model_type, ckpt_type, fsdp_sharding_strategy",
+        [
+            ("llama2/7B_full", "llama2", "hf", None),
+            ("llama3/8B_full", "llama3", "tune", None),
+            ("llama3/8B_full", "llama3", "tune", "NO_SHARD"),
+        ],
+    )
+    @gpu_test(gpu_count=2)
+    def test_loss(
+        self, config, model_type, ckpt_type, fsdp_sharding_strategy, tmpdir, monkeypatch
+    ):
+        ckpt_component = CKPT_COMPONENT_MAP[ckpt_type]
+        ckpt = model_type + "_" + ckpt_type
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        tokenizer_path = Path(TOKENIZER_PATHS[model_type])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        # Config file needed for model conversion.
+        write_hf_ckpt_config(ckpt_dir)
+
+        cmd = f"""
+        tune run --nnodes 1 --nproc_per_node 2 full_finetune_distributed \
+            --config {config} \
+            output_dir={tmpdir} \
+            checkpointer._component_={ckpt_component} \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type={model_type.upper()} \
+            tokenizer.path='{tokenizer_path}' \
+            tokenizer.prompt_template=null \
+            metric_logger.filename={log_file} \
+        """.split()
+        if fsdp_sharding_strategy:
+            cmd.append(f"fsdp_sharding_strategy={fsdp_sharding_strategy}")
+        model_config = MODEL_TEST_CONFIGS[model_type]
+        cmd = cmd + self._get_test_config_overrides() + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        expected_loss_values = self._fetch_expected_loss_values(model_type)
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-4, atol=1e-4
+        )
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_full_finetune_single_device.py marc/third_party/torchtune/tests/recipes/test_full_finetune_single_device.py
--- marc_original/third_party/torchtune/tests/recipes/test_full_finetune_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_full_finetune_single_device.py	2025-02-20 17:49:29.622024364 -0500
@@ -0,0 +1,267 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+
+import runpy
+
+import sys
+from pathlib import Path
+
+import numpy as np
+
+import pytest
+
+import torch
+from tests.common import TUNE_PATH
+
+from tests.recipes.utils import (
+    CKPT_COMPONENT_MAP,
+    dummy_alpaca_dataset_config,
+    MODEL_TEST_CONFIGS,
+    write_hf_ckpt_config,
+)
+from tests.test_utils import (
+    CKPT_MODEL_PATHS,
+    gen_log_file_name,
+    get_loss_values_from_metric_logger,
+    TOKENIZER_PATHS,
+)
+
+
+class TestFullFinetuneSingleDeviceRecipe:
+    def _get_test_config_overrides(self):
+        return [
+            "batch_size=8",
+            "device=cpu",
+            "dtype=fp32",
+            "enable_activation_checkpointing=False",
+            "dataset.train_on_input=False",
+            "seed=9",
+            "epochs=2",
+            "max_steps_per_epoch=2",
+            "optimizer=torch.optim.AdamW",
+            "optimizer.lr=2e-5",
+            "lr_scheduler.num_warmup_steps=0",
+            "lr_scheduler.num_cycles=0",
+            "log_every_n_steps=1",
+            "clip_grad_norm=100",
+        ] + dummy_alpaca_dataset_config()
+
+    def _fetch_expected_loss_values(self, model_type):
+        loss_values_map = {
+            "llama2": [10.5201, 10.5217, 10.4945, 10.5136],
+            "llama3": [11.9839, 11.9684, 11.9596, 11.9366],
+        }
+
+        return loss_values_map[model_type]
+
+    @pytest.mark.integration_test
+    @pytest.mark.parametrize("compile", [True, False])
+    @pytest.mark.parametrize(
+        "config, model_type, ckpt_type",
+        [
+            ("llama2/7B_full_low_memory", "llama2", "meta"),
+            ("llama3/8B_full_single_device", "llama3", "tune"),
+        ],
+    )
+    def test_loss(self, compile, config, model_type, ckpt_type, tmpdir, monkeypatch):
+        ckpt_component = CKPT_COMPONENT_MAP[ckpt_type]
+        ckpt = model_type + "_" + ckpt_type
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        tokenizer_path = Path(TOKENIZER_PATHS[model_type])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        cmd = f"""
+        tune run full_finetune_single_device \
+            --config {config} \
+            output_dir={tmpdir} \
+            checkpointer._component_={ckpt_component} \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type={model_type.upper()} \
+            tokenizer.path='{tokenizer_path}' \
+            tokenizer.prompt_template=null \
+            metric_logger.filename={log_file} \
+            compile={compile} \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS[model_type]
+        cmd = cmd + self._get_test_config_overrides() + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Make sure to clear compile state in between tests
+        if compile:
+            torch._dynamo.reset()
+
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        expected_loss_values = self._fetch_expected_loss_values(model_type)
+
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-4, atol=1e-4
+        )
+
+    @pytest.mark.integration_test
+    def test_training_state_on_resume(self, tmpdir, monkeypatch):
+        """Test whether the recipe state is correctly updated on resume. Since this
+        is model agnostic, we should run this on the small model only. The test
+        consists of three stages:
+            - Train a model for 2 epochs
+            - Resume training after epoch 1
+            - Make sure final loss matches the expected value of a model successfully resumed from a ckpt
+        """
+
+        ckpt = "llama2_hf"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        # Config file needed for model conversion.
+        # Create a second copy for training resume
+        write_hf_ckpt_config(ckpt_dir)
+        write_hf_ckpt_config(tmpdir)
+
+        # Train for two epochs
+        cmd_1 = f"""
+        tune run full_finetune_single_device \
+            --config llama2/7B_full_low_memory \
+            output_dir={tmpdir} \
+            checkpointer._component_=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama2"]
+        cmd_1 = cmd_1 + self._get_test_config_overrides() + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd_1)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Resume training
+        cmd_2 = f"""
+        tune run full_finetune_single_device \
+            --config llama2/7B_full_low_memory \
+            output_dir={tmpdir} \
+            checkpointer._component_=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir={tmpdir} \
+            checkpointer.checkpoint_files=[{os.path.join(tmpdir, "hf_model_0001_0.pt")}]\
+            checkpointer.recipe_checkpoint={os.path.join(tmpdir, "recipe_state.pt")}
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            resume_from_checkpoint=True \
+            metric_logger.filename={log_file} \
+        """.split()
+
+        cmd_2 = cmd_2 + self._get_test_config_overrides() + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd_2)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        expected_loss_values = self._fetch_expected_loss_values("llama2")[2:]
+
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-4, atol=1e-4
+        )
+
+
+class TestFullFinetuneSingleDeviceGradientAccumulation:
+    def _get_test_config_overrides(self):
+        return [
+            "device=cpu",
+            "dtype=fp32",
+            "enable_activation_checkpointing=False",
+            "tokenizer.path=/tmp/test-artifacts/tokenizer.model",
+            "tokenizer.prompt_template=null",
+            "dataset=tests.recipes.utils.DummyDataset",
+            "dataset.train_on_input=False",
+            "seed=9",
+            "epochs=1",
+            "max_steps_per_epoch=1",
+            "optimizer=torch.optim.AdamW",
+            "optimizer.lr=2e-5",
+            "log_every_n_steps=1",
+            "optimizer_in_bwd=False",
+        ]
+
+    @pytest.mark.integration_test
+    def test_gradient_accumulation(self, tmpdir, monkeypatch):
+        """Test whether gradient accumulation runs properly in the recipe. In general
+        the sum of loss across minibatches should equal the loss over the full batch,
+        but since our loss is normalized by the number of unmasked tokens, this does not
+        hold in for our case. We use a dummy dataset where all tokens are unmasked, and
+        in this test check that a single batch size of N yields the same loss as N accumulated
+        microbatches of size 1.
+        """
+        full_batch_size = 4
+        micro_batch_size = 1
+        gradient_accumulation_steps = full_batch_size // micro_batch_size
+        ckpt = "llama2_tune"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+        no_grad_accum_log_file = gen_log_file_name(tmpdir, suffix="no_grad_accum")
+        grad_accum_log_file = gen_log_file_name(tmpdir, suffix="grad_accum")
+
+        cmd_1 = f"""
+        tune run full_finetune_single_device \
+            --config llama2/7B_full_low_memory \
+            checkpointer._component_=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir={ckpt_dir} \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            batch_size={full_batch_size} \
+            output_dir={tmpdir} \
+            log_every_n_steps=1 \
+            metric_logger.filename={no_grad_accum_log_file} \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama2"]
+        cmd_1 = cmd_1 + self._get_test_config_overrides() + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd_1)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        no_accum_loss = get_loss_values_from_metric_logger(no_grad_accum_log_file)[
+            0
+        ]  # List of a single element
+
+        # Update the cmd with new values for gradient accumulation
+        cmd_2 = f"""
+        tune run full_finetune_single_device \
+            --config llama2/7B_full_low_memory \
+            checkpointer._component_=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir={ckpt_dir} \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=llama2 \
+            batch_size={micro_batch_size} \
+            gradient_accumulation_steps={gradient_accumulation_steps} \
+            output_dir={tmpdir} \
+            metric_logger.filename={grad_accum_log_file} \
+        """.split()
+        cmd_2 = cmd_2 + self._get_test_config_overrides() + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd_2)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        accum_loss = np.mean(get_loss_values_from_metric_logger(grad_accum_log_file))
+        torch.testing.assert_close(no_accum_loss, accum_loss, atol=1e-5, rtol=1e-5)
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_knowledge_distillation_single_device.py marc/third_party/torchtune/tests/recipes/test_knowledge_distillation_single_device.py
--- marc_original/third_party/torchtune/tests/recipes/test_knowledge_distillation_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_knowledge_distillation_single_device.py	2025-02-20 17:49:29.622024364 -0500
@@ -0,0 +1,298 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+import torch
+from omegaconf import OmegaConf
+from tests.common import TUNE_PATH
+from tests.recipes.utils import (
+    CKPT_COMPONENT_MAP,
+    dummy_alpaca_dataset_config,
+    MODEL_TEST_CONFIGS,
+    write_hf_ckpt_config,
+)
+from tests.test_utils import (
+    CKPT_MODEL_PATHS,
+    gen_log_file_name,
+    get_loss_values_from_metric_logger,
+    TOKENIZER_PATHS,
+)
+from torchtune import config
+
+
+class TestKDSingleDeviceRecipe:
+    def _get_test_config_overrides(self, dtype_str: str = "fp32", epochs: int = 2):
+        return [
+            "batch_size=8",
+            "device=cpu",
+            f"dtype={dtype_str}",
+            "enable_activation_checkpointing=False",
+            "dataset.train_on_input=False",
+            "seed=9",
+            f"epochs={epochs}",
+            "max_steps_per_epoch=2",
+            "optimizer.lr=2e-5",
+            "log_every_n_steps=1",
+            "gradient_accumulation_steps=1",
+            "clip_grad_norm=100",
+        ] + dummy_alpaca_dataset_config()
+
+    def _fetch_expected_loss_values(self, model_type):
+        loss_values_map = {
+            "llama3": [11.0651, 11.0577, 11.0540, 11.7671],
+        }
+        return loss_values_map[model_type]
+
+    @pytest.mark.integration_test
+    @pytest.mark.parametrize("compile", [True, False])
+    @pytest.mark.parametrize(
+        "config, model_type, ckpt_type",
+        [
+            ("qwen2/knowledge_distillation_single_device", "llama3", "tune"),
+        ],
+    )
+    def test_loss(self, compile, config, model_type, ckpt_type, tmpdir, monkeypatch):
+        ckpt_component = CKPT_COMPONENT_MAP[ckpt_type]
+        ckpt = model_type + "_" + ckpt_type
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        tokenizer_path = Path(TOKENIZER_PATHS[model_type])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        cmd = f"""
+        tune run knowledge_distillation_single_device \
+            --config {config} \
+            output_dir={tmpdir} \
+            checkpointer._component_={ckpt_component} \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}] \
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type={model_type.upper()} \
+            teacher_checkpointer._component_={ckpt_component} \
+            teacher_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            teacher_checkpointer.checkpoint_files=[{ckpt_path}] \
+            teacher_checkpointer.output_dir={tmpdir} \
+            teacher_checkpointer.model_type={model_type.upper()} \
+            tokenizer._component_=torchtune.models.llama3.llama3_tokenizer \
+            tokenizer.path='{tokenizer_path}' \
+            tokenizer.prompt_template=null \
+            ~tokenizer.merges_file \
+            metric_logger._component_=torchtune.training.metric_logging.DiskLogger \
+            metric_logger.filename={log_file} \
+            compile={compile} \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS[model_type + "_lora"]
+        teacher_config = [
+            "teacher_" + config for config in MODEL_TEST_CONFIGS[model_type]
+        ]
+
+        cmd = (
+            cmd
+            + self._get_test_config_overrides(dtype_str="fp32")
+            + model_config
+            + teacher_config
+        )
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Make sure to clear compile state in between tests
+        if compile:
+            torch._dynamo.reset()
+
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        # only take the first loss
+        num_losses = int(len(loss_values) / 4)  # 2 steps per epoch, 2 epochs
+        loss_values = loss_values[0::num_losses]
+        expected_loss_values = self._fetch_expected_loss_values(model_type)
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-5, atol=1e-5
+        )
+
+    @pytest.mark.integration_test
+    def test_training_state_on_resume(self, tmpdir, monkeypatch):
+        """Test whether the recipe state is correctly updated on resume. Since this
+        is model agnostic, we should run this on the small model only. The test
+        consists of three stages:
+            - Train a model for 2 epochs
+            - Resume training after epoch 1
+            - Make sure final loss matches the expected value of a model successfully resumed from a ckpt
+        """
+
+        ckpt = "llama3_tune"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+        tokenizer_path = Path(TOKENIZER_PATHS["llama3"])
+
+        # Config file needed for model conversion.
+        # Create a second copy for training resume
+        write_hf_ckpt_config(ckpt_dir)
+        write_hf_ckpt_config(tmpdir)
+
+        # Train for two epochs
+        cmd_1 = f"""
+        tune run knowledge_distillation_single_device \
+            --config qwen2/knowledge_distillation_single_device \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA3 \
+            teacher_checkpointer._component_=torchtune.training.FullModelTorchTuneCheckpointer \
+            teacher_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            teacher_checkpointer.checkpoint_files=[{ckpt_path}] \
+            teacher_checkpointer.output_dir={tmpdir} \
+            teacher_checkpointer.model_type=LLAMA3 \
+            tokenizer._component_=torchtune.models.llama3.llama3_tokenizer \
+            tokenizer.path={tokenizer_path} \
+            tokenizer.prompt_template=null \
+            ~tokenizer.merges_file \
+            metric_logger._component_=torchtune.training.metric_logging.DiskLogger \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama3_lora"]
+        teacher_config = [
+            "teacher_" + config for config in MODEL_TEST_CONFIGS["llama3"]
+        ]
+
+        cmd_1 = (
+            cmd_1 + self._get_test_config_overrides() + model_config + teacher_config
+        )
+        monkeypatch.setattr(sys, "argv", cmd_1)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Resume training
+        cmd_2 = f"""
+        tune run knowledge_distillation_single_device \
+            --config qwen2/knowledge_distillation_single_device \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir={tmpdir} \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.adapter_checkpoint={os.path.join(tmpdir, "adapter_0.pt")}
+            checkpointer.recipe_checkpoint={os.path.join(tmpdir, "recipe_state.pt")}
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA3 \
+            teacher_checkpointer._component_=torchtune.training.FullModelTorchTuneCheckpointer \
+            teacher_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            teacher_checkpointer.checkpoint_files=[{ckpt_path}] \
+            teacher_checkpointer.output_dir={tmpdir} \
+            teacher_checkpointer.model_type=LLAMA3 \
+            resume_from_checkpoint=True \
+            metric_logger._component_=torchtune.training.metric_logging.DiskLogger \
+            metric_logger.filename={log_file} \
+            tokenizer._component_=torchtune.models.llama3.llama3_tokenizer \
+            tokenizer.path={tokenizer_path} \
+            tokenizer.prompt_template=null \
+            ~tokenizer.merges_file \
+        """.split()
+        cmd_2 = (
+            cmd_2
+            + self._get_test_config_overrides(epochs=3)
+            + model_config
+            + teacher_config
+        )
+        monkeypatch.setattr(sys, "argv", cmd_2)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Second epoch only
+        expected_loss_values = self._fetch_expected_loss_values("llama3")[2:]
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        # only take the first loss
+        num_losses = int(len(loss_values) / 4)  # 2 steps per epoch, 2 epochs
+        loss_values = loss_values[0::num_losses][:2]
+
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-5, atol=1e-5
+        )
+
+    @pytest.mark.integration_test
+    def test_save_and_load_merged_weights(self, tmpdir, monkeypatch):
+        ckpt_type = "tune"
+        model_type = "llama3"
+        ckpt_component = CKPT_COMPONENT_MAP[ckpt_type]
+        ckpt = model_type + "_" + ckpt_type
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        tokenizer_path = Path(TOKENIZER_PATHS[model_type])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        cmd = f"""
+        tune run knowledge_distillation_single_device \
+            --config qwen2/knowledge_distillation_single_device \
+            output_dir={tmpdir} \
+            checkpointer._component_={ckpt_component} \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}] \
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type={model_type.upper()} \
+            teacher_checkpointer._component_={ckpt_component} \
+            teacher_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            teacher_checkpointer.checkpoint_files=[{ckpt_path}] \
+            teacher_checkpointer.output_dir={tmpdir} \
+            teacher_checkpointer.model_type={model_type.upper()} \
+            tokenizer._component_=torchtune.models.llama3.llama3_tokenizer \
+            tokenizer.path='{tokenizer_path}' \
+            tokenizer.prompt_template=null \
+            ~tokenizer.merges_file \
+            metric_logger._component_=torchtune.training.metric_logging.DiskLogger \
+            metric_logger.filename={log_file} \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS[model_type + "_lora"]
+        teacher_config = [
+            "teacher_" + config for config in MODEL_TEST_CONFIGS[model_type]
+        ]
+
+        cmd = (
+            cmd
+            + self._get_test_config_overrides(dtype_str="fp32")
+            + model_config
+            + teacher_config
+        )
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Next load both the merged weights in a Llama3 base model
+        # and the base model weights + trained adapter weights in the LoRA Llama 3 model
+        # The results of calling forward on dummy inputs should be the same.
+        inputs = torch.randint(low=0, high=32_000, size=(2, 100))
+
+        # Build LoRA model for loading base + adapter weights separately
+        lora_model = config.instantiate(OmegaConf.from_dotlist(model_config).model)
+
+        # Build base llama3 model for loading merged weights
+        base_llama3_config = MODEL_TEST_CONFIGS[model_type]
+        llama3_model = config.instantiate(
+            OmegaConf.from_dotlist(base_llama3_config).model
+        )
+
+        # Load base model and trained adapter weights into LoRA model and call fwd
+        with open(f"{tmpdir}/adapter_1.pt", "rb") as f:
+            lora_sd = torch.load(f, weights_only=True)
+        with open(ckpt_path, "rb") as f:
+            base_model_sd = torch.load(f, weights_only=True)
+        lora_model.load_state_dict(lora_sd, strict=False)
+        lora_model.load_state_dict(base_model_sd, strict=False)
+        baseline_out = lora_model(inputs)
+
+        # Load merged final ckpt directly into 3 and call fwd
+        with open(f"{tmpdir}/torchtune_model_1.pt", "rb") as f:
+            sd = torch.load(f, weights_only=True)
+        llama3_model.load_state_dict(sd)
+        merged_ckpt_out = llama3_model(inputs)
+        torch.testing.assert_close(baseline_out, merged_ckpt_out, rtol=1e-5, atol=1e-5)
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_lora_dpo_single_device.py marc/third_party/torchtune/tests/recipes/test_lora_dpo_single_device.py
--- marc_original/third_party/torchtune/tests/recipes/test_lora_dpo_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_lora_dpo_single_device.py	2025-02-20 17:49:29.626024370 -0500
@@ -0,0 +1,184 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+import torch
+from omegaconf import OmegaConf
+from tests.common import TUNE_PATH
+from tests.recipes.utils import (
+    dummy_stack_exchange_dataset_config,
+    MODEL_TEST_CONFIGS,
+    write_hf_ckpt_config,
+)
+from tests.test_utils import (
+    CKPT_MODEL_PATHS,
+    gen_log_file_name,
+    get_loss_values_from_metric_logger,
+)
+from torchtune import config
+
+
+class TestLoRADPOSingleDeviceRecipe:
+    def _get_test_config_overrides(self, dtype_str: str = "fp32", epochs: int = 2):
+        return [
+            "batch_size=8",
+            "device=cpu",
+            f"dtype={dtype_str}",
+            "dataset.train_on_input=False",
+            "seed=9",
+            f"epochs={epochs}",
+            "max_steps_per_epoch=2",
+            "optimizer.lr=2e-5",
+            "log_every_n_steps=1",
+            "gradient_accumulation_steps=1",
+            "clip_grad_norm=100",
+            "tokenizer.max_seq_len=512",
+        ] + dummy_stack_exchange_dataset_config()
+
+    @pytest.mark.parametrize("save_adapter_weights_only", [False, True])
+    @pytest.mark.integration_test
+    def test_training_state_on_resume(
+        self, tmpdir, monkeypatch, save_adapter_weights_only
+    ):
+        """Test whether the recipe state is correctly updated on resume. Since this
+        is model agnostic, we should run this on the small model only. The test
+        consists of three stages:
+            - Train a model for 2 epochs
+            - Resume training after epoch 1
+            - Make sure final loss matches the expected value of a model successfully resumed from a ckpt
+        Unlike `tests.recipes.test_lora_finetune_single_device`, this test does not use pre-computed loss
+        values to benchmark against. This test just ensures the loss values are identical when resuming.
+        """
+
+        ckpt = "llama2_hf"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        # Config file needed for model conversion.
+        # Create a second copy for training resume
+        write_hf_ckpt_config(ckpt_dir)
+        write_hf_ckpt_config(tmpdir)
+
+        # Train for two epochs
+        cmd_1 = f"""
+        tune run lora_dpo_single_device \
+            --config llama2/7B_lora_dpo_single_device \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            save_adapter_weights_only={save_adapter_weights_only} \
+            metric_logger.filename={log_file} \
+            enable_activation_checkpointing=True \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama2_lora"]
+
+        cmd_1 = cmd_1 + self._get_test_config_overrides() + model_config
+        monkeypatch.setattr(sys, "argv", cmd_1)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        expected_loss_values = get_loss_values_from_metric_logger(log_file)
+
+        resumed_log_dir = (tmpdir / "resumed/").mkdir()
+        resumed_log_file = gen_log_file_name(resumed_log_dir)
+        # Resume training
+        cmd_2 = f"""
+        tune run lora_dpo_single_device \
+            --config llama2/7B_lora_dpo_single_device \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir={tmpdir} \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.adapter_checkpoint={os.path.join(tmpdir, "adapter_0.pt")}
+            checkpointer.recipe_checkpoint={os.path.join(tmpdir, "recipe_state.pt")}
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            resume_from_checkpoint=True \
+            metric_logger.filename={resumed_log_file} \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            enable_activation_checkpointing=True \
+        """.split()
+        cmd_2 = cmd_2 + self._get_test_config_overrides(epochs=3) + model_config
+        monkeypatch.setattr(sys, "argv", cmd_2)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Second epoch only
+        resumed_loss_values = get_loss_values_from_metric_logger(resumed_log_file)
+
+        torch.testing.assert_close(
+            resumed_loss_values[:2], expected_loss_values[2:], rtol=1e-5, atol=1e-5
+        )
+
+    @pytest.mark.integration_test
+    def test_save_and_load_merged_weights(self, tmpdir, monkeypatch):
+        ckpt = "llama2_tune"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+
+        cmd = f"""
+        tune run lora_dpo_single_device \
+            --config llama2/7B_lora_dpo_single_device \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            enable_activation_checkpointing=False \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama2_lora"]
+
+        cmd = cmd + self._get_test_config_overrides() + model_config
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Next load both the merged weights in a Llama2 base model
+        # and the base model weights + trained adapter weights in the LoRA Llama 2 model
+        # The results of calling forward on dummy inputs should be the same.
+        inputs = torch.randint(low=0, high=32_000, size=(2, 100))
+
+        # Build LoRA model for loading base + adapter weights separately
+        lora_model = config.instantiate(OmegaConf.from_dotlist(model_config).model)
+
+        # Build base llama2 model for loading merged weights
+        base_llama2_config = MODEL_TEST_CONFIGS["llama2"]
+        llama2_model = config.instantiate(
+            OmegaConf.from_dotlist(base_llama2_config).model
+        )
+
+        # Load base model and trained adapter weights into LoRA model and call fwd
+        with open(f"{tmpdir}/adapter_1.pt", "rb") as f:
+            lora_sd = torch.load(f, weights_only=True)
+        with open(ckpt_path, "rb") as f:
+            base_model_sd = torch.load(f, weights_only=True)
+        lora_model.load_state_dict(lora_sd, strict=False)
+        lora_model.load_state_dict(base_model_sd, strict=False)
+        baseline_out = lora_model(inputs)
+
+        # Load merged final ckpt directly into llama2 and call fwd
+        with open(f"{tmpdir}/torchtune_model_1.pt", "rb") as f:
+            sd = torch.load(f, weights_only=True)
+        llama2_model.load_state_dict(sd)
+        merged_ckpt_out = llama2_model(inputs)
+        torch.testing.assert_close(baseline_out, merged_ckpt_out, rtol=1e-5, atol=1e-5)
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_lora_finetune_distributed.py marc/third_party/torchtune/tests/recipes/test_lora_finetune_distributed.py
--- marc_original/third_party/torchtune/tests/recipes/test_lora_finetune_distributed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_lora_finetune_distributed.py	2025-02-20 17:49:29.630024377 -0500
@@ -0,0 +1,254 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+import torch
+from omegaconf import OmegaConf
+from tests.common import TUNE_PATH
+from tests.recipes.utils import (
+    CKPT_COMPONENT_MAP,
+    dummy_alpaca_dataset_config,
+    MODEL_TEST_CONFIGS,
+    write_hf_ckpt_config,
+)
+from tests.test_utils import (
+    CKPT_MODEL_PATHS,
+    gen_log_file_name,
+    get_loss_values_from_metric_logger,
+    gpu_test,
+    TOKENIZER_PATHS,
+)
+from torchtune import config
+
+
+class TestLoRAFinetuneDistributedRecipe:
+    def _get_test_config_overrides(self):
+        return [
+            "batch_size=4",
+            "dataset.train_on_input=False",
+            "seed=9",
+            "epochs=2",
+            "dtype=fp32",
+            "max_steps_per_epoch=2",
+            "optimizer.lr=2e-5",
+            "log_every_n_steps=1",
+            "gradient_accumulation_steps=1",
+            "compile=False",
+        ] + dummy_alpaca_dataset_config()
+
+    def _fetch_expected_loss_values(self, model_type):
+        # These values have been validated against single device recipe test via
+        # https://gist.github.com/ebsmothers/f1c3db7c66655a23a91e0290360960c4
+        loss_values_map = {
+            "llama2": [10.5136, 10.4856, 10.5292, 10.5345],
+            "llama3": [11.9325, 11.9325, 11.9325, 11.9369],
+        }
+        return loss_values_map[model_type]
+
+    @pytest.mark.integration_test
+    @gpu_test(gpu_count=2)
+    @pytest.mark.parametrize(
+        "reshard_after_forward",
+        [
+            True,
+            False,
+        ],
+    )
+    def test_loss(self, reshard_after_forward, tmpdir, monkeypatch):
+        ckpt = "llama2_tune"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+        cmd = f"""
+        tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed
+            --config llama2/7B_lora \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            metric_logger.filename={log_file} \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            reshard_after_forward={reshard_after_forward} \
+            enable_activation_checkpointing=False \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama2_lora"]
+
+        cmd = cmd + self._get_test_config_overrides() + model_config
+        monkeypatch.setattr(sys, "argv", cmd)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        expected_loss_values = self._fetch_expected_loss_values("llama2")
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-5, atol=1e-5
+        )
+
+    @pytest.mark.integration_test
+    @gpu_test(gpu_count=2)
+    @pytest.mark.parametrize(
+        "config, model_type, ckpt_type, save_adapter_weights_only",
+        [
+            ("llama2/7B_lora", "llama2", "hf", False),
+            ("llama3/8B_lora", "llama3", "tune", False),
+            ("llama2/7B_lora", "llama2", "hf", True),
+        ],
+    )
+    def test_training_state_on_resume(
+        self,
+        config,
+        model_type,
+        ckpt_type,
+        tmpdir,
+        monkeypatch,
+        save_adapter_weights_only,
+    ):
+        """Test whether the recipe state is correctly updated on resume. Since this
+        is model agnostic, we should run this on the small model only. The test
+        consists of three stages:
+            - Train a model for 2 epochs
+            - Resume training after epoch 1
+            - Make sure final loss matches the expected value of a model successfully resumed from a ckpt
+        """
+        ckpt_component = CKPT_COMPONENT_MAP[ckpt_type]
+        ckpt = model_type + "_" + ckpt_type
+        expected_loss_values = self._fetch_expected_loss_values(model_type)
+
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        tokenizer_path = Path(TOKENIZER_PATHS[model_type])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        # Config file needed for model conversion.
+        # Create a second copy for training resume
+        write_hf_ckpt_config(ckpt_dir)
+        write_hf_ckpt_config(tmpdir)
+
+        # Train for two epochs
+        cmd_1 = f"""
+        tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed \
+            --config {config} \
+            output_dir={tmpdir} \
+            checkpointer._component_={ckpt_component} \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type={model_type.upper()} \
+            tokenizer.path='{tokenizer_path}' \
+            tokenizer.prompt_template=null \
+            save_adapter_weights_only={save_adapter_weights_only} \
+            enable_activation_checkpointing=True \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS[model_type + "_lora"]
+
+        cmd_1 = cmd_1 + self._get_test_config_overrides() + model_config
+        monkeypatch.setattr(sys, "argv", cmd_1)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Resume training
+        cmd_2 = f"""
+        tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed \
+            --config {config} \
+            output_dir={tmpdir} \
+            checkpointer._component_={ckpt_component} \
+            checkpointer.checkpoint_dir={tmpdir} \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.adapter_checkpoint={os.path.join(tmpdir, "adapter_0.pt")}
+            checkpointer.recipe_checkpoint={os.path.join(tmpdir, "recipe_state.pt")}
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type={model_type.upper()} \
+            tokenizer.path='{tokenizer_path}' \
+            tokenizer.prompt_template=null \
+            resume_from_checkpoint=True \
+            metric_logger.filename={log_file} \
+            enable_activation_checkpointing=True \
+        """.split()
+
+        cmd_2 = cmd_2 + self._get_test_config_overrides() + model_config
+        monkeypatch.setattr(sys, "argv", cmd_2)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        expected_loss_values = self._fetch_expected_loss_values(model_type)[2:]
+
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-5, atol=1e-5
+        )
+
+    @pytest.mark.integration_test
+    @pytest.mark.parametrize(
+        "recipe_config, model_type, ckpt_type",
+        [
+            ("llama2/7B_lora", "llama2", "tune"),
+            ("llama3/8B_lora", "llama3", "tune"),
+        ],
+    )
+    @gpu_test(gpu_count=2)
+    def test_save_and_load_merged_weights(
+        self, recipe_config, model_type, ckpt_type, tmpdir, monkeypatch
+    ):
+        ckpt_component = CKPT_COMPONENT_MAP[ckpt_type]
+        ckpt = model_type + "_" + ckpt_type
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        tokenizer_path = Path(TOKENIZER_PATHS[model_type])
+        ckpt_dir = ckpt_path.parent
+        cmd = f"""
+        tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed \
+            --config {recipe_config} \
+            output_dir={tmpdir} \
+            model=torchtune.models.lora_small_test_model \
+            checkpointer._component_={ckpt_component} \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type={model_type.upper()} \
+            tokenizer.path='{tokenizer_path}' \
+            tokenizer.prompt_template=null \
+            enable_activation_checkpointing=True \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS[model_type + "_lora"]
+
+        cmd = cmd + self._get_test_config_overrides() + model_config
+        monkeypatch.setattr(sys, "argv", cmd)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Next load both the merged weights in a base model
+        # and the base model weights + trained adapter weights in the LoRA model
+        # The results of calling forward on dummy inputs should be the same.
+        inputs = torch.randint(low=0, high=32_000, size=(2, 100))
+
+        # Build LoRA model for loading base + adapter weights separately
+        lora_model = config.instantiate(OmegaConf.from_dotlist(model_config).model)
+
+        # Build base model for loading merged weights
+        base_config = MODEL_TEST_CONFIGS[model_type]
+        model = config.instantiate(OmegaConf.from_dotlist(base_config).model)
+
+        # Load base model and trained adapter weights into LoRA model and call fwd
+        with open(f"{tmpdir}/adapter_1.pt", "rb") as f:
+            lora_sd = torch.load(f, weights_only=True)
+        with open(ckpt_path, "rb") as f:
+            base_model_sd = torch.load(f, weights_only=True)
+        lora_model.load_state_dict(lora_sd, strict=False)
+        lora_model.load_state_dict(base_model_sd, strict=False)
+        baseline_out = lora_model(inputs)
+
+        # Load merged final ckpt directly into model and call fwd
+        with open(f"{tmpdir}/torchtune_model_1.pt", "rb") as f:
+            sd = torch.load(f, weights_only=True)
+        model.load_state_dict(sd)
+        merged_ckpt_out = model(inputs)
+
+        torch.testing.assert_close(baseline_out, merged_ckpt_out, rtol=1e-5, atol=1e-5)
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_lora_finetune_single_device.py marc/third_party/torchtune/tests/recipes/test_lora_finetune_single_device.py
--- marc_original/third_party/torchtune/tests/recipes/test_lora_finetune_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_lora_finetune_single_device.py	2025-02-20 17:49:29.634024384 -0500
@@ -0,0 +1,287 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+import torch
+from omegaconf import OmegaConf
+from tests.common import TUNE_PATH
+from tests.recipes.utils import (
+    CKPT_COMPONENT_MAP,
+    dummy_alpaca_dataset_config,
+    MODEL_TEST_CONFIGS,
+    write_hf_ckpt_config,
+)
+from tests.test_utils import (
+    CKPT_MODEL_PATHS,
+    gen_log_file_name,
+    get_loss_values_from_metric_logger,
+    TOKENIZER_PATHS,
+)
+from torchtune import config
+from torchtune.utils import torch_version_ge
+
+
+class TestLoRAFinetuneSingleDeviceRecipe:
+    def _get_test_config_overrides(self, dtype_str: str = "fp32", epochs: int = 2):
+        return [
+            "batch_size=8",
+            "device=cpu",
+            f"dtype={dtype_str}",
+            "dataset.train_on_input=False",
+            "seed=9",
+            f"epochs={epochs}",
+            "max_steps_per_epoch=2",
+            "optimizer.lr=2e-5",
+            "log_every_n_steps=1",
+            "gradient_accumulation_steps=1",
+            "clip_grad_norm=100",
+        ] + dummy_alpaca_dataset_config()
+
+    def _fetch_expected_loss_values(self, model_type):
+        loss_values_map = {
+            "llama2": [10.5209, 10.5269, 10.5130, 10.5242],
+            "llama3": [11.9838, 11.9691, 11.9616, 11.9383],
+        }
+        return loss_values_map[model_type]
+
+    def _fetch_qlora_expected_loss_values(self, dtype):
+        if dtype == "bf16":
+            return [10.5197, 10.5272, 10.5129, 10.5243]
+        return [10.5198, 10.5271, 10.5131, 10.5244]
+
+    @pytest.mark.integration_test
+    @pytest.mark.parametrize("compile", [True, False])
+    @pytest.mark.parametrize(
+        "config, model_type, ckpt_type",
+        [
+            ("llama2/7B_lora_single_device", "llama2", "meta"),
+            ("llama3/8B_lora_single_device", "llama3", "tune"),
+        ],
+    )
+    def test_loss(self, compile, config, model_type, ckpt_type, tmpdir, monkeypatch):
+        ckpt_component = CKPT_COMPONENT_MAP[ckpt_type]
+        ckpt = model_type + "_" + ckpt_type
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        tokenizer_path = Path(TOKENIZER_PATHS[model_type])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        cmd = f"""
+        tune run lora_finetune_single_device \
+            --config {config} \
+            output_dir={tmpdir} \
+            checkpointer._component_={ckpt_component} \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}] \
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type={model_type.upper()} \
+            tokenizer.path='{tokenizer_path}' \
+            tokenizer.prompt_template=null \
+            metric_logger.filename={log_file} \
+            compile={compile} \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS[model_type + "_lora"]
+
+        cmd = cmd + self._get_test_config_overrides(dtype_str="fp32") + model_config
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Make sure to clear compile state in between tests
+        if compile:
+            torch._dynamo.reset()
+
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        expected_loss_values = self._fetch_expected_loss_values(model_type)
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-5, atol=1e-5
+        )
+
+    @pytest.mark.integration_test
+    @pytest.mark.parametrize("dtype", ["fp32", "bf16"])
+    @pytest.mark.parametrize("compile", [True, False])
+    @pytest.mark.skipif(
+        not torch_version_ge("2.4.0"),
+        reason="Please install a nightly build of torch to run this test.",
+    )
+    def test_loss_qlora(self, compile, dtype, tmpdir, monkeypatch):
+        ckpt = "llama2_meta"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        cmd = f"""
+        tune run lora_finetune_single_device
+            --config llama2/7B_qlora_single_device \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelMetaCheckpointer
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            metric_logger.filename={log_file} \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            compile={compile} \
+            enable_activation_checkpointing=False \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama2_qlora"]
+
+        cmd = cmd + self._get_test_config_overrides(dtype_str=dtype) + model_config
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(SystemExit):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Make sure to clear compile state in between tests
+        if compile:
+            torch._dynamo.reset()
+
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        expected_loss_values = self._fetch_qlora_expected_loss_values(dtype=dtype)
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-4, atol=1e-4
+        )
+
+    @pytest.mark.parametrize("save_adapter_weights_only", [False, True])
+    @pytest.mark.integration_test
+    def test_training_state_on_resume(
+        self, tmpdir, monkeypatch, save_adapter_weights_only
+    ):
+        """Test whether the recipe state is correctly updated on resume. Since this
+        is model agnostic, we should run this on the small model only. The test
+        consists of three stages:
+            - Train a model for 2 epochs
+            - Resume training after epoch 1
+            - Make sure final loss matches the expected value of a model successfully resumed from a ckpt
+        """
+
+        ckpt = "llama2_hf"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        # Config file needed for model conversion.
+        # Create a second copy for training resume
+        write_hf_ckpt_config(ckpt_dir)
+        write_hf_ckpt_config(tmpdir)
+
+        # Train for two epochs
+        cmd_1 = f"""
+        tune run lora_finetune_single_device \
+            --config llama2/7B_lora_single_device \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            save_adapter_weights_only={save_adapter_weights_only} \
+            enable_activation_checkpointing=True \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama2_lora"]
+
+        cmd_1 = cmd_1 + self._get_test_config_overrides() + model_config
+        monkeypatch.setattr(sys, "argv", cmd_1)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Resume training
+        cmd_2 = f"""
+        tune run lora_finetune_single_device \
+            --config llama2/7B_lora_single_device \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir={tmpdir} \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.adapter_checkpoint={os.path.join(tmpdir, "adapter_0.pt")}
+            checkpointer.recipe_checkpoint={os.path.join(tmpdir, "recipe_state.pt")}
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            resume_from_checkpoint=True \
+            metric_logger.filename={log_file} \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            enable_activation_checkpointing=True \
+        """.split()
+        cmd_2 = cmd_2 + self._get_test_config_overrides(epochs=3) + model_config
+        monkeypatch.setattr(sys, "argv", cmd_2)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Second epoch only
+        expected_loss_values = self._fetch_expected_loss_values("llama2")[2:]
+        loss_values = get_loss_values_from_metric_logger(log_file)[:2]
+
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-5, atol=1e-5
+        )
+
+    @pytest.mark.integration_test
+    def test_save_and_load_merged_weights(self, tmpdir, monkeypatch):
+        ckpt = "llama2_tune"
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        ckpt_dir = ckpt_path.parent
+
+        cmd = f"""
+        tune run lora_finetune_single_device \
+            --config llama2/7B_lora_single_device \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tokenizer.prompt_template=null \
+            enable_activation_checkpointing=True \
+        """.split()
+
+        model_config = MODEL_TEST_CONFIGS["llama2_lora"]
+
+        cmd = cmd + self._get_test_config_overrides() + model_config
+        monkeypatch.setattr(sys, "argv", cmd)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        # Next load both the merged weights in a Llama2 base model
+        # and the base model weights + trained adapter weights in the LoRA Llama 2 model
+        # The results of calling forward on dummy inputs should be the same.
+        inputs = torch.randint(low=0, high=32_000, size=(2, 100))
+
+        # Build LoRA model for loading base + adapter weights separately
+        lora_model = config.instantiate(OmegaConf.from_dotlist(model_config).model)
+
+        # Build base llama2 model for loading merged weights
+        base_llama2_config = MODEL_TEST_CONFIGS["llama2"]
+        llama2_model = config.instantiate(
+            OmegaConf.from_dotlist(base_llama2_config).model
+        )
+
+        # Load base model and trained adapter weights into LoRA model and call fwd
+        with open(f"{tmpdir}/adapter_1.pt", "rb") as f:
+            lora_sd = torch.load(f, weights_only=True)
+        with open(ckpt_path, "rb") as f:
+            base_model_sd = torch.load(f, weights_only=True)
+        lora_model.load_state_dict(lora_sd, strict=False)
+        lora_model.load_state_dict(base_model_sd, strict=False)
+        baseline_out = lora_model(inputs)
+
+        # Load merged final ckpt directly into llama2 and call fwd
+        with open(f"{tmpdir}/torchtune_model_1.pt", "rb") as f:
+            sd = torch.load(f, weights_only=True)
+        llama2_model.load_state_dict(sd)
+        merged_ckpt_out = llama2_model(inputs)
+        torch.testing.assert_close(baseline_out, merged_ckpt_out, rtol=1e-5, atol=1e-5)
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_ppo_full_finetune_single_device.py marc/third_party/torchtune/tests/recipes/test_ppo_full_finetune_single_device.py
--- marc_original/third_party/torchtune/tests/recipes/test_ppo_full_finetune_single_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_ppo_full_finetune_single_device.py	2025-02-20 17:49:29.638024391 -0500
@@ -0,0 +1,378 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+import torch
+from tests.common import TUNE_PATH
+
+from tests.recipes.utils import (
+    dummy_text_completion_alpaca_dataset_config,
+    llama2_classifier_test_config,
+    llama2_test_config,
+    write_hf_ckpt_config,
+)
+from tests.test_utils import (
+    CKPT_MODEL_PATHS,
+    gen_log_file_name,
+    get_loss_values_from_metric_logger,
+    mps_ignored_test,
+)
+
+
+class TestPPOFullFinetuneSingleDeviceRecipe:
+    def _get_test_config_overrides(self):
+        return [
+            "batch_size=4",
+            "forward_batch_size=4",
+            "ppo_batch_size=4",
+            "ppo_epochs=1",
+            "num_steps=16",
+            "temperature=1.0",
+            "gradient_accumulation_steps=1",
+            "device=cpu",
+            "dtype=fp32",
+            "enable_activation_checkpointing=False",
+            "tokenizer.path=/tmp/test-artifacts/tokenizer.model",
+            "tokenizer._component_=torchtune.models.llama2.llama2_tokenizer",
+            "tokenizer.prompt_template=null",
+            "tokenizer.max_seq_len=64",
+            "seed=9",
+            "optimizer=torch.optim.AdamW",
+            "optimizer.lr=2e-5",
+            "log_every_n_steps=1",
+            "compile=False",
+        ] + dummy_text_completion_alpaca_dataset_config()
+
+    @pytest.mark.integration_test
+    @mps_ignored_test()
+    def test_loss(self, tmpdir, monkeypatch):
+
+        reward_ckpt = "llama2_reward_hf"
+        policy_ckpt = "llama2_hf"
+        reward_ckpt_path = Path(CKPT_MODEL_PATHS[reward_ckpt])
+        policy_ckpt_path = Path(CKPT_MODEL_PATHS[policy_ckpt])
+
+        ckpt_dir = policy_ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+        policy_tmpdir = (tmpdir / "policy").mkdir()
+        value_tmpdir = (tmpdir / "value").mkdir()
+
+        write_hf_ckpt_config(ckpt_dir)
+        cmd_1 = f"""
+        tune run ppo_full_finetune_single_device \
+            --config mistral/7B_full_ppo_low_memory \
+            output_dir={tmpdir} \
+            checkpointer._component_=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{policy_ckpt_path}]\
+            checkpointer.output_dir={policy_tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+
+            ref_policy_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            ref_policy_checkpointer.checkpoint_files=[{policy_ckpt_path}]\
+
+            value_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            value_checkpointer.checkpoint_files=[{reward_ckpt_path}]\
+            value_checkpointer.output_dir={value_tmpdir} \
+
+            reward_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            reward_checkpointer.checkpoint_files=[{reward_ckpt_path}]\
+
+            metric_logger._component_=torchtune.training.metric_logging.DiskLogger \
+            metric_logger.filename={log_file} \
+        """.split()
+
+        model_config = llama2_test_config()
+        model_config = [k.replace("model.", "policy_model.") for k in model_config]
+        model_config += ["policy_model.intermediate_dim=null"]
+
+        reward_and_value_model_config = llama2_classifier_test_config()
+        reward_and_value_model_config = [
+            k.replace("model.", "reward_and_value_model.")
+            for k in reward_and_value_model_config
+        ]
+        reward_and_value_model_config += [
+            "reward_and_value_model.intermediate_dim=null"
+        ]
+        cmd_1 = (
+            cmd_1
+            + self._get_test_config_overrides()
+            + model_config
+            + reward_and_value_model_config
+        )
+
+        monkeypatch.setattr(sys, "argv", cmd_1)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        expected_loss_values = [
+            1.0403,
+            0.9495,
+            0.9084,
+            1.0494,
+            0.9609,
+            0.8846,
+            1.0282,
+            0.9390,
+            0.8915,
+            1.0166,
+            0.9231,
+            0.9352,
+        ]
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, atol=1e-4, rtol=1e-5
+        )
+
+    @pytest.mark.integration_test
+    def test_training_state_on_resume(self, tmpdir, monkeypatch):
+        """Test whether the recipe state correctly saved and restored after training."""
+
+        reward_ckpt = "llama2_reward_hf"
+        policy_ckpt = "llama2_hf"
+        reward_ckpt_path = Path(CKPT_MODEL_PATHS[reward_ckpt])
+        policy_ckpt_path = Path(CKPT_MODEL_PATHS[policy_ckpt])
+
+        ckpt_dir = policy_ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+        policy_tmpdir = (tmpdir / "policy").mkdir()
+        value_tmpdir = (tmpdir / "value").mkdir()
+
+        # Config file needed for model conversion.
+        # Create a second copy for training resume
+        write_hf_ckpt_config(ckpt_dir)
+        write_hf_ckpt_config(policy_tmpdir)
+        write_hf_ckpt_config(value_tmpdir)
+        # There are 4 steps in total (num_steps / batch size)
+        # and the dataset has 8 samples, so each epoch will be 2 batches
+        # a single step is a single batch update, and we checkpoint at every epoch (2 steps)
+        # so we're expecting an intermediate checkpoint at step 2. The idea here is to train for 4 steps,
+        # resume after 2, and ensure the losses for the final two steps after resuming are identical
+        cmd_1 = f"""
+        tune run ppo_full_finetune_single_device \
+            --config mistral/7B_full_ppo_low_memory \
+            output_dir={tmpdir} \
+            checkpointer._component_=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{policy_ckpt_path}]\
+            checkpointer.output_dir={policy_tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+
+            ref_policy_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            ref_policy_checkpointer.checkpoint_files=[{policy_ckpt_path}]\
+
+            value_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            value_checkpointer.checkpoint_files=[{reward_ckpt_path}]\
+            value_checkpointer.output_dir={value_tmpdir} \
+
+            reward_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            reward_checkpointer.checkpoint_files=[{reward_ckpt_path}]\
+
+            metric_logger._component_=torchtune.training.metric_logging.DiskLogger \
+            metric_logger.filename={log_file} \
+        """.split()
+
+        model_config = llama2_test_config()
+        model_config = [k.replace("model.", "policy_model.") for k in model_config]
+        model_config += ["policy_model.intermediate_dim=null"]
+
+        reward_and_value_model_config = llama2_classifier_test_config()
+        reward_and_value_model_config = [
+            k.replace("model.", "reward_and_value_model.")
+            for k in reward_and_value_model_config
+        ]
+        reward_and_value_model_config += [
+            "reward_and_value_model.intermediate_dim=null"
+        ]
+        cmd_1 = (
+            cmd_1
+            + self._get_test_config_overrides()
+            + model_config
+            + reward_and_value_model_config
+        )
+
+        monkeypatch.setattr(sys, "argv", cmd_1)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        loss_values = get_loss_values_from_metric_logger(log_file)
+
+        # Resume training at step 2
+        resumed_log_dir = (tmpdir / "resumed/").mkdir()
+        resumed_log_file = gen_log_file_name(resumed_log_dir)
+        cmd_2 = f"""
+        tune run ppo_full_finetune_single_device \
+            --config mistral/7B_full_ppo_low_memory \
+            output_dir={tmpdir} \
+            checkpointer._component_=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir='{policy_tmpdir}' \
+            checkpointer.checkpoint_files=[{os.path.join(policy_tmpdir, "hf_model_0001_0.pt")}]\
+            checkpointer.recipe_checkpoint={os.path.join(policy_tmpdir, "recipe_state.pt")}\
+            checkpointer.output_dir={policy_tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+
+            ref_policy_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            ref_policy_checkpointer.checkpoint_files=[{policy_ckpt_path}]\
+
+            value_checkpointer.checkpoint_dir='{value_tmpdir}' \
+            value_checkpointer.checkpoint_files=[{os.path.join(value_tmpdir, "hf_model_0001_0.pt")}]\
+            value_checkpointer.output_dir={value_tmpdir} \
+
+            reward_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            reward_checkpointer.checkpoint_files=[{reward_ckpt_path}]\
+
+            resume_from_checkpoint=True \
+            metric_logger._component_=torchtune.training.metric_logging.DiskLogger \
+            metric_logger.filename={resumed_log_file} \
+        """.split()
+
+        cmd_2 = (
+            cmd_2
+            + self._get_test_config_overrides()
+            + model_config
+            + reward_and_value_model_config
+        )
+
+        monkeypatch.setattr(sys, "argv", cmd_2)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        resumed_loss_values = get_loss_values_from_metric_logger(resumed_log_file)
+
+        # losses at each step are (loss, policy_loss, value_loss)
+        torch.testing.assert_close(
+            loss_values[6:], resumed_loss_values, rtol=1e-4, atol=1e-4
+        )
+
+    @pytest.mark.integration_test
+    def test_training_state_on_resume_with_optimizer_in_bwd(self, tmpdir, monkeypatch):
+        """Test whether the recipe state correctly saves and restores optimizer state
+        when using ``optimizer_in_bwd``, since the optimizer checkpoint dict will include
+        parameters for two models.
+
+        This is identical to ``test_training_state_on_resume``, but adds optimizer_in_bwd.
+        """
+
+        reward_ckpt = "llama2_reward_hf"
+        policy_ckpt = "llama2_hf"
+        reward_ckpt_path = Path(CKPT_MODEL_PATHS[reward_ckpt])
+        policy_ckpt_path = Path(CKPT_MODEL_PATHS[policy_ckpt])
+
+        ckpt_dir = policy_ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+        policy_tmpdir = (tmpdir / "policy").mkdir()
+        value_tmpdir = (tmpdir / "value").mkdir()
+
+        # Config file needed for model conversion.
+        # Create a second copy for training resume
+        write_hf_ckpt_config(ckpt_dir)
+        write_hf_ckpt_config(policy_tmpdir)
+        write_hf_ckpt_config(value_tmpdir)
+        cmd_1 = f"""
+        tune run ppo_full_finetune_single_device \
+            --config mistral/7B_full_ppo_low_memory \
+            output_dir={tmpdir} \
+            checkpointer._component_=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{policy_ckpt_path}]\
+            checkpointer.output_dir={policy_tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+
+            ref_policy_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            ref_policy_checkpointer.checkpoint_files=[{policy_ckpt_path}]\
+
+            value_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            value_checkpointer.checkpoint_files=[{reward_ckpt_path}]\
+            value_checkpointer.output_dir={value_tmpdir} \
+
+            reward_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            reward_checkpointer.checkpoint_files=[{reward_ckpt_path}]\
+
+            metric_logger._component_=torchtune.training.metric_logging.DiskLogger \
+            metric_logger.filename={log_file} \
+
+            optimizer_in_bwd=True
+        """.split()
+
+        model_config = llama2_test_config()
+        model_config = [k.replace("model.", "policy_model.") for k in model_config]
+        model_config += ["policy_model.intermediate_dim=null"]
+
+        reward_and_value_model_config = llama2_classifier_test_config()
+        reward_and_value_model_config = [
+            k.replace("model.", "reward_and_value_model.")
+            for k in reward_and_value_model_config
+        ]
+        reward_and_value_model_config += [
+            "reward_and_value_model.intermediate_dim=null"
+        ]
+        cmd_1 = (
+            cmd_1
+            + self._get_test_config_overrides()
+            + model_config
+            + reward_and_value_model_config
+        )
+
+        monkeypatch.setattr(sys, "argv", cmd_1)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        loss_values = get_loss_values_from_metric_logger(log_file)
+
+        # Resume training at step 2
+        resumed_log_dir = (tmpdir / "resumed/").mkdir()
+        resumed_log_file = gen_log_file_name(resumed_log_dir)
+        cmd_2 = f"""
+        tune run ppo_full_finetune_single_device \
+            --config mistral/7B_full_ppo_low_memory \
+            output_dir={tmpdir} \
+            checkpointer._component_=torchtune.training.FullModelHFCheckpointer \
+            checkpointer.checkpoint_dir='{policy_tmpdir}' \
+            checkpointer.checkpoint_files=[{os.path.join(policy_tmpdir, "hf_model_0001_0.pt")}]\
+            checkpointer.recipe_checkpoint={os.path.join(policy_tmpdir, "recipe_state.pt")}\
+            checkpointer.output_dir={policy_tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+
+            ref_policy_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            ref_policy_checkpointer.checkpoint_files=[{policy_ckpt_path}]\
+
+            value_checkpointer.checkpoint_dir='{value_tmpdir}' \
+            value_checkpointer.checkpoint_files=[{os.path.join(value_tmpdir, "hf_model_0001_0.pt")}]\
+            value_checkpointer.output_dir={value_tmpdir} \
+
+            reward_checkpointer.checkpoint_dir='{ckpt_dir}' \
+            reward_checkpointer.checkpoint_files=[{reward_ckpt_path}]\
+
+            resume_from_checkpoint=True \
+            metric_logger._component_=torchtune.training.metric_logging.DiskLogger \
+            metric_logger.filename={resumed_log_file} \
+
+            optimizer_in_bwd=True
+        """.split()
+
+        cmd_2 = (
+            cmd_2
+            + self._get_test_config_overrides()
+            + model_config
+            + reward_and_value_model_config
+        )
+
+        monkeypatch.setattr(sys, "argv", cmd_2)
+        with pytest.raises(SystemExit, match=""):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        resumed_loss_values = get_loss_values_from_metric_logger(resumed_log_file)
+
+        # losses at each step are (loss, policy_loss, value_loss)
+        torch.testing.assert_close(
+            loss_values[6:], resumed_loss_values, rtol=1e-4, atol=1e-4
+        )
diff -ruN marc_original/third_party/torchtune/tests/recipes/test_qat_distributed.py marc/third_party/torchtune/tests/recipes/test_qat_distributed.py
--- marc_original/third_party/torchtune/tests/recipes/test_qat_distributed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/test_qat_distributed.py	2025-02-20 17:49:29.642024397 -0500
@@ -0,0 +1,99 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import runpy
+
+import sys
+from pathlib import Path
+
+import pytest
+import torch
+from tests.common import TUNE_PATH
+
+from tests.recipes.utils import (
+    CKPT_COMPONENT_MAP,
+    dummy_alpaca_dataset_config,
+    MODEL_TEST_CONFIGS,
+    write_hf_ckpt_config,
+)
+from tests.test_utils import (
+    CKPT_MODEL_PATHS,
+    gen_log_file_name,
+    get_loss_values_from_metric_logger,
+    gpu_test,
+    TOKENIZER_PATHS,
+)
+from torchao.utils import TORCH_VERSION_AFTER_2_4
+
+
+class TestQATDistributedRecipe:
+    def _get_test_config_overrides(self):
+        return [
+            "batch_size=4",
+            "dtype=fp32",
+            "enable_activation_checkpointing=False",
+            "dataset.train_on_input=False",
+            "seed=9",
+            "epochs=2",
+            "max_steps_per_epoch=2",
+            "optimizer=torch.optim.AdamW",
+            "optimizer.lr=2e-5",
+            "log_every_n_steps=1",
+        ] + dummy_alpaca_dataset_config()
+
+    def _fetch_expected_loss_values(self, model_type):
+        loss_values_map = {
+            "llama2": [10.5164, 10.4830, 10.5138, 10.5199],
+            "llama3": [12.0672, 11.9067, 11.9304, 11.9351],
+        }
+        return loss_values_map[model_type]
+
+    @pytest.mark.integration_test
+    @pytest.mark.parametrize(
+        "config, model_type, ckpt_type",
+        [
+            ("llama2/7B_qat_full", "llama2", "hf"),
+            ("llama3/8B_qat_full", "llama3", "tune"),
+        ],
+    )
+    @gpu_test(gpu_count=2)
+    @pytest.mark.skipif(
+        not TORCH_VERSION_AFTER_2_4, reason="QAT only supported for PyTorch 2.4+"
+    )
+    def test_loss(self, config, model_type, ckpt_type, tmpdir, monkeypatch):
+        ckpt_component = CKPT_COMPONENT_MAP[ckpt_type]
+        ckpt = model_type + "_" + ckpt_type
+        ckpt_path = Path(CKPT_MODEL_PATHS[ckpt])
+        tokenizer_path = Path(TOKENIZER_PATHS[model_type])
+        ckpt_dir = ckpt_path.parent
+        log_file = gen_log_file_name(tmpdir)
+
+        # Config file needed for model conversion.
+        write_hf_ckpt_config(ckpt_dir)
+
+        cmd = f"""
+        tune run --nnodes 1 --nproc_per_node 2 qat_distributed \
+            --config {config} \
+            output_dir={tmpdir} \
+            checkpointer._component_={ckpt_component} \
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type={model_type.upper()} \
+            tokenizer.path='{tokenizer_path}' \
+            tokenizer.prompt_template=null \
+            metric_logger.filename={log_file} \
+        """.split()
+        model_config = MODEL_TEST_CONFIGS[model_type]
+        cmd = cmd + self._get_test_config_overrides() + model_config
+
+        monkeypatch.setattr(sys, "argv", cmd)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+        loss_values = get_loss_values_from_metric_logger(log_file)
+        expected_loss_values = self._fetch_expected_loss_values(model_type)
+        torch.testing.assert_close(
+            loss_values, expected_loss_values, rtol=1e-3, atol=1e-3
+        )
diff -ruN marc_original/third_party/torchtune/tests/recipes/utils.py marc/third_party/torchtune/tests/recipes/utils.py
--- marc_original/third_party/torchtune/tests/recipes/utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/recipes/utils.py	2025-02-20 17:49:29.642024397 -0500
@@ -0,0 +1,225 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import json
+import os
+from pathlib import Path
+from typing import List
+
+import torch
+from torch.utils.data import Dataset
+
+CKPT_COMPONENT_MAP = {
+    "tune": "torchtune.training.FullModelTorchTuneCheckpointer",
+    "meta": "torchtune.training.FullModelMetaCheckpointer",
+    "hf": "torchtune.training.FullModelHFCheckpointer",
+}
+
+
+class DummyDataset(Dataset):
+    def __init__(self, *args, **kwargs):
+        self._data = torch.LongTensor(
+            [
+                [0, 2, 4, 2, 5, 6, 7, 8, 9, 1, 2, 4, 3, 3, 5, 6, 8, 2, 1, 1],
+                [1, 2, 5, 6, 7, 8, 2, 3, 1, 9, 9, 9, 5, 6, 7, 0, 0, 0, 1, 2],
+                [5, 6, 8, 2, 1, 0, 3, 4, 0, 0, 0, 2, 4, 7, 8, 8, 2, 2, 1, 0],
+                [4, 6, 7, 1, 0, 2, 0, 2, 0, 2, 3, 9, 9, 9, 7, 5, 1, 8, 4, 1],
+            ]
+        )
+        self._labels = torch.LongTensor(
+            [
+                [2, 6, 7, 8, 2, 2, 1, 0, 0, 1],
+                [1, 2, 5, 6, 7, 8, 2, 3, 1, 9],
+                [6, 1, 1, 2, 5, 0, 9, 0, 2, 1],
+                [5, 8, 6, 0, 2, 0, 0, 3, 2, 1],
+            ]
+        )
+
+    def __getitem__(self, index):
+        return {"tokens": self._data[index], "labels": self._labels[index]}
+
+    def __len__(self):
+        return len(self._data)
+
+
+def get_assets_path():
+    return Path(__file__).parent.parent / "assets"
+
+
+def dummy_stack_exchange_dataset_config():
+    data_files = os.path.join(get_assets_path(), "stack_exchange_paired_tiny.json")
+    out = [
+        "dataset._component_=torchtune.datasets.stack_exchange_paired_dataset",
+        "dataset.source='json'",
+        f"dataset.data_files={data_files}",
+        "dataset.split='train'",
+    ]
+    return out
+
+
+def dummy_alpaca_dataset_config():
+    data_files = os.path.join(get_assets_path(), "alpaca_tiny.json")
+    out = [
+        "dataset._component_=torchtune.datasets.alpaca_dataset",
+        "dataset.source='json'",
+        f"dataset.data_files={data_files}",
+        "dataset.split='train'",
+    ]
+    return out
+
+
+def dummy_text_completion_alpaca_dataset_config():
+    """
+    Constructs a minimal text-completion-style dataset from ``alpaca_tiny.json``.
+    This is used for testing PPO fine-tuning.
+    """
+    data_files = os.path.join(get_assets_path(), "alpaca_tiny.json")
+    out = [
+        "dataset._component_=torchtune.datasets.text_completion_dataset",
+        "dataset.source='json'",
+        f"dataset.data_files={data_files}",
+        "dataset.column='instruction'",
+        "dataset.split='train[:10%]'",  # 10% of the dataset gets us 8 batches
+        "dataset.add_eos=False",
+    ]
+    return out
+
+
+def llama2_test_config() -> List[str]:
+    return [
+        "model._component_=torchtune.models.llama2.llama2",
+        "model.vocab_size=32_000",
+        "model.num_layers=4",
+        "model.num_heads=16",
+        "model.embed_dim=256",
+        "model.max_seq_len=2048",
+        "model.norm_eps=1e-5",
+        "model.num_kv_heads=8",
+    ]
+
+
+def llama2_classifier_test_config() -> List[str]:
+    return [
+        "model._component_=torchtune.models.llama2.llama2_classifier",
+        "model.num_classes=1",
+        "model.vocab_size=32_000",
+        "model.num_layers=4",
+        "model.num_heads=16",
+        "model.embed_dim=256",
+        "model.max_seq_len=2048",
+        "model.norm_eps=1e-5",
+        "model.num_kv_heads=8",
+    ]
+
+
+def llama3_test_config() -> List[str]:
+    return [
+        "model._component_=torchtune.models.llama3.llama3",
+        "model.vocab_size=128_256",
+        "model.num_layers=2",
+        "model.num_heads=8",
+        "model.embed_dim=64",
+        "model.max_seq_len=1024",
+        "model.norm_eps=1e-5",
+        "model.num_kv_heads=4",
+    ]
+
+
+def lora_llama2_test_config(
+    lora_attn_modules,
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    quantize_base: bool = False,
+) -> List[str]:
+    return [
+        # Note: we explicitly use _component_ so that we can also call
+        # config.instantiate directly for easier comparison
+        "model._component_=torchtune.models.llama2.lora_llama2",
+        f"model.lora_attn_modules={lora_attn_modules}",
+        f"model.apply_lora_to_mlp={apply_lora_to_mlp}",
+        f"model.apply_lora_to_output={apply_lora_to_output}",
+        "model.vocab_size=32000",
+        "model.num_layers=4",
+        "model.num_heads=16",
+        "model.embed_dim=256",
+        "model.max_seq_len=2048",
+        "model.norm_eps=1e-5",
+        "model.num_kv_heads=8",
+        f"model.lora_rank={lora_rank}",
+        f"model.lora_alpha={lora_alpha}",
+        "model.lora_dropout=0.0",
+        f"model.quantize_base={quantize_base}",
+    ]
+
+
+def lora_llama3_test_config(
+    lora_attn_modules,
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    quantize_base: bool = False,
+) -> List[str]:
+    return [
+        # Note: we explicitly use _component_ so that we can also call
+        # config.instantiate directly for easier comparison
+        "model._component_=torchtune.models.llama3.lora_llama3",
+        f"model.lora_attn_modules={lora_attn_modules}",
+        f"model.apply_lora_to_mlp={apply_lora_to_mlp}",
+        f"model.apply_lora_to_output={apply_lora_to_output}",
+        "model.vocab_size=128_256",
+        "model.num_layers=2",
+        "model.num_heads=8",
+        "model.embed_dim=64",
+        "model.max_seq_len=1024",
+        "model.norm_eps=1e-5",
+        "model.num_kv_heads=4",
+        f"model.lora_rank={lora_rank}",
+        f"model.lora_alpha={lora_alpha}",
+        "model.lora_dropout=0.0",
+        f"model.quantize_base={quantize_base}",
+    ]
+
+
+def write_hf_ckpt_config(ckpt_dir: str):
+    config = {
+        "hidden_size": 256,
+        "num_attention_heads": 16,
+        "num_key_value_heads": 8,
+    }
+    config_file = Path.joinpath(Path(ckpt_dir), "config.json")
+    with config_file.open("w") as f:
+        json.dump(config, f)
+
+
+MODEL_TEST_CONFIGS = {
+    "llama2": llama2_test_config(),
+    "llama3": llama3_test_config(),
+    "llama2_lora": lora_llama2_test_config(
+        lora_attn_modules=["q_proj", "k_proj", "v_proj", "output_proj"],
+        apply_lora_to_mlp=False,
+        apply_lora_to_output=False,
+        lora_rank=8,
+        lora_alpha=16,
+    ),
+    "llama2_qlora": lora_llama2_test_config(
+        lora_attn_modules=["q_proj", "k_proj", "v_proj", "output_proj"],
+        apply_lora_to_mlp=True,
+        apply_lora_to_output=False,
+        lora_rank=8,
+        lora_alpha=16,
+        quantize_base=True,
+    ),
+    "llama3_lora": lora_llama3_test_config(
+        lora_attn_modules=["q_proj", "k_proj", "v_proj", "output_proj"],
+        apply_lora_to_mlp=False,
+        apply_lora_to_output=False,
+        lora_rank=8,
+        lora_alpha=16,
+    ),
+}
diff -ruN marc_original/third_party/torchtune/tests/regression_tests/test_llama2_7b.py marc/third_party/torchtune/tests/regression_tests/test_llama2_7b.py
--- marc_original/third_party/torchtune/tests/regression_tests/test_llama2_7b.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/regression_tests/test_llama2_7b.py	2025-02-20 17:49:29.646024403 -0500
@@ -0,0 +1,74 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import re
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+import torchtune
+from tests.common import TUNE_PATH
+from tests.test_utils import CKPT_MODEL_PATHS, gpu_test
+
+
+CKPT = "llama2_7b"
+
+# TODO: remove this once we have eval configs exposed properly
+pkg_path = Path(torchtune.__file__).parent.absolute()
+EVAL_CONFIG_PATH = Path.joinpath(
+    pkg_path, "_cli", "eval_configs", "default_eval_config.yaml"
+)
+
+
+@gpu_test(gpu_count=2)
+class TestLoRA7BDistributedFinetuneEval:
+    @pytest.mark.slow_integration_test
+    def test_finetune_and_eval(self, tmpdir, capsys, monkeypatch):
+
+        ckpt_path = Path(CKPT_MODEL_PATHS[CKPT])
+        ckpt_dir = ckpt_path.parent
+
+        # Run on prod LoRA FT config but with only 10 steps for now
+        ft_cmd = f"""
+        tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed
+            --config llama2/7B_lora \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer
+            checkpointer.checkpoint_dir='{ckpt_dir}' \
+            checkpointer.checkpoint_files=[{ckpt_path}]\
+            checkpointer.output_dir={tmpdir} \
+            checkpointer.model_type=LLAMA2 \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            max_steps_per_epoch=10 \
+        """.split()
+
+        monkeypatch.setattr(sys, "argv", ft_cmd)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+        eval_cmd = f"""
+        tune run eleuther_eval \
+            --config eleuther_evaluation \
+            output_dir={tmpdir} \
+            checkpointer=torchtune.training.FullModelTorchTuneCheckpointer \
+            checkpointer.checkpoint_dir='{tmpdir}' \
+            checkpointer.checkpoint_files=[torchtune_model_0.pt] \
+            checkpointer.output_dir={tmpdir} \
+            tokenizer.path=/tmp/test-artifacts/tokenizer.model \
+            tasks=['truthfulqa_mc2']
+            limit=10 \
+            device=cuda \
+        """.split()
+        monkeypatch.setattr(sys, "argv", eval_cmd)
+        with pytest.raises(SystemExit):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        out = capsys.readouterr().out
+        search_results = re.search(
+            r"acc(?:_norm)?\s*\|?\s*(?:\\s*\|?)?([\d.]+)", out.strip()
+        )
+        assert search_results is not None
+        acc_result = float(search_results.group(1))
+        assert acc_result >= 0.4
diff -ruN marc_original/third_party/torchtune/tests/test_import_recipes.py marc/third_party/torchtune/tests/test_import_recipes.py
--- marc_original/third_party/torchtune/tests/test_import_recipes.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/test_import_recipes.py	2025-02-20 17:49:29.650024410 -0500
@@ -0,0 +1,14 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+
+def test_import_recipes():
+    with pytest.raises(
+        ModuleNotFoundError, match="The torchtune recipes directory isn't a package"
+    ):
+        import recipes  # noqa
diff -ruN marc_original/third_party/torchtune/tests/test_utils.py marc/third_party/torchtune/tests/test_utils.py
--- marc_original/third_party/torchtune/tests/test_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/test_utils.py	2025-02-20 17:49:29.654024417 -0500
@@ -0,0 +1,372 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+import os
+import re
+import sys
+import unittest
+from contextlib import contextmanager
+from functools import partial
+from io import StringIO
+from pathlib import Path
+from typing import Any, Dict, Generator, List, Mapping, Optional, TextIO, Tuple, Union
+
+import pytest
+
+import torch
+from torch import nn
+from torchtune.data import Message, PromptTemplate, truncate
+from torchtune.modules.tokenizers import ModelTokenizer
+from torchtune.modules.transforms import Transform
+
+skip_if_cuda_not_available = unittest.skipIf(
+    not torch.cuda.is_available(), "CUDA is not available"
+)
+
+CKPT_MODEL_PATHS = {
+    "llama2_tune": "/tmp/test-artifacts/small-ckpt-tune-03082024.pt",
+    "llama2_meta": "/tmp/test-artifacts/small-ckpt-meta-03082024.pt",
+    "llama2_hf": "/tmp/test-artifacts/small-ckpt-hf-03082024.pt",
+    "llama2_reward_hf": "/tmp/test-artifacts/small-ckpt-hf-reward-07122024.pt",
+    "llama3_tune": "/tmp/test-artifacts/small-ckpt-tune-llama3-05052024.pt",
+    "llama2_7b": "/tmp/test-artifacts/llama2-7b-torchtune.pt",
+}
+
+TOKENIZER_PATHS = {
+    "llama2": "/tmp/test-artifacts/tokenizer.model",
+    "llama3": "/tmp/test-artifacts/tokenizer_llama3.model",
+}
+
+# Taken from Open-Orca/SlimOrca-Dedup on Hugging Face:
+# https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup
+CHAT_SAMPLE = {
+    "system": "You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",  # noqa: B950
+    "user": "Please briefly summarize this news article:\n\nAOL.com Video - Father Lets 8-Year-Old Drive On Icy Road\n\nDescription:Would you let your 8-year-old drive your car? How about on an icy road? Well one father in Russia did just that, and recorded the entire thing. To her credit, the child seemed to be doing a great job. (0:44)\n\nTags: 8-year-old driver , caught on camera , child driver , pix11\n\nSummary:",  # noqa: B950
+    "assistant": "A father in Russia allowed his 8-year-old child to drive his car on an icy road and recorded the event. The child appeared to be handling the situation well, showcasing their driving skills despite the challenging conditions.",  # noqa: B950
+}
+
+MESSAGE_SAMPLE_TRAIN_ON_INPUT = [
+    Message(
+        role="system",
+        content=CHAT_SAMPLE["system"],
+    ),
+    Message(
+        role="user",
+        content=CHAT_SAMPLE["user"],
+    ),
+    Message(
+        role="assistant",
+        content=CHAT_SAMPLE["assistant"],
+    ),
+]
+
+MESSAGE_SAMPLE = [
+    Message(role="system", content=CHAT_SAMPLE["system"], masked=True),
+    Message(role="user", content=CHAT_SAMPLE["user"], masked=True),
+    Message(
+        role="assistant",
+        content=CHAT_SAMPLE["assistant"],
+    ),
+]
+
+
+class DummyTokenizer(ModelTokenizer, Transform):
+    def __init__(self, max_seq_len: Optional[int] = None):
+        self.max_seq_len = max_seq_len
+
+    def encode(self, text, add_bos=True, add_eos=True, **kwargs) -> List[int]:
+        words = text.split()
+        tokens = [len(word) for word in words]
+        if add_bos:
+            tokens = [self.bos_id] + tokens
+        if add_eos:
+            tokens = tokens + [self.eos_id]
+        return tokens
+
+    def tokenize_messages(
+        self,
+        messages: List[Message],
+    ) -> Tuple[List[int], List[bool]]:
+        """
+        A simplified version of Llama2Tokenizer's ``tokenize_messages`` for testing purposes.
+        """
+        start_of_turn = True
+        end_of_turn = False
+        tokenized_messages = []
+        mask = []
+        for message in messages:
+            # If assistant message, this is the end of a turn
+            end_of_turn = message.role == "assistant"
+
+            # Prepend BOS on start of new turns
+            if start_of_turn:
+                tokenized_messages.append(self.bos_id)
+                mask.append(message.masked)
+
+            # Tokenize current message, append with masks
+            tokens = []
+            for item in message.content:
+                if item["type"] == "text":
+                    tokens = tokens + self.encode(
+                        item["content"],
+                        add_bos=False,
+                        add_eos=False,
+                    )
+                elif item["type"] == "image":
+                    tokens = tokens + [self.image_id]
+
+            tokenized_messages.extend(tokens)
+            mask.extend([message.masked] * len(tokens))
+
+            # If assistant message, append EOS at end
+            if end_of_turn:
+                tokenized_messages.append(self.eos_id)
+                mask.append(message.masked)
+                end_of_turn = False
+                start_of_turn = True
+            else:
+                start_of_turn = False
+
+            # Break out early if we reach max_seq_len
+            if self.max_seq_len and len(tokenized_messages) >= self.max_seq_len:
+                break
+
+        # Finally, truncate if necessary
+        if self.max_seq_len:
+            tokenized_messages = truncate(
+                tokenized_messages, self.max_seq_len, self.eos_id
+            )
+            mask = truncate(mask, self.max_seq_len, message.masked)
+
+        return tokenized_messages, mask
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        messages: List[Message] = sample.pop("messages")
+        images = []
+        for message in messages:
+            images += message.get_media()
+        tokens, mask = self.tokenize_messages(messages)
+        sample["tokens"] = tokens
+        sample["mask"] = mask
+        sample["images"] = images
+        return sample
+
+    @property
+    def eos_id(self):
+        return -1
+
+    @property
+    def bos_id(self):
+        return 0
+
+    @property
+    def image_id(self):
+        return -2
+
+
+class DummyChatFormat:
+
+    B_SYS, E_SYS = "System:\n", "\n"
+    B_INST, E_INST = "User:\n", "\nAssistant:\n"
+    B_ASST, E_ASST = "", ""
+    system = f"{B_SYS}{{content}}{E_SYS}"
+    user = f"{B_INST}{{content}}{E_INST}"
+    assistant = f"{B_ASST}{{content}}{E_ASST}"
+
+    @classmethod
+    def format(
+        cls,
+        messages,
+    ):
+        formats = {"system": cls.system, "user": cls.user, "assistant": cls.assistant}
+        formatted_dialogue = []
+        for message in messages:
+            content = formats.get(message.role).format(
+                content=message.content[0]["content"]
+            )
+            formatted_dialogue.append(
+                Message(role=message.role, content=content, masked=message.masked),
+            )
+        return formatted_dialogue
+
+
+DummyPromptTemplate = partial(
+    PromptTemplate,
+    template={
+        "system": ("System:\n", "\n"),
+        "user": ("User:\n", "\n"),
+        "assistant": ("Assistant:\n", "\n"),
+    },
+)
+
+
+def get_assets_path():
+    return Path(__file__).parent / "assets"
+
+
+def fixed_init_tensor(
+    shape: torch.Size,
+    min_val: Union[float, int] = 0.0,
+    max_val: Union[float, int] = 1.0,
+    nonlinear: bool = False,
+    dtype: torch.dtype = torch.float,
+):
+    """
+    Utility for generating deterministic tensors of a given shape. In general stuff
+    like torch.ones, torch.eye, etc can result in trivial outputs. This utility
+    generates a range tensor [min_val, max_val) of a specified dtype, applies
+    a sine function if nonlinear=True, then reshapes to the appropriate shape.
+    """
+    n_elements = math.prod(shape)
+    step_size = (max_val - min_val) / n_elements
+    x = torch.arange(min_val, max_val, step_size, dtype=dtype)
+    x = x.reshape(shape)
+    if nonlinear:
+        return torch.sin(x)
+    return x
+
+
+@torch.no_grad
+def fixed_init_model(
+    model: nn.Module,
+    min_val: Union[float, int] = 0.0,
+    max_val: Union[float, int] = 1.0,
+    nonlinear: bool = False,
+    dtype: Optional[torch.dtype] = None,
+):
+    """
+    This utility initializes all parameters of a model deterministically using the
+    function fixed_init_tensor above. See that docstring for details of each parameter.
+    """
+    for _, param in model.named_parameters():
+        param.copy_(
+            fixed_init_tensor(
+                param.shape,
+                min_val=min_val,
+                max_val=max_val,
+                nonlinear=nonlinear,
+                dtype=param.dtype if dtype is None else dtype,
+            )
+        )
+
+
+def assert_expected(
+    actual: Any,
+    expected: Any,
+    rtol: float = 1e-5,
+    atol: float = 1e-8,
+    check_device: bool = True,
+):
+    torch.testing.assert_close(
+        actual,
+        expected,
+        rtol=rtol,
+        atol=atol,
+        check_device=check_device,
+        msg=f"actual: {actual}, expected: {expected}",
+    )
+
+
+@contextmanager
+def single_box_init(init_pg: bool = True):
+    env_vars = ["MASTER_ADDR", "MASTER_PORT", "LOCAL_RANK", "RANK", "WORLD_SIZE"]
+    initial_os = {k: os.environ.get(k, None) for k in env_vars}
+    os.environ.get("MASTER_ADDR", None)
+    os.environ["MASTER_ADDR"] = "localhost"
+    # TODO: Don't hardcode ports as this could cause flakiness if tests execute
+    # in parallel.
+    os.environ["MASTER_PORT"] = str(12345)
+    os.environ["LOCAL_RANK"] = str(0)
+    os.environ["RANK"] = str(0)
+    os.environ["WORLD_SIZE"] = str(1)
+    if init_pg:
+        torch.distributed.init_process_group(
+            backend="gloo",
+            world_size=int(os.environ["WORLD_SIZE"]),
+            rank=int(os.environ["RANK"]),
+        )
+    try:
+        yield
+    finally:
+        if init_pg:
+            torch.distributed.destroy_process_group()
+        for k in env_vars:
+            if initial_os.get(k) is None:
+                del os.environ[k]
+            else:
+                os.environ[k] = initial_os[k]
+
+
+@contextmanager
+def set_dtype(dtype: torch.dtype) -> Generator[None, None, None]:
+    old_dtype = torch.get_default_dtype()
+    torch.set_default_dtype(dtype)
+    try:
+        yield
+    finally:
+        torch.set_default_dtype(old_dtype)
+
+
+@contextmanager
+def captured_output() -> Generator[Tuple[TextIO, TextIO], None, None]:
+    new_out, new_err = StringIO(), StringIO()
+    old_out, old_err = sys.stdout, sys.stderr
+    try:
+        sys.stdout, sys.stderr = new_out, new_err
+        yield sys.stdout, sys.stderr
+    finally:
+        sys.stdout, sys.stderr = old_out, old_err
+
+
+def gpu_test(gpu_count: int = 1):
+    """
+    Annotation for GPU tests, skipping the test if the
+    required amount of GPU is not available
+    """
+    message = f"Not enough GPUs to run the test: requires {gpu_count}"
+    local_gpu_count: int = torch.cuda.device_count()
+    return pytest.mark.skipif(local_gpu_count < gpu_count, reason=message)
+
+
+def get_loss_values_from_metric_logger(log_file_path: str) -> Dict[str, float]:
+    """
+    Given an output directory containing metric logger .txt file,
+    parse the .txt and return a list of losses from each logged iteration.
+    """
+    with open(log_file_path, "r") as f:
+        logs = f.read()
+    losses = [float(x) for x in re.findall(r"loss:(\d+\.\d+)", logs)]
+    return losses
+
+
+def gen_log_file_name(tmpdir, suffix: Optional[str] = None) -> str:
+    """
+    Take the tmpdir and just append a non-path version of it as the
+    filename, optionally adding specified suffix. This is used to
+    write metric logs to a deterministic file per test run.
+    E.g. /tmp/my/dir -> /tmp/my/dir/tmpmydir.txt
+    """
+    filename = str(tmpdir) + str(tmpdir).replace("/", "")
+    if suffix:
+        filename += suffix
+    filename += ".txt"
+    return filename
+
+
+def assert_dialogue_equal(actual, expected):
+    assert len(actual) == len(expected)
+    for i in range(len(actual)):
+        assert actual[i].role == expected[i].role
+        assert actual[i].text_content == expected[i].text_content
+
+
+def mps_ignored_test() -> bool:
+    return pytest.mark.skipif(
+        torch.backends.mps.is_available() and torch.backends.mps.is_built(),
+        reason="Test skipped due to torch being compiled with MPS"
+        "see https://github.com/pytorch/torchtune/issues/1707 for more information",
+    )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/_cli/__init__.py marc/third_party/torchtune/tests/torchtune/_cli/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/_cli/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/_cli/__init__.py	2025-02-20 17:49:29.662024430 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/_cli/test_cp.py marc/third_party/torchtune/tests/torchtune/_cli/test_cp.py
--- marc_original/third_party/torchtune/tests/torchtune/_cli/test_cp.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/_cli/test_cp.py	2025-02-20 17:49:29.666024437 -0500
@@ -0,0 +1,127 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+from tests.common import TUNE_PATH
+
+
+class TestTuneCLIWithCopyScript:
+    """This class tests the `tune cp` command."""
+
+    @pytest.mark.parametrize("already_exists", (True, False))
+    def test_copy_successful(self, capsys, monkeypatch, tmpdir, already_exists):
+        tmpdir_path = Path(tmpdir)
+        dest = tmpdir_path / "my_custom_finetune.yaml"
+
+        if already_exists:
+            dest.touch()
+
+        args = f"tune cp llama2/7B_full {dest}".split()
+
+        monkeypatch.setattr(sys, "argv", args)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        captured = capsys.readouterr()
+        out = captured.out.rstrip("\n")
+
+        assert dest.exists(), f"Expected {dest} to exist"
+        assert f"Copied file to {dest}" in out
+
+    def test_copy_successful_with_cwd_as_path(self, capsys, monkeypatch, tmpdir):
+        tmpdir_path = Path(tmpdir)
+
+        # Needed so we can run test from tmpdir
+        tune_path_as_absolute = Path(TUNE_PATH).absolute()
+
+        # Change cwd to tmpdir
+        monkeypatch.chdir(tmpdir_path)
+
+        args = "tune cp llama2/7B_full .".split()
+        monkeypatch.setattr(sys, "argv", args)
+        runpy.run_path(str(tune_path_as_absolute), run_name="__main__")
+
+        captured = capsys.readouterr()
+        out = captured.out.rstrip("\n")
+
+        dest = tmpdir_path / "7B_full.yaml"
+
+        assert dest.exists()
+        assert "Copied file to ./7B_full.yaml" in out
+
+    def test_copy_skips_when_dest_already_exists_and_no_clobber_is_true(
+        self, capsys, monkeypatch, tmpdir
+    ):
+        tmpdir_path = Path(tmpdir)
+        existing_file = tmpdir_path / "existing_file.yaml"
+        existing_file.touch()
+
+        args = f"tune cp llama2/7B_full_low_memory {existing_file} -n".split()
+
+        monkeypatch.setattr(sys, "argv", args)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        captured = capsys.readouterr()
+        out = captured.out.rstrip("\n")
+        err = captured.err.rstrip("\n")
+
+        assert err == ""
+        assert "not overwriting" in out
+
+    def test_adds_correct_suffix_to_dest_when_no_suffix_is_provided(
+        self, capsys, monkeypatch, tmpdir
+    ):
+        tmpdir_path = Path(tmpdir)
+        dest = tmpdir_path / "my_custom_finetune"
+
+        args = f"tune cp llama2/7B_full_low_memory {dest}".split()
+
+        monkeypatch.setattr(sys, "argv", args)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        captured = capsys.readouterr()
+        out = captured.out.rstrip("\n")
+
+        assert dest.with_suffix(".yaml").exists(), f"Expected {dest} to exist"
+        assert f"Copied file to {dest}.yaml" in out
+
+    @pytest.mark.parametrize(
+        "tune_command,expected_error_message",
+        [
+            (
+                "tune cp non_existent_recipe .",
+                "error: Invalid file name: non_existent_recipe. Try `tune ls` to see all available files to copy.",
+            ),
+            (
+                "tune cp non_existent_config .",
+                "error: Invalid file name: non_existent_config. Try `tune ls` to see all available files to copy.",
+            ),
+            (
+                "tune cp full_finetune_single_device /home/mr_bean/full_finetune_single_device.py",
+                "error: Cannot create regular file: '/home/mr_bean/full_finetune_single_device.py'. No such file or directory.",
+            ),
+            (
+                "tune cp",
+                "error: the following arguments are required: file, destination",
+            ),
+        ],
+    )
+    def test_copy_fails_when_given_invalid_recipe(
+        self, capsys, monkeypatch, tune_command, expected_error_message
+    ):
+        args = tune_command.split()
+
+        monkeypatch.setattr(sys, "argv", args)
+        with pytest.raises(SystemExit):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        captured = capsys.readouterr()
+        err = captured.err.rstrip("\n")
+
+        assert expected_error_message in err
diff -ruN marc_original/third_party/torchtune/tests/torchtune/_cli/test_download.py marc/third_party/torchtune/tests/torchtune/_cli/test_download.py
--- marc_original/third_party/torchtune/tests/torchtune/_cli/test_download.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/_cli/test_download.py	2025-02-20 17:49:29.670024444 -0500
@@ -0,0 +1,108 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import runpy
+import sys
+
+import pytest
+from tests.common import TUNE_PATH
+
+
+class TestTuneDownloadCommand:
+    """This class tests the `tune download` command."""
+
+    @pytest.fixture
+    def snapshot_download(self, mocker, tmpdir):
+
+        from huggingface_hub.utils import GatedRepoError, RepositoryNotFoundError
+
+        yield mocker.patch(
+            "torchtune._cli.download.snapshot_download",
+            return_value=tmpdir,
+            # Side effects are iterated through on each call
+            side_effect=[
+                GatedRepoError("test"),
+                RepositoryNotFoundError("test"),
+                mocker.DEFAULT,
+            ],
+        )
+
+    def test_download_calls_snapshot(self, capsys, monkeypatch, snapshot_download):
+        model = "meta-llama/Llama-2-7b"
+        testargs = f"tune download {model}".split()
+        monkeypatch.setattr(sys, "argv", testargs)
+
+        # Call the first time and get GatedRepoError
+        with pytest.raises(SystemExit, match="2"):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+        out_err = capsys.readouterr()
+        assert (
+            "Ignoring files matching the following patterns: *.safetensors"
+            in out_err.out
+        )
+        assert (
+            "It looks like you are trying to access a gated repository." in out_err.err
+        )
+
+        # Call the second time and get RepositoryNotFoundError
+        with pytest.raises(SystemExit, match="2"):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+        out_err = capsys.readouterr()
+        assert (
+            "Ignoring files matching the following patterns: *.safetensors"
+            in out_err.out
+        )
+        assert "not found on the Hugging Face Hub" in out_err.err
+
+        # Call the third time and get the expected output
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+        output = capsys.readouterr().out
+        assert "Ignoring files matching the following patterns: *.safetensors" in output
+        assert "Successfully downloaded model repo" in output
+
+        # Make sure it was called twice
+        assert snapshot_download.call_count == 3
+
+    # GatedRepoError without --hf-token (expect prompt for token)
+    def test_gated_repo_error_no_token(self, capsys, monkeypatch, snapshot_download):
+        model = "meta-llama/Llama-2-7b"
+        testargs = f"tune download {model}".split()
+        monkeypatch.setattr(sys, "argv", testargs)
+
+        # Expect GatedRepoError without --hf-token provided
+        with pytest.raises(SystemExit, match="2"):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        out_err = capsys.readouterr()
+        # Check that error message prompts for --hf-token
+        assert (
+            "It looks like you are trying to access a gated repository." in out_err.err
+        )
+        assert (
+            "Please ensure you have access to the repository and have provided the proper Hugging Face API token"
+            in out_err.err
+        )
+
+    # GatedRepoError with --hf-token (should not ask for token)
+    def test_gated_repo_error_with_token(self, capsys, monkeypatch, snapshot_download):
+        model = "meta-llama/Llama-2-7b"
+        testargs = f"tune download {model} --hf-token valid_token".split()
+        monkeypatch.setattr(sys, "argv", testargs)
+
+        # Expect GatedRepoError with --hf-token provided
+        with pytest.raises(SystemExit, match="2"):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        out_err = capsys.readouterr()
+        # Check that error message does not prompt for --hf-token again
+        assert (
+            "It looks like you are trying to access a gated repository." in out_err.err
+        )
+        assert "Please ensure you have access to the repository." in out_err.err
+        assert (
+            "Please ensure you have access to the repository and have provided the proper Hugging Face API token"
+            not in out_err.err
+        )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/_cli/test_ls.py marc/third_party/torchtune/tests/torchtune/_cli/test_ls.py
--- marc_original/third_party/torchtune/tests/torchtune/_cli/test_ls.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/_cli/test_ls.py	2025-02-20 17:49:29.674024449 -0500
@@ -0,0 +1,29 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import runpy
+import sys
+
+from tests.common import TUNE_PATH
+
+from torchtune._recipe_registry import get_all_recipes
+
+
+class TestTuneListCommand:
+    """This class tests the `tune ls` command."""
+
+    def test_ls_lists_all_recipes_and_configs(self, capsys, monkeypatch):
+        testargs = "tune ls".split()
+
+        monkeypatch.setattr(sys, "argv", testargs)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        captured = capsys.readouterr()
+        output = captured.out.rstrip("\n")
+
+        for recipe in get_all_recipes():
+            assert recipe.name in output
+            for config in recipe.configs:
+                assert config.name in output
diff -ruN marc_original/third_party/torchtune/tests/torchtune/_cli/test_run.py marc/third_party/torchtune/tests/torchtune/_cli/test_run.py
--- marc_original/third_party/torchtune/tests/torchtune/_cli/test_run.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/_cli/test_run.py	2025-02-20 17:49:29.678024456 -0500
@@ -0,0 +1,75 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import runpy
+import sys
+
+import pytest
+
+from tests.common import TUNE_PATH
+
+
+class TestTuneRunCommand:
+    def test_run_calls_distributed_run_for_distributed_recipe(
+        self, capsys, monkeypatch, mocker
+    ):
+        testargs = "tune run --nproc_per_node 4 full_finetune_distributed --config llama2/7B_full".split()
+
+        monkeypatch.setattr(sys, "argv", testargs)
+        distributed_run = mocker.patch("torchtune._cli.tune.Run._run_distributed")
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+        distributed_run.assert_called_once()
+
+    def test_run_calls_single_device_run_for_single_device_recipe(
+        self, capsys, monkeypatch, mocker
+    ):
+        testargs = "tune run full_finetune_single_device --config llama2/7B_full_single_device".split()
+
+        monkeypatch.setattr(sys, "argv", testargs)
+        single_device_run = mocker.patch("torchtune._cli.tune.Run._run_single_device")
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+        single_device_run.assert_called_once()
+
+    def test_run_fails_when_called_with_distributed_args_for_single_device_recipe(
+        self, capsys, monkeypatch
+    ):
+        testargs = "tune run --nproc_per_node 4 full_finetune_single_device --config llama2/7B_full_single_device".split()
+
+        monkeypatch.setattr(sys, "argv", testargs)
+        with pytest.raises(SystemExit, match="2"):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        output = capsys.readouterr()
+        assert "does not support distributed training" in output.err
+
+    def test_run_fails_when_config_not_passed_in(self, capsys, monkeypatch):
+        testargs = "tune run full_finetune_single_device batch_size=3".split()
+
+        monkeypatch.setattr(sys, "argv", testargs)
+        with pytest.raises(SystemExit, match="2"):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        output = capsys.readouterr()
+        assert "The '--config' argument is required" in output.err
+
+    def test_run_succeeds_with_local_recipe_file_and_default_config(
+        self, capsys, monkeypatch, mocker
+    ):
+        testargs = "tune run my_custom_recipe.py --config llama2/7B_full".split()
+        monkeypatch.setattr(sys, "argv", testargs)
+        local_file_run = mocker.patch("torchtune._cli.tune.Run._run_single_device")
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+        local_file_run.assert_called_once()
+
+    def test_run_calls_local_file_run_for_local_file_recipe(
+        self, capsys, monkeypatch, mocker
+    ):
+        testargs = "tune run my_custom_recipe.py --config custom_config.yaml".split()
+
+        monkeypatch.setattr(sys, "argv", testargs)
+        local_file_run = mocker.patch("torchtune._cli.tune.Run._run_single_device")
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+        local_file_run.assert_called_once()
diff -ruN marc_original/third_party/torchtune/tests/torchtune/_cli/test_tune.py marc/third_party/torchtune/tests/torchtune/_cli/test_tune.py
--- marc_original/third_party/torchtune/tests/torchtune/_cli/test_tune.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/_cli/test_tune.py	2025-02-20 17:49:29.682024463 -0500
@@ -0,0 +1,24 @@
+#!/usr/bin/env python3
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import runpy
+import sys
+
+from tests.common import TUNE_PATH
+
+
+class TestTuneCLI:
+    def test_tune_without_args_returns_help(self, capsys, monkeypatch):
+        testargs = ["tune"]
+
+        monkeypatch.setattr(sys, "argv", testargs)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        captured = capsys.readouterr()
+        output = captured.out.rstrip("\n")
+
+        assert "Welcome to the torchtune CLI!" in output
diff -ruN marc_original/third_party/torchtune/tests/torchtune/_cli/test_validate.py marc/third_party/torchtune/tests/torchtune/_cli/test_validate.py
--- marc_original/third_party/torchtune/tests/torchtune/_cli/test_validate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/_cli/test_validate.py	2025-02-20 17:49:29.682024463 -0500
@@ -0,0 +1,41 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import runpy
+import sys
+
+import pytest
+from tests.common import ASSETS, TUNE_PATH
+
+
+class TestTuneValidateCommand:
+    """This class tests the `tune validate` command."""
+
+    VALID_CONFIG_PATH = ASSETS / "valid_dummy_config.yaml"
+    INVALID_CONFIG_PATH = ASSETS / "invalid_dummy_config.yaml"
+
+    def test_validate_good_config(self, capsys, monkeypatch):
+        args = f"tune validate {self.VALID_CONFIG_PATH}".split()
+
+        monkeypatch.setattr(sys, "argv", args)
+        runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        captured = capsys.readouterr()
+        out = captured.out.rstrip("\n")
+
+        assert out == "Config is well-formed!"
+
+    def test_validate_bad_config(self, monkeypatch, capsys):
+        args = f"tune validate {self.INVALID_CONFIG_PATH}".split()
+
+        monkeypatch.setattr(sys, "argv", args)
+        with pytest.raises(SystemExit):
+            runpy.run_path(TUNE_PATH, run_name="__main__")
+
+        captured = capsys.readouterr()
+        err = captured.err.rstrip("\n")
+
+        assert "got an unexpected keyword argument 'dummy'" in err
diff -ruN marc_original/third_party/torchtune/tests/torchtune/config/test_config_utils.py marc/third_party/torchtune/tests/torchtune/config/test_config_utils.py
--- marc_original/third_party/torchtune/tests/torchtune/config/test_config_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/config/test_config_utils.py	2025-02-20 17:49:29.690024476 -0500
@@ -0,0 +1,189 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import copy
+import logging
+from io import StringIO
+from unittest import mock
+
+import pytest
+from omegaconf import OmegaConf
+from torchtune.config._parse import TuneRecipeArgumentParser
+from torchtune.config._utils import (
+    _get_component_from_path,
+    _merge_yaml_and_cli_args,
+    _remove_key_by_dotpath,
+    InstantiationError,
+    log_config,
+)
+
+_CONFIG = {
+    "a": 1,
+    "b": {
+        "_component_": 2,
+        "c": 3,
+    },
+    "d": 4,
+    "f": 8,
+}
+
+
+class TestUtils:
+    def test_get_component_from_path(self):
+        good_paths = [
+            "torchtune",  # Test single module without dot
+            "torchtune.models",  # Test dotpath for a module
+            "torchtune.models.llama2.llama2_7b",  # Test dotpath for an object
+        ]
+        for path in good_paths:
+            _ = _get_component_from_path(path)
+
+        # Test that a relative path fails
+        with pytest.raises(ValueError, match="Relative imports are not supported"):
+            _ = _get_component_from_path(".test")
+        # Test that a non-existent path fails
+        with pytest.raises(
+            InstantiationError, match="Error loading 'torchtune.models.dummy'"
+        ):
+            _ = _get_component_from_path("torchtune.models.dummy")
+
+    @mock.patch("torchtune.config._parse.OmegaConf.load", return_value=_CONFIG)
+    def test_merge_yaml_and_cli_args(self, mock_load):
+        parser = TuneRecipeArgumentParser("test parser")
+        yaml_args, cli_args = parser.parse_known_args(
+            [
+                "--config",
+                "test.yaml",
+                "b.c=4",  # Test overriding a flat param in a component
+                "b=5",  # Test overriding component path
+                "b.b.c=6",  # Test nested dotpath
+                "d=6",  # Test overriding a flat param
+                "e=7",  # Test adding a new param
+                "~f",  # Test removing a param
+            ]
+        )
+        conf = _merge_yaml_and_cli_args(yaml_args, cli_args)
+        assert conf.a == 1, f"a == {conf.a}, not 1 as set in the config."
+        assert (
+            conf.b._component_ == 5
+        ), f"b == {conf.b._component_}, not 5 as set in overrides."
+        assert conf.b.c == 4, f"b.c == {conf.b.c}, not 4 as set in overrides."
+        assert conf.b.b.c == 6, f"b.b.c == {conf.b.b.c}, not 6 as set in overrides."
+        assert conf.d == 6, f"d == {conf.d}, not 6 as set in overrides."
+        assert conf.e == 7, f"e == {conf.e}, not 7 as set in overrides."
+        assert "f" not in conf, f"f == {conf.f}, not removed as set in overrides."
+        mock_load.assert_called_once()
+
+        yaml_args, cli_args = parser.parse_known_args(
+            [
+                "--config",
+                "test.yaml",
+                "b=5",  # Test overriding component path but keeping other kwargs
+            ]
+        )
+        conf = _merge_yaml_and_cli_args(yaml_args, cli_args)
+        assert (
+            conf.b._component_ == 5
+        ), f"b == {conf.b._component_}, not 5 as set in overrides."
+        assert conf.b.c == 3, f"b.c == {conf.b.c}, not 3 as set in the config."
+        assert mock_load.call_count == 2
+
+        yaml_args, cli_args = parser.parse_known_args(
+            [
+                "--config",
+                "test.yaml",
+                "b.c=5",  # Test overriding kwarg but keeping component path
+            ]
+        )
+        conf = _merge_yaml_and_cli_args(yaml_args, cli_args)
+        assert (
+            conf.b._component_ == 2
+        ), f"b == {conf.b._component_}, not 2 as set in the config."
+        assert conf.b.c == 5, f"b.c == {conf.b.c}, not 5 as set in overrides."
+        assert mock_load.call_count == 3
+
+        yaml_args, cli_args = parser.parse_known_args(
+            [
+                "--config",
+                "test.yaml",
+                "b",  # Test invalid override
+            ]
+        )
+        with pytest.raises(
+            ValueError, match="Command-line overrides must be in the form of key=value"
+        ):
+            _ = _merge_yaml_and_cli_args(yaml_args, cli_args)
+
+    def test_log_config(self, capsys):
+        cfg = OmegaConf.create({"test": {"a": 1, "b": 2}})
+
+        # Create a logger and add a StreamHandler to it so we can patch the
+        # config logger and assert on logged strings
+        logger = logging.getLogger(__name__)
+        logger.setLevel("DEBUG")
+        stream = StringIO()
+        handler = logging.StreamHandler(stream)
+        logger.addHandler(handler)
+
+        with mock.patch(
+            "torchtune.config._utils.get_logger", return_value=logger
+        ), mock.patch(
+            "torchtune.utils._logging.dist.is_available", return_value=True
+        ), mock.patch(
+            "torchtune.utils._logging.dist.is_initialized", return_value=True
+        ):
+            # Make sure rank 0 logs as expected
+            with mock.patch(
+                "torchtune.utils._logging.dist.get_rank",
+                return_value=0,
+            ):
+                log_config("test", cfg)
+                output = stream.getvalue().strip()
+                assert (
+                    "Running test with resolved config:\n\ntest:\n  a: 1\n  b: 2"
+                    in output
+                )
+
+            # Clear the stream
+            stream.truncate(0)
+            stream.seek(0)
+
+            # Make sure all other ranks do not log anything
+            with mock.patch(
+                "torchtune.utils._logging.dist.get_rank",
+                return_value=1,
+            ):
+                log_config("test", cfg)
+                output = stream.getvalue().strip()
+                assert not output
+
+    def test_remove_key_by_dotpath(self):
+        # Test removing a component raises
+        cfg = copy.deepcopy(_CONFIG)
+        with pytest.raises(
+            ValueError, match="Removing components from CLI is not supported"
+        ):
+            _remove_key_by_dotpath(cfg, "b")
+
+        # Test removing a top-level param
+        cfg = copy.deepcopy(_CONFIG)
+        _remove_key_by_dotpath(cfg, "a")
+        assert "a" not in cfg
+
+        # Test removing a component param
+        cfg = copy.deepcopy(_CONFIG)
+        _remove_key_by_dotpath(cfg, "b.c")
+        assert "c" not in cfg["b"]
+
+        # Test removing nested one level too deep fails
+        cfg = copy.deepcopy(_CONFIG)
+        with pytest.raises(TypeError, match="'int' object is not subscriptable"):
+            _remove_key_by_dotpath(cfg, "b.c.d")
+
+        # Test removing non-existent param fails
+        cfg = copy.deepcopy(_CONFIG)
+        with pytest.raises(KeyError, match="'g'"):
+            _remove_key_by_dotpath(cfg, "g")
diff -ruN marc_original/third_party/torchtune/tests/torchtune/config/test_instantiate.py marc/third_party/torchtune/tests/torchtune/config/test_instantiate.py
--- marc_original/third_party/torchtune/tests/torchtune/config/test_instantiate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/config/test_instantiate.py	2025-02-20 17:49:29.690024476 -0500
@@ -0,0 +1,94 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from pathlib import Path
+from textwrap import dedent
+
+import pytest
+from omegaconf import OmegaConf
+from torchtune.config._errors import InstantiationError
+from torchtune.config._instantiate import (
+    _create_component,
+    _instantiate_node,
+    instantiate,
+)
+from torchtune.config._utils import _has_component
+from torchtune.modules import RMSNorm
+
+
+class TestInstantiate:
+    @pytest.fixture
+    def config(self):
+        s = """
+        a: b
+        b: c
+        test:
+          _component_: torchtune.modules.RMSNorm
+          dim: 5
+        """
+        return OmegaConf.create(s)
+
+    @pytest.fixture
+    def module(self):
+        return RMSNorm(dim=5, eps=1e-4)
+
+    def get_dim(self, rms_norm: RMSNorm):
+        return rms_norm.scale.shape[0]
+
+    def test_has_path(self, config):
+        assert _has_component(config.test)
+        assert not _has_component(config.a)
+
+    def test_call_object(self, module):
+        obj = RMSNorm
+        args = (5,)
+        kwargs = {"eps": 1e-4}
+        actual = _create_component(obj, args, kwargs)
+        expected = module
+        assert isinstance(actual, RMSNorm)
+        assert self.get_dim(actual) == self.get_dim(expected)
+        assert actual.eps == expected.eps
+
+    def test_instantiate_node(self, config, module):
+        actual = _instantiate_node(config.test)
+        expected = module
+        assert isinstance(actual, RMSNorm)
+        assert self.get_dim(actual) == self.get_dim(expected)
+
+        with pytest.raises(
+            InstantiationError, match="Cannot instantiate specified object"
+        ):
+            _ = _instantiate_node(config.a)
+
+    def test_instantiate(self, config, module):
+        actual = instantiate(config.test)
+        expected = module
+        assert isinstance(actual, RMSNorm)
+        assert self.get_dim(actual) == self.get_dim(expected)
+
+        # Test passing in kwargs
+        actual = instantiate(config.test, eps=1e-4)
+        assert actual.eps == expected.eps
+
+        # Test passing in positional args
+        del config.test.dim
+        actual = instantiate(config.test, 3)
+        assert self.get_dim(actual) == 3
+
+    def test_tokenizer_config_with_null(self):
+        assets = Path(__file__).parent.parent.parent / "assets"
+        s = dedent(
+            f"""\
+        tokenizer:
+          _component_: torchtune.models.llama2.llama2_tokenizer
+          max_seq_len: null
+          path: {assets / 'm.model'}
+        """
+        )
+        config = OmegaConf.create(s)
+
+        tokenizer = instantiate(config.tokenizer)
+        assert tokenizer.max_seq_len is None
diff -ruN marc_original/third_party/torchtune/tests/torchtune/config/test_parse.py marc/third_party/torchtune/tests/torchtune/config/test_parse.py
--- marc_original/third_party/torchtune/tests/torchtune/config/test_parse.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/config/test_parse.py	2025-02-20 17:49:29.694024483 -0500
@@ -0,0 +1,67 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from argparse import Namespace
+from unittest.mock import patch
+
+import pytest
+from omegaconf import OmegaConf
+from torchtune import config
+
+from torchtune.config._parse import TuneRecipeArgumentParser
+
+_CONFIG = {"a": 1, "b": 2}
+
+
+class TestParse:
+    def test_parse(self):
+        a = 1
+        b = 3
+
+        @config.parse
+        def func(cfg):
+            assert cfg.a == a
+            assert cfg.b != b
+
+        with patch(
+            "torchtune.config._parse.TuneRecipeArgumentParser.parse_known_args",
+            return_value=(Namespace(**_CONFIG), []),
+        ) as mock_parse_args:
+            with pytest.raises(SystemExit):
+                func()
+            mock_parse_args.assert_called_once()
+
+
+class TestArgParse:
+    @pytest.fixture
+    def parser(self):
+        parser = TuneRecipeArgumentParser("Test parser")
+        return parser
+
+    @patch("torchtune.config._parse.OmegaConf.load", return_value=_CONFIG)
+    def test_parse_known_args(self, mock_load, parser):
+        """
+        Test that the parser can load a config and override parameters provided on CLI.
+        The actual load is mocked to return the test config above.
+        """
+        config_args, cli_args = parser.parse_known_args(
+            ["--config", "test.yaml", "b=3", "c=4"]
+        )
+        assert config_args.a == 1, f"a == {config_args.a} not 1 as set in the config."
+        assert config_args.b == 2, f"b == {config_args.b} not 2 as set in the config."
+
+        cli_kwargs = OmegaConf.from_dotlist(cli_args)
+        assert (
+            cli_kwargs.b == 3
+        ), f"b == {cli_kwargs.b} not 3 as set in the command args."
+        assert (
+            cli_kwargs.c == 4
+        ), f"c == {cli_kwargs.c} not 4 as set in the command args."
+
+        with pytest.raises(ValueError, match="Additional flag arguments not supported"):
+            _ = parser.parse_known_args(
+                ["--config", "test.yaml", "--b", "3"],
+            )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/config/test_validate.py marc/third_party/torchtune/tests/torchtune/config/test_validate.py
--- marc_original/third_party/torchtune/tests/torchtune/config/test_validate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/config/test_validate.py	2025-02-20 17:49:29.698024489 -0500
@@ -0,0 +1,29 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from omegaconf import OmegaConf
+from torchtune import config
+from torchtune.config._errors import ConfigError
+
+VALID_CONFIG_PATH = "tests/assets/valid_dummy_config.yaml"
+INVALID_CONFIG_PATH = "tests/assets/invalid_dummy_config.yaml"
+
+
+class TestValidate:
+    def test_validate(self):
+        conf = OmegaConf.load(VALID_CONFIG_PATH)
+        # Test a valid component
+        config.validate(conf)
+        # Test an invalid component
+        conf = OmegaConf.load(INVALID_CONFIG_PATH)
+        with pytest.raises(ConfigError) as excinfo:
+            config.validate(conf)
+        exc_config = excinfo.value
+        assert len(exc_config.errors) == 2
+        for e in exc_config.errors:
+            assert isinstance(e, TypeError)
+            assert str(e) == "get_dtype got an unexpected keyword argument 'dummy'"
diff -ruN marc_original/third_party/torchtune/tests/torchtune/data/test_collate.py marc/third_party/torchtune/tests/torchtune/data/test_collate.py
--- marc_original/third_party/torchtune/tests/torchtune/data/test_collate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/data/test_collate.py	2025-02-20 17:49:29.702024495 -0500
@@ -0,0 +1,394 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.
+
+from unittest import mock
+
+import pytest
+import torch
+import torch.nn.functional as F
+from tests.test_utils import gpu_test
+from torchtune.data import (
+    left_pad_sequence,
+    padded_collate,
+    padded_collate_dpo,
+    padded_collate_packed,
+    padded_collate_sft,
+    padded_collate_tiled_images_and_mask,
+)
+from torchtune.modules.attention_utils import _SUPPORTS_FLEX_ATTENTION
+
+
+class TestPaddedCollateSFT:
+    def test_batch_pad_sequence(self):
+        """
+        Tests that shorter input, label sequences are padded to the max seq len.
+        """
+        padding_idx = -8
+        ignore_idx = -9
+        token_pairs = [
+            {
+                "tokens": [1, 2, 3],
+                "labels": [4, 5, 6],
+            },
+            {
+                "tokens": [7],
+                "labels": [10],
+            },
+        ]
+        padded = padded_collate_sft(
+            batch=token_pairs,
+            padding_idx=padding_idx,
+            ignore_idx=ignore_idx,
+        )
+        padded_input = padded["tokens"][1]
+        padded_label = padded["labels"][1]
+        torch.testing.assert_close(
+            padded_input, torch.tensor([7, padding_idx, padding_idx])
+        )
+        torch.testing.assert_close(
+            padded_label, torch.tensor([10, ignore_idx, ignore_idx])
+        )
+
+
+class TestPaddedCollateTiledImagesAndMask:
+    @pytest.fixture
+    def batch(self):
+        return [
+            {
+                "tokens": [1, 2, 1, 3],
+                "labels": [4, 5, 6, 7],
+                "encoder_input": {
+                    "images": [torch.ones(2, 1, 1, 1), torch.ones(3, 1, 1, 1)],
+                    "aspect_ratio": [torch.tensor([1, 2]), torch.tensor([1, 3])],
+                },
+                "encoder_mask": [torch.ones(4, 5 * 2), torch.ones(4, 5 * 3)],
+            },
+            {
+                "tokens": [1, 4],
+                "labels": [8, 9],
+                "encoder_input": {
+                    "images": [torch.ones(4, 1, 1, 1)],
+                    "aspect_ratio": [torch.tensor([2, 2])],
+                },
+                "encoder_mask": [torch.ones(2, 5 * 4)],
+            },
+        ]
+
+    def test_right_pad_sequence(self, batch):
+        actual = padded_collate_tiled_images_and_mask(
+            batch=batch, padding_idx=0, ignore_idx=-100, pad_direction="right"
+        )
+
+        mask_1 = torch.concat([torch.ones(4, 5 * 2), torch.zeros(4, 10)], dim=1)
+        mask_2 = torch.concat([torch.ones(4, 5 * 3), torch.zeros(4, 5)], dim=1)
+        mask_3 = torch.concat([torch.ones(2, 5 * 4), torch.zeros(2, 20)], dim=0)
+        sample_1 = torch.stack([mask_1, mask_2])
+        sample_2 = torch.stack([mask_3, torch.zeros(4, 20)])
+        expected_mask = torch.stack([sample_1, sample_2]).view(2, 4, -1)
+
+        expected = {
+            "tokens": torch.tensor([[1, 2, 1, 3], [1, 4, 0, 0]]),
+            "labels": torch.tensor([[4, 5, 6, 7], [8, 9, -100, -100]]),
+            "encoder_input": {
+                "images": torch.tensor(
+                    [
+                        [
+                            [[[[1.0]]], [[[1.0]]], [[[0.0]]], [[[0.0]]]],
+                            [[[[1.0]]], [[[1.0]]], [[[1.0]]], [[[0.0]]]],
+                        ],
+                        [
+                            [[[[1.0]]], [[[1.0]]], [[[1.0]]], [[[1.0]]]],
+                            [[[[0.0]]], [[[0.0]]], [[[0.0]]], [[[0.0]]]],
+                        ],
+                    ]
+                ),
+                "aspect_ratio": torch.tensor([[[1, 2], [1, 3]], [[2, 2], [1, 1]]]),
+            },
+            "encoder_mask": expected_mask,
+        }
+
+        for k in expected:
+            if isinstance(expected[k], dict):
+                for k1 in expected[k]:
+                    torch.testing.assert_close(actual[k][k1], expected[k][k1])
+            else:
+                torch.testing.assert_close(actual[k], expected[k])
+
+    def test_left_pad_sequence(self, batch):
+        actual = padded_collate_tiled_images_and_mask(
+            batch=batch,
+            padding_idx=0,
+            ignore_idx=-100,
+            pad_direction="left",
+            pad_max_images=4,
+        )
+
+        mask_1 = torch.concat([torch.ones(4, 5 * 2), torch.zeros(4, 10)], dim=1)
+        mask_2 = torch.concat([torch.ones(4, 5 * 3), torch.zeros(4, 5)], dim=1)
+        mask_3 = torch.concat([torch.zeros(2, 20), torch.ones(2, 5 * 4)], dim=0)
+        sample_1 = torch.stack([mask_1, mask_2])
+        sample_2 = torch.stack([mask_3, torch.zeros(4, 20)])
+        expected_mask = torch.stack([sample_1, sample_2]).view(2, 4, -1)
+        expected_mask = F.pad(expected_mask, (0, 40), value=0)
+
+        expected = {
+            "tokens": torch.tensor([[1, 2, 1, 3], [0, 0, 1, 4]]),
+            "encoder_input": {
+                "images": torch.tensor(
+                    [
+                        [
+                            [[[[1.0]]], [[[1.0]]], [[[0.0]]], [[[0.0]]]],
+                            [[[[1.0]]], [[[1.0]]], [[[1.0]]], [[[0.0]]]],
+                        ],
+                        [
+                            [[[[1.0]]], [[[1.0]]], [[[1.0]]], [[[1.0]]]],
+                            [[[[0.0]]], [[[0.0]]], [[[0.0]]], [[[0.0]]]],
+                        ],
+                    ]
+                ),
+                "aspect_ratio": torch.tensor([[[1, 2], [1, 3]], [[2, 2], [1, 1]]]),
+            },
+            "encoder_mask": expected_mask,
+        }
+
+        for k in expected:
+            if isinstance(expected[k], dict):
+                for k1 in expected[k]:
+                    torch.testing.assert_close(actual[k][k1], expected[k][k1])
+            else:
+                torch.testing.assert_close(actual[k], expected[k])
+
+
+class TestPaddedCollatePacked:
+    @mock.patch("torchtune.modules.attention_utils._SUPPORTS_FLEX_ATTENTION", False)
+    def test_padded_collate_packed_sdpa(self):
+        token_pairs = [
+            {
+                "tokens": torch.tensor([1, 2, 3, 4, 5, 6]),
+                "labels": torch.tensor([7, 8, 9, 10, 11, 12]),
+                "input_pos": torch.tensor([0, 1, 2, 0, 1, 0]),
+                "seq_lens": torch.tensor([3, 2, 1]),
+            },
+            {
+                "tokens": torch.tensor([13, 14, 15, 16, 17, 18]),
+                "labels": torch.tensor([19, 20, 21, 22, 23, 24]),
+                "input_pos": torch.tensor([0, 1, 0, 1, 0, 1]),
+                "seq_lens": torch.tensor([2, 2, 2]),
+            },
+        ]
+        collated = padded_collate_packed(
+            batch=token_pairs,
+        )
+        torch.testing.assert_close(
+            collated["tokens"],
+            torch.tensor([[1, 2, 3, 4, 5, 6], [13, 14, 15, 16, 17, 18]]),
+        )
+        torch.testing.assert_close(
+            collated["labels"],
+            torch.tensor([[7, 8, 9, 10, 11, 12], [19, 20, 21, 22, 23, 24]]),
+        )
+        torch.testing.assert_close(
+            collated["input_pos"],
+            torch.tensor([[0, 1, 2, 0, 1, 0], [0, 1, 0, 1, 0, 1]]),
+        )
+        torch.testing.assert_close(
+            collated["mask"],
+            torch.tensor(
+                [
+                    [
+                        [1, 0, 0, 0, 0, 0],
+                        [1, 1, 0, 0, 0, 0],
+                        [1, 1, 1, 0, 0, 0],
+                        [0, 0, 0, 1, 0, 0],
+                        [0, 0, 0, 1, 1, 0],
+                        [0, 0, 0, 0, 0, 1],
+                    ],
+                    [
+                        [1, 0, 0, 0, 0, 0],
+                        [1, 1, 0, 0, 0, 0],
+                        [0, 0, 1, 0, 0, 0],
+                        [0, 0, 1, 1, 0, 0],
+                        [0, 0, 0, 0, 1, 0],
+                        [0, 0, 0, 0, 1, 1],
+                    ],
+                ],
+                dtype=torch.bool,
+            ),
+        )
+
+    @pytest.mark.skipif(
+        not _SUPPORTS_FLEX_ATTENTION,
+        reason="Please install a nightly build of torch to run this test.",
+    )
+    @gpu_test(gpu_count=1)
+    def test_padded_collate_packed_flex(self):
+        # create_block_mask requires that seq_len be divisible by 128, the default block size.
+        # see https://github.com/pytorch/pytorch/blob/main/torch/nn/attention/flex_attention.py#L636
+        batch = [
+            {
+                "tokens": torch.arange(128, dtype=torch.long),
+                "labels": torch.arange(128, dtype=torch.long),
+                "input_pos": torch.arange(128, dtype=torch.long),
+                "seq_lens": torch.ones(64, dtype=torch.long) * 2,
+            },
+            {
+                "tokens": torch.arange(128, 256, dtype=torch.long),
+                "labels": torch.arange(128, 256, dtype=torch.long),
+                "input_pos": torch.arange(128, 256, dtype=torch.long),
+                "seq_lens": torch.ones(32, dtype=torch.long) * 4,
+            },
+        ]
+        collated = padded_collate_packed(
+            batch=batch,
+        )
+        torch.testing.assert_close(
+            collated["tokens"],
+            torch.stack(
+                [
+                    torch.arange(128, dtype=torch.long),
+                    torch.arange(128, 256, dtype=torch.long),
+                ]
+            ),
+        )
+        torch.testing.assert_close(
+            collated["labels"],
+            torch.stack(
+                [
+                    torch.arange(128, dtype=torch.long),
+                    torch.arange(128, 256, dtype=torch.long),
+                ]
+            ),
+        )
+        torch.testing.assert_close(
+            collated["input_pos"],
+            torch.stack(
+                [
+                    torch.arange(128, dtype=torch.long),
+                    torch.arange(128, 256, dtype=torch.long),
+                ]
+            ),
+        )
+        torch.testing.assert_close(
+            collated["mask"].to_dense(),
+            torch.tensor([[[[1]]], [[[1]]]], dtype=torch.int32, device="cuda"),
+        )
+
+
+class TestLeftPadSequence:
+    def test_left_pad_sequence(self):
+        a = torch.tensor([1, 2, 3])
+        b = torch.tensor([4, 5, 6, 7])
+        c = torch.tensor([8, 9, 10, 11, 12])
+        result = left_pad_sequence([a, b, c], batch_first=True, padding_value=0)
+        expected = torch.tensor([[0, 0, 1, 2, 3], [0, 4, 5, 6, 7], [8, 9, 10, 11, 12]])
+        assert torch.equal(result, expected)
+
+        result = left_pad_sequence([a, b, c], batch_first=False, padding_value=0)
+        expected = torch.tensor(
+            [[0, 0, 8], [0, 4, 9], [1, 5, 10], [2, 6, 11], [3, 7, 12]]
+        )
+        assert torch.equal(result, expected)
+
+
+class TestPaddedCollate:
+    def test_padded_collate_classifier_labels(self):
+        batch = [
+            {"tokens": [1, 2, 3], "labels": 1},
+            {"tokens": [4, 5], "labels": 2},
+            {"tokens": [6, 7, 8, 9], "labels": 3},
+        ]
+        result = padded_collate(
+            batch,
+            pad_direction="right",
+            keys_to_pad=["tokens"],
+            padding_idx=-10,
+        )
+        expected_tokens = torch.tensor([[1, 2, 3, -10], [4, 5, -10, -10], [6, 7, 8, 9]])
+        expected_labels = torch.tensor([1, 2, 3])
+        assert torch.equal(result["tokens"], expected_tokens)
+        assert torch.equal(result["labels"], expected_labels)
+
+    def test_padded_collate_multiple_keys_to_pad(self):
+        batch = [
+            {"tokens": [1, 2], "labels_0": [3, 4], "labels_1": 1},
+            {"tokens": [5, 6, 7], "labels_0": [8, 9, 10], "labels_1": 2},
+        ]
+        result = padded_collate(
+            batch,
+            pad_direction="left",
+            keys_to_pad=["tokens", "labels_0"],
+            padding_idx={"tokens": 0, "labels_0": -1},
+        )
+        expected_tokens = torch.tensor([[0, 1, 2], [5, 6, 7]])
+        expected_labels_0 = torch.tensor([[-1, 3, 4], [8, 9, 10]])
+        expected_labels_1 = torch.tensor([1, 2])
+        assert torch.equal(result["tokens"], expected_tokens)
+        assert torch.equal(result["labels_0"], expected_labels_0)
+        assert torch.equal(result["labels_1"], expected_labels_1)
+
+    def test_value_error_raised_when_empty_keys_to_pad(self):
+        batch = [{"labels": [1]}, {"labels": [2]}]
+        with pytest.raises(ValueError):
+            padded_collate(batch, pad_direction="left", keys_to_pad=[], padding_idx=0)
+
+    def test_value_error_raised_when_mismatched_padding_idx_keys(self):
+        batch = [{"tokens": [1, 2], "labels": [1, 1]}]
+        with pytest.raises(ValueError):
+            padded_collate(
+                batch,
+                pad_direction="left",
+                keys_to_pad=["tokens", "labels"],
+                padding_idx={"tokens": 0},
+            )
+
+    def test_value_error_raised_when_mismatched_keys_to_pad(self):
+        batch = [{"tokens": [1, 2], "labels": [1, 1]}]
+        with pytest.raises(ValueError):
+            padded_collate(
+                batch,
+                pad_direction="left",
+                keys_to_pad=["tokens", "labels_0"],
+                padding_idx={"tokens": 0},
+            )
+
+    def test_value_error_raised_when_invalid_pad_direction(self):
+        batch = [{"tokens": [1, 2], "labels": [1, 1]}]
+        with pytest.raises(ValueError):
+            padded_collate(
+                batch,
+                pad_direction="oogabooga",
+                keys_to_pad=["tokens", "labels_0"],
+                padding_idx={"tokens": 0},
+            )
+
+
+class TestPaddedCollateDPO:
+    def test_dpo_collate(self):
+        batch = [
+            {
+                "chosen_input_ids": [1, 2, 3],
+                "chosen_labels": [4, 5, 6],
+                "rejected_input_ids": [7, 8],
+                "rejected_labels": [9, 10],
+            },
+            {
+                "chosen_input_ids": [11, 12],
+                "chosen_labels": [13, 14],
+                "rejected_input_ids": [15, 16, 17],
+                "rejected_labels": [18, 19, 20],
+            },
+        ]
+        input_ids, labels = padded_collate_dpo(batch, padding_idx=0, ignore_idx=-100)
+        expected_input_ids = torch.tensor(
+            [[1, 2, 3], [11, 12, 0], [7, 8, 0], [15, 16, 17]]
+        )
+        expected_labels = torch.tensor(
+            [[4, 5, 6], [13, 14, -100], [9, 10, -100], [18, 19, 20]]
+        )
+        assert torch.equal(input_ids, expected_input_ids)
+        assert torch.equal(labels, expected_labels)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/data/test_converters.py marc/third_party/torchtune/tests/torchtune/data/test_converters.py
--- marc_original/third_party/torchtune/tests/torchtune/data/test_converters.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/data/test_converters.py	2025-02-20 17:49:29.706024502 -0500
@@ -0,0 +1,94 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from tests.test_utils import (
+    assert_dialogue_equal,
+    CHAT_SAMPLE,
+    MESSAGE_SAMPLE,
+    MESSAGE_SAMPLE_TRAIN_ON_INPUT,
+)
+from torchtune.data import get_openai_messages, get_sharegpt_messages
+
+
+class TestShareGPTToLlama2Messages:
+    samples = {
+        "conversations": [
+            {
+                "from": "system",
+                "value": CHAT_SAMPLE["system"],
+            },
+            {
+                "from": "human",
+                "value": CHAT_SAMPLE["user"],
+            },
+            {
+                "from": "gpt",
+                "value": CHAT_SAMPLE["assistant"],
+            },
+        ]
+    }
+
+    def test_conversion(self):
+        converted_messages = get_sharegpt_messages(self.samples)
+        assert_dialogue_equal(converted_messages, MESSAGE_SAMPLE)
+
+    def test_conversion_train_on_input(self):
+        converted_messages = get_sharegpt_messages(self.samples, train_on_input=True)
+        assert_dialogue_equal(converted_messages, MESSAGE_SAMPLE_TRAIN_ON_INPUT)
+
+
+class TestOpenAIToLlama2Messages:
+    samples_1 = {
+        "id": "DUMMY",
+        "conversations": [
+            {
+                "role": "system",
+                "content": CHAT_SAMPLE["system"],
+            },
+            {
+                "role": "user",
+                "content": CHAT_SAMPLE["user"],
+            },
+            {
+                "role": "assistant",
+                "content": CHAT_SAMPLE["assistant"],
+            },
+        ],
+    }
+
+    samples_2 = {
+        "id": "DUMMY",
+        "messages": [
+            {
+                "role": "system",
+                "content": CHAT_SAMPLE["system"],
+            },
+            {
+                "role": "user",
+                "content": CHAT_SAMPLE["user"],
+            },
+            {
+                "role": "assistant",
+                "content": CHAT_SAMPLE["assistant"],
+            },
+        ],
+    }
+
+    def test_conversion_conversations_key(self):
+        converted_messages_1 = get_openai_messages(self.samples_1)
+        assert_dialogue_equal(converted_messages_1, MESSAGE_SAMPLE)
+
+    def test_conversion_messages_key(self):
+        converted_messages_2 = get_openai_messages(self.samples_2)
+        assert_dialogue_equal(converted_messages_2, MESSAGE_SAMPLE)
+
+    def test_conversion_conversations_key_train_on_input(self):
+        converted_messages_1 = get_openai_messages(self.samples_1, train_on_input=True)
+        assert_dialogue_equal(converted_messages_1, MESSAGE_SAMPLE_TRAIN_ON_INPUT)
+
+    def test_conversion_messages_key_train_on_input(self):
+        converted_messages_2 = get_openai_messages(self.samples_2, train_on_input=True)
+        assert_dialogue_equal(converted_messages_2, MESSAGE_SAMPLE_TRAIN_ON_INPUT)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/data/test_data_utils.py marc/third_party/torchtune/tests/torchtune/data/test_data_utils.py
--- marc_original/third_party/torchtune/tests/torchtune/data/test_data_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/data/test_data_utils.py	2025-02-20 17:49:29.710024509 -0500
@@ -0,0 +1,161 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+
+import pytest
+from PIL import Image
+
+from tests.common import ASSETS
+from torchtune.data._utils import format_content_with_images, load_image, truncate
+
+
+def test_truncate():
+    tokens = [1, 2, 3, 4, -1]
+
+    # Test no truncation
+    truncated_tokens = truncate(
+        tokens=tokens,
+        max_seq_len=5,
+        eos_id=-1,
+    )
+    assert truncated_tokens == tokens
+
+    masks = [True, True, False, True, False]
+    # Test truncated mask
+    truncated_masks = truncate(tokens=masks, max_seq_len=4, eos_id=False)
+    assert truncated_masks == [True, True, False, False]
+
+
+def test_format_content_with_images():
+    test_image_1 = Image.new(mode="RGB", size=(4, 4))
+    test_image_2 = Image.new(mode="RGB", size=(4, 4))
+    test_image_3 = Image.new(mode="RGB", size=(4, 4))
+
+    # Test single image tag in the middle
+    text = "hello <image>world"
+    assert format_content_with_images(
+        text,
+        image_tag="<image>",
+        images=[test_image_1],
+    ) == [
+        {"type": "text", "content": "hello "},
+        {"type": "image", "content": test_image_1},
+        {"type": "text", "content": "world"},
+    ]
+
+    # Test multiple image tags and image tag in beginning
+    text = "[image]hello [image]world"
+    assert format_content_with_images(
+        text,
+        image_tag="[image]",
+        images=[test_image_1, test_image_2],
+    ) == [
+        {"type": "image", "content": test_image_1},
+        {"type": "text", "content": "hello "},
+        {"type": "image", "content": test_image_2},
+        {"type": "text", "content": "world"},
+    ]
+
+    # Test an image tag that is not present in the text
+    text = "hello world"
+    assert format_content_with_images(text, image_tag="asdfghjkl;", images=[]) == [
+        {"type": "text", "content": "hello world"}
+    ]
+
+    # Test consecutive image tags
+    text = "<image><image>hello <image>world"
+    assert format_content_with_images(
+        text,
+        image_tag="<image>",
+        images=[test_image_1, test_image_2, test_image_3],
+    ) == [
+        {"type": "image", "content": test_image_1},
+        {"type": "image", "content": test_image_2},
+        {"type": "text", "content": "hello "},
+        {"type": "image", "content": test_image_3},
+        {"type": "text", "content": "world"},
+    ]
+
+    # Test image tag at the end
+    text = "hello <image>"
+    assert format_content_with_images(
+        text,
+        image_tag="<image>",
+        images=[test_image_1],
+    ) == [
+        {"type": "text", "content": "hello "},
+        {"type": "image", "content": test_image_1},
+    ]
+
+    # Test errors when the number of images does not match the number of image tags
+    text = "hello <image>world"
+    with pytest.raises(
+        ValueError,
+        match="does not match number of image tags",
+    ):
+        format_content_with_images(
+            text, image_tag="<image>", images=[test_image_1, test_image_2]
+        )
+
+
+def test_load_image(monkeypatch, tmp_path):
+    tmp_image = str(ASSETS / "dog_on_skateboard.jpg")
+
+    # Test loading from local file
+    image = load_image(tmp_image)
+    assert isinstance(image, Image.Image)
+    assert image.size == (580, 403)
+
+    # Test loading from remote file
+    # Mock the urlopen function to return a BytesIO object
+    def mock_urlopen(url):
+        return open(tmp_image, "rb")
+
+    monkeypatch.setattr("urllib.request.urlopen", mock_urlopen)
+    image = load_image("http://example.com/test_image.jpg")
+    assert isinstance(image, Image.Image)
+    assert image.size == (580, 403)
+
+    # Test that a ValueError is raised when the image path is invalid
+    with pytest.raises(ValueError, match="Failed to open image as PIL.Image"):
+        load_image("invalid_path")
+
+    # Test a temporary file with invalid image data
+    image_path = tmp_path / "test_image.jpg"
+    with open(image_path, "w") as f:
+        f.write("Invalid image data")
+
+    # Test that a ValueError is raised when the image data is invalid
+    with pytest.raises(ValueError, match="Failed to open image as PIL.Image"):
+        load_image(str(image_path))
+
+    # Test that a ValueError is raised when there is an HTTP error
+    # Mock the urlopen function to raise an exception
+    def mock_urlopen(url):
+        raise Exception("Failed to load image")
+
+    monkeypatch.setattr("urllib.request.urlopen", mock_urlopen)
+    with pytest.raises(ValueError, match="Failed to load image"):
+        load_image("http://example.com/test_image.jpg")
+
+    # Test that a ValueError is raised when there is an IO error
+    # Create a temporary file that cannot be read
+    image_path = tmp_path / "test_image.jpg"
+    with open(image_path, "w") as f:
+        f.write("Test data")
+    os.chmod(image_path, 0o000)  # Remove read permissions
+    with pytest.raises(ValueError, match="Failed to open image as PIL.Image"):
+        load_image(str(image_path))
+    os.chmod(image_path, 0o644)  # Restore read permissions
+
+    # Test that a ValueError is raised with invalid image data is read
+    # Create a temporary file with invalid image data
+    image_path = tmp_path / "test_image.jpg"
+    with open(image_path, "wb") as f:
+        f.write(b"Invalid image data")
+    with pytest.raises(ValueError, match="Failed to open image as PIL.Image"):
+        load_image(str(image_path))
diff -ruN marc_original/third_party/torchtune/tests/torchtune/data/test_messages.py marc/third_party/torchtune/tests/torchtune/data/test_messages.py
--- marc_original/third_party/torchtune/tests/torchtune/data/test_messages.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/data/test_messages.py	2025-02-20 17:49:29.714024515 -0500
@@ -0,0 +1,487 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from unittest import mock
+
+import pytest
+
+from PIL import Image
+from tests.test_utils import (
+    assert_dialogue_equal,
+    CHAT_SAMPLE,
+    MESSAGE_SAMPLE,
+    MESSAGE_SAMPLE_TRAIN_ON_INPUT,
+)
+from torchtune.data._messages import (
+    ChosenRejectedToMessages,
+    InputOutputToMessages,
+    Message,
+    OpenAIToMessages,
+    ShareGPTToMessages,
+    validate_messages,
+)
+
+
+class TestMessage:
+    @pytest.fixture
+    def text_message(self):
+        return Message(role="user", content="hello world")
+
+    @pytest.fixture
+    def test_image(self):
+        return Image.new(mode="RGB", size=(4, 4))
+
+    @pytest.fixture
+    def image_message(self, test_image):
+        return Message(
+            role="user",
+            content=[
+                {"type": "text", "content": "hello"},
+                {"type": "image", "content": test_image},
+                {"type": "text", "content": " world"},
+            ],
+        )
+
+    def test_message_validation(self, text_message, test_image):
+        message = text_message
+        assert message.role == "user"
+        assert message.content == [{"type": "text", "content": "hello world"}]
+
+        with pytest.raises(
+            ValueError,
+            match="Only assistant messages can be tool calls. Found role user in message: hello world",
+        ):
+            message = Message(role="user", content="hello world", ipython=True)
+
+        with pytest.raises(
+            ValueError,
+            match="Media tokens in tool calls are not supported. Both are set in message: ",
+        ):
+            message = Message(
+                role="user",
+                content=[{"type": "image", "content": test_image}],
+                ipython=True,
+            )
+
+    def test_from_dict(self):
+        message = Message.from_dict({"role": "user", "content": "hello world"})
+        assert message.role == "user"
+        assert message.content[0]["content"] == "hello world"
+        assert not message.masked
+        assert not message.ipython
+        assert message.eot
+
+    def test_contains_media(self, text_message, image_message):
+        assert not text_message.contains_media
+        assert image_message.contains_media
+
+    def test_get_media(self, text_message, image_message, test_image):
+        assert text_message.get_media() == []
+        assert image_message.get_media() == [test_image]
+
+    def test_text_content(self, text_message, image_message):
+        assert text_message.text_content == "hello world"
+        assert image_message.text_content == "hello world"
+
+    def test_repr_text(self, text_message):
+        expected_repr = "Message(role='user', content=['hello world'])"
+        assert str(text_message) == expected_repr
+        assert repr(text_message) == expected_repr
+
+    def test_repr_image(self, image_message, test_image):
+        img_repr = str(test_image)
+        expected_repr = f"Message(role='user', content=['hello', {img_repr}, ' world'])"
+        assert str(image_message) == expected_repr
+        assert repr(image_message) == expected_repr
+
+
+class TestInputOutputToMessages:
+    @pytest.fixture
+    def sample(self):
+        return {
+            "maybe_input": "hello world",
+            "maybe_output": "hello world",
+        }
+
+    def test_call(self, sample):
+        transform = InputOutputToMessages(
+            column_map={"input": "maybe_input", "output": "maybe_output"}
+        )
+        actual = transform(sample)
+        expected = [
+            Message(role="user", content="hello world", masked=True, eot=True),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["messages"], expected)
+
+    def test_call_train_on_input(self, sample):
+        transform = InputOutputToMessages(
+            column_map={"input": "maybe_input", "output": "maybe_output"},
+            train_on_input=True,
+        )
+        actual = transform(sample)
+        expected = [
+            Message(role="user", content="hello world", masked=False, eot=True),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["messages"], expected)
+
+    def test_system_prompt(self, sample):
+        transform = InputOutputToMessages(
+            column_map={"input": "maybe_input", "output": "maybe_output"},
+            new_system_prompt="you are a robot",
+        )
+        actual = transform(sample)
+        expected = [
+            Message(role="system", content="you are a robot", masked=True, eot=True),
+            Message(role="user", content="hello world", masked=True, eot=True),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["messages"], expected)
+
+    def test_raise_value_error_when_input_not_in_column_map(self):
+        with pytest.raises(ValueError, match="Expected a key of 'input'"):
+            InputOutputToMessages(
+                column_map={"bananas": "maybe_input", "output": "maybe_output"},
+            )
+
+    def test_raise_value_error_when_output_not_in_column_map(self):
+        with pytest.raises(ValueError, match="Expected a key of 'output'"):
+            InputOutputToMessages(
+                column_map={"input": "maybe_input", "bananas": "maybe_output"},
+            )
+
+
+class TestChosenRejectedToMessages:
+    @pytest.fixture
+    def sample(self):
+        return {
+            "maybe_chosen": [
+                {"role": "user", "content": "hello world"},
+                {"role": "assistant", "content": "hello world"},
+            ],
+            "maybe_rejected": [
+                {"role": "user", "content": "hello world"},
+                {"role": "assistant", "content": "bye world"},
+            ],
+        }
+
+    def test_call(self, sample):
+        transform = ChosenRejectedToMessages(
+            column_map={
+                "chosen": "maybe_chosen",
+                "rejected": "maybe_rejected",
+            },
+        )
+        actual = transform(sample)
+        expected_chosen = [
+            Message(role="user", content="hello world", masked=True, eot=True),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["chosen"], expected_chosen)
+
+        expected_rejected = [
+            Message(role="user", content="hello world", masked=True, eot=True),
+            Message(role="assistant", content="bye world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["rejected"], expected_rejected)
+
+    def test_call_train_on_input(self, sample):
+        transform = ChosenRejectedToMessages(
+            column_map={
+                "chosen": "maybe_chosen",
+                "rejected": "maybe_rejected",
+            },
+            train_on_input=True,
+        )
+        actual = transform(sample)
+        expected_chosen = [
+            Message(role="user", content="hello world", masked=False, eot=True),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["chosen"], expected_chosen)
+
+        expected_rejected = [
+            Message(role="user", content="hello world", masked=False, eot=True),
+            Message(role="assistant", content="bye world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["rejected"], expected_rejected)
+
+    def test_system_prompt(self, sample):
+        transform = ChosenRejectedToMessages(
+            column_map={
+                "chosen": "maybe_chosen",
+                "rejected": "maybe_rejected",
+            },
+            new_system_prompt="you are a robot",
+        )
+        actual = transform(sample)
+        expected_chosen = [
+            Message(role="system", content="you are a robot", masked=True, eot=True),
+            Message(role="user", content="hello world", masked=True, eot=True),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["chosen"], expected_chosen)
+
+        expected_rejected = [
+            Message(role="system", content="you are a robot", masked=True, eot=True),
+            Message(role="user", content="hello world", masked=True, eot=True),
+            Message(role="assistant", content="bye world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["rejected"], expected_rejected)
+
+    def test_raise_value_error_when_chosen_not_in_column_map(self):
+        with pytest.raises(ValueError, match="Expected a key of 'chosen'"):
+            ChosenRejectedToMessages(
+                column_map={"bananas": "maybe_chosen", "rejected": "maybe_rejected"},
+            )
+
+    def test_raise_value_error_when_rejected_not_in_column_map(self):
+        with pytest.raises(ValueError, match="Expected a key of 'rejected'"):
+            ChosenRejectedToMessages(
+                column_map={"chosen": "maybe_chosen", "bananas": "maybe_rejected"},
+            )
+
+
+class TestShareGPTToMessages:
+    samples = {
+        "conversations": [
+            {
+                "from": "system",
+                "value": CHAT_SAMPLE["system"],
+            },
+            {
+                "from": "human",
+                "value": CHAT_SAMPLE["user"],
+            },
+            {
+                "from": "gpt",
+                "value": CHAT_SAMPLE["assistant"],
+            },
+        ]
+    }
+
+    def test_call(self):
+        transform = ShareGPTToMessages()
+        converted_messages = transform(self.samples)
+        assert_dialogue_equal(converted_messages["messages"], MESSAGE_SAMPLE)
+
+    def test_call_train_on_input(self):
+        transform = ShareGPTToMessages(train_on_input=True)
+        converted_messages = transform(self.samples)
+        assert_dialogue_equal(
+            converted_messages["messages"], MESSAGE_SAMPLE_TRAIN_ON_INPUT
+        )
+
+    def test_system_prompt(self):
+        transform = ShareGPTToMessages(new_system_prompt="you are a robot")
+        converted_messages = transform(self.samples)
+        assert_dialogue_equal(
+            converted_messages["messages"],
+            [
+                Message(
+                    role="system", content="you are a robot", masked=True, eot=True
+                ),
+            ]
+            + MESSAGE_SAMPLE[1:],
+        )
+
+    def test_raise_value_error_when_conversations_not_in_column_map(self):
+        with pytest.raises(ValueError, match="Expected a key of 'conversations'"):
+            ShareGPTToMessages(
+                column_map={"bananas": "maybe_conversations"},
+            )
+
+    @mock.patch("torchtune.data._messages.load_image")
+    def test_image_only_in_first_user_message(self, mock_load_image):
+        mock_load_image.return_value = Image.new(mode="RGB", size=(4, 4))
+        sample = {
+            "conversations": [
+                {"from": "human", "value": "<image>\nFirst message."},
+                {"from": "gpt", "value": "First response."},
+                {"from": "human", "value": "Second message."},
+                {"from": "gpt", "value": "Second response."},
+            ],
+            "image": "test_image.jpg",
+        }
+        transform = ShareGPTToMessages(image_tag="<image>")
+        messages = transform(sample)
+        for idx, message in enumerate(messages["messages"]):
+            if idx == 0:
+                assert message.contains_media
+            else:
+                assert not message.contains_media
+
+
+class TestOpenAIToMessages:
+    samples = {
+        "messages": [
+            {
+                "role": "system",
+                "content": CHAT_SAMPLE["system"],
+            },
+            {
+                "role": "user",
+                "content": CHAT_SAMPLE["user"],
+            },
+            {
+                "role": "assistant",
+                "content": CHAT_SAMPLE["assistant"],
+            },
+        ],
+    }
+
+    image_samples = {
+        "messages": [
+            {
+                "role": "system",
+                "content": CHAT_SAMPLE["system"],
+            },
+            {
+                "role": "user",
+                "content": [
+                    {"type": "text", "text": CHAT_SAMPLE["user"]},
+                    {"type": "image_url", "image_url": {"url": "https://example.com"}},
+                ],
+            },
+            {
+                "role": "assistant",
+                "content": CHAT_SAMPLE["assistant"],
+            },
+        ],
+    }
+
+    def test_call(self):
+        transform = OpenAIToMessages()
+        converted_messages = transform(self.samples)
+        assert_dialogue_equal(converted_messages["messages"], MESSAGE_SAMPLE)
+
+    def test_call_train_on_input(self):
+        transform = OpenAIToMessages(train_on_input=True)
+        converted_messages = transform(self.samples)
+        assert_dialogue_equal(
+            converted_messages["messages"], MESSAGE_SAMPLE_TRAIN_ON_INPUT
+        )
+
+    def test_system_prompt(self):
+        transform = OpenAIToMessages(new_system_prompt="you are a robot")
+        converted_messages = transform(self.samples)
+        assert_dialogue_equal(
+            converted_messages["messages"],
+            [
+                Message(
+                    role="system", content="you are a robot", masked=True, eot=True
+                ),
+            ]
+            + MESSAGE_SAMPLE[1:],
+        )
+
+    def test_raise_value_error_when_messages_not_in_column_map(self):
+        with pytest.raises(ValueError, match="Expected a key of 'messages'"):
+            OpenAIToMessages(
+                column_map={"bananas": "maybe_messages"},
+            )
+
+    @mock.patch("torchtune.data._messages.load_image")
+    def test_convert_from_openai_content(self, mock_load_image):
+        test_img = Image.new(mode="RGB", size=(4, 4))
+        mock_load_image.return_value = test_img
+        transform = OpenAIToMessages()
+        converted_content = transform._convert_from_openai_content(
+            self.image_samples["messages"][1]["content"]
+        )
+        assert converted_content == [
+            {"type": "text", "content": CHAT_SAMPLE["user"]},
+            {"type": "image", "content": test_img},
+        ]
+        mock_load_image.assert_called_once_with("https://example.com")
+
+    @mock.patch("torchtune.data._messages.load_image")
+    def test_call_image_messages(self, mock_load_image):
+        test_img = Image.new(mode="RGB", size=(4, 4))
+        mock_load_image.return_value = test_img
+        transform = OpenAIToMessages()
+        converted_messages = transform(self.image_samples)
+        assert_dialogue_equal(
+            converted_messages["messages"],
+            [
+                MESSAGE_SAMPLE[0],
+                Message(
+                    role="user",
+                    content=[
+                        {"type": "text", "content": CHAT_SAMPLE["user"]},
+                        {"type": "image", "content": test_img},
+                    ],
+                ),
+                MESSAGE_SAMPLE[2],
+            ],
+        )
+        mock_load_image.assert_called_once_with("https://example.com")
+
+
+def test_validate_messages():
+    messages = [
+        Message(role="system", content="hello"),
+        Message(role="user", content="hello"),
+        Message(role="assistant", content="world"),
+    ]
+
+    # Test valid conversation with system
+    validate_messages(messages)
+
+    # Test valid conversation without system
+    validate_messages(messages[1:])
+
+    # Test system not first
+    messages = [
+        Message(role="user", content="hello"),
+        Message(role="system", content="hello"),
+        Message(role="assistant", content="world"),
+    ]
+    with pytest.raises(
+        ValueError,
+        match="System message at index 1 in messages, but system messages must come first",
+    ):
+        validate_messages(messages)
+
+    # Test empty assistant message
+    messages = [
+        Message(role="system", content="hello"),
+        Message(role="user", content="world"),
+        Message(role="assistant", content=""),
+    ]
+    validate_messages(messages)
+
+    # Test single message
+    messages = [
+        Message(role="user", content="hello"),
+    ]
+    with pytest.raises(
+        ValueError, match="Messages must be at least length 2, but got 1 messages"
+    ):
+        validate_messages(messages)
+
+    # Test repeated user message
+    messages = [
+        Message(role="user", content="hello"),
+        Message(role="user", content="world"),
+        Message(role="assistant", content="world"),
+    ]
+    with pytest.raises(
+        ValueError, match="Two consecutive user messages at index 1 and 0 in messages"
+    ):
+        validate_messages(messages)
+
+    # Test assistant message comes first
+    messages = [
+        Message(role="assistant", content="hello"),
+        Message(role="user", content="world"),
+    ]
+    with pytest.raises(
+        ValueError,
+        match="Assistant message before expected user message at index 0 in messages",
+    ):
+        validate_messages(messages)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/data/test_prompt_templates.py marc/third_party/torchtune/tests/torchtune/data/test_prompt_templates.py
--- marc_original/third_party/torchtune/tests/torchtune/data/test_prompt_templates.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/data/test_prompt_templates.py	2025-02-20 17:49:29.714024515 -0500
@@ -0,0 +1,200 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from tests.test_utils import assert_dialogue_equal, MESSAGE_SAMPLE
+from torchtune.data._messages import Message
+from torchtune.data._prompt_templates import (
+    _get_prompt_template,
+    ChatMLTemplate,
+    GrammarErrorCorrectionTemplate,
+    PromptTemplate,
+    SummarizeTemplate,
+)
+from torchtune.models.llama2 import Llama2ChatTemplate
+
+
+class TestChatMLTemplate:
+    expected_dialogue = [
+        Message(
+            role="system",
+            content="<|im_start|>system\nYou are an AI assistant. User will you give you a task. "
+            "Your goal is to complete the task as faithfully as you can. While performing "
+            "the task think step-by-step and justify your steps.<|im_end|>\n",
+        ),
+        Message(
+            role="user",
+            content="<|im_start|>user\nPlease "
+            "briefly summarize this news article:\n\nAOL.com Video - Father Lets 8-Year-Old "
+            "Drive On Icy Road\n\nDescription:Would you let your 8-year-old drive your car? "
+            "How about on an icy road? Well one father in Russia did just that, and recorded "
+            "the entire thing. To her credit, the child seemed to be doing a great job. "
+            "(0:44)\n\nTags: 8-year-old driver , caught on camera , child driver , pix11\n\n"
+            "Summary:<|im_end|>\n",
+        ),
+        Message(
+            role="assistant",
+            content="<|im_start|>assistant\nA father in Russia allowed his 8-year-old child to drive his car on an "
+            "icy road and recorded the event. The child appeared to be handling the situation well, "
+            "showcasing their driving skills despite the challenging conditions.<|im_end|>\n",
+        ),
+    ]
+
+    def test_format(self):
+        actual = ChatMLTemplate()(MESSAGE_SAMPLE)
+        assert_dialogue_equal(actual, self.expected_dialogue)
+
+    def test_format_generation(self):
+        messages_generation = MESSAGE_SAMPLE[:2] + [
+            Message(role="assistant", content="")
+        ]
+        expected = self.expected_dialogue[:2] + [
+            Message(role="assistant", content="<|im_start|>assistant\n")
+        ]
+        actual = ChatMLTemplate()(messages_generation)
+        assert_dialogue_equal(actual, expected)
+
+
+class TestGrammarErrorCorrectionTemplate:
+    samples = [
+        {
+            "messages": [
+                Message(
+                    role="user",
+                    content="Bitcoin is for $7,094 this morning, which CoinDesk says.",
+                ),
+                Message(
+                    role="assistant",
+                    content="Bitcoin goes for $7,094 this morning, according to CoinDesk.",
+                ),
+            ]
+        },
+        {
+            "messages": [
+                Message(
+                    role="user",
+                    content="Much many brands and sellers still in the market.",
+                ),
+                Message(
+                    role="assistant",
+                    content="Many brands and sellers still in the market.",
+                ),
+            ],
+        },
+    ]
+    expected_prompts = [
+        [
+            Message(
+                role="user",
+                content="Correct this to standard English: Bitcoin is for $7,094 this morning, which CoinDesk says.\n"
+                "---\n"
+                "Corrected: ",
+            ),
+            Message(
+                role="assistant",
+                content="Bitcoin goes for $7,094 this morning, according to CoinDesk.",
+            ),
+        ],
+        [
+            Message(
+                role="user",
+                content="Correct this to standard English: Much many brands and sellers still in the market.\n"
+                "---\n"
+                "Corrected: ",
+            ),
+            Message(
+                role="assistant",
+                content="Many brands and sellers still in the market.",
+            ),
+        ],
+    ]
+
+    template = GrammarErrorCorrectionTemplate()
+
+    def test_call(self):
+        for sample, expected_prompt in zip(self.samples, self.expected_prompts):
+            actual = self.template(sample["messages"])
+            assert_dialogue_equal(actual, expected_prompt)
+
+
+class TestSummarizeTemplate:
+    samples = [
+        {
+            "messages": [
+                Message(
+                    role="user",
+                    content="Amanda: I baked cookies. Do you want some? Jerry: Sure! Amanda: I'll bring you tomorrow :-)",
+                ),
+                Message(
+                    role="assistant",
+                    content="Amanda baked cookies and will bring Jerry some tomorrow.",
+                ),
+            ],
+        },
+        {
+            "messages": [
+                Message(
+                    role="user",
+                    content="Olivia: Who are you voting for in this election? Oliver: Liberals as always. Olivia: Me too!! Oliver: Great",  # noqa: B950
+                ),
+                Message(
+                    role="assistant",
+                    content="Olivia and Olivier are voting for liberals in this election.",
+                ),
+            ],
+        },
+    ]
+    expected_prompts = [
+        [
+            Message(
+                role="user",
+                content="Summarize this dialogue:\n"
+                "Amanda: I baked cookies. Do you want some? Jerry: Sure! Amanda: I'll bring you tomorrow :-)\n"
+                "---\n"
+                "Summary:\n",
+            ),
+            Message(
+                role="assistant",
+                content="Amanda baked cookies and will bring Jerry some tomorrow.",
+            ),
+        ],
+        [
+            Message(
+                role="user",
+                content="Summarize this dialogue:\n"
+                "Olivia: Who are you voting for in this election? Oliver: Liberals as always. Olivia: Me too!! Oliver: Great\n"
+                "---\n"
+                "Summary:\n",
+            ),
+            Message(
+                role="assistant",
+                content="Olivia and Olivier are voting for liberals in this election.",
+            ),
+        ],
+    ]
+
+    template = SummarizeTemplate()
+
+    def test_call(self):
+        for sample, expected_prompt in zip(self.samples, self.expected_prompts):
+            actual = self.template(sample["messages"])
+            assert_dialogue_equal(actual, expected_prompt)
+
+
+def test_get_prompt_template():
+    template = _get_prompt_template("torchtune.models.llama2.Llama2ChatTemplate")
+    assert isinstance(template, Llama2ChatTemplate)
+
+    template = _get_prompt_template({"user": ("1", "2"), "assistant": ("3", "4")})
+    assert isinstance(template, PromptTemplate)
+    assert template.template["user"] == ("1", "2")
+    assert template.template["assistant"] == ("3", "4")
+
+    with pytest.raises(
+        ValueError,
+        match="Prompt template must be a dotpath string or dictionary with custom template",
+    ):
+        _ = _get_prompt_template(["user", "assistant"])
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/__init__.py marc/third_party/torchtune/tests/torchtune/datasets/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/__init__.py	2025-02-20 17:49:29.722024529 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/multimodal/test_llava_instruct_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/multimodal/test_llava_instruct_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/multimodal/test_llava_instruct_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/multimodal/test_llava_instruct_dataset.py	2025-02-20 17:49:29.726024535 -0500
@@ -0,0 +1,88 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from collections import Counter
+from unittest.mock import patch
+
+import PIL
+
+import pytest
+from datasets import Dataset
+
+from tests.test_utils import DummyTokenizer
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+
+from torchtune.datasets.multimodal import llava_instruct_dataset
+
+
+class TestLLaVAInstructDataset:
+    @pytest.fixture
+    def tokenizer(self):
+        return DummyTokenizer()
+
+    @pytest.fixture
+    def test_image_pil(self):
+        return PIL.Image.new(mode="RGB", size=(4, 4))
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    @patch("torchtune.data._messages.load_image")
+    def test_get_item(self, load_image, load_dataset, tokenizer, test_image_pil):
+        """
+        WARNING: careful with these mocks, they are applied in bottom up order
+        """
+        # mock the call to load_image
+        load_image.return_value = test_image_pil
+
+        # mock the call to HF datasets
+        load_dataset.return_value = Dataset.from_list(
+            [
+                {
+                    "image": "test_image.jpg",
+                    "conversations": [
+                        {
+                            "from": "human",
+                            "value": "<image>\nWhat can you infer about the man's outdoor activity?",
+                        },
+                        {
+                            "from": "gpt",
+                            "value": "From the image, we can infer that the man is engaging in a "
+                            "recreational activity involving a frisbee in a park or grass field. "
+                            "The frisbee is in the air, and the man appears to be either catching "
+                            "or throwing it. This suggests that he might be playing a casual game "
+                            "of catch with a friend or practicing his frisbee skills, enjoying the "
+                            "outdoors and getting some physical activity at the same time.",
+                        },
+                    ],
+                }
+            ]
+        )
+
+        ds = llava_instruct_dataset(
+            model_transform=tokenizer,
+        )
+
+        input, labels, images = ds[0]["tokens"], ds[0]["labels"], ds[0]["images"]
+
+        expected_count = {
+            3: 17,
+            2: 15,
+            4: 11,
+            8: 9,
+            5: 8,
+            7: 8,
+            6: 5,
+            1: 5,
+            9: 2,
+            0: 1,
+            -2: 1,
+            12: 1,
+            10: 1,
+            -1: 1,
+        }
+
+        assert Counter(input) == expected_count
+        assert labels.count(CROSS_ENTROPY_IGNORE_IDX) == 11
+        assert images == [test_image_pil]
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/multimodal/test_the_cauldron_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/multimodal/test_the_cauldron_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/multimodal/test_the_cauldron_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/multimodal/test_the_cauldron_dataset.py	2025-02-20 17:49:29.730024541 -0500
@@ -0,0 +1,81 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from unittest.mock import patch
+
+import PIL
+
+import pytest
+from tests.test_utils import DummyTokenizer
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+
+from torchtune.datasets.multimodal import the_cauldron_dataset
+
+
+class TestTheCauldronDataset:
+    @pytest.fixture
+    def tokenizer(self):
+        return DummyTokenizer()
+
+    @pytest.fixture
+    def test_image_pil(self):
+        return PIL.Image.new(mode="RGB", size=(4, 4))
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    def test_get_item(self, load_dataset, tokenizer, test_image_pil):
+        # mock the call to HF datasets
+        load_dataset.return_value = [
+            {
+                "images": [test_image_pil],
+                "texts": [
+                    {
+                        "user": "Question: What do respiration and combustion give out"
+                        "\nChoices:\nA. Oxygen\nB. Carbon dioxide\nC. Nitrogen\nD. Heat"
+                        "\nAnswer with the letter.",
+                        "assistant": "Answer: B",
+                        "source": "AI2D",
+                    }
+                ],
+            }
+        ]
+
+        ds = the_cauldron_dataset(
+            model_transform=tokenizer,
+            subset="dummy",
+        )
+        input, labels, images = (ds[0]["tokens"], ds[0]["labels"], ds[0]["images"])
+
+        assert input == [
+            0,
+            -2,
+            9,
+            4,
+            2,
+            11,
+            3,
+            10,
+            4,
+            3,
+            8,
+            2,
+            6,
+            2,
+            6,
+            7,
+            2,
+            8,
+            2,
+            4,
+            6,
+            4,
+            3,
+            7,
+            7,
+            1,
+            -1,
+        ]
+        assert labels.count(CROSS_ENTROPY_IGNORE_IDX) == 24
+        assert images == [test_image_pil]
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_alpaca_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_alpaca_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_alpaca_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_alpaca_dataset.py	2025-02-20 17:49:29.734024548 -0500
@@ -0,0 +1,156 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from unittest.mock import patch
+
+import pytest
+from datasets import Dataset
+
+from tests.test_utils import assert_dialogue_equal, DummyTokenizer
+from torchtune.data import Message
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+from torchtune.datasets._alpaca import (
+    alpaca_cleaned_dataset,
+    alpaca_dataset,
+    AlpacaToMessages,
+)
+
+
+class TestAlpacaDataset:
+    @pytest.fixture
+    def tokenizer(self):
+        return DummyTokenizer()
+
+    @pytest.fixture
+    def sample(self):
+        return {
+            "instruction": "Give three tips for staying healthy.",
+            "input": "",
+            "output": (
+                "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables."
+                "2. Exercise regularly to keep your body active and strong."
+                "3. Get enough sleep and maintain a consistent sleep schedule."
+            ),
+        }
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    def test_label_no_masking(self, load_dataset, tokenizer, sample):
+        """
+        Test whether the input and the labels are correctly created when the input is not masked.
+        """
+
+        # mock the call to HF datasets
+        load_dataset.return_value = Dataset.from_list([sample])
+
+        alpaca_ds = alpaca_dataset(tokenizer=tokenizer)
+        input, labels = alpaca_ds[0]["tokens"], alpaca_ds[0]["labels"]
+
+        assert len(input) == len(labels)
+        assert labels[-1] == tokenizer.eos_id
+        assert input[0] == tokenizer.bos_id
+        assert CROSS_ENTROPY_IGNORE_IDX not in labels
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    def test_label_masking(self, load_dataset, tokenizer, sample):
+        """
+        Test whether the input and the labels are correctly created when the input is masked.
+        """
+
+        # mock the call to HF datasets
+        load_dataset.return_value = Dataset.from_list([sample])
+
+        alpaca_ds = alpaca_dataset(tokenizer=tokenizer, train_on_input=False)
+
+        # Generate the input and labels
+        input, labels = alpaca_ds[0]["tokens"], alpaca_ds[0]["labels"]
+
+        assert len(input) == len(labels)
+        assert labels[-1] == tokenizer.eos_id
+        assert input[0] == tokenizer.bos_id
+        assert labels.count(CROSS_ENTROPY_IGNORE_IDX) == 27
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    def test_alpaca_clean(self, load_dataset, tokenizer, sample):
+        """
+        Test whether the input and the labels are correctly created when the input is not masked.
+        """
+
+        # mock the call to HF datasets
+        load_dataset.return_value = Dataset.from_list([sample])
+
+        alpaca_ds = alpaca_cleaned_dataset(tokenizer=tokenizer)
+        input, labels = alpaca_ds[0]["tokens"], alpaca_ds[0]["labels"]
+
+        assert len(input) == len(labels)
+        assert labels[-1] == tokenizer.eos_id
+        assert input[0] == tokenizer.bos_id
+        assert CROSS_ENTROPY_IGNORE_IDX not in labels
+
+
+class TestAlpacaToMessages:
+    @pytest.fixture
+    def sample(self):
+        return {
+            "maybe_instruction": "hello",
+            "maybe_input": "world",
+            "maybe_output": "hello world",
+        }
+
+    @pytest.fixture
+    def sample_no_input(self):
+        return {
+            "maybe_instruction": "hello world",
+            "maybe_input": "",
+            "maybe_output": "hello world",
+        }
+
+    @pytest.mark.parametrize("train_on_input", [True, False])
+    def test_call(self, train_on_input, sample):
+        transform = AlpacaToMessages(
+            column_map={
+                "instruction": "maybe_instruction",
+                "input": "maybe_input",
+                "output": "maybe_output",
+            },
+            train_on_input=train_on_input,
+        )
+        actual = transform(sample)
+        expected = [
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task, paired with an input that provides further context. "
+                "Write a response that appropriately completes the request.\n\n"
+                "### Instruction:\nhello\n\n### Input:\nworld\n\n### Response:\n",
+                masked=True,
+                eot=True,
+            ),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["messages"], expected)
+
+    @pytest.mark.parametrize("train_on_input", [True, False])
+    def test_call_no_input(self, train_on_input, sample_no_input):
+        transform = AlpacaToMessages(
+            column_map={
+                "instruction": "maybe_instruction",
+                "input": "maybe_input",
+                "output": "maybe_output",
+            },
+            train_on_input=train_on_input,
+        )
+        actual = transform(sample_no_input)
+        expected = [
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task. "
+                "Write a response that appropriately completes the request.\n\n"
+                "### Instruction:\nhello world\n\n### Response:\n",
+                masked=True,
+                eot=True,
+            ),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["messages"], expected)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_chat_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_chat_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_chat_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_chat_dataset.py	2025-02-20 17:49:29.738024555 -0500
@@ -0,0 +1,71 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from tests.common import ASSETS
+from tests.test_utils import DummyChatFormat, DummyTokenizer
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+from torchtune.datasets import chat_dataset
+
+
+class TestChatDataset:
+    @pytest.fixture
+    def chat_format(self):
+        return DummyChatFormat
+
+    def test_get_item(self, chat_format):
+        expected_tokenized_prompts = [
+            [
+                0,
+                3,
+                3,
+                2,
+                2,
+                10,
+                4,
+                2,
+                3,
+                7,
+                2,
+                5,
+                3,
+                7,
+                2,
+                4,
+                2,
+                3,
+                -1,
+                0,
+                6,
+                11,
+                1,
+                6,
+                -1,
+            ]
+        ]
+        prompt_lengths = (12, 3)
+        expected_labels = [
+            [CROSS_ENTROPY_IGNORE_IDX] * prompt_lengths[0]
+            + [3, 7, 2, 4, 2, 3, -1]
+            + [CROSS_ENTROPY_IGNORE_IDX] * prompt_lengths[1]
+            + [1, 6, -1]
+        ]
+
+        ds = chat_dataset(
+            tokenizer=DummyTokenizer(),
+            source="json",
+            data_files=str(ASSETS / "chat_tiny.json"),
+            conversation_column="conversations",
+            conversation_style="sharegpt",
+            train_on_input=False,
+            packed=False,
+            split="train",
+        )
+
+        assert len(ds) == 1
+        prompt, label = ds[0]["tokens"], ds[0]["labels"]
+        assert prompt == expected_tokenized_prompts[0]
+        assert label == expected_labels[0]
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_cnn_dailymail_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_cnn_dailymail_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_cnn_dailymail_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_cnn_dailymail_dataset.py	2025-02-20 17:49:29.738024555 -0500
@@ -0,0 +1,48 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from unittest.mock import patch
+
+import pytest
+
+from tests.test_utils import DummyTokenizer
+
+from torchtune.datasets import cnn_dailymail_articles_dataset
+
+
+class TestCNNDailyMailArticlesDataset:
+    @pytest.fixture
+    def tokenizer(self):
+        return DummyTokenizer()
+
+    @patch("torchtune.datasets._text_completion.load_dataset")
+    @pytest.mark.parametrize("max_seq_len", [128, 512, 1024, 4096])
+    def test_dataset_get_item(self, load_dataset, tokenizer, max_seq_len):
+        # Sample data from CNN / DailyMail dataset
+        load_dataset.return_value = [
+            {
+                "article": "(CNN) -- An American woman died aboard a cruise ship "
+                "that docked at Rio de Janeiro on Tuesday, the same ship on which "
+                "86 passengers previously fell ill, according to the state-run "
+                "Brazilian news agency, Agencia Brasil. The American tourist died "
+                "aboard the MS Veendam, owned by cruise operator Holland America. "
+                "Federal Police told Agencia Brasil that forensic doctors were "
+                "investigating her death. The ship's doctors told police that the "
+                "woman was elderly and suffered from diabetes and hypertension, "
+                "according the agency. The other passengers came down with diarrhea "
+                "prior to her death during an earlier part of the trip, the ship's "
+                "doctors said. The Veendam left New York 36 days ago for a South "
+                "America tour.",
+            }
+        ]
+        ds = cnn_dailymail_articles_dataset(
+            tokenizer=tokenizer,
+            max_seq_len=max_seq_len,
+        )
+        input, label = ds[0]["tokens"], ds[0]["labels"]
+        assert len(input) <= max_seq_len
+        assert len(label) <= max_seq_len
+        assert len(input) == len(label)
+        assert input[0] == tokenizer.bos_id
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_concat_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_concat_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_concat_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_concat_dataset.py	2025-02-20 17:49:29.742024561 -0500
@@ -0,0 +1,92 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from datasets import Dataset
+from torch.utils.data import Dataset as TorchDataset
+from torchtune.datasets._concat import ConcatDataset
+from torchtune.datasets._packed import PackedDataset
+
+
+class DummyDataset(TorchDataset):
+    def __init__(self, sample_size):
+        self.sample_size = sample_size
+
+    def __getitem__(self, index):
+        if index >= 1000:
+            raise IndexError()
+        return {
+            "tokens": [index] * self.sample_size,
+            "labels": [index] * self.sample_size,
+        }
+
+    def __len__(self):
+        return 1000
+
+
+class TestConcatDataset:
+    @pytest.fixture
+    def datasets(self):
+        ds1 = Dataset.from_list([{"data": f"ds1_{i}"} for i in range(4)])
+        ds2 = Dataset.from_list([{"data": f"ds2_{i}"} for i in range(8)])
+        ds3 = Dataset.from_list([{"data": f"ds3_{i}"} for i in range(15)])
+        ds4 = Dataset.from_list([{"data": f"ds4_{i}"} for i in range(16)])
+        ds5 = Dataset.from_list([{"data": f"ds5_{i}"} for i in range(23)])
+        ds6 = Dataset.from_list([{"data": f"ds6_{i}"} for i in range(42)])
+        return [ds1, ds2, ds3, ds4, ds5, ds6]
+
+    @pytest.fixture
+    def torch_datasets(self):
+        ds1 = DummyDataset(4)
+        ds2 = DummyDataset(8)
+        ds3 = DummyDataset(15)
+        ds4 = DummyDataset(16)
+        ds5 = DummyDataset(23)
+        ds6 = DummyDataset(42)
+        return [ds1, ds2, ds3, ds4, ds5, ds6]
+
+    def test_length(self, datasets):
+        """Test the correct computation of total length"""
+        multi_dataset = ConcatDataset(datasets)
+
+        # sum of individual datasets lengths
+        expected_length = 4 + 8 + 15 + 16 + 23 + 42  # 108
+        assert len(multi_dataset) == expected_length
+
+    def test_getitem(self, datasets):
+        """Test item retrieval across dataset boundaries"""
+        multi_dataset = ConcatDataset(datasets)
+
+        # Testing indices across different datasets
+        assert multi_dataset[-1] is None  # Index out of range
+        assert multi_dataset[0] == {"data": "ds1_0"}
+        assert multi_dataset[3] == {"data": "ds1_3"}
+        assert multi_dataset[4] == {"data": "ds2_0"}
+        assert multi_dataset[10] == {"data": "ds2_6"}
+        assert multi_dataset[20] == {"data": "ds3_8"}
+        assert multi_dataset[35] == {"data": "ds4_8"}
+        assert multi_dataset[50] == {"data": "ds5_7"}
+        assert multi_dataset[70] == {"data": "ds6_4"}
+        assert multi_dataset[90] == {"data": "ds6_24"}
+        assert multi_dataset[108] is None  # Index out of range
+
+    def test_invalid_index_type(self, datasets):
+        """Test handling of invalid index types"""
+        multi_dataset = ConcatDataset(datasets)
+
+        with pytest.raises(TypeError):
+            multi_dataset["invalid_type"]  # Non-integer index
+
+    def test_packed_dataset(self, torch_datasets):
+        torch_datasets[0] = PackedDataset(
+            torch_datasets[0],
+            max_seq_len=25,
+            max_packs=5,
+            split_across_pack=True,
+        )
+
+        with pytest.raises(ValueError):
+            concated_dataset = ConcatDataset(torch_datasets)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_grammar_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_grammar_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_grammar_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_grammar_dataset.py	2025-02-20 17:49:29.746024568 -0500
@@ -0,0 +1,68 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from unittest.mock import patch
+
+import pytest
+from datasets import Dataset
+
+from tests.test_utils import DummyTokenizer
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+
+from torchtune.datasets import grammar_dataset
+
+
+class TestGrammarDataset:
+    @pytest.fixture
+    def tokenizer(self):
+        return DummyTokenizer()
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    def test_label_no_masking(self, load_dataset, tokenizer):
+        """
+        Test whether the input and the labels are correctly created when the input is not masked.
+        """
+
+        # mock the call to HF datasets
+        load_dataset.return_value = Dataset.from_list(
+            [
+                {
+                    "input": "Bitcoin is for $7,094 this morning, which CoinDesk says.",
+                    "output": "Bitcoin goes for $7,094 this morning, according to CoinDesk.",
+                }
+            ]
+        )
+
+        grammar_ds = grammar_dataset(tokenizer=tokenizer, train_on_input=True)
+        input, labels = grammar_ds[0]["tokens"], grammar_ds[0]["labels"]
+
+        assert input == [0, 7, 2, 3, 6, 4, 8, 5, 8, 5, 7, 4, 3, 6, 4, 8, 9, 2, 9, -1]
+        assert labels == input
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    def test_label_masking(self, load_dataset, tokenizer):
+        """
+        Test whether the input and the labels are correctly created when the input is masked.
+        """
+
+        # mock the call to HF datasets
+        load_dataset.return_value = Dataset.from_list(
+            [
+                {
+                    "input": "Bitcoin is for $7,094 this morning, which CoinDesk says.",
+                    "output": "Bitcoin goes for $7,094 this morning, according to CoinDesk.",
+                }
+            ]
+        )
+
+        grammar_ds = grammar_dataset(tokenizer=tokenizer)
+
+        # Generate the input and labels
+        input, labels = grammar_ds[0]["tokens"], grammar_ds[0]["labels"]
+
+        assert input == [0, 7, 2, 3, 6, 4, 8, 5, 8, 5, 7, 4, 3, 6, 4, 8, 9, 2, 9, -1]
+        # Check that the input is masked
+        assert labels.count(CROSS_ENTROPY_IGNORE_IDX) == 10
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_hh_rlhf_helpful_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_hh_rlhf_helpful_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_hh_rlhf_helpful_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_hh_rlhf_helpful_dataset.py	2025-02-20 17:49:29.750024574 -0500
@@ -0,0 +1,109 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from collections import Counter
+from unittest.mock import patch
+
+import pytest
+from datasets import Dataset
+
+from tests.test_utils import DummyTokenizer
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+
+from torchtune.datasets._hh_rlhf_helpful import hh_rlhf_helpful_dataset
+
+
+class TestHHRLHFHelpfulDataset:
+    @patch("torchtune.datasets._preference.load_dataset")
+    @pytest.mark.parametrize("train_on_input", [True, False])
+    def test_dataset_get_item(self, mock_load_dataset, train_on_input):
+        # Truncated sample data from HH RLHF Helpful dataset
+        mock_load_dataset.return_value = Dataset.from_list(
+            [
+                {
+                    "chosen": [
+                        {
+                            "content": "helping my granny with her mobile phone issue",
+                            "role": "user",
+                        },
+                        {
+                            "content": "I see you are chatting with your grandmother "
+                            "about an issue with her mobile phone. How can I help?",
+                            "role": "assistant",
+                        },
+                        {"content": "her phone is not turning on", "role": "user"},
+                        {
+                            "content": "Is it on but it doesnt power up or charge? "
+                            "Or its off and does not turn on?",
+                            "role": "assistant",
+                        },
+                    ],
+                    "rejected": [
+                        {
+                            "content": "helping my granny with her mobile phone issue",
+                            "role": "user",
+                        },
+                        {
+                            "content": "I see you are chatting with your grandmother "
+                            "about an issue with her mobile phone. How can I help?",
+                            "role": "assistant",
+                        },
+                        {"content": "her phone is not turning on", "role": "user"},
+                        {
+                            "content": "Okay, are you concerned that her phone is broken, "
+                            "or simply that it is not turning on?",
+                            "role": "assistant",
+                        },
+                    ],
+                }
+            ]
+        )
+        ds = hh_rlhf_helpful_dataset(
+            tokenizer=DummyTokenizer(),
+            train_on_input=train_on_input,
+        )
+        # Generate the input and labels
+        sample = ds[0]
+
+        expected_chosen_counts = {
+            3: 14,
+            2: 11,
+            4: 7,
+            5: 7,
+            7: 4,
+            6: 4,
+            0: 2,
+            1: 2,
+            -1: 2,
+            8: 1,
+            11: 1,
+        }
+        assert Counter(sample["chosen_input_ids"]) == expected_chosen_counts
+        if train_on_input:
+            assert Counter(sample["chosen_labels"]) == expected_chosen_counts
+        else:
+            # Check that the input is masked
+            assert sample["chosen_labels"].count(CROSS_ENTROPY_IGNORE_IDX) == 16
+
+        expected_rejected_counts = {
+            3: 14,
+            2: 8,
+            5: 8,
+            4: 6,
+            6: 5,
+            7: 4,
+            0: 2,
+            1: 2,
+            -1: 2,
+            8: 1,
+            11: 1,
+            9: 1,
+        }
+        assert Counter(sample["rejected_input_ids"]) == expected_rejected_counts
+        if train_on_input:
+            assert Counter(sample["rejected_labels"]) == expected_rejected_counts
+        else:
+            # Check that the input is masked
+            assert sample["rejected_labels"].count(CROSS_ENTROPY_IGNORE_IDX) == 16
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_instruct_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_instruct_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_instruct_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_instruct_dataset.py	2025-02-20 17:49:29.754024581 -0500
@@ -0,0 +1,67 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from tests.common import ASSETS
+from tests.test_utils import DummyTokenizer
+from torchtune.data import InstructTemplate
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+from torchtune.datasets import instruct_dataset
+
+
+def dummy_transform(sample):
+    sample["instruction"] = sample["instruction"] + " asdfghjkl; "
+    sample["response"] = sample["response"] + " asdfghjkl; "
+    return sample
+
+
+class DummyTemplate(InstructTemplate):
+    template = "Instruction:\n{instruction}\n\nResponse:\n"
+
+    @classmethod
+    def format(cls, sample, column_map):
+        return cls.template.format(**sample)
+
+
+class TestInstructDataset:
+    @pytest.mark.parametrize("train_on_input", [True, False])
+    def test_get_item(self, train_on_input):
+        expected_tokenized_prompts = [
+            [0, 6, 4, 6, 4, 4, 2, 2, 2, 7, 2, 2, 5, 2, 2, 6, -1],
+            [0, 6, 4, 6, 2, 2, 8, 2, 15, 8, 3, 15, 3, 4, 9, 3, 15, -1],
+        ]
+        prompt_lengths = (10, 9)
+        expected_labels = [
+            [CROSS_ENTROPY_IGNORE_IDX] * prompt_lengths[0] + [2, 2, 5, 2, 2, 6, -1],
+            [CROSS_ENTROPY_IGNORE_IDX] * prompt_lengths[1]
+            + [8, 3, 15, 3, 4, 9, 3, 15, -1],
+        ]
+
+        system_prompt = "follow this prompt"
+
+        dataset = instruct_dataset(
+            tokenizer=DummyTokenizer(),
+            source="json",
+            train_on_input=train_on_input,
+            data_files=str(ASSETS / "instruct_tiny.json"),
+            column_map={"input": "instruction", "output": "response"},
+            split="train",
+            new_system_prompt=system_prompt,
+        )
+        system_prompt_offset = len(system_prompt.split(" ")) + 1  # +1 for bos token
+
+        assert len(dataset) == 2
+
+        for i in range(len(dataset)):
+            prompt, label = dataset[i]["tokens"], dataset[i]["labels"]
+            assert prompt == expected_tokenized_prompts[i]
+            if train_on_input:
+                assert (
+                    label[system_prompt_offset:]
+                    == expected_tokenized_prompts[i][system_prompt_offset:]
+                )
+            else:
+                assert label == expected_labels[i]
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_packed_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_packed_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_packed_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_packed_dataset.py	2025-02-20 17:49:29.754024581 -0500
@@ -0,0 +1,266 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import itertools
+
+import pytest
+import torch
+from tests.test_utils import DummyTokenizer
+from torch.utils.data import Dataset
+
+from torchtune.datasets import PackedDataset
+
+
+class DummyDataset(Dataset):
+    def __init__(self, sample_size):
+        self.sample_size = sample_size
+
+    def __getitem__(self, index):
+        if index >= 1000:
+            raise IndexError()
+        return {
+            "tokens": [index] * self.sample_size,
+            "labels": [index] * self.sample_size,
+        }
+
+    def __len__(self):
+        return 1000
+
+
+class DummyRealDataset(Dataset):
+    def __init__(self):
+        self.samples_list = [
+            "This is a packing test",
+            "A fantastic test. It should pack two samples.",
+            "This one will not be fully packed.",
+        ]
+        self.tokenizer = DummyTokenizer()
+
+    def __getitem__(self, index):
+        tokens = self.tokenizer.encode(self.samples_list[index])
+        return {"tokens": tokens, "labels": tokens}
+
+    def __len__(self):
+        return len(self.samples_list)
+
+
+class TestPackedDataset:
+    def _get_expected_seq_lens_and_input_pos(
+        self, max_seq_len, sample_size, split_across_pack
+    ):
+        """
+        Generate expected seq lens and position ids for given max sequence
+        length and sample length
+        """
+        num_samples, remainder = divmod(max_seq_len, sample_size)
+        seq_lens = [sample_size] * num_samples
+        if split_across_pack and remainder > 0:
+            num_samples += 1
+        input_pos = [list(range(sample_size)) for i in range(1, num_samples + 1)]
+        input_pos = list(itertools.chain(*input_pos))
+
+        # Emulate seq len and position id padding
+        if remainder > 0:
+            if not split_across_pack:
+                input_pos.extend(list(range(sample_size, sample_size + remainder)))
+            seq_lens.extend([remainder])
+
+        return torch.tensor(seq_lens), torch.tensor(input_pos[:max_seq_len])
+
+    def _calculate_num_packs(
+        self, dataset_size, max_seq_len, sample_size, split_across_pack, max_packs
+    ):
+        # First see how many samples we can fit in a single pack
+        num_samples_per_pack, remainder = divmod(max_seq_len, sample_size)
+
+        # If we split across pack (and the samples don't fit perfectly in max_seq_len), we can fit more
+        if split_across_pack and remainder > 0:
+            # Now we need the fractional to see how many we can partially fit in each pack
+            num_samples_per_pack = max_seq_len / sample_size
+
+        # If we don't split across pack, we will need more packs
+        num_packs, remainder = divmod(dataset_size, num_samples_per_pack)
+
+        # If there's leftover, we need to add one more pack
+        if remainder > 0:
+            num_packs += 1
+
+        return num_packs if num_packs < max_packs else max_packs
+
+    @pytest.mark.parametrize("max_seq_len", [25])
+    @pytest.mark.parametrize("sample_size", [2, 5])
+    @pytest.mark.parametrize("max_packs", [5, 200])
+    @pytest.mark.parametrize("split_across_pack", [True, False])
+    def test_packed_dataset(
+        self, max_seq_len, sample_size, max_packs, split_across_pack
+    ):
+        dataset = DummyDataset(sample_size)
+        packed = PackedDataset(
+            dataset,
+            max_seq_len=max_seq_len,
+            max_packs=max_packs,
+            split_across_pack=split_across_pack,
+        )
+
+        # Check we get right number of packs
+        correct_num_packs = self._calculate_num_packs(
+            len(dataset), max_seq_len, sample_size, split_across_pack, max_packs
+        )
+        assert len(packed) == correct_num_packs
+
+        # Check all fields are same length
+        assert (
+            len(packed[0]["tokens"])
+            == len(packed[0]["labels"])
+            == len(packed[0]["input_pos"])
+        )
+        # Check that samples are packed correctly - very last individual sample
+        # should have index value of the number of times dataset was iterated over
+        if split_across_pack:
+            # If we split samples, we'll know how many samples by taking the
+            # full length and dividing by sample size
+            last_index, remainder = divmod(len(packed) * max_seq_len, sample_size)
+            # Account for remaining sample that didn't fit in window
+            last_index = last_index if remainder > 0 else last_index - 1
+        else:
+            # If we don't split samples, we know how many samples by taking
+            # how much fits in a single window and multiplying by max rows.
+            # If there is a remainder, this will end up being a pad token.
+            last_index = (
+                (max_seq_len // sample_size) * len(packed) - 1
+                if max_seq_len % sample_size == 0
+                else 0
+            )
+
+        assert packed[-1]["tokens"][-1].item() == last_index
+
+        (
+            expected_seq_lens,
+            expected_input_pos,
+        ) = self._get_expected_seq_lens_and_input_pos(
+            max_seq_len, sample_size, split_across_pack
+        )
+
+        torch.testing.assert_close(packed[0]["seq_lens"], expected_seq_lens)
+        torch.testing.assert_close(packed[0]["input_pos"], expected_input_pos)
+
+    @pytest.mark.parametrize("max_seq_len", [13])
+    @pytest.mark.parametrize("sample_size", [14, 27, 40])
+    @pytest.mark.parametrize("max_packs", [5, 200, 3100])
+    @pytest.mark.parametrize("split_across_pack", [True])
+    def test_chunked_case(self, max_seq_len, sample_size, max_packs, split_across_pack):
+        dataset = DummyDataset(sample_size)
+        packed = PackedDataset(
+            dataset,
+            max_seq_len=max_seq_len,
+            max_packs=max_packs,
+            split_across_pack=split_across_pack,
+        )
+
+        # Check we get right number of packs
+        correct_num_packs = self._calculate_num_packs(
+            len(dataset), max_seq_len, sample_size, split_across_pack, max_packs
+        )
+        assert len(packed) == correct_num_packs
+
+        # Check all fields are same length
+        assert all(
+            len(pack["tokens"]) == len(pack["labels"]) == len(pack["input_pos"])
+            for pack in packed
+        )
+
+        # Check that all sum(seq_lens) are equal to max_seq_len
+        assert all(pack["seq_lens"].sum().item() == max_seq_len for pack in packed)
+
+    def test_packed_dataset_real_data(self):
+        expected_tokenized_prompts = [
+            torch.tensor([0, 4, 2, 1, 7, 4, -1, 0, 1, 9]),
+            torch.tensor([5, 2, 6, 4, 3, 8, -1, 0, 4, 3]),
+            torch.tensor([4, 3, 2, 5, 7, -1, 0, 0, 0, 0]),
+        ]
+        expected_tokenized_labels = [
+            torch.tensor([0, 4, 2, 1, 7, 4, -1, 0, 1, 9]),
+            torch.tensor([5, 2, 6, 4, 3, 8, -1, 0, 4, 3]),
+            torch.tensor([4, 3, 2, 5, 7, -1, -100, -100, -100, -100]),
+        ]
+        expected_seq_lens = [
+            torch.tensor(
+                [7, 3],
+            ),
+            torch.tensor(
+                [7, 3],
+            ),
+            torch.tensor(
+                [6, 4],
+            ),
+        ]
+        expected_input_pos = [
+            torch.tensor([0, 1, 2, 3, 4, 5, 6, 0, 1, 2]),
+            torch.tensor([3, 4, 5, 6, 7, 8, 9, 0, 1, 2]),
+            # Padded position ids cannot go beyond max seq_len - 1
+            torch.tensor([3, 4, 5, 6, 7, 8, 9, 9, 9, 9]),
+        ]
+        packed = PackedDataset(
+            DummyRealDataset(),
+            max_seq_len=10,
+            split_across_pack=True,
+        )
+
+        for i in range(len(packed)):
+            prompt, label, seq_lens, input_pos = (
+                packed[i]["tokens"],
+                packed[i]["labels"],
+                packed[i]["seq_lens"],
+                packed[i]["input_pos"],
+            )
+            torch.testing.assert_close(prompt, expected_tokenized_prompts[i])
+            torch.testing.assert_close(label, expected_tokenized_labels[i])
+            torch.testing.assert_close(input_pos, expected_input_pos[i])
+            torch.testing.assert_close(seq_lens, expected_seq_lens[i])
+
+    def test_pad_pack(self):
+        padding_idx = -8
+        ignore_idx = -100  # Same as CROSS_ENTROPY_IGNORE_IDX
+        pack = {
+            "tokens": [2, 5],
+            "labels": [3, 7],
+            "seq_lens": [1, 1],
+            # Let the first token be the end of the previous sample (pos 8),
+            # and the second token the start of the next sample (pos 0). Collate
+            # should continue from 0 -> 1, 2, ...
+            "input_pos": [8, 0],
+        }
+
+        dataset = DummyDataset(2)
+        packed = PackedDataset(
+            dataset,
+            max_seq_len=4,
+        )
+
+        pack = packed._convert_to_tensors(pack)
+        padded = packed._pad_pack(pack, padding_idx=padding_idx)
+
+        padded_input = padded["tokens"]
+        padded_label = padded["labels"]
+        padded_input_pos = padded["input_pos"]
+        padded_seq_lens = padded["seq_lens"]
+
+        torch.testing.assert_close(
+            padded_input, torch.tensor([2, 5, padding_idx, padding_idx])
+        )
+        torch.testing.assert_close(
+            padded_label, torch.tensor([3, 7, ignore_idx, ignore_idx])
+        )
+        torch.testing.assert_close(padded_input_pos, torch.tensor([8, 0, 1, 2]))
+        torch.testing.assert_close(padded_seq_lens, torch.tensor([1, 1, 2]))
+
+    def test_pack_errors_if_sample_too_long(self):
+        dataset = DummyDataset(8)
+        with pytest.raises(ValueError, match="Dataset sample is too long"):
+            PackedDataset(
+                dataset,
+                max_seq_len=4,
+            )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_preference_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_preference_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_preference_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_preference_dataset.py	2025-02-20 17:49:29.758024588 -0500
@@ -0,0 +1,157 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Mapping
+from unittest import mock
+
+import pytest
+from tests.common import ASSETS
+from tests.test_utils import DummyTokenizer
+from torchtune.data import Message
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+from torchtune.datasets._preference import preference_dataset, PreferenceDataset
+from torchtune.modules.transforms import Transform
+
+
+class ToDummyPreferenceMessages(Transform):
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        chosen_messages = [
+            Message.from_dict(sample["prompt"][0]),
+            Message.from_dict(sample["chosen"][0]),
+        ]
+
+        rejected_messages = [
+            Message.from_dict(sample["prompt"][0]),
+            Message.from_dict(sample["rejected"][0]),
+        ]
+
+        return {"chosen": chosen_messages, "rejected": rejected_messages}
+
+
+class TestPreferenceDataset:
+    @pytest.fixture
+    def dialogue(self):
+        return [
+            {
+                "prompt": [
+                    {
+                        "role": "user",
+                        "content": "What is 2+2?",
+                        "masked": True,
+                    },
+                ],
+                "chosen": [
+                    {
+                        "role": "assistant",
+                        "content": "The answer is 4.",
+                        "masked": False,
+                    },
+                ],
+                "rejected": [
+                    {
+                        "role": "assistant",
+                        "content": "The answer is 12.",
+                        "masked": False,
+                    },
+                ],
+            },
+        ]
+
+    @pytest.fixture
+    def expected(self):
+        return {
+            "prompt": [
+                0,
+                4,
+                2,
+                4,
+            ],
+            "chosen": [
+                3,
+                6,
+                2,
+                2,
+                -1,
+            ],
+            "rejected": [
+                3,
+                6,
+                2,
+                3,
+                -1,
+            ],
+        }
+
+    @mock.patch("torchtune.datasets._preference.load_dataset")
+    def test_get_item(self, mock_load_dataset, dialogue, expected):
+        mock_load_dataset.return_value = dialogue
+        expected_chosen_tokens = expected["prompt"] + expected["chosen"]
+        expected_chosen_labels = [CROSS_ENTROPY_IGNORE_IDX] * len(
+            expected["prompt"]
+        ) + expected["chosen"]
+        expected_rejected_tokens = expected["prompt"] + expected["rejected"]
+        expected_rejected_labels = [CROSS_ENTROPY_IGNORE_IDX] * len(
+            expected["prompt"]
+        ) + expected["rejected"]
+
+        ds = PreferenceDataset(
+            source="iam/agoofy/goober",
+            message_transform=ToDummyPreferenceMessages(),
+            tokenizer=DummyTokenizer(),
+        )
+        assert len(ds) == 1
+        mock_load_dataset.assert_called_once()
+
+        prompt, label = ds[0]["chosen_input_ids"], ds[0]["chosen_labels"]
+        assert prompt == expected_chosen_tokens
+        assert label == expected_chosen_labels
+
+        prompt, label = ds[0]["rejected_input_ids"], ds[0]["rejected_labels"]
+        assert prompt == expected_rejected_tokens
+        assert label == expected_rejected_labels
+
+    def test_load_local_json(self):
+        expected_tokenized_chosen_prompts = [
+            [0, 4, 2, 1, 2, 4, 1, 4, 1, 4, 2, 2, 9, 3, 3, 5, -1]
+        ]
+        expected_tokenized_rejected_prompts = [
+            [0, 4, 2, 1, 2, 4, 1, 4, 1, 4, 2, 2, 9, 4, 4, 4, -1]
+        ]
+
+        # prompt length is number of tokens shared between
+        # the tokenized rejected and chosen messages
+        prompt_length = 13
+        expected_chosen_labels = [
+            [CROSS_ENTROPY_IGNORE_IDX] * prompt_length + [3, 3, 5, -1]
+        ]
+        expected_rejected_labels = [
+            [CROSS_ENTROPY_IGNORE_IDX] * prompt_length + [4, 4, 4, -1]
+        ]
+
+        ds = preference_dataset(
+            tokenizer=DummyTokenizer(),
+            source="json",
+            data_files=str(ASSETS / "hh_rlhf_tiny.json"),
+            train_on_input=False,
+            split="train",
+        )
+
+        assert len(ds) == 1
+
+        expected_keys = [
+            "chosen_input_ids",
+            "chosen_labels",
+            "rejected_input_ids",
+            "rejected_labels",
+        ]
+        assert set(ds[0].keys()) == set(expected_keys)
+        assert len(ds[0].keys()) == 4
+
+        assert expected_tokenized_chosen_prompts[0] == ds[0]["chosen_input_ids"]
+        assert expected_tokenized_rejected_prompts[0] == ds[0]["rejected_input_ids"]
+
+        assert expected_chosen_labels[0] == ds[0]["chosen_labels"]
+        assert expected_rejected_labels[0] == ds[0]["rejected_labels"]
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_samsum_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_samsum_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_samsum_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_samsum_dataset.py	2025-02-20 17:49:29.762024594 -0500
@@ -0,0 +1,125 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from unittest.mock import patch
+
+import pytest
+from datasets import Dataset
+
+from tests.test_utils import DummyTokenizer
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+
+from torchtune.datasets import samsum_dataset
+
+
+class TestSamsumDataset:
+    @pytest.fixture
+    def tokenizer(self):
+        return DummyTokenizer()
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    def test_label_no_masking(self, load_dataset, tokenizer):
+        """
+        Test whether the input and the labels are correctly created when the input is not masked.
+        """
+
+        # mock the call to HF datasets
+        load_dataset.return_value = Dataset.from_list(
+            [
+                {
+                    "id": "13818513",
+                    "dialogue": "Amanda: I baked cookies. Do you want some? Jerry: Sure! Amanda: I'll bring you tomorrow :-)",
+                    "summary": "Amanda baked cookies and will bring Jerry some tomorrow.",
+                },
+            ]
+        )
+
+        samsum_ds = samsum_dataset(tokenizer=tokenizer, train_on_input=True)
+        input, labels = samsum_ds[0]["tokens"], samsum_ds[0]["labels"]
+
+        assert input == [
+            0,
+            7,
+            1,
+            5,
+            8,
+            2,
+            3,
+            4,
+            5,
+            6,
+            5,
+            7,
+            4,
+            5,
+            3,
+            8,
+            3,
+            6,
+            5,
+            7,
+            3,
+            4,
+            5,
+            5,
+            4,
+            9,
+            -1,
+        ]
+        assert labels == input
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    def test_label_masking(self, load_dataset, tokenizer):
+        """
+        Test whether the input and the labels are correctly created when the input is masked.
+        """
+
+        # mock the call to HF datasets
+        load_dataset.return_value = Dataset.from_list(
+            [
+                {
+                    "id": "13818513",
+                    "dialogue": "Amanda: I baked cookies. Do you want some? Jerry: Sure! Amanda: I'll bring you tomorrow :-)",
+                    "summary": "Amanda baked cookies and will bring Jerry some tomorrow.",
+                },
+            ]
+        )
+
+        samsum_ds = samsum_dataset(tokenizer=tokenizer)
+
+        # Generate the input and labels
+        input, labels = samsum_ds[0]["tokens"], samsum_ds[0]["labels"]
+
+        assert input == [
+            0,
+            7,
+            1,
+            5,
+            8,
+            2,
+            3,
+            4,
+            5,
+            6,
+            5,
+            7,
+            4,
+            5,
+            3,
+            8,
+            3,
+            6,
+            5,
+            7,
+            3,
+            4,
+            5,
+            5,
+            4,
+            9,
+            -1,
+        ]
+        assert labels.count(CROSS_ENTROPY_IGNORE_IDX) == 17
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_sft_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_sft_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_sft_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_sft_dataset.py	2025-02-20 17:49:29.766024601 -0500
@@ -0,0 +1,159 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Mapping
+from unittest import mock
+
+import pytest
+from tests.test_utils import DummyTokenizer
+from torchtune.data import Message
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.transforms import Transform
+
+
+class ToDummyMessages(Transform):
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        dialogue = sample["dialogue"]
+        messages = [Message.from_dict(d) for d in dialogue]
+        return {"messages": messages}
+
+
+class DummyTokenizerInvalidModelTransform(DummyTokenizer):
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        sample = super().__call__(sample)
+        del sample["tokens"]
+        del sample["images"]
+        return sample
+
+
+class TestSFTDataset:
+    @pytest.fixture
+    def dialogue(self):
+        return [
+            {
+                "dialogue": [
+                    {
+                        "role": "system",
+                        "content": "You are an AI assistant.",
+                        "masked": True,
+                    },
+                    {
+                        "role": "user",
+                        "content": "What is the meaning of life?",
+                        "masked": True,
+                    },
+                    {
+                        "role": "assistant",
+                        "content": "The meaning of life is 42.",
+                        "masked": False,
+                    },
+                    {
+                        "role": "user",
+                        "content": "That's ridiculous.",
+                        "masked": True,
+                    },
+                    {"role": "assistant", "content": "I agree.", "masked": False},
+                ],
+            },
+        ]
+
+    @mock.patch("torchtune.datasets._sft.load_dataset")
+    def test_get_item(self, mock_load_dataset, dialogue):
+        mock_load_dataset.return_value = dialogue
+        expected_tokenized_prompts = [
+            [
+                0,
+                3,
+                3,
+                2,
+                2,
+                10,
+                4,
+                2,
+                3,
+                7,
+                2,
+                5,
+                3,
+                7,
+                2,
+                4,
+                2,
+                3,
+                -1,
+                0,
+                6,
+                11,
+                1,
+                6,
+                -1,
+            ]
+        ]
+        prompt_lengths = (12, 3)
+        expected_labels = [
+            [CROSS_ENTROPY_IGNORE_IDX] * prompt_lengths[0]
+            + [3, 7, 2, 4, 2, 3, -1]
+            + [CROSS_ENTROPY_IGNORE_IDX] * prompt_lengths[1]
+            + [1, 6, -1]
+        ]
+        ds = SFTDataset(
+            source="iam/agoofy/goober",
+            message_transform=ToDummyMessages(),
+            model_transform=DummyTokenizer(),
+        )
+        assert len(ds) == 1
+        mock_load_dataset.assert_called_once()
+        prompt, label = ds[0]["tokens"], ds[0]["labels"]
+        assert prompt == expected_tokenized_prompts[0]
+        assert label == expected_labels[0]
+
+    @pytest.fixture
+    def invalid_dialogue(self):
+        return [
+            {
+                "dialogue": [
+                    {
+                        "role": "user",
+                        "content": "What is the meaning of life?",
+                        "masked": True,
+                    },
+                    {
+                        "role": "system",
+                        "content": "You are an AI assistant.",
+                        "masked": True,
+                    },
+                ],
+            },
+        ]
+
+    @mock.patch("torchtune.datasets._sft.load_dataset")
+    def test_error_for_invalid_messages(self, mock_load_dataset, invalid_dialogue):
+        mock_load_dataset.return_value = invalid_dialogue
+
+        ds = SFTDataset(
+            source="iam/agoofy/goober",
+            message_transform=ToDummyMessages(),
+            model_transform=DummyTokenizer(),
+        )
+
+        msg = "system messages must come first"
+        with pytest.raises(ValueError, match=msg):
+            ds[0]
+
+    @mock.patch("torchtune.datasets._sft.load_dataset")
+    def test_error_for_invalid_tokenized_dict(self, mock_load_dataset, dialogue):
+        mock_load_dataset.return_value = dialogue
+
+        ds = SFTDataset(
+            source="iam/agoofy/goober",
+            message_transform=ToDummyMessages(),
+            model_transform=DummyTokenizerInvalidModelTransform(),
+        )
+
+        msg = "model_transform returned the following keys: mask. Must return 'tokens' and 'mask' as keys."
+        with pytest.raises(ValueError, match=msg):
+            ds[0]
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_slimorca_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_slimorca_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_slimorca_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_slimorca_dataset.py	2025-02-20 17:49:29.770024608 -0500
@@ -0,0 +1,76 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from collections import Counter
+from unittest.mock import patch
+
+import pytest
+from datasets import Dataset
+
+from tests.test_utils import DummyTokenizer
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+
+from torchtune.datasets import slimorca_dataset
+
+
+class TestSlimOrcaDataset:
+    @pytest.fixture
+    def tokenizer(self):
+        return DummyTokenizer()
+
+    @patch("torchtune.datasets._sft.load_dataset")
+    @pytest.mark.parametrize("train_on_input", [True, False])
+    def test_dataset_get_item(self, mock_load_dataset, train_on_input, tokenizer):
+        # Sample data from slimorca dataset
+        mock_load_dataset.return_value = Dataset.from_list(
+            [
+                {
+                    "conversations": [
+                        {
+                            "from": "system",
+                            "value": "You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",  # noqa: B950
+                        },
+                        {
+                            "from": "human",
+                            "value": "Please briefly summarize this news article:\n\nAOL.com Video - Father Lets 8-Year-Old Drive On Icy Road\n\nDescription:Would you let your 8-year-old drive your car? How about on an icy road? Well one father in Russia did just that, and recorded the entire thing. To her credit, the child seemed to be doing a great job. (0:44)\n\nTags: 8-year-old driver , caught on camera , child driver , pix11\n\nSummary:",  # noqa: B950
+                        },
+                        {
+                            "from": "gpt",
+                            "value": "A father in Russia allowed his 8-year-old child to drive his car on an icy road and recorded the event. The child appeared to be handling the situation well, showcasing their driving skills despite the challenging conditions.",  # noqa: B950
+                        },
+                    ]
+                }
+            ]
+        )
+        ds = slimorca_dataset(
+            tokenizer=tokenizer,
+            train_on_input=train_on_input,
+        )
+        # Generate the input and labels
+        input, labels = ds[0]["tokens"], ds[0]["labels"]
+
+        expected_counts = {
+            3: 28,
+            2: 20,
+            4: 20,
+            5: 20,
+            6: 17,
+            10: 8,
+            1: 7,
+            8: 7,
+            7: 7,
+            9: 2,
+            11: 2,
+            0: 1,
+            12: 1,
+            17: 1,
+            -1: 1,
+        }
+        assert Counter(input) == expected_counts
+        if train_on_input:
+            assert Counter(labels) == expected_counts
+        else:
+            # Check that the input is masked
+            assert labels.count(CROSS_ENTROPY_IGNORE_IDX) == 104
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_stack_exchange_paired_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_stack_exchange_paired_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_stack_exchange_paired_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_stack_exchange_paired_dataset.py	2025-02-20 17:49:29.774024614 -0500
@@ -0,0 +1,154 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from collections import Counter
+from unittest.mock import patch
+
+import pytest
+from datasets import Dataset
+
+from tests.test_utils import assert_dialogue_equal, DummyTokenizer
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+from torchtune.data._messages import Message
+
+from torchtune.datasets._stack_exchange_paired import (
+    stack_exchange_paired_dataset,
+    StackExchangePairedToMessages,
+)
+
+
+class TestStackExchangePairedDataset:
+    @patch("torchtune.datasets._preference.load_dataset")
+    @pytest.mark.parametrize("train_on_input", [True, False])
+    def test_dataset_get_item(self, mock_load_dataset, train_on_input):
+        # Truncated sample data from stack exchange paired dataset
+        mock_load_dataset.return_value = Dataset.from_list(
+            [
+                {
+                    "question": "I have a question about if a animation ends that it "
+                    "will like `gotoAndStop()` to another frame ``` if (bird.hitTestObject(pipe1))"
+                    " { bird.gotoAndStop(3); //frame 3 = animation } ``` after it ends it will need"
+                    " to go the Game Over frame (frame 3) and I use the `Flash Timeline` not `.as` "
+                    "thanks!",
+                    "response_j": "Java does not provide a convenient way to list the 'files' "
+                    "in a 'directory', when that directory is backed by a JAR file on the classpath"
+                    " (see [How do I list the files inside a JAR file?](https://stackoverflow.com/"
+                    "questions/1429172/how-do-i-list-the-files-inside-a-jar-file) for some work-arounds)",
+                    "response_k": "If you are still looking for an actual answer here is [mine]"
+                    "(https://pastebin.com/R0jMh4ui) (it is kinda hacky but its work). To use it "
+                    "you simply have to call one of the 2 options below",
+                }
+            ]
+        )
+        ds = stack_exchange_paired_dataset(
+            tokenizer=DummyTokenizer(),
+            train_on_input=train_on_input,
+        )
+        # Generate the input and labels
+        sample = ds[0]
+
+        expected_chosen_counts = {
+            4: 20,
+            2: 15,
+            3: 15,
+            1: 13,
+            5: 6,
+            9: 5,
+            7: 5,
+            6: 4,
+            0: 1,
+            8: 1,
+            15: 1,
+            27: 1,
+            20: 1,
+            10: 1,
+            12: 1,
+            93: 1,
+            13: 1,
+            -1: 1,
+        }
+        assert Counter(sample["chosen_input_ids"]) == expected_chosen_counts
+        if train_on_input:
+            assert Counter(sample["chosen_labels"]) == expected_chosen_counts
+        else:
+            # Check that the input is masked
+            assert sample["chosen_labels"].count(CROSS_ENTROPY_IGNORE_IDX) == 52
+
+        expected_rejected_counts = {
+            2: 17,
+            3: 17,
+            4: 13,
+            1: 9,
+            5: 9,
+            6: 6,
+            7: 5,
+            9: 3,
+            0: 1,
+            8: 1,
+            15: 1,
+            27: 1,
+            20: 1,
+            37: 1,
+            -1: 1,
+        }
+        assert Counter(sample["rejected_input_ids"]) == expected_rejected_counts
+        if train_on_input:
+            assert Counter(sample["rejected_labels"]) == expected_rejected_counts
+        else:
+            # Check that the input is masked
+            assert sample["rejected_labels"].count(CROSS_ENTROPY_IGNORE_IDX) == 52
+
+
+class TestStackExchangePairedToMessages:
+    @pytest.fixture
+    def sample(self):
+        return {
+            "maybe_prompt": "hello world",
+            "maybe_chosen": "hello world",
+            "maybe_rejected": "bye world",
+        }
+
+    def test_call(self, sample):
+        transform = StackExchangePairedToMessages(
+            column_map={
+                "prompt": "maybe_prompt",
+                "chosen": "maybe_chosen",
+                "rejected": "maybe_rejected",
+            },
+        )
+        actual = transform(sample)
+        expected_chosen = [
+            Message(role="user", content="hello world", masked=True, eot=False),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["chosen"], expected_chosen)
+
+        expected_rejected = [
+            Message(role="user", content="hello world", masked=True, eot=False),
+            Message(role="assistant", content="bye world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["rejected"], expected_rejected)
+
+    def test_call_train_on_input(self, sample):
+        transform = StackExchangePairedToMessages(
+            column_map={
+                "prompt": "maybe_prompt",
+                "chosen": "maybe_chosen",
+                "rejected": "maybe_rejected",
+            },
+            train_on_input=True,
+        )
+        actual = transform(sample)
+        expected_chosen = [
+            Message(role="user", content="hello world", masked=False, eot=False),
+            Message(role="assistant", content="hello world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["chosen"], expected_chosen)
+
+        expected_rejected = [
+            Message(role="user", content="hello world", masked=False, eot=False),
+            Message(role="assistant", content="bye world", masked=False, eot=True),
+        ]
+        assert_dialogue_equal(actual["rejected"], expected_rejected)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_text_completion_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_text_completion_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_text_completion_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_text_completion_dataset.py	2025-02-20 17:49:29.778024620 -0500
@@ -0,0 +1,69 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from unittest import mock
+
+from tests.test_utils import DummyTokenizer
+
+from torchtune.datasets import TextCompletionDataset
+
+
+class TestTextCompletionDataset:
+    expected_tokenized_prompts = [
+        [0, 4, 2, 2, 7, 5, -1],
+        [0, 4, 2, 7, 7, 5, -1],
+    ]
+
+    def get_samples(self):
+        return [
+            {
+                "text": "This is an example text.",
+            },
+            {
+                "text": "This is another example text.",
+            },
+        ]
+
+    @mock.patch("torchtune.datasets._text_completion.load_dataset")
+    def test_get_item(self, mock_load_dataset):
+        mock_load_dataset.return_value = self.get_samples()
+        expected_labels = self.expected_tokenized_prompts
+
+        dataset = TextCompletionDataset(
+            tokenizer=DummyTokenizer(),
+            source="iam/agoofy/goober",
+            column="text",
+            max_seq_len=100,
+        )
+        assert len(dataset) == 2
+        mock_load_dataset.assert_called_once()
+
+        for i in range(len(dataset)):
+            prompt, label = dataset[i]["tokens"], dataset[i]["labels"]
+            assert prompt == self.expected_tokenized_prompts[i]
+            assert label == expected_labels[i]
+
+    @mock.patch("torchtune.datasets._text_completion.load_dataset")
+    def test_get_item_no_eos(self, mock_load_dataset):
+        mock_load_dataset.return_value = self.get_samples()
+        expected_labels = self.expected_tokenized_prompts
+
+        dataset = TextCompletionDataset(
+            tokenizer=DummyTokenizer(),
+            source="iam/agoofy/goober",
+            column="text",
+            max_seq_len=100,
+            add_eos=False,
+        )
+        assert len(dataset) == 2
+        mock_load_dataset.assert_called_once()
+
+        for i in range(len(dataset)):
+            prompt, label = dataset[i]["tokens"], dataset[i]["labels"]
+            # trimming EOS IDs from the expected tokens, assertion is against:
+            # [0, 4, 2, 2, 7, 5]
+            assert prompt == self.expected_tokenized_prompts[i][:-1]
+            assert label == expected_labels[i][:-1]
diff -ruN marc_original/third_party/torchtune/tests/torchtune/datasets/test_wikitext_dataset.py marc/third_party/torchtune/tests/torchtune/datasets/test_wikitext_dataset.py
--- marc_original/third_party/torchtune/tests/torchtune/datasets/test_wikitext_dataset.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/datasets/test_wikitext_dataset.py	2025-02-20 17:49:29.782024627 -0500
@@ -0,0 +1,41 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from unittest.mock import patch
+
+import pytest
+
+from tests.test_utils import DummyTokenizer
+
+from torchtune.datasets import wikitext_dataset
+
+
+class TestWikiTextDataset:
+    @pytest.fixture
+    def tokenizer(self):
+        return DummyTokenizer()
+
+    @patch("torchtune.datasets._text_completion.load_dataset")
+    @pytest.mark.parametrize("max_seq_len", [128, 512, 1024, 4096])
+    def test_dataset_get_item(self, load_dataset, tokenizer, max_seq_len):
+        # Sample data from wikitext dataset
+        load_dataset.return_value = [
+            {
+                "page": "Bart , like the rest of his family , has yellow skin . "
+                "Bart usually wears a red T @-@ shirt , blue shorts and blue trainers . "
+                "When the Simpson family goes to church in the episodes , or to school "
+                "events or shows , Bart wears a blue suit with a white shirt , a purple "
+                "tie , blue shorts and a blue jacket .",
+            }
+        ]
+        ds = wikitext_dataset(
+            tokenizer=tokenizer,
+            max_seq_len=max_seq_len,
+        )
+        input, label = ds[0]["tokens"], ds[0]["labels"]
+        assert len(input) <= max_seq_len
+        assert len(label) <= max_seq_len
+        assert len(input) == len(label)
+        assert input[0] == tokenizer.bos_id
diff -ruN marc_original/third_party/torchtune/tests/torchtune/generation/__init__.py marc/third_party/torchtune/tests/torchtune/generation/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/generation/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/generation/__init__.py	2025-02-20 17:49:29.786024634 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/generation/test_generation.py marc/third_party/torchtune/tests/torchtune/generation/test_generation.py
--- marc_original/third_party/torchtune/tests/torchtune/generation/test_generation.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/generation/test_generation.py	2025-02-20 17:49:29.790024640 -0500
@@ -0,0 +1,587 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+from tests.test_utils import fixed_init_model, mps_ignored_test
+
+from torchtune.generation._generation import (
+    generate,
+    get_causal_mask_from_padding_mask,
+    get_position_ids_from_padding_mask,
+    sample,
+)
+from torchtune.models.llama2 import llama2
+
+
+class TestGenerate:
+    """
+    Test class for text generation functionality in :func:`~torchtune.generation.geneate`.
+    """
+
+    @pytest.fixture
+    def generation_model_no_kv_cache(self):
+        model = llama2(
+            vocab_size=4_000,
+            embed_dim=128,
+            num_layers=2,
+            num_heads=4,
+            num_kv_heads=4,
+            max_seq_len=2048,
+        )
+        fixed_init_model(model)
+        model.eval()
+        return model
+
+    @pytest.fixture
+    def generation_model_kv_cache(self):
+        model = llama2(
+            vocab_size=4_000,
+            embed_dim=128,
+            num_layers=2,
+            num_heads=4,
+            num_kv_heads=4,
+            max_seq_len=2048,
+        )
+        fixed_init_model(model)
+        model.setup_caches(batch_size=1, dtype=torch.float32)
+        model.eval()
+        return model
+
+    @pytest.fixture
+    def generation_model_kv_cache_batched(self):
+        model = llama2(
+            vocab_size=4_000,
+            embed_dim=128,
+            num_layers=2,
+            num_heads=4,
+            num_kv_heads=4,
+            max_seq_len=2048,
+        )
+        fixed_init_model(model)
+        model.setup_caches(batch_size=3, dtype=torch.float32)
+        model.eval()
+        return model
+
+    @pytest.fixture
+    def generation_model_batched_fixed_cache_seq_len(self, dtype=torch.float32):
+        model = llama2(
+            vocab_size=4_000,
+            embed_dim=128,
+            num_layers=2,
+            num_heads=4,
+            num_kv_heads=4,
+            max_seq_len=2048,
+        )
+        fixed_init_model(model)
+        model.setup_caches(batch_size=3, dtype=dtype, decoder_max_seq_len=1024)
+        model.eval()
+        return model
+
+    @pytest.fixture
+    def prompt_tokens(self):
+        """
+        Pytest fixture to create a list of prompt tokens for testing.
+        """
+        return torch.arange(2, 10)
+
+    @pytest.fixture
+    def prompt_tokens_batched(self):
+        """
+        Pytest fixture to create a list of batched prompt tokens for testing.
+        """
+        return torch.arange(2, 10).repeat(3, 1)
+
+    @pytest.fixture
+    def prompt_tokens_left_padded(self):
+        """
+        Pytest fixture to create a list of left-padded prompt tokens for testing.
+        """
+        return torch.cat([torch.tensor([0, 0]), torch.arange(2, 10)])
+
+    @pytest.fixture
+    def prompt_tokens_batched_left_padded(self):
+        """
+        Pytest fixture to create a list of left-padded batched prompt tokens for testing.
+        """
+        return torch.cat([torch.tensor([0, 0]), torch.arange(2, 10)]).repeat(3, 1)
+
+    @pytest.fixture
+    def expected_tokens(self):
+        """
+        The numbers here are the first 10 tokens generated by the model
+        with constantly initialized weights, a tensor input with range 2 through 10,
+        and the manual seed set to 42. They do not correspond to "recognizable" tokens.
+        """
+        return torch.tensor(
+            [
+                [
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9,
+                    3987,
+                    3991,
+                    3953,
+                    3957,
+                    3983,
+                    3964,
+                    3928,
+                    3932,
+                    3986,
+                    3982,
+                ]
+            ]
+        )
+
+    @pytest.fixture
+    def expected_tokens_batched(self):
+        """
+        The numbers here are the first 10 tokens generated by the model
+        with constantly initialized weights, a tensor input with range 2 through 10,
+        and the manual seed set to 42. They do not correspond to "recognizable" tokens.
+        """
+        return torch.tensor(
+            [
+                [
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9,
+                    3987,
+                    3991,
+                    3953,
+                    3957,
+                    3983,
+                    3964,
+                    3928,
+                    3932,
+                    3986,
+                    3982,
+                ],
+                [
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9,
+                    3958,
+                    3979,
+                    3934,
+                    3945,
+                    3993,
+                    3904,
+                    3950,
+                    3988,
+                    3948,
+                    3999,
+                ],
+                [
+                    2,
+                    3,
+                    4,
+                    5,
+                    6,
+                    7,
+                    8,
+                    9,
+                    3989,
+                    3976,
+                    3997,
+                    3960,
+                    3989,
+                    3956,
+                    3917,
+                    3949,
+                    3917,
+                    3987,
+                ],
+            ]
+        )
+
+    def test_sample_consistency(self):
+        """
+        Test token sampling produces the right output.
+        """
+        # set all probabilities except for token_id=100 to 0
+        logits = torch.zeros(2000)
+        logits[100] = 1
+        token = sample(logits, top_k=1)
+        assert token.item() == 100
+
+    @pytest.mark.parametrize(
+        "model1",
+        ["generation_model_no_kv_cache", "generation_model_kv_cache"],
+    )
+    @pytest.mark.parametrize(
+        "model2",
+        ["generation_model_no_kv_cache", "generation_model_kv_cache"],
+    )
+    def test_reproducibility(self, request, model1, model2, prompt_tokens):
+        """
+        Test to check if the generate function produces the same output when run with the same
+        fixed seed and models with and without kv-cacheing.
+        """
+
+        model1 = request.getfixturevalue(model1)
+        model2 = request.getfixturevalue(model2)
+
+        temperature = 0.6
+        top_k = 100
+
+        torch.manual_seed(42)
+        outputs_first, _ = generate(
+            model=model1,
+            prompt=prompt_tokens,
+            max_generated_tokens=10,
+            temperature=temperature,
+            top_k=top_k,
+        )
+
+        torch.manual_seed(42)
+        outputs_second, _ = generate(
+            model=model2,
+            prompt=prompt_tokens,
+            max_generated_tokens=10,
+            temperature=temperature,
+            top_k=top_k,
+        )
+
+        # slicing for the last 18 tokens - this is the whole sequence for unpadded inputs
+        # and excludes the first two tokens for padded inputs, which are padding tokens
+        assert torch.equal(outputs_first, outputs_second)
+
+    @pytest.mark.parametrize(
+        "model1",
+        [
+            "generation_model_no_kv_cache",
+            "generation_model_kv_cache_batched",
+            "generation_model_batched_fixed_cache_seq_len",
+        ],
+    )
+    @pytest.mark.parametrize(
+        "model2",
+        [
+            "generation_model_no_kv_cache",
+            "generation_model_kv_cache_batched",
+            "generation_model_batched_fixed_cache_seq_len",
+        ],
+    )
+    @pytest.mark.parametrize(
+        "prompt1", ["prompt_tokens_batched", "prompt_tokens_batched_left_padded"]
+    )
+    @pytest.mark.parametrize(
+        "prompt2", ["prompt_tokens_batched", "prompt_tokens_batched_left_padded"]
+    )
+    def test_reproducibility_batched(self, request, model1, model2, prompt1, prompt2):
+        """
+        Test to check if the generate function produces the same output when run with the same
+        fixed seed, models with and without kv-cacheing, and batched inputs with and without left-padding.
+        """
+
+        model1 = request.getfixturevalue(model1)
+        model2 = request.getfixturevalue(model2)
+        prompt1 = request.getfixturevalue(prompt1)
+        prompt2 = request.getfixturevalue(prompt2)
+
+        temperature = 0.6
+        top_k = 100
+
+        torch.manual_seed(42)
+        outputs_first, _ = generate(
+            model=model1,
+            prompt=prompt1,
+            max_generated_tokens=10,
+            temperature=temperature,
+            top_k=top_k,
+        )
+
+        torch.manual_seed(42)
+        outputs_second, _ = generate(
+            model=model2,
+            prompt=prompt2,
+            max_generated_tokens=10,
+            temperature=temperature,
+            top_k=top_k,
+        )
+
+        # slicing for the last 18 tokens - this is the whole sequence for unpadded inputs
+        # and excludes the first two tokens for padded inputs, which are padding tokens
+        assert torch.equal(outputs_first[:, -18:], outputs_second[:, -18:])
+
+    @pytest.mark.parametrize(
+        "model",
+        ["generation_model_no_kv_cache", "generation_model_kv_cache_batched"],
+    )
+    @pytest.mark.parametrize(
+        "prompt", ["prompt_tokens_batched", "prompt_tokens_batched_left_padded"]
+    )
+    @mps_ignored_test()
+    def test_stop_tokens_batched(self, request, model, prompt, expected_tokens_batched):
+        """
+        Test to check if the `generate` function produces the right output when stop tokens are
+        provided.
+        """
+        model = request.getfixturevalue(model)
+        prompt = request.getfixturevalue(prompt)
+        temperature = 0.6
+        top_k = 100
+
+        # This is the first token generated by the model
+        # so it should stop immediately
+        stop_tokens = [3987, 3958, 3989]
+
+        torch.manual_seed(42)
+
+        outputs, _ = generate(
+            model=model,
+            prompt=prompt,
+            max_generated_tokens=10,
+            temperature=temperature,
+            top_k=top_k,
+            stop_tokens=stop_tokens,
+        )
+
+        assert torch.equal(outputs[:, -9:], expected_tokens_batched[:, :9])
+
+    @pytest.mark.parametrize(
+        "model",
+        ["generation_model_no_kv_cache", "generation_model_kv_cache"],
+    )
+    @mps_ignored_test()
+    def test_stop_tokens(self, request, model, prompt_tokens, expected_tokens):
+        """
+        Test to check if the `generate` function produces the right output when stop tokens are
+        provided.
+        """
+        model = request.getfixturevalue(model)
+        temperature = 0.6
+        top_k = 100
+
+        # This is the first token generated by the model
+        # so it should stop immediately
+        stop_tokens = [3987]
+
+        torch.manual_seed(42)
+
+        outputs, _ = generate(
+            model=model,
+            prompt=prompt_tokens,
+            max_generated_tokens=10,
+            temperature=temperature,
+            top_k=top_k,
+            stop_tokens=stop_tokens,
+        )
+
+        assert torch.equal(outputs, expected_tokens[:, :9])
+
+    @pytest.mark.parametrize(
+        "model",
+        ["generation_model_no_kv_cache", "generation_model_kv_cache_batched"],
+    )
+    @mps_ignored_test()
+    def test_stop_tokens_batched_uneven_stopping(
+        self, request, model, prompt_tokens_batched
+    ):
+        """
+        Test to check if the `generate` function produces the right output when different sequences
+        in the batch stop at different lengths.
+        """
+        model = request.getfixturevalue(model)
+        temperature = 0.6
+        top_k = 100
+
+        stop_tokens = [3953, 3979, 3989]
+
+        torch.manual_seed(42)
+
+        outputs, _ = generate(
+            model=model,
+            prompt=prompt_tokens_batched,
+            max_generated_tokens=10,
+            temperature=temperature,
+            top_k=top_k,
+            stop_tokens=stop_tokens,
+        )
+
+        expected_output = torch.tensor(
+            [
+                [2, 3, 4, 5, 6, 7, 8, 9, 3987, 3991, 3953],
+                [2, 3, 4, 5, 6, 7, 8, 9, 3958, 3979, 0],
+                [2, 3, 4, 5, 6, 7, 8, 9, 3989, 0, 0],
+            ]
+        )
+
+        assert torch.equal(outputs, expected_output)
+
+    @pytest.mark.parametrize(
+        "model",
+        ["generation_model_no_kv_cache", "generation_model_kv_cache_batched"],
+    )
+    @mps_ignored_test()
+    def test_stop_tokens_batched_uneven_stopping_left_padded(
+        self, request, model, prompt_tokens_batched_left_padded
+    ):
+        """
+        Test to check if the `generate` function produces the right output when different sequences
+        in the batch stop at different lengths.
+        """
+        model = request.getfixturevalue(model)
+        temperature = 0.6
+        top_k = 100
+
+        stop_tokens = [3953, 3979, 3989]
+
+        torch.manual_seed(42)
+
+        outputs, _ = generate(
+            model=model,
+            prompt=prompt_tokens_batched_left_padded,
+            max_generated_tokens=10,
+            temperature=temperature,
+            top_k=top_k,
+            stop_tokens=stop_tokens,
+        )
+
+        expected_output = torch.tensor(
+            [
+                [0, 0, 2, 3, 4, 5, 6, 7, 8, 9, 3987, 3991, 3953],
+                [0, 0, 2, 3, 4, 5, 6, 7, 8, 9, 3958, 3979, 0],
+                [0, 0, 2, 3, 4, 5, 6, 7, 8, 9, 3989, 0, 0],
+            ]
+        )
+
+        assert torch.equal(outputs, expected_output)
+
+
+class TestGetPositionIDsFromPaddingMask:
+    def test_get_position_ids_padding(self):
+        outputs = get_position_ids_from_padding_mask(
+            torch.Tensor([False, False, False, True, True, True, True, True])
+        )
+        expected_outputs = torch.Tensor([0, 0, 0, 0, 1, 2, 3, 4])
+        assert torch.equal(outputs, expected_outputs)
+
+    def test_get_position_ids_no_padding(self):
+        outputs = get_position_ids_from_padding_mask(
+            torch.Tensor([True, True, True, True, True, True, True, True])
+        )
+        expected_outputs = torch.Tensor([0, 1, 2, 3, 4, 5, 6, 7])
+        assert torch.equal(outputs, expected_outputs)
+
+    def test_get_position_ids_batched(self):
+        outputs = get_position_ids_from_padding_mask(
+            torch.Tensor(
+                [
+                    [False, False, False, True, True, True, True, True],
+                    [False, False, False, False, False, False, True, True],
+                    [True, True, True, True, True, True, True, True],
+                ]
+            )
+        )
+        expected_outputs = torch.Tensor(
+            [
+                [0, 0, 0, 0, 1, 2, 3, 4],
+                [0, 0, 0, 0, 0, 0, 0, 1],
+                [0, 1, 2, 3, 4, 5, 6, 7],
+            ]
+        )
+        assert torch.equal(outputs, expected_outputs)
+
+
+class TestGetCausalMaskFromPaddingMask:
+    @pytest.fixture
+    def prompt_tokens_batched(self):
+        """
+        Pytest fixture to create a list of batched prompt tokens for testing.
+        """
+        return torch.arange(2, 10).repeat(3, 1)
+
+    @pytest.fixture
+    def left_padded_prompt_tokens(self):
+        """
+        Pytest fixture to create a list of left-padded prompt tokens for testing.
+        """
+        return torch.cat([torch.tensor([0, 0]), torch.arange(2, 6)]).unsqueeze(0)
+
+    @pytest.fixture
+    def left_padded_prompt_tokens_batched(self):
+        """
+        Pytest fixture to create a list of left-padded batched prompt tokens for testing.
+        """
+        return torch.tensor(
+            [[0, 0, 0, 1, 2, 3], [0, 1, 2, 3, 4, 5], [0, 0, 0, 0, 0, 1]]
+        )
+
+    def test_get_causal_mask_for_left_padded_inputs(self, left_padded_prompt_tokens):
+        """
+        Test to check if the `get_causal_mask` function produces the right output for left-padded prompts.
+        """
+        expected_casual_mask = torch.tensor(
+            [
+                [True, False, False, False, False, False],
+                [False, True, False, False, False, False],
+                [False, False, True, False, False, False],
+                [False, False, True, True, False, False],
+                [False, False, True, True, True, False],
+                [False, False, True, True, True, True],
+            ]
+        ).unsqueeze(0)
+
+        causal_mask = get_causal_mask_from_padding_mask(left_padded_prompt_tokens != 0)
+        assert torch.equal(causal_mask, expected_casual_mask)
+
+    def test_get_causal_mask_for_left_padded_inputs_batched(
+        self, left_padded_prompt_tokens_batched
+    ):
+        """
+        Test to check if the `get_causal_mask` function produces the right output for left-padded batched prompts.
+        """
+        expected_causal_mask = torch.tensor(
+            [
+                [
+                    [True, False, False, False, False, False],
+                    [False, True, False, False, False, False],
+                    [False, False, True, False, False, False],
+                    [False, False, False, True, False, False],
+                    [False, False, False, True, True, False],
+                    [False, False, False, True, True, True],
+                ],
+                [
+                    [True, False, False, False, False, False],
+                    [False, True, False, False, False, False],
+                    [False, True, True, False, False, False],
+                    [False, True, True, True, False, False],
+                    [False, True, True, True, True, False],
+                    [False, True, True, True, True, True],
+                ],
+                [
+                    [True, False, False, False, False, False],
+                    [False, True, False, False, False, False],
+                    [False, False, True, False, False, False],
+                    [False, False, False, True, False, False],
+                    [False, False, False, False, True, False],
+                    [False, False, False, False, False, True],
+                ],
+            ]
+        )
+
+        causal_mask = get_causal_mask_from_padding_mask(
+            left_padded_prompt_tokens_batched != 0
+        )
+        assert torch.equal(causal_mask, expected_causal_mask)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/__init__.py marc/third_party/torchtune/tests/torchtune/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/__init__.py	2025-02-20 17:49:29.658024423 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/clip/__init__.py marc/third_party/torchtune/tests/torchtune/models/clip/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/models/clip/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/clip/__init__.py	2025-02-20 17:49:29.798024654 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/clip/test_clip_image_transform.py marc/third_party/torchtune/tests/torchtune/models/clip/test_clip_image_transform.py
--- marc_original/third_party/torchtune/tests/torchtune/models/clip/test_clip_image_transform.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/clip/test_clip_image_transform.py	2025-02-20 17:49:29.802024660 -0500
@@ -0,0 +1,181 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+import numpy as np
+import PIL
+import pytest
+
+import torch
+
+from tests.test_utils import assert_expected
+
+from torchtune.models.clip._transform import CLIPImageTransform
+from torchtune.models.clip.inference._transform import (
+    CLIPImageTransform as CLIPImageTransformInference,
+)
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(0)
+
+
+class TestCLIPImageTransform:
+    @pytest.mark.parametrize(
+        "params",
+        [
+            {
+                "image_size": (100, 400, 3),
+                "expected_shape": torch.Size([2, 3, 224, 224]),
+                "resize_to_max_canvas": False,
+                "expected_tile_means": [0.2230, 0.1763],
+                "expected_tile_max": [1.0, 1.0],
+                "expected_tile_min": [0.0, 0.0],
+                "expected_aspect_ratio": [1, 2],
+                "pad_max_tiles": False,
+            },
+            {
+                "image_size": (100, 400, 3),
+                "expected_shape": torch.Size([4, 3, 224, 224]),
+                "resize_to_max_canvas": False,
+                "expected_tile_means": [0.2230, 0.1763, 0.0, 0.0],
+                "expected_tile_max": [1.0, 1.0, 0.0, 0.0],
+                "expected_tile_min": [0.0, 0.0, 0.0, 0.0],
+                "expected_aspect_ratio": [1, 2],
+                "pad_max_tiles": True,
+            },
+            {
+                "image_size": (1000, 300, 3),
+                "expected_shape": torch.Size([4, 3, 224, 224]),
+                "resize_to_max_canvas": True,
+                "expected_tile_means": [0.5007, 0.4995, 0.5003, 0.1651],
+                "expected_tile_max": [0.9705, 0.9694, 0.9521, 0.9314],
+                "expected_tile_min": [0.0353, 0.0435, 0.0528, 0.0],
+                "expected_aspect_ratio": [4, 1],
+                "pad_max_tiles": False,
+            },
+            {
+                "image_size": (200, 200, 3),
+                "expected_shape": torch.Size([4, 3, 224, 224]),
+                "resize_to_max_canvas": True,
+                "expected_tile_means": [0.5012, 0.5020, 0.5011, 0.4991],
+                "expected_tile_max": [0.9922, 0.9926, 0.9970, 0.9908],
+                "expected_tile_min": [0.0056, 0.0069, 0.0059, 0.0033],
+                "expected_aspect_ratio": [2, 2],
+                "pad_max_tiles": False,
+                "pad_tiles": 1,
+            },
+            {
+                "image_size": (600, 200, 3),
+                "expected_shape": torch.Size([3, 3, 224, 224]),
+                "resize_to_max_canvas": False,
+                "expected_tile_means": [0.4473, 0.4469, 0.3032],
+                "expected_tile_max": [1.0, 1.0, 1.0],
+                "expected_tile_min": [0.0, 0.0, 0.0],
+                "expected_aspect_ratio": [3, 1],
+                "pad_max_tiles": False,
+            },
+            {
+                "image_size": (600, 200, 3),
+                "expected_shape": torch.Size([4, 3, 224, 224]),
+                "resize_to_max_canvas": False,
+                "expected_tile_means": [0.4473, 0.4469, 0.3032, 0.0],
+                "expected_tile_max": [1.0, 1.0, 1.0, 0.0],
+                "expected_tile_min": [0.0, 0.0, 0.0, 0.0],
+                "expected_aspect_ratio": [3, 1],
+                "pad_max_tiles": True,
+            },
+        ],
+    )
+    def test_clip_image_transform(self, params):
+        # Initialize the image transformation with specified parameters
+        image_transform = CLIPImageTransform(
+            image_mean=None,
+            image_std=None,
+            tile_size=224,
+            possible_resolutions=None,
+            max_num_tiles=4,
+            resample="bilinear",
+            dtype=torch.float32,
+            resize_to_max_canvas=params["resize_to_max_canvas"],
+            pad_max_tiles=params["pad_max_tiles"],
+        )
+
+        image_transform_inference = CLIPImageTransformInference(
+            image_mean=None,
+            image_std=None,
+            tile_size=224,
+            possible_resolutions=None,
+            max_num_tiles=4,
+            resample="bilinear",
+            resize_to_max_canvas=params["resize_to_max_canvas"],
+            antialias=True,
+            pad_max_tiles=params["pad_max_tiles"],
+        )
+
+        # Generate a deterministic image using np.arange for reproducibility
+        image_size = params["image_size"]
+        image = (
+            np.random.randint(0, 256, np.prod(image_size))
+            .reshape(image_size)
+            .astype(np.uint8)
+        )
+        image = PIL.Image.fromarray(image)
+
+        # Apply the transformation
+        output = image_transform({"image": image})
+        output_image = output["image"]
+        output_ar = output["aspect_ratio"]
+
+        inference_output = image_transform_inference(image=image)
+        inference_output_image = inference_output["image"]
+        inference_output_ar = inference_output["aspect_ratio"]
+
+        # Check output is the same across CLIPImageTransform and CLIPImageTransformInference.
+        assert torch.allclose(output_image, inference_output_image)
+        assert output_ar[0] == inference_output_ar[0]
+        assert output_ar[1] == inference_output_ar[1]
+
+        # Check if the output shape matches the expected shape
+        assert (
+            output_image.shape == params["expected_shape"]
+        ), f"Expected shape {params['expected_shape']} but got {output_image.shape}"
+
+        # Check if the pixel values are within the expected range [0, 1]
+        assert (
+            0 <= output_image.min() <= output_image.max() <= 1
+        ), f"Expected pixel values to be in range [0, 1] but got {output_image.min()} and {output_image.max()}"
+
+        # Check if the mean, max, and min values of the tiles match the expected values
+        for i, tile in enumerate(output_image):
+            assert_expected(
+                tile.mean().item(), params["expected_tile_means"][i], rtol=0, atol=1e-4
+            )
+            assert_expected(
+                tile.max().item(), params["expected_tile_max"][i], rtol=0, atol=1e-4
+            )
+            assert_expected(
+                tile.min().item(), params["expected_tile_min"][i], rtol=0, atol=1e-4
+            )
+
+        #  aspect ratio matches the expected aspect ratio
+        assert tuple(output_ar.numpy()) == tuple(
+            params["expected_aspect_ratio"]
+        ), f"Expected aspect ratio {params['expected_aspect_ratio']} but got {tuple(output_ar.numpy())}"
+
+        # number of tiles matches the product of the aspect ratio
+        if params["pad_max_tiles"]:
+            # max_num_tiles=4.
+            assert (
+                4 == output_image.shape[0]
+            ), f"Expected 4 tiles but got {output_image.shape[0]}"
+        else:
+            expected_num_tiles = output_ar[0] * output_ar[1]
+            assert (
+                expected_num_tiles == output_image.shape[0]
+            ), f"Expected {expected_num_tiles} tiles but got {output_image.shape[0]}"
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/clip/test_pos_embedding_interpolation.py marc/third_party/torchtune/tests/torchtune/models/clip/test_pos_embedding_interpolation.py
--- marc_original/third_party/torchtune/tests/torchtune/models/clip/test_pos_embedding_interpolation.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/clip/test_pos_embedding_interpolation.py	2025-02-20 17:49:29.806024666 -0500
@@ -0,0 +1,349 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+
+import pytest
+import torch
+
+from tests.test_utils import assert_expected
+
+from torchtune.models.clip._position_embeddings import (
+    TiledTokenPositionalEmbedding,
+    TilePositionalEmbedding,
+)
+
+# generated comparing vs fairinternal/internal-llama-models
+tile_pos_emb_test_cases = [
+    {
+        "tgt_max_num_tiles": 1,
+        "input_tensor": torch.tensor(
+            [[[[0.0, 1.0]], [[2.0, 3.0]]], [[[4.0, 5.0]], [[6.0, 7.0]]]]
+        ),
+        "expected_output": torch.tensor([[[[0.0, 1.0]]]]),
+    },
+    {
+        "tgt_max_num_tiles": 3,
+        "input_tensor": torch.tensor([[[[0.0]]]]),
+        "expected_output": torch.tensor(
+            [
+                [[[0.0]], [[0.0]], [[0.0]]],
+                [[[0.0]], [[0.0]], [[0.0]]],
+                [[[0.0]], [[0.0]], [[0.0]]],
+            ]
+        ),
+    },
+    {
+        "tgt_max_num_tiles": 2,
+        "input_tensor": torch.tensor(
+            [
+                [[[0.0, 1.0]], [[2.0, 3.0]], [[4.0, 5.0]]],
+                [[[6.0, 7.0]], [[8.0, 9.0]], [[10.0, 11.0]]],
+                [[[12.0, 13.0]], [[14.0, 15.0]], [[16.0, 17.0]]],
+            ]
+        ),
+        "expected_output": torch.tensor(
+            [[[[0.0, 1.0]], [[4.0, 5.0]]], [[[12.0, 13.0]], [[16.0, 17.0]]]]
+        ),
+    },
+]
+
+local_pos_emb_test_cases = [
+    {
+        "tgt_patch_grid_size": 2,
+        "expected_shape": torch.Size([5, 2]),
+        "input_tensor": torch.tensor(
+            [[0.0, 1.0], [2.0, 3.0], [4.0, 5.0], [6.0, 7.0], [8.0, 9.0]]
+        ),
+        "expected_output": torch.tensor(
+            [[0.0, 1.0], [2.0, 3.0], [4.0, 5.0], [6.0, 7.0], [8.0, 9.0]]
+        ),
+    },
+    {
+        "tgt_patch_grid_size": 1,
+        "expected_shape": torch.Size([2, 1]),
+        "input_tensor": torch.tensor([[0.0], [1.0], [2.0], [3.0], [4.0]]),
+        "expected_output": torch.tensor([[0.0], [1.0]]),
+    },
+    {
+        "tgt_patch_grid_size": 2,
+        "expected_shape": torch.Size([5, 2]),
+        "input_tensor": torch.tensor([[0.0, 1.0], [2.0, 3.0]]),
+        "expected_output": torch.tensor(
+            [[0.0, 1.0], [2.0, 3.0], [2.0, 3.0], [2.0, 3.0], [2.0, 3.0]]
+        ),
+    },
+]
+
+global_pos_emb_test_cases = [
+    {
+        "tgt_max_num_tiles": 1,
+        "tgt_patch_grid_size": 2,
+        "input_tensor": torch.tensor(
+            [
+                [
+                    [[0.0, 1.0], [2.0, 3.0], [4.0, 5.0], [6.0, 7.0], [8.0, 9.0]],
+                    [
+                        [10.0, 11.0],
+                        [12.0, 13.0],
+                        [14.0, 15.0],
+                        [16.0, 17.0],
+                        [18.0, 19.0],
+                    ],
+                ],
+                [
+                    [
+                        [20.0, 21.0],
+                        [22.0, 23.0],
+                        [24.0, 25.0],
+                        [26.0, 27.0],
+                        [28.0, 29.0],
+                    ],
+                    [
+                        [30.0, 31.0],
+                        [32.0, 33.0],
+                        [34.0, 35.0],
+                        [36.0, 37.0],
+                        [38.0, 39.0],
+                    ],
+                ],
+            ]
+        ),
+        "expected_output": torch.tensor(
+            [[[[0.0, 1.0], [2.0, 3.0], [14.0, 15.0], [26.0, 27.0], [38.0, 39.0]]]]
+        ),
+    },
+    {
+        "tgt_max_num_tiles": 3,
+        "tgt_patch_grid_size": 1,
+        "input_tensor": torch.tensor([[[[0.0], [1.0], [2.0], [3.0], [4.0]]]]),
+        "expected_output": torch.tensor(
+            [
+                [[[0.0000], [1.0000]], [[0.0000], [1.5000]], [[0.0000], [2.0000]]],
+                [[[0.0000], [2.0000]], [[0.0000], [2.5000]], [[0.0000], [3.0000]]],
+                [[[0.0000], [3.0000]], [[0.0000], [3.5000]], [[0.0000], [4.0000]]],
+            ]
+        ),
+    },
+    {
+        "tgt_max_num_tiles": 2,
+        "tgt_patch_grid_size": 2,
+        "input_tensor": torch.tensor(
+            [
+                [
+                    [[0.0, 1.0], [2.0, 3.0]],
+                    [[4.0, 5.0], [6.0, 7.0]],
+                    [[8.0, 9.0], [10.0, 11.0]],
+                ],
+                [
+                    [[12.0, 13.0], [14.0, 15.0]],
+                    [[16.0, 17.0], [18.0, 19.0]],
+                    [[20.0, 21.0], [22.0, 23.0]],
+                ],
+                [
+                    [[24.0, 25.0], [26.0, 27.0]],
+                    [[28.0, 29.0], [30.0, 31.0]],
+                    [[32.0, 33.0], [34.0, 35.0]],
+                ],
+            ]
+        ),
+        "expected_output": torch.tensor(
+            [
+                [
+                    [
+                        [0.0000, 1.0000],
+                        [2.0000, 3.0000],
+                        [4.6667, 5.6667],
+                        [10.0000, 11.0000],
+                        [12.6667, 13.6667],
+                    ],
+                    [
+                        [8.0000, 9.0000],
+                        [7.3333, 8.3333],
+                        [10.0000, 11.0000],
+                        [15.3333, 16.3333],
+                        [18.0000, 19.0000],
+                    ],
+                ],
+                [
+                    [
+                        [24.0000, 25.0000],
+                        [18.0000, 19.0000],
+                        [20.6667, 21.6667],
+                        [26.0000, 27.0000],
+                        [28.6667, 29.6667],
+                    ],
+                    [
+                        [32.0000, 33.0000],
+                        [23.3333, 24.3333],
+                        [26.0000, 27.0000],
+                        [31.3333, 32.3333],
+                        [34.0000, 35.0000],
+                    ],
+                ],
+            ]
+        ),
+    },
+]
+
+
+class TestPositionalEmbeddingsInterpolation:
+    @pytest.mark.parametrize("params", tile_pos_emb_test_cases)
+    def test_tile_resize_position_embedding(self, params):
+        tgt_max_num_tiles = params["tgt_max_num_tiles"]
+        expected_output = params["expected_output"]
+        embedding = params["input_tensor"]
+
+        resized_pos_embed = TilePositionalEmbedding._resize_position_embedding(
+            embedding, tgt_max_num_tiles
+        )
+
+        assert_expected(resized_pos_embed, expected_output, atol=1e-3, rtol=1e-4)
+
+    @pytest.mark.parametrize("params", local_pos_emb_test_cases)
+    def test_resize_local_position_embedding(self, params):
+        input_tensor = params["input_tensor"]
+        tgt_patch_grid_size = params["tgt_patch_grid_size"]
+        expected_output = params["expected_output"]
+
+        resized_pos_embed = (
+            TiledTokenPositionalEmbedding._resize_local_position_embedding(
+                input_tensor, tgt_patch_grid_size
+            )
+        )
+
+        assert_expected(resized_pos_embed, expected_output, atol=1e-3, rtol=1e-4)
+
+    @pytest.mark.parametrize("params", global_pos_emb_test_cases)
+    def test_resize_global_position_embedding(self, params):
+        input_tensor = params["input_tensor"]
+        tgt_max_num_tiles = params["tgt_max_num_tiles"]
+        tgt_patch_grid_size = params["tgt_patch_grid_size"]
+        expected_output = params["expected_output"]
+
+        resized_pos_embed = (
+            TiledTokenPositionalEmbedding._resize_global_position_embedding(
+                input_tensor, tgt_max_num_tiles, tgt_patch_grid_size
+            )
+        )
+
+        assert_expected(resized_pos_embed, expected_output, atol=1e-3, rtol=1e-4)
+
+    @pytest.mark.parametrize(
+        "local_params, global_params",
+        zip(local_pos_emb_test_cases, global_pos_emb_test_cases),
+    )
+    def test_load_state_dict_hook_tiled_token(self, local_params, global_params):
+        # Corrected parameters for instantiation
+        global_max_num_tiles = global_params["expected_output"].shape[0]
+        global_embed_dim = global_params["expected_output"].shape[-1]
+        n_tokens_per_tile = local_params["expected_output"].shape[
+            0
+        ]  # Assuming first dimension is tokens per tile
+        patch_grid_size = int(math.sqrt(n_tokens_per_tile - 1))
+        tile_size = patch_grid_size * 1  # Assuming patch_size is 1 for simplicity
+        patch_size = 1
+
+        # Instantiate the model
+        model = TiledTokenPositionalEmbedding(
+            max_num_tiles=global_max_num_tiles,
+            embed_dim=global_embed_dim,
+            tile_size=tile_size,
+            patch_size=patch_size,
+        )
+
+        # Create state_dict mimicking loading scenario
+        state_dict = {
+            "model.local_token_positional_embedding": local_params["input_tensor"],
+            "model.global_token_positional_embedding": global_params["input_tensor"],
+        }
+
+        # Call the hook directly (simulating loading process)
+        state_dict_copy = state_dict.copy()
+        model._load_state_dict_hook(state_dict_copy, "model.")
+
+        # Assert expected outputs
+        assert_expected(
+            state_dict_copy["model.local_token_positional_embedding"],
+            local_params["expected_output"],
+            atol=1e-3,
+            rtol=1e-4,
+        )
+        assert_expected(
+            state_dict_copy["model.global_token_positional_embedding"],
+            global_params["expected_output"],
+            atol=1e-3,
+            rtol=1e-4,
+        )
+
+        # Check for errors with non-square token grid sizes
+        with pytest.raises(ValueError):
+            bad_state_dict = state_dict.copy()
+
+            # add +1 to num_token dimension to make it non-square
+            local_pos_emb = bad_state_dict["model.local_token_positional_embedding"]
+            bad_local_pos_emb = torch.cat(
+                (local_pos_emb, local_pos_emb[0].unsqueeze(0)), dim=0
+            )
+            bad_state_dict["model.local_token_positional_embedding"] = bad_local_pos_emb
+
+            # call
+            model._load_state_dict_hook(bad_state_dict, "model.")
+
+        # Check for errors with non-square tile grid sizes
+        with pytest.raises(ValueError):
+            bad_state_dict = state_dict.copy()
+
+            # add +1 to num_token dimension to make it non-square
+            global_pos_emb = bad_state_dict["model.global_token_positional_embedding"]
+            bad_global_pos_emb = torch.cat(
+                (global_pos_emb, global_pos_emb[:, :, [0]]), dim=2
+            )
+            bad_state_dict[
+                "model.global_token_positional_embedding"
+            ] = bad_global_pos_emb
+
+            # call
+            model._load_state_dict_hook(bad_state_dict, "model.")
+
+    @pytest.mark.parametrize("params", tile_pos_emb_test_cases)
+    def test_load_state_dict_hook_tile(self, params):
+
+        # Extract parameters for instantiation
+        max_num_tiles = params["expected_output"].shape[0]
+        embed_dim = params["expected_output"].shape[-1]
+
+        # Instantiate the model
+        model = TilePositionalEmbedding(
+            max_num_tiles=max_num_tiles,
+            embed_dim=embed_dim,
+        )
+        # Create state_dict mimicking loading scenario
+        state_dict = {
+            "model.embedding": params["input_tensor"],
+        }
+
+        # Call the hook
+        state_dict_copy = state_dict.copy()
+        model._load_state_dict_hook(state_dict_copy, "model.")
+
+        # Assert expected outputs
+        assert_expected(
+            state_dict_copy["model.embedding"],
+            params["expected_output"],
+            atol=1e-3,
+            rtol=1e-4,
+        )
+
+        # Check for errors with non-square tile grid sizes
+        with pytest.raises(ValueError):
+            bad_state_dict = state_dict.copy()
+            # Manipulate the tensor to have non-equal max_num_tiles_x and max_num_tiles_y
+            bad_tensor = torch.cat(
+                (params["input_tensor"], params["input_tensor"][:, [0], :, :]), dim=1
+            )
+            bad_state_dict["model.embedding"] = bad_tensor
+            model._load_state_dict_hook(bad_state_dict, "model.")
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/clip/test_positional_embeddings.py marc/third_party/torchtune/tests/torchtune/models/clip/test_positional_embeddings.py
--- marc_original/third_party/torchtune/tests/torchtune/models/clip/test_positional_embeddings.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/clip/test_positional_embeddings.py	2025-02-20 17:49:29.814024680 -0500
@@ -0,0 +1,87 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+
+from tests.test_utils import assert_expected, fixed_init_model, fixed_init_tensor
+from torchtune.models.clip._position_embeddings import (
+    TiledTokenPositionalEmbedding,
+    TilePositionalEmbedding,
+    TokenPositionalEmbedding,
+)
+
+
+class TestPositionalEmbeddings:
+    @pytest.fixture(autouse=True)
+    def setup_class(self):
+
+        self.embed_dim = 16
+        self.tile_size = 14
+        self.max_num_tiles = 3
+        self.bsz_and_n_imgs = 2
+        self.patch_size = 2
+        self.aspect_ratio = torch.tensor([[3, 1], [1, 2]])
+        self.patch_grid_size = self.tile_size // self.patch_size
+
+        input_tensor = torch.randn(
+            (
+                self.bsz_and_n_imgs,
+                self.max_num_tiles,
+                self.patch_grid_size**2 + 1,
+                self.embed_dim,
+            )
+        )
+        self.input_tensor = fixed_init_tensor(input_tensor.shape, min_val=-1, max_val=1)
+
+    def test_token_positional_embedding(self):
+        # call model
+        embedding = TokenPositionalEmbedding(
+            self.embed_dim, patch_size=self.patch_size, tile_size=self.tile_size
+        )
+        fixed_init_model(embedding, min_val=-1, max_val=1)
+
+        inpt = self.input_tensor.clone().reshape(
+            self.bsz_and_n_imgs * self.max_num_tiles, -1, self.embed_dim
+        )
+        output = embedding(inpt)
+
+        # assertion
+        assert_expected(output.shape, inpt.shape)
+        assert_expected(output.mean(), torch.tensor(-0.001458), atol=1e-3, rtol=1e-3)
+
+    def test_tiled_token_positional_embedding(self):
+        # call model
+        embedding = TiledTokenPositionalEmbedding(
+            self.max_num_tiles,
+            self.embed_dim,
+            patch_size=self.patch_size,
+            tile_size=self.tile_size,
+        )
+        fixed_init_model(embedding, min_val=-1, max_val=1)
+
+        # replace gate 0 -> 0.5
+        embedding.gate = torch.nn.Parameter(torch.full(embedding.gate.shape, 0.5))
+
+        inpt = self.input_tensor.clone()
+        output = embedding(inpt, self.aspect_ratio)
+
+        # assertion
+        assert_expected(output.shape, self.input_tensor.shape)
+        assert_expected(output.mean(), torch.tensor(-0.17208), atol=1e-3, rtol=1e-3)
+
+    def test_tile_positional_embedding(self):
+        # call model
+        embedding = TilePositionalEmbedding(self.max_num_tiles, self.embed_dim)
+        fixed_init_model(embedding, min_val=-1, max_val=1)
+
+        inpt = self.input_tensor.clone()
+        output = embedding(inpt, self.aspect_ratio)
+
+        # assertion
+        assert_expected(output.shape, self.input_tensor.shape)
+        assert_expected(output.mean(), torch.tensor(0.28627), atol=1e-3, rtol=1e-3)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/flamingo/test_flamingo_decoder.py marc/third_party/torchtune/tests/torchtune/models/flamingo/test_flamingo_decoder.py
--- marc_original/third_party/torchtune/tests/torchtune/models/flamingo/test_flamingo_decoder.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/flamingo/test_flamingo_decoder.py	2025-02-20 17:49:29.834024712 -0500
@@ -0,0 +1,61 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from tests.test_utils import assert_expected, fixed_init_model, fixed_init_tensor
+from torchtune.models.llama3_2_vision._component_builders import llama3_2_vision_decoder
+
+
+@pytest.fixture
+def decoder_config():
+    return {
+        "vocab_size": 10000,
+        "num_layers": 6,
+        "fusion_interval": 2,
+        "num_special_tokens": 3,
+        "num_heads": 8,
+        "num_kv_heads": 4,
+        "embed_dim": 512,
+        "max_seq_len": 512,
+        "encoder_max_seq_len": 512,
+        "rope_base": 500000.0,
+        "intermediate_dim": 2048,
+    }
+
+
+class TestLlama3VisionDecoder:
+    @pytest.fixture(autouse=True)
+    def setup_class(self, decoder_config):
+        self.batch_size = 1
+        self.dim = decoder_config["embed_dim"]
+        self.vocab_size = decoder_config["vocab_size"]
+        self.seq_len = 128
+        self.input = {
+            "tokens": torch.arange(self.batch_size * self.seq_len).reshape(
+                self.batch_size, self.seq_len
+            ),
+            "encoder_input": fixed_init_tensor(
+                (self.batch_size, self.seq_len, self.dim), min_val=-1, max_val=1
+            ),
+            "encoder_mask": None,
+        }
+        self.decoder = llama3_2_vision_decoder(**decoder_config)
+        fixed_init_model(self.decoder, min_val=-1, max_val=1)
+
+    @torch.no_grad()
+    def test_llama3_2_vision_decoder(self):
+        # call model
+        output = self.decoder(**self.input)
+
+        # assertion
+        expected_shape = (self.batch_size, self.seq_len, self.vocab_size)
+
+        assert (
+            output.shape == expected_shape
+        ), f"Expected shape {expected_shape}, but got {output.shape}"
+
+        assert_expected(output.mean(), torch.tensor(-9.47548e-5), atol=1e-3, rtol=1e-3)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/flamingo/test_flamingo_encoder.py marc/third_party/torchtune/tests/torchtune/models/flamingo/test_flamingo_encoder.py
--- marc_original/third_party/torchtune/tests/torchtune/models/flamingo/test_flamingo_encoder.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/flamingo/test_flamingo_encoder.py	2025-02-20 17:49:29.838024719 -0500
@@ -0,0 +1,130 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from tests.test_utils import assert_expected, fixed_init_model, fixed_init_tensor
+from torchtune.models.llama3_2_vision._component_builders import llama3_2_vision_encoder
+
+
+@pytest.fixture
+def transformer_config():
+    return {
+        "clip_embed_dim": 32,
+        "clip_num_layers": 2,
+        "num_heads": 4,
+        "tile_size": 49,
+        "patch_size": 9,
+        "max_num_tiles": 4,
+        "in_channels": 3,
+        "clip_hidden_states": [0, 1],
+        "num_layers_projection": 2,
+        "decoder_embed_dim": 128,
+    }
+
+
+@pytest.fixture
+def vision_transformer(transformer_config):
+    vision_transformer = llama3_2_vision_encoder(**transformer_config).eval()
+    fixed_init_model(vision_transformer, min_val=-1, max_val=1)
+    return vision_transformer
+
+
+class TestLlama3VisionEncoder:
+    @pytest.fixture(autouse=True)
+    def setup_class(self, transformer_config):
+        self.batch_size = 1
+        self.n_imgs = 2
+        self.num_tiles = 4
+        self.aspect_ratio = torch.tensor([[1, 3], [2, 2]]).reshape(
+            self.batch_size, self.n_imgs, 2
+        )
+        image = torch.rand(
+            (
+                self.batch_size,
+                self.n_imgs,
+                self.num_tiles,
+                transformer_config["in_channels"],
+                transformer_config["tile_size"],
+                transformer_config["tile_size"],
+            )
+        )
+        self.image = fixed_init_tensor(image.shape, min_val=-1, max_val=1)
+
+    def test_llama3_2_vision_encoder(self, vision_transformer, transformer_config):
+        # call model
+        output = vision_transformer(self.image, self.aspect_ratio)
+
+        # assertion
+        expected_shape = (
+            self.batch_size,
+            self.n_imgs
+            * self.num_tiles
+            * vision_transformer.clip.get_image_tokens_per_tile(),
+            transformer_config["decoder_embed_dim"],
+        )
+        assert (
+            output.shape == expected_shape
+        ), f"Expected shape {expected_shape}, but got {output.shape}"
+
+        assert_expected(output.mean(), torch.tensor(5.28800), atol=1e-3, rtol=1e-3)
+
+    def test_fails_if_ar_none_and_multiple_tiles(self, vision_transformer):
+        assert self.image.shape[2] > 1, "This test is not valid for num_tiles=1"
+        try:
+            vision_transformer(self.image, aspect_ratio=None)
+            pytest.fail(
+                "Expected ValueError: If num_tiles>1, aspect_ratio should not be None"
+            )
+        except ValueError:
+            pass  # If ValueError is raised, the test passes
+
+    def test_llama3_2_vision_encoder_single_tile(self, transformer_config):
+        transformer_config = transformer_config.copy()
+        transformer_config["max_num_tiles"] = 1
+        images = self.image[:, :, [0], :, :, :]
+
+        model_with_multiple_tiles = llama3_2_vision_encoder(**transformer_config).eval()
+        fixed_init_model(model_with_multiple_tiles, min_val=-1, max_val=1)
+
+        output = model_with_multiple_tiles(images, aspect_ratio=None)
+
+        expected_shape = (
+            self.batch_size,
+            self.n_imgs * model_with_multiple_tiles.clip.get_image_tokens_per_tile(),
+            transformer_config["decoder_embed_dim"],
+        )
+        assert (
+            output.shape == expected_shape
+        ), f"Expected shape {expected_shape}, but got {output.shape}"
+
+        assert_expected(output.mean(), torch.tensor(5.38046), atol=1e-3, rtol=1e-3)
+
+    def test_llama3_2_no_hidden_layers(self, vision_transformer, transformer_config):
+        # Modify the transformer_config for this specific test
+        transformer_config = transformer_config.copy()
+        transformer_config["clip_hidden_states"] = None
+
+        # Reinitialize the model with the updated configuration
+        model_with_no_hidden = llama3_2_vision_encoder(**transformer_config).eval()
+        fixed_init_model(model_with_no_hidden, min_val=-1, max_val=1)
+
+        # Call model
+        output = model_with_no_hidden(self.image, self.aspect_ratio)
+
+        # Assertion
+        expected_shape = (
+            self.batch_size,
+            self.n_imgs
+            * self.num_tiles
+            * model_with_no_hidden.clip.get_image_tokens_per_tile(),
+            transformer_config["decoder_embed_dim"],
+        )
+        assert (
+            output.shape == expected_shape
+        ), f"Expected shape {expected_shape}, but got {output.shape}"
+
+        assert_expected(output.mean(), torch.tensor(-77.3419), atol=1e-3, rtol=1e-3)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/gemma/test_gemma_tokenizer.py marc/third_party/torchtune/tests/torchtune/models/gemma/test_gemma_tokenizer.py
--- marc_original/third_party/torchtune/tests/torchtune/models/gemma/test_gemma_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/gemma/test_gemma_tokenizer.py	2025-02-20 17:49:29.842024726 -0500
@@ -0,0 +1,473 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from tests.common import ASSETS
+from torchtune.data import Message
+from torchtune.models.gemma import gemma_tokenizer
+
+
+class TestGemmaTokenizer:
+    @pytest.fixture
+    def tokenizer(self):
+        # m.model is a pretrained Sentencepiece model using the following command:
+        # spm.SentencePieceTrainer.train('--input=<TRAIN_FILE> --model_prefix=m --vocab_size=2000')
+        return gemma_tokenizer(str(ASSETS / "m.model"))
+
+    def test_tokenize_messages(self, tokenizer):
+        messages = [
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task. Write a response "
+                "that appropriately completes the request.\n\n### Instruction:\nGenerate "
+                "a realistic dating profile bio.\n\n### Response:\n",
+                masked=True,
+            ),
+            Message(
+                role="assistant",
+                content="I'm an outgoing and friendly person who loves spending time with "
+                "friends and family. I'm also a big-time foodie and love trying out new "
+                "restaurants and different cuisines. I'm a big fan of the arts and enjoy "
+                "going to museums and galleries. I'm looking for someone who shares my "
+                "interest in exploring new places, as well as someone who appreciates a "
+                "good conversation over coffee.",
+            ),
+        ]
+        tokens, mask = tokenizer.tokenize_messages(messages)
+        expected_tokens = [
+            1,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+            2,
+        ]
+        expected_mask = [True] * 75 + [False] * 125
+        assert expected_tokens == tokens
+        assert expected_mask == mask
+
+    def test_tokenize_messages_drop_eos(self, tokenizer):
+        messages = [
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task. Write a response "
+                "that appropriately completes the request.\n\n### Instruction:\nGenerate "
+                "a realistic dating profile bio.\n\n### Response:\n",
+                masked=True,
+            ),
+            Message(
+                role="assistant",
+                content="I'm an outgoing and friendly person who loves spending time with "
+                "friends and family. I'm also a big-time foodie and love trying out new "
+                "restaurants and different cuisines. I'm a big fan of the arts and enjoy "
+                "going to museums and galleries. I'm looking for someone who shares my "
+                "interest in exploring new places, as well as someone who appreciates a "
+                "good conversation over coffee.",
+            ),
+        ]
+        tokens, mask = tokenizer.tokenize_messages(messages, add_eos=False)
+        expected_tokens = [
+            1,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+            2,
+        ]
+        # Drop eos token.
+        expected_tokens = expected_tokens[:-1]
+        # On 1 less then with eos
+        expected_mask = [True] * 75 + [False] * 124
+        assert expected_tokens == tokens
+        assert expected_mask == mask
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/__init__.py marc/third_party/torchtune/tests/torchtune/models/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/models/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/__init__.py	2025-02-20 17:49:29.794024647 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_attention.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_attention.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_attention.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_attention.py	2025-02-20 17:49:29.854024745 -0500
@@ -0,0 +1,276 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+
+import torch
+
+from torch import nn
+
+from torchtune.modules import MultiHeadAttention, RotaryPositionalEmbeddings
+
+
+"""
+Reference implementation of Attention from:
+https://github.com/facebookresearch/llama/blob/main/llama/model.py#L176
+
+Replicating code here to minimize dependencies. The code is modified to
+remove dependencies like FAIRSCale and features like KV Caching.
+The latter is not supported by the current implementation.
+"""
+
+
+def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
+    """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
+    bs, slen, n_kv_heads, head_dim = x.shape
+    if n_rep == 1:
+        return x
+    return (
+        x[:, :, :, None, :]
+        .expand(bs, slen, n_kv_heads, n_rep, head_dim)
+        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
+    )
+
+
+class Attention(nn.Module):
+    def __init__(self, n_heads: int, n_kv_heads: int, dim: int):
+        super().__init__()
+        self.n_kv_heads = n_heads if n_kv_heads is None else n_kv_heads
+        self.n_heads = n_heads
+        self.n_rep = self.n_heads // self.n_kv_heads
+        self.head_dim = dim // n_heads
+        self.dim = dim
+
+        self.wq = nn.Linear(self.dim, self.dim, bias=False)
+        self.wk = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False)
+        self.wv = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False)
+        self.wo = nn.Linear(self.dim, self.dim, bias=False)
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        freqs_cis: torch.Tensor,
+        mask: torch.Tensor,
+    ):
+        bsz, seqlen, _ = x.shape
+        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
+
+        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)
+        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
+        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
+
+        xq = apply_rotary_emb(xq, freqs_cis=freqs_cis)
+        xk = apply_rotary_emb(xk, freqs_cis=freqs_cis)
+
+        # repeat k/v heads if n_kv_heads < n_heads
+        keys = repeat_kv(xk, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)
+        values = repeat_kv(xv, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)
+
+        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
+        keys = keys.transpose(1, 2)
+        values = values.transpose(1, 2)
+        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
+        if mask is not None:
+            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)
+        scores = nn.functional.softmax(scores.float(), dim=-1).type_as(xq)
+        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)
+        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
+        output = self.wo(output)
+        return output
+
+
+"""
+Reference implementation of RoPE from:
+https://github.com/facebookresearch/llama/blob/main/llama/model.py#L80
+
+The original code structures this as stand-alone functions instead of
+a class. Replicating code here to minimize dependencies.
+"""
+
+
+def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:
+    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
+    t = torch.arange(end, device=freqs.device)  # type: ignore
+    freqs = torch.outer(t, freqs).float()  # type: ignore
+    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64
+    return freqs_cis
+
+
+def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
+    ndim = x.ndim
+    assert 0 <= 1 < ndim
+    assert freqs_cis.shape == (
+        x.shape[1],
+        x.shape[-1],
+    ), f"{freqs_cis.shape} does not match {x.shape[1], x.shape[-1]}"
+    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]
+    return freqs_cis.view(*shape)
+
+
+def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:
+    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))
+    freqs_cis = reshape_for_broadcast(freqs_cis, x_)
+    x_out = torch.view_as_real(x_ * freqs_cis).flatten(3)
+    return x_out.type_as(x)
+
+
+def compare_rope(
+    bsz: int, num_heads: int, embed_dim: int, seq_len: int, max_seq_len: int
+) -> None:
+
+    # make sure we have the right seed for generating outputs
+    torch.manual_seed(0)
+
+    head_dim = embed_dim // num_heads
+
+    # generate input tensor
+    x = torch.randn(bsz, seq_len, num_heads, head_dim)
+
+    # Compute the reference tensors
+    freq_cis = precompute_freqs_cis(dim=head_dim, end=max_seq_len * 2)
+    x_out_ref = apply_rotary_emb(x, freqs_cis=freq_cis[:seq_len])
+
+    # Compute the tensors from current implementation
+    rope_emb = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+    x_out = rope_emb(x)
+
+    # Validate correctness
+    torch.testing.assert_close(x_out_ref, x_out, atol=1e-6, rtol=1e-5)
+
+    # value: tensor(6.4543e-05)
+    print(x_out.mean())
+
+    # value: tensor(2165.7053)
+    print(x_out.sum())
+
+    # value: tensor(5.4546)
+    print(x_out.max())
+
+    curr_pos = 10
+    x_out_ref = apply_rotary_emb(x, freqs_cis=freq_cis[curr_pos : curr_pos + seq_len])
+    x_out = rope_emb(x, curr_pos=10)
+
+    # Validate correctness
+    torch.testing.assert_close(x_out_ref, x_out, atol=1e-6, rtol=1e-5)
+
+    # value: tensor(0.0002)
+    print(x_out.mean())
+
+    # value: tensor(5158.3159)
+    print(x_out.sum())
+
+    # value: tensor(5.4543)
+    print(x_out.max())
+
+
+def compare_attention(
+    bsz: int,
+    seq_len: int,
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+) -> None:
+
+    # make sure we have the right seed for generating outputs
+    # this should match up the seed value set in the corresponding
+    # unit test
+    torch.manual_seed(16)
+
+    head_dim = embed_dim // num_heads
+
+    # generate input tensor used by both implementations
+    input_t = torch.randn(bsz, seq_len, embed_dim)
+
+    # generate mask and frequencies tensor needed for the reference
+    # implementation
+    mask = torch.full((1, 1, seq_len, seq_len), float("-inf"))
+    mask = torch.triu(mask, diagonal=1)
+    freq_cis = precompute_freqs_cis(dim=head_dim, end=seq_len)
+
+    # reference implementation; initialize with constant to compare outputs
+    attn_ref = Attention(n_heads=num_heads, n_kv_heads=num_kv_heads, dim=embed_dim)
+    for p in attn_ref.parameters():
+        nn.init.constant_(p, 0.05)
+
+    with torch.no_grad():
+        attn_out_ref = attn_ref(input_t, freq_cis, mask)
+
+    # current implementation; initialize with constant to compare outputs
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+    attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=0.0,
+    )
+    for p in attn.parameters():
+        nn.init.constant_(p, 0.05)
+
+    with torch.no_grad():
+        attn_out = attn(input_t)
+
+    # value: tensor(-27.5074)
+    print(attn_out.mean())
+
+    # output tensors should be similar
+    torch.testing.assert_close(attn_out, attn_out_ref, atol=1e-5, rtol=1e-3)
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser = argparse.ArgumentParser(description="Compare Attention implementations")
+    parser.add_argument("--bsz", type=int, default=4, help="Batch size of input tensor")
+    parser.add_argument(
+        "--seq_len", type=int, default=2048, help="input sequence length"
+    )
+    parser.add_argument(
+        "--embed_dim",
+        type=int,
+        default=4096,
+        help="Embedding dimension used to compute the dim for RopE",
+    )
+    parser.add_argument(
+        "--num_heads",
+        type=int,
+        default=32,
+        help="Number of heads in the attention layer",
+    )
+    parser.add_argument(
+        "--num_kv_heads",
+        type=int,
+        default=8,
+        help="Number of key/value heads in the attention layer",
+    )
+    parser.add_argument(
+        "--max_seq_len", type=int, default=4096, help="max sequence length"
+    )
+
+    args = parser.parse_args()
+
+    compare_rope(
+        args.bsz, args.num_heads, args.embed_dim, args.seq_len, args.max_seq_len
+    )
+
+    compare_attention(
+        args.bsz,
+        args.seq_len,
+        args.embed_dim,
+        args.num_heads,
+        args.num_kv_heads,
+        args.max_seq_len,
+    )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_decoder_layer.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_decoder_layer.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_decoder_layer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_decoder_layer.py	2025-02-20 17:49:29.862024758 -0500
@@ -0,0 +1,201 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional
+
+import torch
+
+from tests.torchtune.models.llama2.scripts.compare_attention import (
+    Attention,
+    precompute_freqs_cis,
+)
+from tests.torchtune.models.llama2.scripts.compare_feed_forward import FeedForwardRef
+
+from torch import nn
+from torchtune.models.llama2._model_utils import scale_hidden_dim_for_mlp
+
+from torchtune.modules import (
+    FeedForward,
+    MultiHeadAttention,
+    RMSNorm,
+    RotaryPositionalEmbeddings,
+    TransformerSelfAttentionLayer,
+)
+
+
+"""
+Reference implementation of RMSNorm from:
+https://github.com/facebookresearch/llama/blob/main/llama/model.py#L34
+
+Replicating code here to minimize dependencies. The code is modified to
+include params for the constructor and remove start_pos (not supported).
+"""
+
+
+class RMSNormRef(torch.nn.Module):
+    def __init__(self, dim: int, eps: float = 1e-5):
+        super().__init__()
+        self.eps = eps
+        self.weight = nn.Parameter(torch.ones(dim))
+
+    def _norm(self, x):
+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
+
+    def forward(self, x):
+        output = self._norm(x.float()).type_as(x)
+        return output * self.weight
+
+
+"""
+Reference implementation of Transformer Decoder Layer from:
+https://github.com/facebookresearch/llama/blob/main/llama/model.py#L351
+
+Replicating code here to minimize dependencies. The code is modified to
+include params for the constructor and remove start_pos (not supported).
+"""
+
+
+class TransformerBlock(nn.Module):
+    def __init__(self, n_heads: int, dim: int, n_kv_heads: int):
+        super().__init__()
+        self.n_heads = n_heads
+        self.dim = dim
+        # self.head_dim = args.dim // args.n_heads
+        self.attention = Attention(n_heads=n_heads, n_kv_heads=n_kv_heads, dim=dim)
+        self.feed_forward = FeedForwardRef(dim=dim, hidden_dim=4 * dim)
+        self.attention_norm = RMSNormRef(dim=dim)
+        self.ffn_norm = RMSNormRef(dim=dim)
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        freqs_cis: torch.Tensor,
+        mask: Optional[torch.Tensor],
+    ):
+        norm_out = self.attention_norm(x)
+        attn_out = self.attention.forward(norm_out, freqs_cis, mask)
+        h = x + attn_out
+        ffn_norm_out = self.ffn_norm(h)
+        mlp_out = self.feed_forward.forward(ffn_norm_out)
+        out = h + mlp_out
+        return out
+
+
+def compare_decoder_layer(
+    bsz: int,
+    seq_len: int,
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+) -> None:
+    # make sure we have the right seed for generating outputs
+    # this should match up the seed value set in the corresponding
+    # unit test
+    torch.manual_seed(16)
+
+    head_dim = embed_dim // num_heads
+
+    # generate input tensor used by both implementations
+    input_t = torch.randn(bsz, seq_len, embed_dim)
+
+    # generate mask and frequencies tensor needed for the reference
+    # implementation
+    mask = torch.full((1, 1, seq_len, seq_len), float("-inf"))
+    mask = torch.triu(mask, diagonal=1)
+    freq_cis = precompute_freqs_cis(dim=head_dim, end=seq_len)
+
+    # reference implementation; initialize with constant to compare outputs
+    transformer_block = TransformerBlock(
+        n_heads=num_heads, n_kv_heads=num_kv_heads, dim=embed_dim
+    )
+    for p in transformer_block.parameters():
+        nn.init.constant_(p, 0.05)
+
+    with torch.no_grad():
+        block_out = transformer_block(x=input_t, freqs_cis=freq_cis, mask=mask)
+
+    # current implementation; initialize with constant to compare outputs
+    norm_eps = 1e-5
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=0.0,
+    )
+    hidden_dim = _scale_hidden_dim_for_mlp(embed_dim)
+    mlp = FeedForward(
+        dim=embed_dim, hidden_dim=hidden_dim, linear_class=torch.nn.Linear
+    )
+    transformer_layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+    for p in transformer_layer.parameters():
+        nn.init.constant_(p, 0.05)
+
+    with torch.no_grad():
+        layer_out = transformer_layer(input_t)
+
+    # value: torch.tensor(18261.0156)
+    print(layer_out.mean())
+
+    torch.testing.assert_close(block_out, layer_out, atol=1e-2, rtol=1e-2)
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser = argparse.ArgumentParser(description="Compare Attention implementations")
+    parser.add_argument("--bsz", type=int, default=4, help="Batch size of input tensor")
+    parser.add_argument(
+        "--seq_len", type=int, default=2048, help="input sequence length"
+    )
+    parser.add_argument(
+        "--embed_dim",
+        type=int,
+        default=4096,
+        help="Embedding dimension used to compute the dim for RopE",
+    )
+    parser.add_argument(
+        "--num_heads",
+        type=int,
+        default=32,
+        help="Number of heads in the attention layer",
+    )
+    parser.add_argument(
+        "--num_kv_heads",
+        type=int,
+        default=8,
+        help="Number of key/value heads in the attention layer",
+    )
+    parser.add_argument(
+        "--max_seq_len", type=int, default=4096, help="max sequence length"
+    )
+
+    args = parser.parse_args()
+
+    compare_decoder_layer(
+        args.bsz,
+        args.seq_len,
+        args.embed_dim,
+        args.num_heads,
+        args.num_kv_heads,
+        args.max_seq_len,
+    )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_decoder.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_decoder.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_decoder.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_decoder.py	2025-02-20 17:49:29.858024752 -0500
@@ -0,0 +1,176 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+
+from tests.torchtune.models.llama2.scripts.compare_attention import precompute_freqs_cis
+from tests.torchtune.models.llama2.scripts.compare_decoder_layer import (
+    RMSNormRef,
+    TransformerBlock,
+)
+
+from torch import nn
+
+from torchtune.models.llama2 import llama2
+
+"""
+Reference implementation of Transformer from:
+https://github.com/facebookresearch/llama/blob/main/llama/model.py#L413
+
+Replicating code here to minimize dependencies. The code is modified to
+include params for the constructor and remove start_pos (not supported).
+"""
+
+# TODO: Move this to standalone ref implementation
+class Transformer(nn.Module):
+    def __init__(
+        self,
+        vocab_size: int,
+        dim: int,
+        n_layers: int,
+        n_heads: int,
+        max_seq_len: int,
+        n_kv_heads: int,
+    ):
+
+        super().__init__()
+        self.vocab_size = vocab_size
+        self.n_layers = n_layers
+
+        self.tok_embeddings = nn.Embedding(vocab_size, dim)
+
+        self.layers = torch.nn.ModuleList()
+        for _ in range(n_layers):
+            self.layers.append(
+                TransformerBlock(n_heads=n_heads, dim=dim, n_kv_heads=n_kv_heads)
+            )
+
+        self.norm = RMSNormRef(dim)
+        self.output = nn.Linear(dim, vocab_size, bias=False)
+
+        self.freqs_cis = precompute_freqs_cis(dim // n_heads, max_seq_len * 2)
+
+    def forward(self, tokens: torch.Tensor, start_pos: int = 0):
+        _bsz, seqlen = tokens.shape
+        h = self.tok_embeddings(tokens)
+        self.freqs_cis = self.freqs_cis.to(h.device)
+        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]
+
+        mask = None
+        if seqlen > 1:
+            mask = torch.full(
+                (1, 1, seqlen, seqlen), float("-inf"), device=tokens.device
+            )
+            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)
+
+        for layer in self.layers:
+            h = layer(h, freqs_cis, mask)
+        h = self.norm(h)
+        output = self.output(h).float()
+        return output
+
+
+def compare_decoder(
+    bsz: int,
+    seq_len: int,
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    vocab_size: int,
+    num_layers: int,
+) -> None:
+
+    # make sure we have the right seed for generating outputs
+    # this should match up the seed value set in the corresponding
+    # unit test
+    torch.manual_seed(16)
+
+    # generate input tensor used by both implementations
+    x_input = torch.randint(low=0, high=vocab_size, size=(bsz, seq_len))
+
+    # reference implementation; initialize with constant to compare outputs
+    decoder_ref = Transformer(
+        vocab_size=vocab_size,
+        dim=embed_dim,
+        n_layers=num_layers,
+        n_heads=num_heads,
+        max_seq_len=max_seq_len,
+        n_kv_heads=num_kv_heads,
+    )
+    for p in model.parameters():
+        nn.init.constant_(decoder_ref, 0.2)
+
+    with torch.no_grad():
+        output_ref = decoder_ref(x_input)
+
+    # current implementation; initialize with constant to compare outputs
+    decoder = llama2(
+        vocab_size=vocab_size,
+        embed_dim=embed_dim,
+        num_layers=num_layers,
+        num_heads=num_heads,
+        max_seq_len=max_seq_len,
+        num_kv_heads=num_kv_heads,
+    )
+    for p in model.parameters():
+        nn.init.constant_(decoder, 0.2)
+
+    with torch.no_grad():
+        output = decoder(x_input)
+
+    # value: tensor(20.4800)
+    print(output.mean())
+
+    torch.testing.assert_close(output_ref, output, atol=1e-6, rtol=1e-6)
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser = argparse.ArgumentParser(description="Compare Attention implementations")
+    parser.add_argument("--bsz", type=int, default=4, help="Batch size of input tensor")
+    parser.add_argument(
+        "--seq_len", type=int, default=512, help="input sequence length"
+    )
+    parser.add_argument(
+        "--embed_dim",
+        type=int,
+        default=512,
+        help="Embedding dimension used to compute the dim for RopE",
+    )
+    parser.add_argument(
+        "--num_heads",
+        type=int,
+        default=8,
+        help="Number of heads in the attention layer",
+    )
+    parser.add_argument(
+        "--num_kv_heads",
+        type=int,
+        default=8,
+        help="Number of key/value heads in the attention layer",
+    )
+    parser.add_argument(
+        "--max_seq_len", type=int, default=512, help="max sequence length"
+    )
+    parser.add_argument("--vocab_size", type=int, default=256, help="vocab size")
+    parser.add_argument(
+        "--num_layers", type=int, default=4, help="number of transformer layers"
+    )
+
+    args = parser.parse_args()
+
+    compare_decoder(
+        args.bsz,
+        args.seq_len,
+        args.embed_dim,
+        args.num_heads,
+        args.num_kv_heads,
+        args.max_seq_len,
+        args.vocab_size,
+        args.num_layers,
+    )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_dora.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_dora.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_dora.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_dora.py	2025-02-20 17:49:29.866024765 -0500
@@ -0,0 +1,259 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+
+import pytest
+
+import torch
+import torch.nn.functional as F
+from torch import nn
+from torchao.dtypes.nf4tensor import linear_nf4, to_nf4
+from torchtune import training
+from torchtune.modules.peft import (
+    DoRALinear,
+    get_merged_lora_ckpt,
+    load_dora_magnitudes,
+    LoRALinear,
+)
+from torchtune.training.seed import set_seed
+
+
+def compare_dora(self, dtype, use_bias, quantize_base):
+    dropout = 0.0
+    batch_size = 2
+    in_dim = 64
+    out_dim = 128
+    rank = 4
+    alpha = 1.0
+    use_bias = False
+    quantize_base = False
+    dtype = torch.bfloat16
+
+    constructor_kwargs = {
+        "in_dim": in_dim,
+        "out_dim": out_dim,
+        "rank": rank,
+        "alpha": alpha,
+        "dropout": dropout,
+        "use_bias": use_bias,
+        "quantize_base": quantize_base,
+    }
+
+    # this combo is not supported yet
+    if use_bias:
+        with pytest.raises(
+            NotImplementedError, match="DoRALinear does not support using bias"
+        ):
+            DoRALinear(**constructor_kwargs)
+        return
+
+    # build our DoRA module and a reference module for comparison
+    with training.set_default_dtype(dtype):
+        module = DoRALinear(**constructor_kwargs)
+        ref = _DoraReference(dtype=dtype, **constructor_kwargs)
+        lora_module = LoRALinear(**constructor_kwargs)
+
+    # make the initial parameters equal
+    state_dict = ref.state_dict()
+    lora_state_dict = ref.state_dict()
+    if quantize_base:
+        state_dict["weight"] = state_dict["weight"].to(torch.float32)
+        lora_state_dict["weight"] = lora_state_dict["weight"].to(torch.float32)
+    state_dict["magnitude"] = state_dict.pop("lora_magnitude")
+    lora_state_dict.pop("lora_magnitude")
+    module.load_state_dict(state_dict)
+    lora_module.load_state_dict(lora_state_dict)
+
+    # freeze the base params
+    module.weight.requires_grad_(False)
+    lora_module.weight.requires_grad_(False)
+    ref.weight.requires_grad_(False)
+    if use_bias:
+        module.bias.requires_grad_(False)
+        module.lora_module.requires_grad_(False)
+        ref.bias.requires_grad_(False)
+
+    @torch.no_grad
+    def _dora_is_the_same_as_lora():
+        module.eval()
+        lora_module.eval()
+        x = torch.randn(batch_size, in_dim, dtype=dtype)
+        lora_out = lora_module(x)
+        dora_out = module(x)
+        return torch.equal(lora_out, dora_out)
+
+    # DoRA initializes the magnitude vector (after the base params are loaded)
+    # such that its outputs are initially identical to standard LoRA's outputs.
+    # Verify that this is true.
+    assert not _dora_is_the_same_as_lora()
+    module.initialize_dora_magnitude()
+    load_dora_magnitudes(module)
+    assert _dora_is_the_same_as_lora()
+
+    def _compare_params():
+        assert torch.allclose(
+            module.weight.to(torch.float32), ref.weight.to(torch.float32)
+        )
+        assert torch.allclose(module.lora_a.weight, ref.lora_a.weight)
+        assert torch.allclose(module.lora_b.weight, ref.lora_b.weight)
+        assert torch.allclose(module.magnitude, ref.lora_magnitude)
+
+    # verify that the param values match the reference
+    ref.initialize_dora()
+    _compare_params()
+
+    # compare a single training step to the reference
+    module.train()
+    ref.train()
+    opt = torch.optim.Adam(module.parameters())
+    opt_ref = torch.optim.Adam(ref.parameters())
+    opt.zero_grad()
+    opt_ref.zero_grad()
+    # x = torch.randn(batch_size, in_dim, dtype=dtype)
+    x = torch.randn(batch_size, 32, in_dim)
+    y = torch.randn(batch_size, out_dim)
+    torch.manual_seed(0)
+    y1 = module(x.detach())
+    torch.manual_seed(0)
+    y2 = ref(x.detach())
+    F.mse_loss(y1.to(torch.float32), y.detach()).backward()
+    F.mse_loss(y2.to(torch.float32), y.detach()).backward()
+    assert torch.allclose(y1, y2)
+    assert torch.allclose(module.magnitude.grad, ref.lora_magnitude.grad)
+    assert torch.allclose(module.lora_a.weight.grad, ref.lora_a.weight.grad)
+    assert torch.allclose(module.lora_b.weight.grad, ref.lora_b.weight.grad)
+    opt.step()
+    opt_ref.step()
+    _compare_params()
+
+    # verify that the merged and unmerged DoRA modules have nearly identical outputs
+    state_dict = get_merged_lora_ckpt(_Wrapper(module).state_dict(), rank, alpha)
+    merged = _Wrapper(nn.Linear(in_dim, out_dim, bias=use_bias, dtype=dtype))
+    merged.load_state_dict(state_dict)
+    merged = merged.layer
+    module.eval()
+    merged.eval()
+    with torch.no_grad():
+        x = torch.randn(batch_size, in_dim, dtype=dtype)
+        y1 = module(x)
+        y2 = merged(x)
+        mse = F.mse_loss(y1.float(), y2.float())
+        assert mse < (1e-8 if dtype == torch.float32 else 1e-2)
+
+
+class _Wrapper(nn.Module):
+    """
+    For testing the merged checkpoint which requires that the LoRA layer has a parent.
+    """
+
+    def __init__(self, layer):
+        super().__init__()
+        self.layer = layer
+
+    def forward(self, x):
+        return self.layer(x)
+
+
+class _DoraReference(nn.Module):
+    """
+    DoRA linear layer reference.
+
+    Paper: https://arxiv.org/abs/2402.09353
+
+    Based on the code from:
+    https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py
+    https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/dora.py
+
+    For more info, see the discussion here:
+    https://github.com/huggingface/peft/pull/1474
+    """
+
+    def __init__(
+        self,
+        dtype: torch.dtype,
+        in_dim: int,
+        out_dim: int,
+        rank: int,
+        alpha: float,
+        dropout: float = 0.0,
+        use_bias: bool = False,
+        quantize_base: bool = False,
+        use_dora: bool = True,
+    ):
+        super().__init__()
+        self.use_bias = use_bias
+        self.quantize_base = quantize_base
+        self.use_dora = use_dora
+
+        linear = nn.Linear(
+            in_features=in_dim, out_features=out_dim, bias=use_bias, dtype=dtype
+        )
+        weight = linear.weight if not quantize_base else to_nf4(linear.weight)
+        bias = None
+        if use_bias:
+            if quantize_base:
+                raise NotImplementedError()
+            bias = linear.bias
+        self.register_parameter("weight", nn.Parameter(weight))
+        self.register_parameter(
+            "bias", nn.Parameter(bias) if bias is not None else None
+        )
+
+        self.lora_a = nn.Linear(in_dim, rank, bias=False, dtype=dtype)
+        self.lora_b = nn.Linear(rank, out_dim, bias=False, dtype=dtype)
+        nn.init.kaiming_uniform_(self.lora_a.weight, a=math.sqrt(5))
+        nn.init.zeros_(self.lora_b.weight)
+        self.scaling = alpha / rank
+        if use_dora:
+            self.lora_magnitude = nn.Parameter(torch.randn(out_dim, dtype=dtype))
+        self.dropout = nn.Dropout(p=dropout)
+
+    def initialize_dora(self):
+        weight = self.weight.to(self.lora_a.weight.dtype)
+        lora_weight = self.lora_b.weight @ self.lora_a.weight
+        weight_norm = self._get_weight_norm(weight, lora_weight)
+        self.lora_magnitude = nn.Parameter(weight_norm, requires_grad=True)
+
+    def forward(self, x):
+        result = self._base_forward(x)
+        torch_result_dtype = result.dtype
+        x = x.to(self.lora_a.weight.dtype)
+        if not self.use_dora:
+            result = result + self.lora_b(self.lora_a(self.dropout(x))) * self.scaling
+        else:
+            x = self.dropout(x)
+            result = result + self._dora_forward(x)
+        result = result.to(torch_result_dtype)
+        print("result mean", result.mean())
+        return result
+
+    def _base_forward(self, x):
+        if self.quantize_base:
+            return linear_nf4(input=x, weight=self.weight)
+        return F.linear(x, self.weight, self.bias)
+
+    def _dora_forward(self, x):
+        lora_result = self.lora_b(self.lora_a(x))
+        x_eye = torch.eye(
+            self.lora_a.weight.shape[1], device=self.lora_a.weight.device, dtype=x.dtype
+        )
+        lora_weight = self.lora_b(self.lora_a(x_eye)).T
+
+        magnitude = self.lora_magnitude
+        weight = self.weight.to(x.dtype)
+        weight_norm = self._get_weight_norm(weight, lora_weight.detach())
+        weight_norm = weight_norm.detach()
+        mag_norm_scale = (magnitude / weight_norm).view(1, -1)
+        result_dora = (mag_norm_scale - 1) * (
+            F.linear(x, weight)
+        ) + mag_norm_scale * lora_result * self.scaling
+        return result_dora
+
+    def _get_weight_norm(self, weight, lora_weight):
+        weight = weight + self.scaling * lora_weight
+        weight_norm = torch.linalg.norm(weight, dim=1).to(weight.dtype)
+        return weight_norm
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_feed_forward.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_feed_forward.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_feed_forward.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_feed_forward.py	2025-02-20 17:49:29.870024772 -0500
@@ -0,0 +1,98 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional
+
+import torch
+
+from tests.test_utils import fixed_init_model
+
+from torch import nn
+from torchtune.models.llama2._model_utils import scale_hidden_dim_for_mlp
+
+from torchtune.modules import FeedForward
+
+
+"""
+Reference implementation of FeedForward from:
+https://github.com/facebookresearch/llama/blob/main/llama/model.py#L307
+
+Replicating code here to minimize dependencies.
+"""
+
+
+class FeedForwardRef(nn.Module):
+    def __init__(
+        self,
+        dim: int,
+        hidden_dim: int,
+        multiple_of: int = 256,
+        ffn_dim_multiplier: Optional[float] = None,
+    ):
+        super().__init__()
+        hidden_dim = int(2 * hidden_dim / 3)
+        # custom dim factor multiplier
+        if ffn_dim_multiplier is not None:
+            hidden_dim = int(ffn_dim_multiplier * hidden_dim)
+        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)
+
+        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
+        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
+        self.w3 = nn.Linear(dim, hidden_dim, bias=False)
+
+    def forward(self, x):
+        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))
+
+
+def compare_feed_forward(embed_dim: int, hidden_dim: int) -> None:
+
+    # make sure we have the right seed for generating outputs
+    # this should match up the seed value set in the corresponding
+    # unit test
+    torch.manual_seed(0)
+
+    # generate input tensor used by both implementations
+    input_t = torch.randn(1, embed_dim)
+
+    # reference implementation; initialize with constant to compare outputs
+    ff_ref = FeedForwardRef(dim=embed_dim, hidden_dim=4 * embed_dim)
+    fixed_init_model(ff_ref)
+
+    with torch.no_grad():
+        ff_out_ref = ff_ref(input_t)
+
+    hidden_dim = _scale_hidden_dim_for_mlp(embed_dim)
+    ff = FeedForward(dim=embed_dim, hidden_dim=hidden_dim, linear_class=torch.nn.Linear)
+    fixed_init_model(ff)
+
+    with torch.no_grad():
+        ff_out = ff(input_t)
+
+    torch.testing.assert_close(ff_out, ff_out_ref, atol=1e-5, rtol=1e-5)
+    print(ff_out.mean())
+    print(ff_out.max())
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser = argparse.ArgumentParser(description="Compare RMS Norm implementations")
+    parser.add_argument(
+        "--embed_dim",
+        type=int,
+        default=4096,
+        help="Embedding dimension used to compute the dim for RopE",
+    )
+    parser.add_argument(
+        "--hidden_dim",
+        type=int,
+        default=4096,
+        help="Hidden dimension in the feed forward layer",
+    )
+
+    args = parser.parse_args()
+
+    compare_feed_forward(args.embed_dim, args.hidden_dim)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_fused_attention.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_fused_attention.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_fused_attention.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_fused_attention.py	2025-02-20 17:49:29.874024778 -0500
@@ -0,0 +1,388 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional, Tuple
+
+import torch
+from tests.test_utils import fixed_init_model
+from torch import nn, Tensor
+from torchtune.modules import KVCache, MultiHeadAttention, RotaryPositionalEmbeddings
+
+
+# Copy-paste of fused attention for comparison
+class FusedMultiHeadAttention(nn.Module):
+    """Multi-headed grouped query self-attention (GQA) layer introduced
+    in https://arxiv.org/pdf/2305.13245v1.pdf.
+
+    GQA is a version of multiheaded attention (MHA) which uses fewer
+    key/value heads than query heads by grouping n query heads for each
+    key and value head. Multi-Query Attention is an extreme
+    version where we have a single key and value head shared by all
+    query heads.
+
+    Following is an example of MHA, GQA and MQA with num_heads = 4
+
+    (credit for the documentation:
+    https://github.com/Lightning-AI/lit-gpt/blob/main/lit_gpt/config.py).
+
+
+    ::
+
+                              
+         v  v  v  v       v      v               v 
+                              
+                                                      
+                              
+         k  k  k  k       k      k               k 
+                              
+                                  
+            
+         q  q  q  q    q  q  q  q    q  q  q  q 
+            
+            
+                MHA                    GQA                   MQA
+        n_kv_heads =4          n_kv_heads=2           n_kv_heads=1
+
+    Args:
+        embed_dim (int): embedding dimension for the model
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        head_dim (int): dimension of each head, calculated by ``embed_dim`` // ``num_heads``.
+        qkv_proj (nn.Module): projection layer for query, key and value.
+        output_proj (nn.Module): projection layer for output.
+        pos_embeddings (nn.Module): positional embeddings layer, e.g. RotaryPositionalEmbeddings.
+        kv_cache (Optional[KVCache]): KVCache object used to cache key and value.
+            If not specified, then no caching is used.
+        max_seq_len (int): maximum sequence length supported by the model.
+            This is needed to compute the RoPE Cache. Default: 4096.
+        attn_dropout (float): dropout value passed onto the
+            scaled_dot_product_attention function. This argument is ignored if the
+            self.training is False. Default value is 0.0.
+
+    Raises:
+        ValueError: If `num_heads` % `num_kv_heads` != 0
+        ValueError: If `embed_dim` % `num_heads` != 0
+        ValueError: If `attn_dropout` < 0 or > 1
+    """
+
+    def __init__(
+        self,
+        embed_dim: int,
+        num_heads: int,
+        num_kv_heads: int,
+        head_dim: int,
+        qkv_proj: nn.Module,
+        output_proj: nn.Module,
+        pos_embeddings: nn.Module,
+        kv_cache: Optional[KVCache] = None,
+        max_seq_len: int = 4096,
+        attn_dropout: float = 0.0,
+    ) -> None:
+        super().__init__()
+        if num_heads % num_kv_heads != 0:
+            raise ValueError(
+                f"num_heads ({num_heads}) must be divisible by "
+                f"num_kv_heads ({num_kv_heads})"
+            )
+
+        if embed_dim % num_heads != 0:
+            raise ValueError(
+                f"embed_dim ({embed_dim}) must be divisible by "
+                f"num_heads ({num_heads})"
+            )
+
+        if attn_dropout < 0 or attn_dropout > 1:
+            raise ValueError(f"attn_dropout ({embed_dim}) must be between 0.0 and 1.0")
+
+        # Set attributes
+        self.num_heads = num_heads
+        self.num_kv_heads = num_kv_heads
+        self.embed_dim = embed_dim
+        self.attn_dropout = attn_dropout
+        self.head_dim = head_dim
+        self.max_seq_len = max_seq_len
+
+        # Set layers
+        self.kv_cache = kv_cache
+        self.qkv_proj = qkv_proj
+        self.output_proj = output_proj
+        self.pos_embeddings = pos_embeddings
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        mask: Optional[torch.Tensor] = None,
+        curr_pos: int = 0,
+    ) -> torch.Tensor:
+        """
+        Args:
+            x (Tensor): input tensor with shape
+                [batch_size x seq_length x embed_dim]
+            mask (Optional[torch.Tensor]): boolean mask, defaults to None.
+            curr_pos (int): current position in the sequence, defaults to 0.
+
+        Returns:
+            Tensor: output tensor with attention applied
+
+        Raises:
+            ValueError: if seq_len of x is bigger than max_seq_len
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - s: sequence length
+            - n_h: num heads
+            - n_kv: num kv heads
+            - d: embed dim
+            - h_d: head dim
+            - qkv_d: qkv_dim computed as (n_h + 2 * n_kv) * h_d
+
+        TODO:
+            - Return the attention weights
+            - Make application of positional embeddings optional
+        """
+
+        # input has shape [b, s, d]
+        bsz, seq_len, _ = x.shape
+
+        if seq_len > self.max_seq_len:
+            raise ValueError(
+                f"seq_len ({seq_len}) of input tensor should be smaller "
+                f"than max_seq_len ({self.max_seq_len})"
+            )
+
+        # qkv has shape [b, s, qkv_d]
+        qkv = self.qkv_proj(x)
+
+        # number of queries per key/value
+        q_per_kv = self.num_heads // self.num_kv_heads
+
+        # Each key and value either has a single query (MHA)
+        # or q_per_kv queries (MQA/GQA). total_qkv will be 3
+        # for MHA
+        total_qkv = q_per_kv + 2
+
+        # decompose the last dimension into n_kv x total_qkv, h_d
+        qkv = qkv.view(bsz, seq_len, self.num_kv_heads, total_qkv, self.head_dim)
+
+        # create the q,k and v tensors by splitting qkv
+        # q: [b, s, n_kv, q_per_kv, h_d]
+        # k: [b, s, n_kv, 1, h_d]
+        # v: [b, s, n_kv, 1, h_d]
+        q, k, v = qkv.split((q_per_kv, 1, 1), dim=3)
+
+        # if needed, expand the key and value tensors to have the same shape
+        # as the query tensor by copying values across the relevant dim
+        if self.num_heads != self.num_kv_heads:
+            k = k.expand(bsz, seq_len, self.num_kv_heads, q_per_kv, self.head_dim)
+            v = v.expand(bsz, seq_len, self.num_kv_heads, q_per_kv, self.head_dim)
+
+        # llama2 applies the RoPE embeddings on tensors with shape
+        # [b, s, n_h, h_d]
+        # Reshape the tensors before we apply RoPE
+        q = q.reshape(bsz, seq_len, -1, self.head_dim)
+        k = k.reshape(bsz, seq_len, -1, self.head_dim)
+        v = v.reshape(bsz, seq_len, -1, self.head_dim)
+
+        # Apply positional embeddings
+        q = self.pos_embeddings(q, curr_pos)
+        k = self.pos_embeddings(k, curr_pos)
+
+        # Update key-value cache
+        if self.kv_cache is not None:
+            k, v = self.kv_cache.update(
+                bsz=bsz, seq_len=seq_len, curr_pos=curr_pos, k_val=k, v_val=v
+            )
+
+        # [b, n_h, s, h_d]
+        q = q.transpose(1, 2)
+        k = k.transpose(1, 2)
+        v = v.transpose(1, 2)
+
+        # Flash attention from https://pytorch.org/blog/accelerating-large-language-models/
+        output = nn.functional.scaled_dot_product_attention(
+            q,
+            k,
+            v,
+            attn_mask=mask,
+            dropout_p=self.attn_dropout,
+            is_causal=self.kv_cache is None,
+        )
+
+        # reshape the output to be the same shape as the input
+        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)
+        return self.output_proj(output)
+
+
+def map_state_dict(
+    sd,
+    head_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+):
+    mapped_sd = {k: v for k, v in sd.items() if "qkv_proj" not in k}
+    q_per_kv = num_heads // num_kv_heads
+    slice_size = q_per_kv + 2
+    ind = range(head_dim * num_kv_heads * slice_size)
+    qkv = sd["qkv_proj.weight"]
+    q_ind = list(filter(lambda x: (x // head_dim) % slice_size < slice_size - 2, ind))
+    k_ind = list(filter(lambda x: (x // head_dim) % slice_size == slice_size - 2, ind))
+    v_ind = list(filter(lambda x: (x // head_dim) % slice_size == slice_size - 1, ind))
+    q = qkv.index_select(0, torch.tensor(q_ind))
+    k = qkv.index_select(0, torch.tensor(k_ind))
+    v = qkv.index_select(0, torch.tensor(v_ind))
+    mapped_sd["q_proj.weight"] = q
+    mapped_sd["k_proj.weight"] = k
+    mapped_sd["v_proj.weight"] = v
+    return mapped_sd
+
+
+def _get_mask(inpt: torch.Tensor) -> torch.Tensor:
+    seq_len = inpt.shape[1]
+    mask = torch.full((1, 1, seq_len, seq_len), float("-inf"), device=inpt.device)
+    mask = torch.triu(mask, diagonal=1).type_as(inpt)
+    return mask
+
+
+def compare_attn(
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    use_kv_cache: bool,
+):
+
+    torch.manual_seed(16)
+    inputs = torch.randn(4, 2048, 4096)
+
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    qkv_dim = (num_heads + 2 * num_kv_heads) * head_dim
+
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+    if use_kv_cache:
+        kv_cache = KVCache(
+            batch_size=4,
+            max_seq_len=max_seq_len,
+            n_kv_heads=num_heads,
+            head_dim=head_dim,
+        )
+    else:
+        kv_cache = None
+
+    attn_ref = FusedMultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        qkv_proj=nn.Linear(embed_dim, qkv_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=kv_cache,
+        max_seq_len=max_seq_len,
+    )
+    fixed_init_model(attn_ref)
+    attn_ref.eval()
+
+    attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=kv_cache,
+        max_seq_len=max_seq_len,
+    )
+    mapped_sd = map_state_dict(attn_ref.state_dict(), head_dim, num_heads, num_kv_heads)
+    attn.load_state_dict(mapped_sd)
+
+    # Compare fused and non-fused with remapped state dict
+    with torch.no_grad():
+        if use_kv_cache:
+            mask = _get_mask(inputs)
+            out_ref = attn_ref(inputs, mask, curr_pos=0)
+            out = attn_ref(inputs, mask, curr_pos=0)
+        else:
+            out_ref = attn_ref(inputs)
+            out = attn(inputs)
+    print(
+        "These values should match the original unit test", out_ref.mean(), out.mean()
+    )
+    torch.testing.assert_close(out_ref, out, atol=1e-8, rtol=1e-3)
+
+    # Determine the new value with fixed initialization
+    fixed_init_model(attn)
+    with torch.no_grad():
+        if use_kv_cache:
+            new_out = attn(inputs, mask, curr_pos=0)
+        else:
+            new_out = attn(inputs)
+    print(f"New unit test value: {new_out.mean()}")
+
+
+if __name__ == "__main__":
+
+    # compare mha
+    mha = {
+        "num_heads": 32,
+        "embed_dim": 4096,
+        "max_seq_len": 4096,
+        "num_kv_heads": None,
+        "use_kv_cache": False,
+    }
+    mqa = {
+        "num_heads": 32,
+        "embed_dim": 4096,
+        "max_seq_len": 4096,
+        "num_kv_heads": 1,
+        "use_kv_cache": False,
+    }
+    gqa = {
+        "num_heads": 32,
+        "embed_dim": 4096,
+        "max_seq_len": 4096,
+        "num_kv_heads": 8,
+        "use_kv_cache": False,
+    }
+    mha_kv = {
+        "num_heads": 32,
+        "embed_dim": 4096,
+        "max_seq_len": 4096,
+        "num_kv_heads": None,
+        "use_kv_cache": True,
+    }
+    mqa_kv = {
+        "num_heads": 32,
+        "embed_dim": 4096,
+        "max_seq_len": 4096,
+        "num_kv_heads": 1,
+        "use_kv_cache": True,
+    }
+    gqa_kv = {
+        "num_heads": 32,
+        "embed_dim": 4096,
+        "max_seq_len": 4096,
+        "num_kv_heads": 8,
+        "use_kv_cache": True,
+    }
+    test_cases = {
+        "mha": mha,
+        "mqa": mqa,
+        "gqa": gqa,
+        "mha_kv": mha_kv,
+        "mqa_kv": mqa_kv,
+        "gqa_kv": gqa_kv,
+    }
+
+    for test_case, params in test_cases.items():
+        print(f"For test case {test_case}")
+        compare_attn(**params)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora_attention.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora_attention.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora_attention.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora_attention.py	2025-02-20 17:49:29.878024785 -0500
@@ -0,0 +1,147 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+from typing import List
+
+import torch
+
+from tests.test_utils import fixed_init_model
+
+from torch import nn
+
+from torchtune.models.llama2._lora_llama2_builders import _lora_llama_self_attention
+from torchtune.modules import KVCache, MultiHeadAttention, RotaryPositionalEmbeddings
+
+try:
+    from peft import inject_adapter_in_model, LoraConfig
+except:
+    raise ImportError("Must have peft installed to run this comparison script")
+
+
+def compare_lora_attention(
+    bsz: int,
+    seq_len: int,
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    lora_modules: List[str],
+    lora_rank: int,
+    lora_alpha: float,
+) -> None:
+
+    # make sure we have the right seed for generating outputs
+    # this should match up the seed value set in the corresponding
+    # unit test
+    torch.manual_seed(16)
+
+    # generate input tensor used by both implementations
+    x = torch.randn(bsz, seq_len, embed_dim)
+
+    # Our implementation
+    lora_llama_attn = _lora_llama_self_attention(
+        lora_modules=lora_modules,
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+    )
+    fixed_init_model(lora_llama_attn)
+
+    with torch.no_grad():
+        out = lora_llama_attn(x)
+
+    batch_size = None
+    attn_dropout = 0.0
+    # Reference implementation: wrap our native causal self-attention with PEFT LoRAConfig
+    # Copy-pasted from llama2.py
+    # https://github.com/pytorch/torchtune/blob/e983194629d7f093257225dafb7cbc4e46505cc8/torchtune/models/llama2.py#L88-L114
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    kv_cache = (
+        KVCache(
+            batch_size=batch_size,
+            max_seq_len=max_seq_len,
+            n_kv_heads=num_heads,
+            head_dim=head_dim,
+        )
+        if batch_size is not None
+        else None
+    )
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+    llama_attn_ref = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=kv_cache,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    lora_config_ref = LoraConfig(
+        lora_alpha=lora_alpha,
+        lora_dropout=0.0,
+        r=lora_rank,
+        bias="none",
+        target_modules=lora_modules,
+    )
+
+    lora_llama_attn_ref = inject_adapter_in_model(lora_config_ref, llama_attn_ref)
+
+    all_keys = ["q_proj", "k_proj", "v_proj", "output_proj"]
+
+    mapped_sd = {}
+    for key in all_keys:
+        if key in lora_modules:
+            mapped_sd[f"{key}.base_layer.weight"] = lora_llama_attn.state_dict()[
+                f"{key}.weight"
+            ]
+            mapped_sd[f"{key}.lora_A.default.weight"] = lora_llama_attn.state_dict()[
+                f"{key}.lora_a.weight"
+            ]
+            mapped_sd[f"{key}.lora_B.default.weight"] = lora_llama_attn.state_dict()[
+                f"{key}.lora_b.weight"
+            ]
+        else:
+            mapped_sd[f"{key}.weight"] = lora_llama_attn.state_dict()[f"{key}.weight"]
+
+    lora_llama_attn_ref.load_state_dict(mapped_sd)
+
+    with torch.no_grad():
+        out_ref = lora_llama_attn_ref(x)
+
+    print(lora_modules, out.mean(), out_ref.mean(), out.shape, out_ref.shape)
+
+    # output tensors should be similar
+    torch.testing.assert_close(out, out_ref, atol=1e-5, rtol=1e-3)
+
+
+if __name__ == "__main__":
+    test_cases = [
+        ["q_proj", "v_proj"],
+        ["q_proj", "k_proj", "v_proj", "output_proj"],
+        ["k_proj"],
+    ]
+    for lora_modules in test_cases:
+        compare_lora_attention(
+            bsz=2,
+            seq_len=32,
+            embed_dim=64,
+            num_heads=4,
+            num_kv_heads=2,
+            max_seq_len=64,
+            lora_modules=lora_modules,
+            lora_rank=4,
+            lora_alpha=1.0,
+        )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora_llama2.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora_llama2.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora_llama2.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora_llama2.py	2025-02-20 17:49:29.882024791 -0500
@@ -0,0 +1,150 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+from typing import List
+
+import torch
+
+from tests.test_utils import fixed_init_model
+
+from torch import nn
+
+from torchtune.models.llama2 import get_lora_module_names, llama2, lora_llama2
+
+try:
+    from peft import inject_adapter_in_model, LoraConfig
+except:
+    raise ImportError("Must have peft installed to run this comparison script")
+
+
+def compare_lora_llama2(
+    bsz: int,
+    seq_len: int,
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    lora_modules: List[str],
+    lora_in_mlp: bool,
+    lora_in_output: bool,
+    lora_rank: int,
+    lora_alpha: float,
+) -> None:
+
+    # make sure we have the right seed for generating outputs
+    # this should match up the seed value set in the corresponding
+    # unit test
+    torch.manual_seed(16)
+
+    # generate input tensor used by both implementations
+    x = torch.randint(low=0, high=vocab_size, size=(bsz, seq_len))
+
+    # Our implementation
+    lora_llama = lora_llama2(
+        lora_attn_modules=lora_modules,
+        apply_lora_to_mlp=lora_in_mlp,
+        apply_lora_to_output=lora_in_output,
+        vocab_size=vocab_size,
+        num_layers=num_layers,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        embed_dim=embed_dim,
+        max_seq_len=max_seq_len,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+    )
+    # This is to make final outputs less trivial
+    lora_llama.norm = nn.Identity()
+    fixed_init_model(lora_llama)
+
+    with torch.no_grad():
+        out = lora_llama(x)
+
+    # Reference implementation: wrap our native llama2 with PEFT LoRAConfig
+    llama_ref = llama2(
+        vocab_size=vocab_size,
+        num_layers=num_layers,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        embed_dim=embed_dim,
+        max_seq_len=max_seq_len,
+    )
+
+    peft_lora_modules = get_lora_module_names(lora_modules, lora_in_mlp, lora_in_output)
+
+    lora_config_ref = LoraConfig(
+        lora_alpha=lora_alpha,
+        lora_dropout=0.0,
+        r=lora_rank,
+        bias="none",
+        target_modules=peft_lora_modules,
+    )
+
+    lora_llama_ref = inject_adapter_in_model(lora_config_ref, llama_ref)
+    lora_llama_ref.norm = nn.Identity()
+
+    mapped_sd = {}
+    for k, v in lora_llama.state_dict().items():
+        new_k = k.replace("lora_a", "lora_A.default").replace(
+            "lora_b", "lora_B.default"
+        )
+        for attn_module in lora_modules:
+            if attn_module in new_k:
+                new_k = new_k.replace(
+                    attn_module + ".weight", attn_module + ".base_layer.weight"
+                )
+            if lora_in_mlp and any([f"mlp.w{i}.weight" in new_k for i in range(1, 4)]):
+                new_k = new_k.replace(".weight", ".base_layer.weight")
+
+            if lora_in_output and "output.weight" in new_k:
+                new_k = new_k.replace(".weight", ".base_layer.weight")
+
+        mapped_sd[new_k] = v
+
+    lora_llama_ref.load_state_dict(mapped_sd)
+
+    with torch.no_grad():
+        out_ref = lora_llama_ref(x)
+
+    print(
+        lora_modules,
+        lora_in_mlp,
+        lora_in_output,
+        out.mean(),
+        out_ref.mean(),
+        out.shape,
+        out_ref.shape,
+    )
+
+    # output tensors should be similar
+    torch.testing.assert_close(out, out_ref, atol=1e-5, rtol=1e-3)
+
+
+if __name__ == "__main__":
+    test_cases = [
+        (["q_proj", "v_proj"], False, False),
+        (["q_proj", "k_proj", "v_proj", "output_proj"], True, False),
+        (["k_proj"], True, True),
+    ]
+    for lora_modules, lora_in_mlp, lora_in_output in test_cases:
+        compare_lora_llama2(
+            bsz=2,
+            seq_len=32,
+            vocab_size=50,
+            num_layers=3,
+            num_heads=4,
+            num_kv_heads=2,
+            embed_dim=64,
+            max_seq_len=64,
+            lora_modules=lora_modules,
+            lora_in_mlp=lora_in_mlp,
+            lora_in_output=lora_in_output,
+            lora_rank=4,
+            lora_alpha=1.0,
+        )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/compare_lora.py	2025-02-20 17:49:29.878024785 -0500
@@ -0,0 +1,214 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+
+import torch
+import torch.nn.functional as F
+from tests.test_utils import fixed_init_model
+from torch import nn
+from torchtune.modules.peft.lora import LoRALinear
+
+# Reference implementation from
+# https://github.com/microsoft/LoRA/blob/main/loralib/layers.py
+class LoRALayer:
+    def __init__(
+        self,
+        r: int,
+        lora_alpha: int,
+        lora_dropout: float,
+        merge_weights: bool,
+    ):
+        self.r = r
+        self.lora_alpha = lora_alpha
+        # Optional dropout
+        if lora_dropout > 0.0:
+            self.lora_dropout = nn.Dropout(p=lora_dropout)
+        else:
+            self.lora_dropout = lambda x: x
+        # Mark the weight as unmerged
+        self.merged = False
+        self.merge_weights = merge_weights
+
+
+class LoRALinearRef(nn.Linear, LoRALayer):
+    # LoRA implemented in a dense layer
+    def __init__(
+        self,
+        in_features: int,
+        out_features: int,
+        r: int = 0,
+        lora_alpha: int = 1,
+        lora_dropout: float = 0.0,
+        fan_in_fan_out: bool = False,
+        # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
+        merge_weights: bool = True,
+        **kwargs,
+    ):
+        nn.Linear.__init__(self, in_features, out_features, **kwargs)
+        LoRALayer.__init__(
+            self,
+            r=r,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            merge_weights=merge_weights,
+        )
+
+        self.fan_in_fan_out = fan_in_fan_out
+        # Actual trainable parameters
+        if r > 0:
+            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))
+            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))
+            self.scaling = self.lora_alpha / self.r
+            # Freezing the pre-trained weight matrix
+            self.weight.requires_grad = False
+        self.reset_parameters()
+        if fan_in_fan_out:
+            self.weight.data = self.weight.data.transpose(0, 1)
+
+    def reset_parameters(self):
+        nn.Linear.reset_parameters(self)
+        if hasattr(self, "lora_A"):
+            # initialize B the same way as the default for nn.Linear and A to zero
+            # this is different than what is described in the paper but should not affect performance
+            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
+            nn.init.zeros_(self.lora_B)
+
+    def train(self, mode: bool = True):
+        def T(w):  # noqa
+            return w.transpose(0, 1) if self.fan_in_fan_out else w
+
+        nn.Linear.train(self, mode)
+        if mode:
+            if self.merge_weights and self.merged:
+                # Make sure that the weights are not merged
+                if self.r > 0:
+                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling
+                self.merged = False
+        else:
+            if self.merge_weights and not self.merged:
+                # Merge the weights and mark it
+                if self.r > 0:
+                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling
+                self.merged = True
+
+    def forward(self, x: torch.Tensor):
+        def T(w):  # noqa
+            return w.transpose(0, 1) if self.fan_in_fan_out else w
+
+        if self.r > 0 and not self.merged:
+            result = F.linear(x, T(self.weight), bias=self.bias)
+            result += (
+                self.lora_dropout(x)
+                @ self.lora_A.transpose(0, 1)
+                @ self.lora_B.transpose(0, 1)
+            ) * self.scaling
+            return result
+        else:
+            return F.linear(x, T(self.weight), bias=self.bias)
+
+
+def compare_lora(
+    bsz: int,
+    seq_len: int,
+    in_dim: int,
+    out_dim: int,
+    rank: int,
+    alpha: float,
+    dropout: float,
+) -> None:
+
+    # make sure we have the right seed for generating outputs
+    # this should match up the seed value set in the corresponding
+    # unit test
+    torch.manual_seed(16)
+
+    # generate input tensor used by both implementations
+    x_input = torch.randn(bsz, seq_len, in_dim)
+
+    # Initialize our implementation
+    lora = LoRALinear(
+        in_dim=in_dim,
+        out_dim=out_dim,
+        rank=rank,
+        alpha=alpha,
+        use_bias=True,
+        dropout=dropout,
+    )
+    fixed_init_model(lora)
+
+    with torch.no_grad():
+        output = lora(x_input)
+
+    # Initialize reference implementation
+    lora_ref = LoRALinearRef(
+        in_features=in_dim,
+        out_features=out_dim,
+        r=rank,
+        lora_alpha=alpha,
+        lora_dropout=dropout,
+    )
+
+    sd_mapping = {
+        "weight": "weight",
+        "bias": "bias",
+        "lora_a.weight": "lora_A",
+        "lora_b.weight": "lora_B",
+    }
+    mapped_sd = {sd_mapping.get(k): v for k, v in lora.state_dict().items()}
+    lora_ref.load_state_dict(mapped_sd)
+    with torch.no_grad():
+        output_ref = lora_ref(x_input)
+
+    print(output_ref.mean())
+    torch.testing.assert_close(output_ref, output, atol=1e-6, rtol=1e-6)
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser = argparse.ArgumentParser(description="Compare LoRA linear implementations")
+    parser.add_argument("--bsz", type=int, default=2, help="Batch size of input tensor")
+    parser.add_argument("--seq_len", type=int, default=32, help="Input sequence length")
+    parser.add_argument(
+        "--in_dim",
+        type=int,
+        default=64,
+        help="Input embedding dimension",
+    )
+    parser.add_argument(
+        "--out_dim",
+        type=int,
+        default=128,
+        help="Input embedding dimension",
+    )
+    parser.add_argument(
+        "--rank",
+        type=int,
+        default=4,
+        help="Rank of LoRA's A and B matrices",
+    )
+    parser.add_argument(
+        "--alpha",
+        type=float,
+        default=1.0,
+        help="Scaling factor for LoRA matrices",
+    )
+    parser.add_argument(
+        "--dropout", type=int, default=0.0, help="Dropout prob after linear layer"
+    )
+
+    args = parser.parse_args()
+
+    compare_lora(
+        args.bsz,
+        args.seq_len,
+        args.in_dim,
+        args.out_dim,
+        args.rank,
+        args.alpha,
+        args.dropout,
+    )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/__init__.py marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/__init__.py	2025-02-20 17:49:29.850024739 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/README.md marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/README.md
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/scripts/README.md	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/scripts/README.md	2025-02-20 17:49:29.846024732 -0500
@@ -0,0 +1,15 @@
+# Verifying Correctness against Reference Implementations
+
+This repository puts a high bar on correctness and testing. To make sure our model and
+module implementations are correct, we compare our implementation against reference implementations
+where possible. This folder contains scripts used for these comparisons.
+
+
+## Running the scripts
+
+You can run the scripts using the following command as an example.
+Each script should print out the value being used in the associated unit tests.
+
+```
+python3 -m tests.llm.llama2.scripts.compare_attention
+```
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/test_llama2_prompt_template.py marc/third_party/torchtune/tests/torchtune/models/llama2/test_llama2_prompt_template.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/test_llama2_prompt_template.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/test_llama2_prompt_template.py	2025-02-20 17:49:29.886024798 -0500
@@ -0,0 +1,36 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from tests.test_utils import assert_dialogue_equal, MESSAGE_SAMPLE
+from torchtune.data import Message
+from torchtune.models.llama2 import Llama2ChatTemplate
+
+
+class TestLlama2ChatTemplate:
+    expected_dialogue = [
+        Message(
+            role="user",
+            content="[INST] <<SYS>>\nYou are an AI assistant. User will you give you a task. "
+            "Your goal is to complete the task as faithfully as you can. While performing "
+            "the task think step-by-step and justify your steps.\n<</SYS>>\n\nPlease "
+            "briefly summarize this news article:\n\nAOL.com Video - Father Lets 8-Year-Old "
+            "Drive On Icy Road\n\nDescription:Would you let your 8-year-old drive your car? "
+            "How about on an icy road? Well one father in Russia did just that, and recorded "
+            "the entire thing. To her credit, the child seemed to be doing a great job. "
+            "(0:44)\n\nTags: 8-year-old driver , caught on camera , child driver , pix11\n\n"
+            "Summary: [/INST] ",
+        ),
+        Message(
+            role="assistant",
+            content="A father in Russia allowed his 8-year-old child to drive his car on an "
+            "icy road and recorded the event. The child appeared to be handling the situation well, "
+            "showcasing their driving skills despite the challenging conditions.",
+        ),
+    ]
+
+    def test_call(self):
+        actual = Llama2ChatTemplate()(MESSAGE_SAMPLE)
+        assert_dialogue_equal(actual, self.expected_dialogue)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/test_llama2_tokenizer.py marc/third_party/torchtune/tests/torchtune/models/llama2/test_llama2_tokenizer.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/test_llama2_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/test_llama2_tokenizer.py	2025-02-20 17:49:29.890024805 -0500
@@ -0,0 +1,495 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from tests.common import ASSETS
+from torchtune.data import Message
+from torchtune.models.llama2 import llama2_tokenizer
+
+
+class TestLlama2Tokenizer:
+    def tokenizer(self, template: bool = False):
+        # m.model is a pretrained Sentencepiece model using the following command:
+        # spm.SentencePieceTrainer.train('--input=<TRAIN_FILE> --model_prefix=m --vocab_size=2000')
+        return llama2_tokenizer(
+            str(ASSETS / "m.model"),
+            prompt_template="torchtune.models.llama2.Llama2ChatTemplate"
+            if template
+            else None,
+        )
+
+    @pytest.fixture
+    def messages(self):
+        return [
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task. Write a response "
+                "that appropriately completes the request.\n\n### Instruction:\nGenerate "
+                "a realistic dating profile bio.\n\n### Response:\n",
+                masked=True,
+            ),
+            Message(
+                role="assistant",
+                content="I'm an outgoing and friendly person who loves spending time with "
+                "friends and family. I'm also a big-time foodie and love trying out new "
+                "restaurants and different cuisines. I'm a big fan of the arts and enjoy "
+                "going to museums and galleries. I'm looking for someone who shares my "
+                "interest in exploring new places, as well as someone who appreciates a "
+                "good conversation over coffee.",
+            ),
+        ]
+
+    def test_tokenize_messages(self, messages):
+        tokenizer = self.tokenizer(template=False)
+        tokens, mask = tokenizer.tokenize_messages(messages)
+        expected_tokens = [
+            1,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+            2,
+        ]
+        # Mask user, unmask assistant, add EOS token
+        expected_mask = [True] * 75 + [False] * 125
+        assert expected_tokens == tokens
+        assert expected_mask == mask
+
+    @pytest.mark.parametrize(
+        "add_start_tokens, add_end_tokens",
+        [
+            (True, True),
+            (False, False),
+        ],
+    )
+    def test_tokenize_messages_chat_template(
+        self, messages, add_start_tokens, add_end_tokens
+    ):
+        tokenizer = self.tokenizer(template=True)
+        tokens, mask = tokenizer.tokenize_messages(
+            messages, add_start_tokens=add_start_tokens, add_end_tokens=add_end_tokens
+        )
+        expected_tokens = [
+            351,
+            82,
+            391,
+            221,
+            220,
+            193,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            351,
+            0,
+            82,
+            391,
+            221,
+            220,
+            193,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+        ]
+
+        # Mask user, unmask assistant
+        expected_mask = [True] * 87 + [False] * 124
+
+        if add_end_tokens:
+            expected_tokens = expected_tokens + [tokenizer.eos_id]
+            expected_mask = expected_mask + [False]
+
+        if add_start_tokens:
+            expected_tokens = [tokenizer.bos_id] + expected_tokens
+            expected_mask = [True] + expected_mask
+
+        assert expected_tokens == tokens
+        assert expected_mask == mask
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama2/test_lora_llama2.py marc/third_party/torchtune/tests/torchtune/models/llama2/test_lora_llama2.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama2/test_lora_llama2.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama2/test_lora_llama2.py	2025-02-20 17:49:29.894024811 -0500
@@ -0,0 +1,318 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from copy import deepcopy
+
+import pytest
+import torch
+
+from tests.test_utils import assert_expected, fixed_init_model
+from torch import nn
+from torchao.dtypes.nf4tensor import NF4Tensor
+from torchtune import training
+from torchtune.models.llama2 import llama2, lora_llama2
+from torchtune.models.llama2._component_builders import lora_llama2_self_attention
+from torchtune.modules.low_precision import FrozenNF4Linear
+from torchtune.modules.peft import get_merged_lora_ckpt, LoRALinear
+from torchtune.training.seed import set_seed
+
+RANK = 4
+ALPHA = 1.0
+BSZ = 2
+SEQ_LEN = 32
+EMBED_DIM = 64
+NUM_HEADS = 4
+NUM_KV_HEADS = 2
+MAX_SEQ_LEN = 64
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestLoRALlamaSelfAttention:
+    @pytest.fixture
+    def inputs(self) -> torch.Tensor:
+        inputs = torch.randn(BSZ, SEQ_LEN, EMBED_DIM)
+        return inputs
+
+    def get_lora_llama_self_attention(self, lora_modules):
+        lora_llama_sa = lora_llama2_self_attention(
+            lora_modules=lora_modules,
+            embed_dim=EMBED_DIM,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            max_seq_len=MAX_SEQ_LEN,
+            lora_rank=RANK,
+            lora_alpha=ALPHA,
+        )
+        fixed_init_model(lora_llama_sa)
+        return lora_llama_sa
+
+    def test_empty_lora_modules(self):
+        with pytest.raises(ValueError, match="Must pass one or more of"):
+            _ = self.get_lora_llama_self_attention([])
+
+    @pytest.mark.parametrize(
+        "lora_modules, expected",
+        [
+            (["q_proj", "v_proj"], torch.tensor(51.3152)),
+            (["q_proj", "k_proj", "v_proj", "output_proj"], torch.tensor(79.8887)),
+            (["k_proj"], torch.tensor(45.9261)),
+        ],
+    )
+    def test_forward(self, inputs, lora_modules, expected):
+        lora_llama_sa = self.get_lora_llama_self_attention(lora_modules)
+        actual = lora_llama_sa(inputs, inputs)
+        assert_expected(actual.shape, (BSZ, SEQ_LEN, EMBED_DIM))
+        assert_expected(actual.mean(), expected, atol=1e-4, rtol=1e-6)
+
+
+class TestLoRALlama2:
+    @pytest.fixture
+    def vocab_size(self):
+        return 50
+
+    @pytest.fixture
+    def inputs(self, vocab_size):
+        return torch.randint(low=0, high=vocab_size, size=(BSZ, SEQ_LEN))
+
+    def get_lora_llama2(
+        self,
+        lora_modules,
+        apply_lora_to_mlp,
+        apply_lora_to_output,
+        vocab_size,
+        reset_norm=True,
+        quantize_base=False,
+        embed_dim=EMBED_DIM,
+        dtype=None,
+    ):
+        num_layers = 3
+        model = lora_llama2(
+            lora_attn_modules=lora_modules,
+            apply_lora_to_mlp=apply_lora_to_mlp,
+            apply_lora_to_output=apply_lora_to_output,
+            vocab_size=vocab_size,
+            num_layers=num_layers,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=embed_dim,
+            max_seq_len=MAX_SEQ_LEN,
+            lora_rank=RANK,
+            lora_alpha=ALPHA,
+            quantize_base=quantize_base,
+        )
+        # To make final outputs less trivial
+        if reset_norm:
+            model.norm = nn.Identity()
+
+        # dtype=None means to just read dtype from parameters
+        # in the model. This dtype is set explicitly to bf16 currently
+        # when initializing QLoRA models, as ops such as `arange` aren't
+        # yet supported with the actual nf4 tensor dtype yet.
+        fixed_init_model(model, dtype=dtype)
+
+        return model
+
+    def get_ref_llama2(self, vocab_size, embed_dim=EMBED_DIM):
+        num_layers = 3
+        model = llama2(
+            vocab_size=vocab_size,
+            num_layers=num_layers,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=embed_dim,
+            max_seq_len=MAX_SEQ_LEN,
+        )
+        return model
+
+    @pytest.mark.parametrize(
+        "lora_modules, apply_lora_to_mlp, apply_lora_to_output, expected",
+        [
+            (["q_proj", "v_proj"], False, False, torch.tensor(5638859.0)),
+            (
+                ["q_proj", "k_proj", "v_proj", "output_proj"],
+                True,
+                False,
+                torch.tensor(21187608.0),
+            ),
+            (["k_proj"], True, True, torch.tensor(32438764.0)),
+        ],
+    )
+    def test_forward(
+        self,
+        vocab_size,
+        inputs,
+        lora_modules,
+        apply_lora_to_mlp,
+        apply_lora_to_output,
+        expected,
+    ):
+        model = self.get_lora_llama2(
+            lora_modules, apply_lora_to_mlp, apply_lora_to_output, vocab_size
+        )
+        actual = model(inputs)
+        assert_expected(actual.shape, (BSZ, SEQ_LEN, vocab_size))
+        assert_expected(actual.mean(), expected, atol=1e-4, rtol=1e-6)
+
+    @pytest.mark.parametrize(
+        "lora_modules, apply_lora_to_mlp, apply_lora_to_output",
+        [
+            (["q_proj", "v_proj"], True, False),
+            (["q_proj", "k_proj", "v_proj", "output_proj"], False, False),
+            (["k_proj"], True, True),
+        ],
+    )
+    def test_lora_llama2_state_dict_parity(
+        self, lora_modules, apply_lora_to_mlp, apply_lora_to_output, vocab_size
+    ):
+        lora_llama = self.get_lora_llama2(
+            lora_modules,
+            apply_lora_to_mlp,
+            apply_lora_to_output,
+            vocab_size,
+            reset_norm=False,
+        )
+        ref_llama = self.get_ref_llama2(vocab_size)
+        # Ensure ref_llama state_dict can be loaded into lora_llama with only "lora"
+        # keys missing.
+        ref_llama_state_dict = ref_llama.state_dict()
+        missing, unexpected = lora_llama.load_state_dict(
+            ref_llama_state_dict, strict=False
+        )
+        assert not unexpected
+        assert all(["lora" in key for key in missing])
+
+    def test_qlora_linear_quantize_base(self):
+        model = self.get_lora_llama2(
+            lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+            apply_lora_to_mlp=True,
+            # quantize_base
+            apply_lora_to_output=False,
+            vocab_size=50,
+            quantize_base=True,
+            embed_dim=512,
+            dtype=torch.bfloat16,
+        )
+        for module in model.modules():
+            if isinstance(module, LoRALinear):
+                assert module._quantize_base
+
+    def test_qlora_linear_quantize_base_weights(self):
+        # this test checks that modules that don't have LoRA applied to them
+        # have their base weights quantized
+        model = self.get_lora_llama2(
+            lora_modules=["q_proj", "v_proj"],
+            apply_lora_to_mlp=True,
+            # quantize_base
+            apply_lora_to_output=False,
+            vocab_size=50,
+            quantize_base=True,
+            embed_dim=512,
+            dtype=torch.bfloat16,
+        )
+        for name, module in model.named_modules():
+            if isinstance(module, LoRALinear):
+                assert module._quantize_base
+            elif name in ["k_proj", "output_proj"]:
+                assert isinstance(module, FrozenNF4Linear)
+                assert isinstance(module.weight, NF4Tensor)
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_qlora_llama2_parity(self, dtype, inputs):
+        with training.set_default_dtype(dtype):
+            model_ref = self.get_lora_llama2(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=False,
+                embed_dim=512,
+                dtype=dtype,
+            )
+            qlora = self.get_lora_llama2(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=True,
+                embed_dim=512,
+                dtype=dtype,
+            )
+        qlora_sd = qlora.state_dict()
+        model_ref.load_state_dict(qlora_sd)
+        # Forward pass of model_ref and qlora should be the same, as QLoRA linear layers should use
+        # a special linear operator that runs the compute in bf16, but only saves the 4 bit tensors
+        # for backward.
+        ref_output = model_ref(inputs)
+        output = qlora(inputs)
+        torch.testing.assert_close(ref_output, output)
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_qlora_llama2_state_dict(self, dtype):
+        with training.set_default_dtype(dtype):
+            model_ref = self.get_lora_llama2(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=False,
+                embed_dim=512,
+                dtype=dtype,
+            )
+            high_prec_sd = model_ref.state_dict()
+            for v in high_prec_sd.values():
+                assert v.dtype == dtype
+
+            # ensure quantized LoRA can load a bf16 state_dict
+            qlora = self.get_lora_llama2(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=True,
+                embed_dim=512,
+                dtype=dtype,
+            )
+            qlora.load_state_dict(high_prec_sd)
+            # LoRALinear base weights should be nf4 still
+            for module in qlora.modules():
+                if isinstance(module, LoRALinear):
+                    assert isinstance(module.weight, NF4Tensor)
+            # saved state_dict should have bf16 weights.
+            qlora_sd = qlora.state_dict()
+            for v in qlora_sd.values():
+                assert v.dtype == dtype
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_qlora_llama2_merged_state_dict(self, dtype):
+        with training.set_default_dtype(dtype):
+            qlora = self.get_lora_llama2(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=True,
+                embed_dim=512,
+                dtype=dtype,
+                reset_norm=False,  # to ensure norm.scale key exists
+            )
+
+        qlora_sd = qlora.state_dict()
+        # Ensure checkpoint merging produces bf16 tensors
+        merged_ckpt = get_merged_lora_ckpt(deepcopy(qlora_sd), rank=RANK, alpha=ALPHA)
+        for v in merged_ckpt.values():
+            # paranoid check for both, as NF4Tensor had issue where NF4Tensor.dtype would return bf16
+            assert not isinstance(v, NF4Tensor)
+            assert v.dtype == dtype
+
+        # Ensure checkpoint can be loaded into non-LoRA model
+        with training.set_default_dtype(dtype):
+            llama2 = self.get_ref_llama2(vocab_size=50, embed_dim=512)
+
+        llama2.load_state_dict(merged_ckpt)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama3/test_llama3.py marc/third_party/torchtune/tests/torchtune/models/llama3/test_llama3.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama3/test_llama3.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama3/test_llama3.py	2025-02-20 17:49:29.898024818 -0500
@@ -0,0 +1,46 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from tests.test_utils import fixed_init_model
+from torchtune.models.llama3 import llama3
+from torchtune.training.seed import set_seed
+
+EMBED_DIM = 128
+NUM_LAYERS = 4
+NUM_HEADS = 16
+NUM_KV_HEADS = 8
+VOCAB_SIZE = 32000
+MAX_SEQ_LEN = 2048
+BSZ = 2
+SEQ_LEN = 100
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestLlama3:
+    @pytest.fixture
+    def inputs(self):
+        return torch.randint(0, VOCAB_SIZE, (BSZ, SEQ_LEN))
+
+    def test_forward(self, inputs):
+        model = llama3(
+            vocab_size=VOCAB_SIZE,
+            num_layers=NUM_LAYERS,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=EMBED_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+        )
+        fixed_init_model(model, min_val=-0.25, max_val=0.5)
+        actual = model(inputs)
+        expected = torch.tensor(3.9763)
+        assert actual.shape == (BSZ, SEQ_LEN, VOCAB_SIZE)
+        torch.testing.assert_close(actual.mean(), expected, atol=1e-4, rtol=1e-4)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama3/test_llama3_tokenizer.py marc/third_party/torchtune/tests/torchtune/models/llama3/test_llama3_tokenizer.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama3/test_llama3_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama3/test_llama3_tokenizer.py	2025-02-20 17:49:29.902024825 -0500
@@ -0,0 +1,444 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from tests.common import ASSETS
+from torchtune.data._messages import Message
+from torchtune.models.llama3 import llama3_tokenizer, Llama3Tokenizer
+
+
+class TestLlama3Tokenizer:
+    @pytest.fixture
+    def tokenizer(self):
+        # Pretrained tiktoken model generated via the script in
+        # https://gist.github.com/ebsmothers/54b133dd87db6679b14318545aaa2de4
+        return llama3_tokenizer(
+            path=str(ASSETS / "tiktoken_small.model"),
+            max_seq_len=2048,
+        )
+
+    @pytest.fixture
+    def user_text_a(self):
+        return "I can see the sun. "
+
+    @pytest.fixture
+    def user_text_b(self):
+        return "But even if I cannot see the sun, I know that it exists."
+
+    @pytest.fixture
+    def assistant_text(self):
+        return "And to know that the sun is there - that is living."
+
+    @pytest.fixture
+    def user_text_message(self, user_text_a, user_text_b):
+        message = Message(
+            role="user",
+            content=user_text_a + user_text_b,
+            masked=True,
+            eot=True,
+        )
+        expected_tokens = [
+            128000,
+            128006,
+            477,
+            273,
+            128007,
+            10,
+            10,
+            73,
+            503,
+            654,
+            262,
+            376,
+            110,
+            46,
+            690,
+            720,
+            428,
+            270,
+            1119,
+            654,
+            262,
+            376,
+            110,
+            44,
+            270,
+            686,
+            334,
+            312,
+            522,
+            511,
+            115,
+            46,
+            128009,
+        ]
+        return message, expected_tokens
+
+    @pytest.fixture
+    def assistant_text_message(self, assistant_text):
+        message = Message(
+            role="assistant",
+            content=assistant_text,
+            masked=False,
+            eot=True,
+        )
+        expected_tokens = [
+            128006,
+            520,
+            511,
+            446,
+            128007,
+            10,
+            10,
+            65,
+            269,
+            277,
+            686,
+            334,
+            262,
+            376,
+            110,
+            351,
+            443,
+            32,
+            45,
+            334,
+            351,
+            1955,
+            46,
+            128009,
+            128001,
+        ]
+        return message, expected_tokens
+
+    @pytest.fixture
+    def user_image_text_message(self, user_text_a, user_text_b):
+        message = Message(
+            role="user",
+            content=[
+                {"type": "image"},
+                {"type": "text", "content": user_text_a + user_text_b},
+            ],
+            masked=True,
+            eot=True,
+        )
+        expected_tokens = [
+            128000,
+            128006,
+            477,
+            273,
+            128007,
+            10,
+            10,
+            128256,
+            73,
+            503,
+            654,
+            262,
+            376,
+            110,
+            46,
+            690,
+            720,
+            428,
+            270,
+            1119,
+            654,
+            262,
+            376,
+            110,
+            44,
+            270,
+            686,
+            334,
+            312,
+            522,
+            511,
+            115,
+            46,
+            128009,
+        ]
+        return message, expected_tokens
+
+    @pytest.fixture
+    def user_interleaved_image_text_message(self, user_text_a, user_text_b):
+        message = Message(
+            role="user",
+            content=[
+                {"type": "image"},
+                {"type": "text", "content": user_text_a},
+                {"type": "image"},
+                {"type": "text", "content": user_text_b},
+            ],
+            masked=True,
+            eot=True,
+        )
+        expected_tokens = [
+            128000,
+            128006,
+            477,
+            273,
+            128007,
+            10,
+            10,
+            128256,
+            73,
+            503,
+            654,
+            262,
+            376,
+            110,
+            46,
+            128256,
+            1542,
+            720,
+            428,
+            270,
+            1119,
+            654,
+            262,
+            376,
+            110,
+            44,
+            270,
+            686,
+            334,
+            312,
+            522,
+            511,
+            115,
+            46,
+            128009,
+        ]
+        return message, expected_tokens
+
+    @pytest.fixture
+    def assistant_tool_message(self):
+        message = Message(
+            role="assistant",
+            content=[
+                {"type": "text", "content": "locate_sun(radius=100_000_000)"},
+            ],
+            masked=False,
+            ipython=True,
+            eot=False,
+        )
+        expected_tokens = [
+            128006,
+            520,
+            511,
+            446,
+            128007,
+            10,
+            10,
+            128010,
+            525,
+            99,
+            534,
+            95,
+            115,
+            433,
+            40,
+            114,
+            338,
+            105,
+            477,
+            61,
+            49,
+            1635,
+            95,
+            1635,
+            48,
+            95,
+            1635,
+            48,
+            41,
+            128008,
+        ]
+        return message, expected_tokens
+
+    @pytest.fixture
+    def ipython_message(self):
+        message = Message(
+            role="ipython",
+            content=[
+                {"type": "text", "content": '{"content": True}'},
+            ],
+            masked=True,
+            eot=False,
+        )
+        expected_tokens = [
+            128006,
+            1558,
+            121,
+            483,
+            279,
+            128007,
+            10,
+            10,
+            123,
+            34,
+            99,
+            957,
+            317,
+            34,
+            58,
+            323,
+            114,
+            979,
+            125,
+            128008,
+        ]
+        return message, expected_tokens
+
+    def test_token_ids(self, tokenizer):
+        assert tokenizer.bos_id == 128000
+        assert tokenizer.eos_id == 128001
+        assert tokenizer.pad_id == 128004
+        assert tokenizer.step_id == 128005
+        assert tokenizer.start_header_id == 128006
+        assert tokenizer.end_header_id == 128007
+        assert tokenizer.eom_id == 128008
+        assert tokenizer.eot_id == 128009
+        assert tokenizer.python_tag == 128010
+        assert tokenizer.image_id == 128256
+
+    def test_tokenizer_vocab_size(self, tokenizer):
+        assert tokenizer.base_vocab_size == 2000
+        assert tokenizer.vocab_size == 128257
+
+    def test_tokenize_text_messages(
+        self, tokenizer, user_text_message, assistant_text_message
+    ):
+        text_messages = [user_text_message[0], assistant_text_message[0]]
+        expected_tokens = user_text_message[1] + assistant_text_message[1]
+        expected_mask = (
+            [True] * len(user_text_message[1])
+            + [False] * (len(assistant_text_message[1]) - 1)
+            + [True]
+        )
+        tokens, mask = tokenizer.tokenize_messages(text_messages)
+        assert tokens == expected_tokens
+        assert mask == expected_mask
+
+    def test_tokenize_message_drop_eot_and_eos(
+        self, tokenizer, user_text_message, assistant_text_message
+    ):
+        """
+        Test that the tokenizer will not add an EOS token or EOT token if user requests it.
+        This is the most common case for inference.
+        """
+        text_messages = [user_text_message[0], assistant_text_message[0]]
+        # Chop the end of the assistant message to remove the EOS token *and* EOT token
+        expected_tokens = user_text_message[1] + assistant_text_message[1][:-2]
+        # No need to mask out the EOS token *or* EOT token at the end since they are not there
+        expected_mask = [True] * len(user_text_message[1]) + [False] * (
+            len(assistant_text_message[1]) - 2
+        )
+        tokens, mask = tokenizer.tokenize_messages(text_messages, add_end_tokens=False)
+        assert tokens == expected_tokens
+        assert mask == expected_mask
+
+    def test_tokenize_image_and_text_messages(
+        self, tokenizer, user_image_text_message, assistant_text_message
+    ):
+        image_and_text_messages = [
+            user_image_text_message[0],
+            assistant_text_message[0],
+        ]
+        expected_tokens = user_image_text_message[1] + assistant_text_message[1]
+        expected_mask = (
+            [True] * len(user_image_text_message[1])
+            + [False] * (len(assistant_text_message[1]) - 1)
+            + [True]
+        )
+        tokens, mask = tokenizer.tokenize_messages(image_and_text_messages)
+        assert tokens == expected_tokens
+        assert mask == expected_mask
+
+    def test_tokenize_interleaved_image_and_text_messages(
+        self,
+        tokenizer,
+        user_interleaved_image_text_message,
+        assistant_text_message,
+    ):
+        interleaved_image_and_text_messages = [
+            user_interleaved_image_text_message[0],
+            assistant_text_message[0],
+        ]
+        expected_tokens = (
+            user_interleaved_image_text_message[1] + assistant_text_message[1]
+        )
+        expected_mask = (
+            [True] * len(user_interleaved_image_text_message[1])
+            + [False] * (len(assistant_text_message[1]) - 1)
+            + [True]
+        )
+        tokens, mask = tokenizer.tokenize_messages(interleaved_image_and_text_messages)
+        assert tokens == expected_tokens
+        assert mask == expected_mask
+
+    def test_tokenize_tool_call_messages(
+        self,
+        tokenizer,
+        user_text_message,
+        assistant_tool_message,
+        ipython_message,
+        assistant_text_message,
+    ):
+        tool_call_messages = [
+            user_text_message[0],
+            assistant_tool_message[0],
+            ipython_message[0],
+            assistant_text_message[0],
+        ]
+        expected_tokens = (
+            user_text_message[1]
+            + assistant_tool_message[1]
+            + ipython_message[1]
+            + assistant_text_message[1]
+        )
+        expected_mask = (
+            [True] * len(user_text_message[1])
+            + [False] * len(assistant_tool_message[1])
+            + [True] * len(ipython_message[1])
+            + [False] * (len(assistant_text_message[1]) - 1)
+            + [True]
+        )
+        tokens, mask = tokenizer.tokenize_messages(tool_call_messages)
+        assert tokens == expected_tokens
+        assert mask == expected_mask
+
+    def test_validate_special_tokens(self):
+        with pytest.raises(
+            ValueError, match="<|begin_of_text|> missing from special_tokens"
+        ):
+            _ = Llama3Tokenizer(
+                path=str(ASSETS / "tiktoken_small.model"),
+                # Same as LLAMA3_SPECIAL_TOKENS but one missing
+                special_tokens={
+                    "<|end_of_text|>": 128001,
+                    "<|start_header_id|>": 128006,
+                    "<|end_header_id|>": 128007,
+                    "<|eot_id|>": 128009,
+                    "<|eom_id|>": 128008,
+                    "<|python_tag|>": 128255,
+                },
+            )
+
+    def test_skip_special_tokens(
+        self,
+        tokenizer,
+        user_text_message,
+        assistant_text_message,
+        user_text_a,
+        user_text_b,
+        assistant_text,
+    ):
+        # This should satisfy text = decode(encode(text))
+        tokens = user_text_message[1] + assistant_text_message[1]
+        text = tokenizer.decode(tokens, skip_special_tokens=True)
+        assert text == user_text_a + user_text_b + assistant_text
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/llama3_1/test_position_embeddings.py marc/third_party/torchtune/tests/torchtune/models/llama3_1/test_position_embeddings.py
--- marc_original/third_party/torchtune/tests/torchtune/models/llama3_1/test_position_embeddings.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/llama3_1/test_position_embeddings.py	2025-02-20 17:49:29.906024831 -0500
@@ -0,0 +1,143 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+
+from tests.test_utils import assert_expected, mps_ignored_test
+from torch import tensor
+
+from torchtune.models.llama3_1._position_embeddings import Llama3ScaledRoPE
+
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(0)
+
+
+class TestLlama3ScaledRoPE:
+    """
+    Class for testing our Scaled RoPE for LLama3.1 (RoPE)
+    implementation. The expected tensors are computed from the
+    reference implementation here:
+    https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/api/model.py#L272
+
+    The expected values are computed using the following code:
+    https://gist.github.com/joecummings/4f1331a9c1e5aa15bad1641acb74fe0e
+    """
+
+    EXPECTED_FREQS_CIS_MEAN = tensor(0.1738)
+    EXPECTED_FREQS_CIS_SUM = tensor(91141.7656)
+    EXPECTED_FREQS_CIS_MAX = tensor(1.0)
+
+    EXPECTED_X_OUT_MEAN = tensor(-2.4781e-06)
+    EXPECTED_X_OUT_SUM = tensor(-83.1523)
+    EXPECTED_X_OUT_MAX = tensor(5.4625)
+
+    @pytest.fixture
+    def input_params(self):
+        bsz = 4
+        num_heads = 32
+        embed_dim = 4096
+        head_dim = embed_dim // num_heads
+        seq_len = 2048
+        max_seq_len = 4096
+        return bsz, num_heads, head_dim, seq_len, max_seq_len
+
+    @pytest.fixture
+    def input(self, input_params) -> tensor:
+        bsz, num_heads, head_dim, seq_len, _ = input_params
+        return torch.randn(bsz, seq_len, num_heads, head_dim)
+
+    @pytest.fixture
+    def rope(self, input_params) -> Llama3ScaledRoPE:
+        _, _, head_dim, _, max_seq_len = input_params
+        return Llama3ScaledRoPE(dim=head_dim, max_seq_len=max_seq_len)
+
+    def test_cache_equality(self, input, rope) -> None:
+        # Have to explicitly call _rope_init() to initialize theta matrix
+        rope.rope_init()
+        cache = rope.cache
+
+        assert_expected(cache.mean(), self.EXPECTED_FREQS_CIS_MEAN, atol=1e-4)
+        assert_expected(cache.sum(), self.EXPECTED_FREQS_CIS_SUM, atol=1e-4)
+        assert_expected(cache.max(), self.EXPECTED_FREQS_CIS_MAX)
+
+    @mps_ignored_test()
+    def test_forward(self, input, rope) -> None:
+        x_out = rope(input)
+
+        # check the numerics of the computed tensor
+        assert_expected(x_out.mean(), self.EXPECTED_X_OUT_MEAN)
+        assert_expected(x_out.sum(), self.EXPECTED_X_OUT_SUM)
+        assert_expected(x_out.max(), self.EXPECTED_X_OUT_MAX)
+
+        # check shapes
+        assert_expected(x_out.shape, input.shape)
+
+    @mps_ignored_test()
+    def test_forward_with_curr_pos(self, input, rope) -> None:
+        (
+            _,
+            seq_len,
+            _,
+            _,
+        ) = input.shape
+        x_out = rope(input, input_pos=torch.arange(seq_len))
+
+        # these values should be exactly the same as test_forward
+        # since in this case input_pos covers the entire input
+        # sequence. This tests that input_pos works as expected i.e.
+        # extracts the embeddings for the relevant positions
+        assert_expected(x_out.mean(), self.EXPECTED_X_OUT_MEAN, atol=1e-4)
+        assert_expected(x_out.sum(), self.EXPECTED_X_OUT_SUM)
+        assert_expected(x_out.max(), self.EXPECTED_X_OUT_MAX)
+
+        # check shapes
+        assert_expected(x_out.shape, input.shape)
+
+    @mps_ignored_test()
+    def test_forward_with_2d_pos_ids(self, input, rope) -> None:
+        """
+        Use input_pos to indicate positions of each token relative to its sequence
+        when sample is packed.
+        """
+        (
+            bsz,
+            seq_len,
+            _,
+            _,
+        ) = input.shape
+        x_out = rope(
+            input, input_pos=torch.arange(seq_len).unsqueeze(0).expand(bsz, seq_len)
+        )
+
+        # these values should be exactly the same as test_forward
+        # AND test_forward_with_current_pos. In this case input_pos
+        # covers the entire batch dim and is defined for each sample separately.
+        # This tests that input_pos works as expected i.e.
+        # extracts the embeddings for the relevant positions for each sample
+        assert_expected(x_out.mean(), self.EXPECTED_X_OUT_MEAN, atol=1e-4)
+        assert_expected(x_out.sum(), self.EXPECTED_X_OUT_SUM)
+        assert_expected(x_out.max(), self.EXPECTED_X_OUT_MAX)
+
+        # check shapes
+        assert_expected(x_out.shape, input.shape)
+
+    def test_rope_init_meta_device(self, input_params):
+        _, _, head_dim, _, max_seq_len = input_params
+        rope_on_device = Llama3ScaledRoPE(dim=head_dim, max_seq_len=max_seq_len)
+        with torch.device("meta"):
+            meta_rope = Llama3ScaledRoPE(dim=head_dim, max_seq_len=max_seq_len)
+
+        meta_rope.rope_init()
+        for p1, p2 in zip(rope_on_device.buffers(), meta_rope.buffers()):
+            torch.testing.assert_close(p1, p2)
+
+        # Assert meta_rope cache is no longer on meta device
+        assert meta_rope.cache.device != torch.device("meta")
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_feed_forward.py marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_feed_forward.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_feed_forward.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_feed_forward.py	2025-02-20 17:49:29.918024851 -0500
@@ -0,0 +1,65 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+import torch
+
+from tests.test_utils import fixed_init_model
+from tests.torchtune.models.mistral.scripts.mistral_reference import FeedForward
+
+from tests.torchtune.models.mistral.scripts.mistral_test_config import MistralTestConfig
+
+from torchtune.models.mistral._component_builders import mistral_mlp
+
+
+def compare_feed_forward(embed_dim: int, intermediate_dim: int) -> None:
+
+    # make sure we have the right seed for generating outputs
+    # this should match up the seed value set in the corresponding
+    # unit test
+    torch.manual_seed(MistralTestConfig.SEED)
+
+    # generate input tensor used by both implementations
+    input_t = torch.randn(1, embed_dim)
+
+    # reference implementation
+    ff_ref = FeedForward(dim=embed_dim, hidden_dim=intermediate_dim)
+    fixed_init_model(ff_ref)
+
+    with torch.no_grad():
+        ff_out_ref = ff_ref(input_t)
+
+    ff = mistral_mlp(embed_dim, intermediate_dim)
+    fixed_init_model(ff)
+
+    with torch.no_grad():
+        ff_out = ff(input_t)
+
+    torch.testing.assert_close(ff_out, ff_out_ref, atol=1e-5, rtol=1e-5)
+    print(f"ff_out.mean(): {ff_out.mean()}")
+    print(f"ff_out.max(): {ff_out.max()}")
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser = argparse.ArgumentParser(description="Compare FeedForward implementations")
+    parser.add_argument(
+        "--embed_dim",
+        type=int,
+        default=MistralTestConfig.EMBED_DIM,
+        help="Embedding dimension for self-attention",
+    )
+    parser.add_argument(
+        "--intermediate_dim",
+        type=int,
+        default=MistralTestConfig.INTERMEDIATE_DIM,
+        help="Intermediate dimension for MLP",
+    )
+
+    args = parser.parse_args()
+
+    compare_feed_forward(args.embed_dim, args.intermediate_dim)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_mistral_classifier.py marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_mistral_classifier.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_mistral_classifier.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_mistral_classifier.py	2025-02-20 17:49:29.926024864 -0500
@@ -0,0 +1,188 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+
+from tests.test_utils import fixed_init_model
+from torch import nn
+from torchtune.models.mistral import mistral_classifier
+from torchtune.models.mistral._component_builders import mistral_mlp
+from torchtune.modules import (
+    MultiHeadAttention,
+    RMSNorm,
+    RotaryPositionalEmbeddings,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+
+
+# Copying our mistral implementation here to allow access to `output_proj`
+def mistral(
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+    output_proj: nn.Linear,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: int = 10_000,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the mistral model. This includes:
+    - Token embeddings
+    - num_layers number of TransformerSelfAttentionLayer blocks
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    This does NOT currently include inference-time optimizations such as
+    sliding-window attention
+
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        intermediate_dim (int): intermediate dimension for MLP
+        max_seq_len (int): maximum sequence length the model will be run with,
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        norm_eps (float): epsilon in RMS norms
+        rope_base (int): base for the rotary positional embeddings. Default: 10_000
+
+    Returns:
+        TransformerDecoder: Instantiation of mistral model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+
+    rope = RotaryPositionalEmbeddings(
+        dim=head_dim, max_seq_len=max_seq_len, base=rope_base
+    )
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    mlp = mistral_mlp(dim=embed_dim, hidden_dim=intermediate_dim)
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+
+def compare_mistral_classifier(
+    bsz: int,
+    seq_len: int,
+    num_classes: int,
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+):
+
+    # setting up the right seed for generating outputs
+    torch.manual_seed(16)
+
+    # generate input tensor to be used by both implementations
+    x = torch.randint(low=0, high=vocab_size, size=(bsz, seq_len))
+
+    # our implementation
+    classifier = mistral_classifier(
+        num_classes=num_classes,
+        vocab_size=vocab_size,
+        num_layers=num_layers,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        embed_dim=embed_dim,
+        intermediate_dim=intermediate_dim,
+        max_seq_len=max_seq_len,
+    )
+    fixed_init_model(classifier)
+
+    with torch.no_grad():
+        out = classifier(x)
+
+    # reference implementation: manually specify nn.Linear after base mistral
+    output_proj = nn.Linear(embed_dim, num_classes, bias=False)
+    classifier_ref = mistral(
+        vocab_size=vocab_size,
+        num_layers=num_layers,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        embed_dim=embed_dim,
+        intermediate_dim=intermediate_dim,
+        max_seq_len=max_seq_len,
+        output_proj=output_proj,
+    )
+
+    fixed_init_model(classifier_ref)
+
+    with torch.no_grad():
+        out_ref = classifier_ref(x)
+
+    print(
+        f"output layer: {classifier.output}\n reference output layer: {classifier_ref.output}"
+    )
+    print(f"output mean: {out.mean()}\n reference output mean: {out_ref.mean()}")
+    print(f"output shape: {out.shape}\n reference output shape: {out_ref.shape}")
+
+    # output tensors should be similar within precision tolerance
+    torch.testing.assert_close(out, out_ref, atol=1e-5, rtol=1e-3)
+    assert out.shape == (bsz, seq_len, num_classes)
+
+
+if __name__ == "__main__":
+    # (bsz, embed_dim, seq_len, n_classes) # expected
+    test_cases = [
+        (2, 64, 64, 2),  # 22.6879
+        (64, 128, 256, 200),  # 36.8238
+        (1, 256, 512, 1),  # 110.2561
+    ]
+    for bsz, embed_dim, seq_len, n_classes in test_cases:
+        compare_mistral_classifier(
+            bsz,
+            seq_len,
+            n_classes,
+            vocab_size=32000,
+            num_layers=4,
+            num_heads=16,
+            num_kv_heads=8,
+            embed_dim=embed_dim,
+            intermediate_dim=512,
+            max_seq_len=2048,
+        )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_mistral.py marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_mistral.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_mistral.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/compare_mistral.py	2025-02-20 17:49:29.922024857 -0500
@@ -0,0 +1,184 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+
+from tests.test_utils import fixed_init_model
+from tests.torchtune.models.mistral.scripts.mistral_reference import Transformer
+from tests.torchtune.models.mistral.scripts.mistral_test_config import MistralTestConfig
+
+from torchtune.models.mistral import mistral
+
+
+def compare_decoder(
+    bsz: int,
+    vocab_size: int,
+    seq_len: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    n_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    rope_base: int,
+    norm_eps: float,
+) -> None:
+    # make sure we have the right seed for generating outputs
+    # this should match up the seed value set in the corresponding
+    # unit test
+    torch.manual_seed(MistralTestConfig.SEED)
+
+    head_dim = embed_dim // num_heads
+
+    # generate input tensor used by both implementations
+    x_input = torch.randint(low=0, high=vocab_size, size=(bsz, seq_len))
+
+    # current implementation; initialize with constant to compare outputs
+    mistral_model = mistral(
+        vocab_size=vocab_size,
+        embed_dim=embed_dim,
+        num_layers=n_layers,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        intermediate_dim=intermediate_dim,
+        norm_eps=norm_eps,
+        rope_base=rope_base,
+    )
+    fixed_init_model(mistral_model)
+
+    with torch.no_grad():
+        mistral_model_out = mistral_model(x_input)
+
+    # initialize reference implementation with constant weights
+    ref_mistral_model = Transformer(
+        vocab_size=vocab_size,
+        n_layers=n_layers,
+        n_heads=num_heads,
+        head_dim=head_dim,
+        dim=embed_dim,
+        n_kv_heads=num_kv_heads,
+        hidden_dim=intermediate_dim,
+        max_seq_len=max_seq_len,
+        rope_base=rope_base,
+        norm_eps=norm_eps,
+    )
+
+    mapped_sd = {}
+    for k, v in mistral_model.state_dict().items():
+        new_k = k.replace("attn", "attention")
+        new_k = (
+            new_k.replace("q_proj", "wq")
+            .replace("k_proj", "wk")
+            .replace("v_proj", "wv")
+            .replace("output_proj", "wo")
+        )
+        new_k = new_k.replace("mlp", "feed_forward")
+        new_k = new_k.replace("feed_forward_norm.scale", "ffn_norm.weight")
+        new_k = new_k.replace("sa_norm.scale", "attention_norm.weight")
+
+        new_k = new_k.replace("norm.scale", "norm.weight")
+        mapped_sd[new_k] = v
+
+    ref_mistral_model.load_state_dict(mapped_sd)
+
+    with torch.no_grad():
+        red_mistral_model_out = ref_mistral_model(x_input, torch.arange(seq_len))
+
+    # # value: torch.tensor(18.2749)
+    print(f"mistral_model_out.mean(): {mistral_model_out.mean()}")
+    print(f"red_mistral_model_out.mean(): {red_mistral_model_out.mean()}")
+
+    torch.testing.assert_close(
+        mistral_model_out, red_mistral_model_out, atol=1e-2, rtol=1e-2
+    )
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser = argparse.ArgumentParser(description="Compare Decoder implementations")
+    parser.add_argument(
+        "--bsz",
+        type=int,
+        default=MistralTestConfig.BSZ,
+        help="Batch size of input tensor",
+    )
+    parser.add_argument(
+        "--seq_len",
+        type=int,
+        default=MistralTestConfig.SEQ_LEN,
+        help="input sequence length",
+    )
+    parser.add_argument(
+        "--vocab_size",
+        type=int,
+        default=MistralTestConfig.VOCAB_SIZE,
+        help="vocab size",
+    )
+    parser.add_argument(
+        "--embed_dim",
+        type=int,
+        default=MistralTestConfig.EMBED_DIM,
+        help="Embedding dimension used to compute the dim for RopE",
+    )
+    parser.add_argument(
+        "--intermediate_dim",
+        type=int,
+        default=MistralTestConfig.INTERMEDIATE_DIM,
+        help="Intermediate dimension for MLP",
+    )
+    parser.add_argument(
+        "--num_layers",
+        type=int,
+        default=MistralTestConfig.NUM_LAYERS,
+        help="number of transformer layers",
+    )
+    parser.add_argument(
+        "--num_heads",
+        type=int,
+        default=MistralTestConfig.NUM_HEADS,
+        help="Number of heads in the attention layer",
+    )
+    parser.add_argument(
+        "--num_kv_heads",
+        type=int,
+        default=MistralTestConfig.NUM_KV_HEADS,
+        help="Number of key/value heads in the attention layer",
+    )
+    parser.add_argument(
+        "--max_seq_len",
+        type=int,
+        default=MistralTestConfig.MAX_SEQ_LEN,
+        help="max sequence length",
+    )
+    parser.add_argument(
+        "--norm_eps",
+        type=float,
+        default=MistralTestConfig.NORM_EPS,
+        help="RMSNorm epsilon",
+    )
+    parser.add_argument(
+        "--rope_base",
+        type=float,
+        default=MistralTestConfig.ROPE_BASE,
+        help="Base for the rotary positional embeddings",
+    )
+    args = parser.parse_args()
+
+    compare_decoder(
+        args.bsz,
+        args.vocab_size,
+        args.seq_len,
+        args.embed_dim,
+        args.intermediate_dim,
+        args.num_layers,
+        args.num_heads,
+        args.num_kv_heads,
+        args.max_seq_len,
+        args.rope_base,
+        args.norm_eps,
+    )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/__init__.py marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/__init__.py	2025-02-20 17:49:29.914024844 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/mistral_reference.py marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/mistral_reference.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/mistral_reference.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/mistral_reference.py	2025-02-20 17:49:29.930024871 -0500
@@ -0,0 +1,285 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional, Tuple
+
+import torch
+from torch import nn
+
+"""
+Reference mistral implementation from the official repo:
+https://github.com/mistralai/mistral-src/blob/main/one_file_ref.py
+
+Components are copied here with minimal modifications.
+"""
+
+
+"""
+Reference implementation of Attention from:
+https://github.com/mistralai/mistral-src/blob/main/one_file_ref.py
+
+Note, there's another implementation in the same repo which uses xformers for attention:
+https://github.com/mistralai/mistral-src/blob/8598cf582091a596671be31990448e0620017851/mistral/model.py#L60
+
+The implementation for this test uses `one_file_ref.py` since the xformers attention implementation
+expects the input `[b, s, ...]` to be flattened `[b * s, ...]` which makes comparison difficult.
+
+Replicating code here to minimize dependencies. The code is modified to
+remove dependencies from xformers and features like KV Caching.
+"""
+
+
+def repeat_kv(keys: torch.Tensor, values: torch.Tensor, repeats: int):
+    keys = torch.repeat_interleave(keys, repeats=repeats, dim=2)
+    values = torch.repeat_interleave(values, repeats=repeats, dim=2)
+    return keys, values
+
+
+class Attention(nn.Module):
+    def __init__(self, n_heads: int, head_dim: int, dim: int, n_kv_heads: int):
+        super().__init__()
+
+        self.n_heads = n_heads
+        self.n_kv_heads = n_kv_heads
+        self.head_dim = head_dim
+
+        self.repeats = self.n_heads // self.n_kv_heads
+
+        self.scale = head_dim**-0.5
+
+        self.wq = nn.Linear(dim, n_heads * head_dim, bias=False)
+        self.wk = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
+        self.wv = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
+        self.wo = nn.Linear(n_heads * head_dim, dim, bias=False)
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        freqs_cis: torch.Tensor,
+        mask: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        # removed positions as it was only used for cache retrieval
+        bsz, seqlen, _ = x.shape
+
+        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
+        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)
+        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
+        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)
+        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
+        key, value = repeat_kv(xk, xv, self.repeats)
+
+        query = xq.transpose(1, 2)
+        key = key.transpose(1, 2)
+        value = value.transpose(1, 2)
+        # scores : [bsz, n_heads, seqlen | 1, seqlen]
+        scores = torch.matmul(query, key.transpose(2, 3)) * self.scale
+        print(scores.mean())
+        if mask is not None:
+            scores += mask[None, None, ...]
+        print(scores.mean())
+        scores = scores.float()
+        scores = nn.functional.softmax(scores, dim=-1).type_as(query)
+        output = torch.matmul(scores, value)  # (bs, n_local_heads, slen, head_dim)
+        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
+
+        return self.wo(output)
+
+
+"""
+Reference implementation of RoPE from:
+https://github.com/mistralai/mistral-src/blob/8598cf582091a596671be31990448e0620017851/one_file_ref.py#L47
+
+The original code structures this as stand-alone functions instead of
+a class. Replicating code here to minimize dependencies.
+"""
+
+
+def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:
+    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
+    t = torch.arange(end, device=freqs.device)  # type: ignore
+    freqs = torch.outer(t, freqs).float()  # type: ignore
+    return torch.polar(torch.ones_like(freqs), freqs)  # complex64
+
+
+def _reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
+    """
+    freqs_cis: complex - (seq_len, head_dim / 2)
+    x: complex - (bsz, seq_len, head_dim / 2)
+    """
+    ndim = x.ndim
+    assert 1 < ndim
+    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), (
+        freqs_cis.shape,
+        (x.shape[1], x.shape[-1]),
+    )
+    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]
+    return freqs_cis.view(*shape)
+
+
+def apply_rotary_emb(
+    xq: torch.Tensor,
+    xk: torch.Tensor,
+    freqs_cis: torch.Tensor,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
+    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
+    freqs_cis = _reshape_for_broadcast(freqs_cis, xq_)
+    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
+    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
+    return xq_out.type_as(xq), xk_out.type_as(xk)
+
+
+"""
+Reference impementation of FeedForward from:
+https://github.com/mistralai/mistral-src/blob/8598cf582091a596671be31990448e0620017851/one_file_ref.py#L152
+
+The original code structures this as stand-alone functions in
+`torchtune.models.mistral._component_builders.mistral_mlp` instead of
+a standalone class.
+"""
+
+
+class FeedForward(nn.Module):
+    def __init__(
+        self,
+        dim: int,
+        hidden_dim: int,
+    ):
+        super().__init__()
+
+        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
+        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
+        self.w3 = nn.Linear(dim, hidden_dim, bias=False)
+
+    def forward(self, x) -> torch.Tensor:
+        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))
+
+
+"""
+Reference implementation of TransformerBlock from:
+https://github.com/mistralai/mistral-src/blob/8598cf582091a596671be31990448e0620017851/one_file_ref.py#L190
+"""
+
+
+class RMSNorm(torch.nn.Module):
+    def __init__(self, dim: int, eps: float = 1e-6):
+        super().__init__()
+        self.eps = eps
+        self.weight = nn.Parameter(torch.ones(dim))
+
+    def _norm(self, x):
+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
+
+    def forward(self, x):
+        output = self._norm(x.float()).type_as(x)
+        return output * self.weight
+
+
+class TransformerBlock(nn.Module):
+    def __init__(
+        self,
+        n_heads: int,
+        head_dim: int,
+        dim: int,
+        n_kv_heads: int,
+        hidden_dim: int,
+        norm_eps: float,
+    ):
+        super().__init__()
+        self.n_heads = n_heads
+        self.dim = dim
+        self.attention = Attention(
+            n_heads=n_heads, head_dim=head_dim, dim=dim, n_kv_heads=n_kv_heads
+        )
+        self.feed_forward = FeedForward(dim=dim, hidden_dim=hidden_dim)
+        self.attention_norm = RMSNorm(dim=dim, eps=norm_eps)
+        self.ffn_norm = RMSNorm(dim, eps=norm_eps)
+
+    def forward(
+        self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]
+    ) -> torch.Tensor:
+        r = self.attention.forward(self.attention_norm(x), freqs_cis, mask)
+        h = x + r
+        r = self.feed_forward.forward(self.ffn_norm(h))
+        out = h + r
+        return out
+
+
+"""
+Reference implementation of Transformer from:
+https://github.com/mistralai/mistral-src/blob/8598cf582091a596671be31990448e0620017851/one_file_ref.py#L217
+"""
+
+
+class Transformer(nn.Module):
+    def __init__(
+        self,
+        vocab_size: int,
+        n_layers: int,
+        n_heads: int,
+        head_dim: int,
+        dim: int,
+        n_kv_heads: int,
+        hidden_dim: int,
+        max_seq_len: int,
+        rope_base: int,
+        norm_eps: float,
+    ):
+        super().__init__()
+        self.vocab_size = vocab_size
+        self.n_layers = n_layers
+        assert self.vocab_size > 0
+
+        self.tok_embeddings = nn.Embedding(vocab_size, dim)
+
+        self.layers = torch.nn.ModuleList(
+            [
+                TransformerBlock(
+                    n_heads=n_heads,
+                    head_dim=head_dim,
+                    dim=dim,
+                    n_kv_heads=n_kv_heads,
+                    hidden_dim=hidden_dim,
+                    norm_eps=norm_eps,
+                )
+                for _ in range(n_layers)
+            ]
+        )
+
+        self.norm = RMSNorm(dim, eps=norm_eps)
+
+        self.output = nn.Linear(dim, vocab_size, bias=False)
+
+        # our RoPE implementation is a bit different from the reference:
+        # mistral hardcodes max_seq_len and uses a `positions` argument
+        # in forward to index `freqs_cis` for the current sequence length
+        # before using it in the attention layer.
+
+        self.freqs_cis = precompute_freqs_cis(
+            head_dim, max_seq_len * 2, theta=rope_base
+        )  # removed .to("cuda")
+
+    def forward(self, input_ids: torch.Tensor, positions: torch.Tensor):
+        _, seqlen = input_ids.shape
+        h = self.tok_embeddings(input_ids)
+        freqs_cis = self.freqs_cis[positions]
+        mask: Optional[torch.Tensor] = None
+        if input_ids.shape[1] > 1:
+            seqlen = input_ids.shape[1]
+            tensor = torch.full(
+                (seqlen, seqlen),
+                dtype=h.dtype,
+                fill_value=1,
+                device=h.device,
+            )
+            mask = torch.tril(tensor, diagonal=0).to(h.dtype)
+            # removed mask banding
+            mask = torch.triu(mask, diagonal=-1)  # setting sliding window to 1
+            mask = torch.log(mask)
+        for layer in self.layers:
+            h = layer(h, freqs_cis, mask)
+
+        return self.output(self.norm(h)).float()
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/mistral_test_config.py marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/mistral_test_config.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/mistral_test_config.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/mistral_test_config.py	2025-02-20 17:49:29.934024877 -0500
@@ -0,0 +1,23 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from dataclasses import dataclass
+
+
+@dataclass
+class MistralTestConfig:
+    BSZ = 2
+    SEQ_LEN = 128
+    EMBED_DIM = 64
+    VOCAB_SIZE = 512
+    NUM_LAYERS = 4
+    NUM_HEADS = 4
+    NUM_KV_HEADS = 2
+    INTERMEDIATE_DIM = 512
+    MAX_SEQ_LEN = 256
+    ROPE_BASE = 10000
+    NORM_EPS = 1e-5
+    SEED = 16
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/README.md marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/README.md
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/scripts/README.md	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/scripts/README.md	2025-02-20 17:49:29.910024837 -0500
@@ -0,0 +1,13 @@
+## Verifying correctness
+This directory compares the current implementation of `mistral` to the reference implementation at https://github.com/mistralai/mistral-src/blob/main/one_file_ref.py. Additionally, `torchtune.models.mistral._component_builders.mistral_mlp` is compared in `tests.torchtune.models.mistral.scripts.compare_feed_forward.py`
+
+Since `torchtune.models.mistral` shares nearly all components with `torchtune.models.llama2`, please see `tests.torchtune.models.llama2.scripts` for comparison scripts for individual components.
+
+## Running the scripts
+
+You can run the scripts using the following command as an example.
+Each script should print out the value being used in the associated unit tests.
+
+```
+python3 -m tests.torchtune.models.mistral.scripts.compare_mistral
+```
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_classifier.py marc/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_classifier.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_classifier.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_classifier.py	2025-02-20 17:49:29.938024883 -0500
@@ -0,0 +1,54 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from tests.test_utils import fixed_init_model
+from torchtune.models.mistral import mistral_classifier
+from torchtune.training.seed import set_seed
+
+NUM_LAYERS = 4
+NUM_HEADS = 16
+NUM_KV_HEADS = 8
+VOCAB_SIZE = 32000
+MAX_SEQ_LEN = 2048
+INTERMEDIATE_DIM = 512
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestMistralClassifier:
+    # expected values are calculated using
+    # `tests.torchtune.models.scripts.compare_mistral_classifier`
+    @pytest.mark.parametrize(
+        "bsz, embed_dim, seq_len, n_classes, expected",
+        [
+            (2, 64, 64, 2, 22.6879),
+            (1, 256, 256, 1, 110.2561),
+        ],
+    )
+    def test_forward(
+        self, bsz: int, embed_dim: int, seq_len: int, n_classes: int, expected: float
+    ):
+        inputs = torch.randint(low=0, high=VOCAB_SIZE, size=(bsz, seq_len))
+        model = mistral_classifier(
+            num_classes=n_classes,
+            vocab_size=VOCAB_SIZE,
+            num_layers=n_classes,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=embed_dim,
+            intermediate_dim=INTERMEDIATE_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+        )
+        fixed_init_model(model)
+        actual = model(inputs)
+        expected = torch.tensor(expected)
+        assert actual.shape == (bsz, seq_len, n_classes)
+        torch.testing.assert_close(actual.mean(), expected, atol=1e-4, rtol=1e-4)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_prompt_template.py marc/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_prompt_template.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_prompt_template.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_prompt_template.py	2025-02-20 17:49:29.942024890 -0500
@@ -0,0 +1,41 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from tests.test_utils import assert_dialogue_equal, MESSAGE_SAMPLE
+from torchtune.data import Message
+from torchtune.models.mistral import MistralChatTemplate
+
+
+class TestMistralChatTemplate:
+    expected_dialogue = [
+        Message(
+            role="user",
+            content="[INST] Please briefly summarize this news article:\n\nAOL.com Video - Father Lets 8-Year-Old "
+            "Drive On Icy Road\n\nDescription:Would you let your 8-year-old drive your car? "
+            "How about on an icy road? Well one father in Russia did just that, and recorded "
+            "the entire thing. To her credit, the child seemed to be doing a great job. "
+            "(0:44)\n\nTags: 8-year-old driver , caught on camera , child driver , pix11\n\n"
+            "Summary: [/INST] ",
+        ),
+        Message(
+            role="assistant",
+            content="A father in Russia allowed his 8-year-old child to drive his car on an "
+            "icy road and recorded the event. The child appeared to be handling the situation well, "
+            "showcasing their driving skills despite the challenging conditions.",
+        ),
+    ]
+
+    def test_format(self):
+        no_system_sample = MESSAGE_SAMPLE[1:]
+        actual = MistralChatTemplate()(no_system_sample)
+        assert_dialogue_equal(actual, self.expected_dialogue)
+
+    def test_format_with_system_prompt_raises(self):
+        with pytest.raises(
+            ValueError, match="System prompts are not supported in MistralChatTemplate"
+        ):
+            _ = MistralChatTemplate()(MESSAGE_SAMPLE)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/test_mistral.py marc/third_party/torchtune/tests/torchtune/models/mistral/test_mistral.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/test_mistral.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/test_mistral.py	2025-02-20 17:49:29.934024877 -0500
@@ -0,0 +1,49 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from tests.test_utils import fixed_init_model
+from tests.torchtune.models.mistral.scripts.mistral_test_config import MistralTestConfig
+from torchtune.models.mistral import mistral
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(MistralTestConfig.SEED)
+
+
+class TestMistral:
+    @pytest.fixture
+    def inputs(self):
+        return torch.randint(
+            0,
+            MistralTestConfig.VOCAB_SIZE,
+            (MistralTestConfig.BSZ, MistralTestConfig.SEQ_LEN),
+        )
+
+    def test_forward(self, inputs):
+        model = mistral(
+            vocab_size=MistralTestConfig.VOCAB_SIZE,
+            embed_dim=MistralTestConfig.EMBED_DIM,
+            num_heads=MistralTestConfig.NUM_HEADS,
+            num_layers=MistralTestConfig.NUM_LAYERS,
+            num_kv_heads=MistralTestConfig.NUM_KV_HEADS,
+            max_seq_len=MistralTestConfig.MAX_SEQ_LEN,
+            intermediate_dim=MistralTestConfig.INTERMEDIATE_DIM,
+            norm_eps=MistralTestConfig.NORM_EPS,
+            rope_base=MistralTestConfig.ROPE_BASE,
+        )
+        fixed_init_model(model)
+        actual = model(inputs)
+        expected = torch.tensor(18.2749)
+        assert actual.shape == (
+            MistralTestConfig.BSZ,
+            MistralTestConfig.SEQ_LEN,
+            MistralTestConfig.VOCAB_SIZE,
+        )
+        torch.testing.assert_close(actual.mean(), expected, atol=1e-4, rtol=1e-4)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_tokenizer.py marc/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_tokenizer.py
--- marc_original/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/mistral/test_mistral_tokenizer.py	2025-02-20 17:49:29.946024897 -0500
@@ -0,0 +1,686 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+from tests.common import ASSETS
+from torchtune.data import Message
+from torchtune.models.mistral import mistral_tokenizer
+
+
+class TestMistralTokenizer:
+    def tokenizer(self, template: bool = False):
+        # m.model is a pretrained Sentencepiece model using the following command:
+        # spm.SentencePieceTrainer.train('--input=<TRAIN_FILE> --model_prefix=m --vocab_size=2000')
+        return mistral_tokenizer(
+            str(ASSETS / "m.model"),
+            prompt_template="torchtune.models.mistral.MistralChatTemplate"
+            if template
+            else None,
+        )
+
+    @pytest.fixture
+    def messages(self):
+        return [
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task. Write a response "
+                "that appropriately completes the request.\n\n### Instruction:\nGenerate "
+                "a realistic dating profile bio.\n\n### Response:\n",
+                masked=True,
+            ),
+            Message(
+                role="assistant",
+                content="I'm an outgoing and friendly person who loves spending time with "
+                "friends and family. I'm also a big-time foodie and love trying out new "
+                "restaurants and different cuisines. I'm a big fan of the arts and enjoy "
+                "going to museums and galleries. I'm looking for someone who shares my "
+                "interest in exploring new places, as well as someone who appreciates a "
+                "good conversation over coffee.",
+            ),
+        ]
+
+    def test_tokenize_messages(self, messages):
+        tokenizer = self.tokenizer(template=False)
+        tokens, mask = tokenizer.tokenize_messages(messages)
+        expected_tokens = [
+            1,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+            2,
+        ]
+        expected_mask = [True] * 75 + [False] * 125
+        assert expected_tokens == tokens
+        assert expected_mask == mask
+
+    def test_tokenize_messages_chat_template(self, messages):
+        tokenizer = self.tokenizer(template=True)
+        tokens, mask = tokenizer.tokenize_messages(messages)
+        expected_tokens = [
+            1,
+            351,
+            82,
+            391,
+            221,
+            220,
+            193,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            351,
+            0,
+            82,
+            391,
+            221,
+            220,
+            193,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+            2,
+        ]
+        expected_mask = [True] * 88 + [False] * 125
+        assert expected_tokens == tokens
+        assert expected_mask == mask
+
+    def test_tokenize_message_drop_eos(self, messages):
+        tokenizer = self.tokenizer(template=False)
+        tokens, mask = tokenizer.tokenize_messages(messages, add_eos=False)
+        expected_tokens = [
+            1,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+            2,
+        ]
+        # Drop eos token.
+        expected_tokens = expected_tokens[:-1]
+        # On 1 less then with eos
+        expected_mask = [True] * 75 + [False] * 124
+        assert expected_tokens == tokens
+        assert expected_mask == mask
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/phi3/__init__.py marc/third_party/torchtune/tests/torchtune/models/phi3/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/models/phi3/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/phi3/__init__.py	2025-02-20 17:49:29.950024903 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/phi3/test_lora_phi3.py marc/third_party/torchtune/tests/torchtune/models/phi3/test_lora_phi3.py
--- marc_original/third_party/torchtune/tests/torchtune/models/phi3/test_lora_phi3.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/phi3/test_lora_phi3.py	2025-02-20 17:49:29.954024910 -0500
@@ -0,0 +1,301 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from copy import deepcopy
+
+import pytest
+import torch
+
+from tests.test_utils import assert_expected, fixed_init_model
+from torch import nn
+from torchao.dtypes.nf4tensor import NF4Tensor
+from torchtune import training
+from torchtune.models.phi3 import lora_phi3, phi3
+from torchtune.models.phi3._component_builders import lora_phi3_self_attention
+from torchtune.modules.peft import get_merged_lora_ckpt, LoRALinear
+from torchtune.training.seed import set_seed
+
+RANK = 4
+ALPHA = 1.0
+BSZ = 2
+SEQ_LEN = 32
+EMBED_DIM = 64
+INTER_DIM = 128
+NUM_HEADS = 4
+NUM_KV_HEADS = 2
+MAX_SEQ_LEN = 64
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestLoRAPhi3SelfAttention:
+    @pytest.fixture
+    def inputs(self) -> torch.Tensor:
+        inputs = torch.randn(BSZ, SEQ_LEN, EMBED_DIM)
+        return inputs
+
+    def get_lora_phi_self_attention(self, lora_modules):
+        lora_phi_sa = lora_phi3_self_attention(
+            lora_modules=lora_modules,
+            embed_dim=EMBED_DIM,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            max_seq_len=MAX_SEQ_LEN,
+            lora_rank=RANK,
+            lora_alpha=ALPHA,
+        )
+        fixed_init_model(lora_phi_sa)
+        return lora_phi_sa
+
+    def test_empty_lora_modules(self):
+        with pytest.raises(ValueError, match="Must pass one or more of"):
+            _ = self.get_lora_phi_self_attention([])
+
+    @pytest.mark.parametrize(
+        "lora_modules, expected",
+        [
+            (["q_proj", "v_proj"], torch.tensor(51.88655)),
+            (["q_proj", "k_proj", "v_proj", "output_proj"], torch.tensor(75.80934)),
+            (["k_proj"], torch.tensor(44.00425)),
+        ],
+    )
+    def test_forward(self, inputs, lora_modules, expected):
+        lora_phi_sa = self.get_lora_phi_self_attention(lora_modules)
+        actual = lora_phi_sa(inputs, inputs)
+        assert_expected(actual.shape, (BSZ, SEQ_LEN, EMBED_DIM))
+        assert_expected(actual.mean(), expected, atol=1e-4, rtol=1e-6)
+
+
+class TestLoRAPhi3:
+    @pytest.fixture
+    def vocab_size(self):
+        return 50
+
+    @pytest.fixture
+    def inputs(self, vocab_size):
+        return torch.randint(low=0, high=vocab_size, size=(BSZ, SEQ_LEN))
+
+    def get_lora_phi3(
+        self,
+        lora_modules,
+        apply_lora_to_mlp,
+        apply_lora_to_output,
+        vocab_size,
+        reset_norm=True,
+        quantize_base=False,
+        embed_dim=EMBED_DIM,
+        dtype=None,
+    ):
+        num_layers = 3
+        model = lora_phi3(
+            lora_attn_modules=lora_modules,
+            apply_lora_to_mlp=apply_lora_to_mlp,
+            apply_lora_to_output=apply_lora_to_output,
+            vocab_size=vocab_size,
+            num_layers=num_layers,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=embed_dim,
+            intermediate_dim=INTER_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+            lora_rank=RANK,
+            lora_alpha=ALPHA,
+            quantize_base=quantize_base,
+        )
+        # To make final outputs less trivial
+        if reset_norm:
+            model.norm = nn.Identity()
+
+        # dtype=None means to just read dtype from parameters
+        # in the model. This dtype is set explicitly to bf16 currently
+        # when initializing QLoRA models, as ops such as `arange` aren't
+        # yet supported with the actual nf4 tensor dtype yet.
+        fixed_init_model(model, dtype=dtype)
+
+        return model
+
+    def get_ref_phi3(self, vocab_size, embed_dim=EMBED_DIM):
+        num_layers = 3
+        model = phi3(
+            vocab_size=vocab_size,
+            num_layers=num_layers,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=embed_dim,
+            intermediate_dim=INTER_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+        )
+        return model
+
+    @pytest.mark.parametrize(
+        "lora_modules, apply_lora_to_mlp, apply_lora_to_output, expected",
+        [
+            (["q_proj", "v_proj"], False, False, torch.tensor(2869687.75)),
+            (
+                ["q_proj", "k_proj", "v_proj", "output_proj"],
+                True,
+                False,
+                torch.tensor(10674772.0),
+            ),
+            (["k_proj"], True, True, torch.tensor(16285834.0)),
+        ],
+    )
+    def test_forward(
+        self,
+        vocab_size,
+        inputs,
+        lora_modules,
+        apply_lora_to_mlp,
+        apply_lora_to_output,
+        expected,
+    ):
+        model = self.get_lora_phi3(
+            lora_modules, apply_lora_to_mlp, apply_lora_to_output, vocab_size
+        )
+        actual = model(inputs)
+        assert_expected(actual.shape, (BSZ, SEQ_LEN, vocab_size))
+        assert_expected(actual.mean(), expected, atol=1e-4, rtol=1e-6)
+
+    @pytest.mark.parametrize(
+        "lora_modules, apply_lora_to_mlp, apply_lora_to_output",
+        [
+            (["q_proj", "v_proj"], True, False),
+            (["q_proj", "k_proj", "v_proj", "output_proj"], False, False),
+            (["k_proj"], True, True),
+        ],
+    )
+    def test_lora_phi3_state_dict_parity(
+        self, lora_modules, apply_lora_to_mlp, apply_lora_to_output, vocab_size
+    ):
+        lora_phi = self.get_lora_phi3(
+            lora_modules,
+            apply_lora_to_mlp,
+            apply_lora_to_output,
+            vocab_size,
+            reset_norm=False,
+        )
+        ref_phi = self.get_ref_phi3(vocab_size)
+        # Ensure ref_phi state_dict can be loaded into lora_phi with only "lora"
+        # keys missing.
+        ref_phi_state_dict = ref_phi.state_dict()
+        missing, unexpected = lora_phi.load_state_dict(ref_phi_state_dict, strict=False)
+        assert not unexpected
+        assert all(["lora" in key for key in missing])
+
+    def test_lora_linear_quantize_base(self):
+        model = self.get_lora_phi3(
+            lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+            apply_lora_to_mlp=True,
+            # quantize_base
+            apply_lora_to_output=False,
+            vocab_size=50,
+            quantize_base=True,
+            embed_dim=512,
+            dtype=torch.bfloat16,
+        )
+        for module in model.modules():
+            if isinstance(module, LoRALinear):
+                assert module._quantize_base
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_qlora_phi3_parity(self, dtype, inputs):
+        with training.set_default_dtype(dtype):
+            model_ref = self.get_lora_phi3(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=False,
+                embed_dim=512,
+                dtype=dtype,
+            )
+            qlora = self.get_lora_phi3(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=True,
+                embed_dim=512,
+                dtype=dtype,
+            )
+        qlora_sd = qlora.state_dict()
+        model_ref.load_state_dict(qlora_sd)
+        # Forward pass of model_ref and qlora should be the same, as QLoRA linear layers should use
+        # a special linear operator that runs the compute in bf16, but only saves the 4 bit tensors
+        # for backward.
+        ref_output = model_ref(inputs)
+        output = qlora(inputs)
+        torch.testing.assert_close(ref_output, output)
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_qlora_phi3_state_dict(self, dtype):
+        with training.set_default_dtype(dtype):
+            model_ref = self.get_lora_phi3(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=False,
+                embed_dim=512,
+                dtype=dtype,
+            )
+            high_prec_sd = model_ref.state_dict()
+            for v in high_prec_sd.values():
+                assert v.dtype == dtype
+
+            # ensure quantized LoRA can load a bf16 state_dict
+            qlora = self.get_lora_phi3(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=True,
+                embed_dim=512,
+                dtype=dtype,
+            )
+            qlora.load_state_dict(high_prec_sd)
+            # LoRALinear base weights should be nf4 still
+            for module in qlora.modules():
+                if isinstance(module, LoRALinear):
+                    assert isinstance(module.weight, NF4Tensor)
+            # saved state_dict should have bf16 weights.
+            qlora_sd = qlora.state_dict()
+            for v in qlora_sd.values():
+                assert v.dtype == dtype
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_qlora_phi3_merged_state_dict(self, dtype):
+        with training.set_default_dtype(dtype):
+            qlora = self.get_lora_phi3(
+                lora_modules=["q_proj", "v_proj", "k_proj", "output_proj"],
+                apply_lora_to_mlp=True,
+                apply_lora_to_output=False,
+                vocab_size=50,
+                quantize_base=True,
+                embed_dim=512,
+                dtype=dtype,
+                reset_norm=False,  # to ensure norm.scale key exists
+            )
+
+        qlora_sd = qlora.state_dict()
+        # Ensure checkpoint merging produces bf16 tensors
+        merged_ckpt = get_merged_lora_ckpt(deepcopy(qlora_sd), rank=RANK, alpha=ALPHA)
+        for k, v in merged_ckpt.items():
+            # paranoid check for both, as NF4Tensor had issue where NF4Tensor.dtype would return bf16
+            assert not isinstance(v, NF4Tensor)
+            if k == "":
+                assert v.dtype == torch.float32
+            else:
+                assert v.dtype == dtype
+
+        # Ensure checkpoint can be loaded into non-LoRA model
+        with training.set_default_dtype(dtype):
+            phi3 = self.get_ref_phi3(vocab_size=50, embed_dim=512)
+
+        phi3.load_state_dict(merged_ckpt)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/phi3/test_phi3.py marc/third_party/torchtune/tests/torchtune/models/phi3/test_phi3.py
--- marc_original/third_party/torchtune/tests/torchtune/models/phi3/test_phi3.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/phi3/test_phi3.py	2025-02-20 17:49:29.958024916 -0500
@@ -0,0 +1,48 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from tests.test_utils import fixed_init_model
+from torchtune.models.phi3 import phi3
+from torchtune.training.seed import set_seed
+
+EMBED_DIM = 128
+INTER_DIM = 256
+NUM_LAYERS = 4
+NUM_HEADS = 16
+NUM_KV_HEADS = 8
+VOCAB_SIZE = 32000
+MAX_SEQ_LEN = 2048
+BSZ = 2
+SEQ_LEN = 100
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestPhi3:
+    @pytest.fixture
+    def inputs(self):
+        return torch.randint(0, VOCAB_SIZE, (BSZ, SEQ_LEN))
+
+    def test_forward(self, inputs):
+        model = phi3(
+            vocab_size=VOCAB_SIZE,
+            num_layers=NUM_LAYERS,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=EMBED_DIM,
+            intermediate_dim=INTER_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+        )
+        fixed_init_model(model, min_val=-0.25, max_val=0.5)
+        actual = model(inputs)
+        expected = torch.tensor(3.9763)
+        assert actual.shape == (BSZ, SEQ_LEN, VOCAB_SIZE)
+        torch.testing.assert_close(actual.mean(), expected, atol=1e-4, rtol=1e-4)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/phi3/test_phi3_tokenizer.py marc/third_party/torchtune/tests/torchtune/models/phi3/test_phi3_tokenizer.py
--- marc_original/third_party/torchtune/tests/torchtune/models/phi3/test_phi3_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/phi3/test_phi3_tokenizer.py	2025-02-20 17:49:29.962024923 -0500
@@ -0,0 +1,734 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+from tests.common import ASSETS
+from torchtune.data import Message
+from torchtune.models.phi3 import phi3_mini_tokenizer
+
+
+class TestPhi3MiniTokenizer:
+    @pytest.fixture
+    def tokenizer(self):
+        # m.model is a pretrained Sentencepiece model using the following command:
+        # spm.SentencePieceTrainer.train('--input=<TRAIN_FILE> --model_prefix=m --vocab_size=2000')
+        return phi3_mini_tokenizer(
+            path=str(ASSETS / "m.model"),
+        )
+
+    def test_tokenize_messages(self, tokenizer):
+        messages = [
+            Message(role="system", content="You are a helpful assistant", masked=True),
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task. Write a response "
+                "that appropriately completes the request.\n\n### Instruction:\nGenerate "
+                "a realistic dating profile bio.\n\n### Response:\n",
+                masked=True,
+            ),
+            Message(
+                role="assistant",
+                content="I'm an outgoing and friendly person who loves spending time with "
+                "friends and family. I'm also a big-time foodie and love trying out new "
+                "restaurants and different cuisines. I'm a big fan of the arts and enjoy "
+                "going to museums and galleries. I'm looking for someone who shares my "
+                "interest in exploring new places, as well as someone who appreciates a "
+                "good conversation over coffee.",
+            ),
+        ]
+        tokens, mask = tokenizer.tokenize_messages(messages)
+        expected_tokens = [
+            1,
+            32006,
+            272,
+            84,
+            9,
+            615,
+            454,
+            1974,
+            19,
+            32007,
+            32010,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            32007,
+            32001,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+            32007,
+        ]
+
+        expected_mask = [True] * 86 + [False] * 126
+        assert expected_tokens == tokens
+        assert expected_mask == mask
+
+    def test_tokenize_messages_no_system_prompt(self, tokenizer):
+        messages = [
+            Message(role="system", content="You are a helpful assistant", masked=True),
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task. Write a response "
+                "that appropriately completes the request.\n\n### Instruction:\nGenerate "
+                "a realistic dating profile bio.\n\n### Response:\n",
+                masked=True,
+            ),
+            Message(
+                role="assistant",
+                content="I'm an outgoing and friendly person who loves spending time with "
+                "friends and family. I'm also a big-time foodie and love trying out new "
+                "restaurants and different cuisines. I'm a big fan of the arts and enjoy "
+                "going to museums and galleries. I'm looking for someone who shares my "
+                "interest in exploring new places, as well as someone who appreciates a "
+                "good conversation over coffee.",
+            ),
+        ]
+        tokens, mask = tokenizer.tokenize_messages(messages, ignore_system_prompt=True)
+        expected_tokens = [
+            1,
+            32010,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            32007,
+            32001,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+            32007,
+        ]
+        expected_mask = [True] * 77 + [False] * 126
+        assert expected_tokens == tokens
+        assert expected_mask == mask
+
+    def test_tokenize_messages_drop_eos(self, tokenizer):
+        messages = [
+            Message(role="system", content="You are a helpful assistant", masked=True),
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task. Write a response "
+                "that appropriately completes the request.\n\n### Instruction:\nGenerate "
+                "a realistic dating profile bio.\n\n### Response:\n",
+                masked=True,
+            ),
+            Message(
+                role="assistant",
+                content="I'm an outgoing and friendly person who loves spending time with "
+                "friends and family. I'm also a big-time foodie and love trying out new "
+                "restaurants and different cuisines. I'm a big fan of the arts and enjoy "
+                "going to museums and galleries. I'm looking for someone who shares my "
+                "interest in exploring new places, as well as someone who appreciates a "
+                "good conversation over coffee.",
+            ),
+        ]
+        tokens, mask = tokenizer.tokenize_messages(messages, add_eos=False)
+        expected_tokens = [
+            1,
+            32006,
+            272,
+            84,
+            9,
+            615,
+            454,
+            1974,
+            19,
+            32007,
+            32010,
+            323,
+            418,
+            202,
+            31,
+            128,
+            15,
+            120,
+            47,
+            88,
+            584,
+            23,
+            1665,
+            182,
+            9,
+            434,
+            295,
+            85,
+            4,
+            780,
+            47,
+            636,
+            9,
+            1094,
+            213,
+            23,
+            9,
+            69,
+            69,
+            164,
+            1153,
+            299,
+            35,
+            961,
+            132,
+            237,
+            7,
+            5,
+            761,
+            4,
+            12,
+            0,
+            313,
+            120,
+            47,
+            88,
+            584,
+            166,
+            493,
+            171,
+            54,
+            299,
+            9,
+            906,
+            244,
+            19,
+            186,
+            767,
+            303,
+            671,
+            92,
+            209,
+            24,
+            190,
+            52,
+            38,
+            4,
+            12,
+            0,
+            1243,
+            7,
+            69,
+            135,
+            213,
+            166,
+            32007,
+            32001,
+            6,
+            21,
+            45,
+            128,
+            71,
+            58,
+            38,
+            14,
+            10,
+            652,
+            35,
+            462,
+            101,
+            1306,
+            7,
+            341,
+            171,
+            20,
+            14,
+            127,
+            26,
+            652,
+            7,
+            10,
+            1268,
+            4,
+            6,
+            21,
+            45,
+            591,
+            9,
+            566,
+            22,
+            994,
+            913,
+            38,
+            20,
+            52,
+            24,
+            10,
+            1306,
+            734,
+            14,
+            71,
+            365,
+            1382,
+            7,
+            10,
+            801,
+            105,
+            88,
+            244,
+            985,
+            7,
+            4,
+            6,
+            21,
+            45,
+            9,
+            566,
+            126,
+            180,
+            11,
+            5,
+            1137,
+            7,
+            10,
+            1089,
+            151,
+            8,
+            1156,
+            213,
+            342,
+            7,
+            10,
+            384,
+            104,
+            54,
+            470,
+            4,
+            6,
+            21,
+            45,
+            287,
+            14,
+            33,
+            125,
+            135,
+            24,
+            101,
+            512,
+            66,
+            7,
+            28,
+            822,
+            15,
+            542,
+            69,
+            59,
+            110,
+            14,
+            365,
+            229,
+            7,
+            3,
+            36,
+            267,
+            36,
+            125,
+            135,
+            24,
+            101,
+            1503,
+            182,
+            9,
+            222,
+            1661,
+            191,
+            332,
+            92,
+            92,
+            24,
+            24,
+            4,
+            32007,
+        ]
+
+        # Drop eos token
+        expected_tokens = expected_tokens[:]
+        # On 1 less then with eos
+        expected_mask = [True] * 86 + [False] * 126
+        assert expected_tokens == tokens
+        assert expected_mask == mask
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/qwen2/__init__.py marc/third_party/torchtune/tests/torchtune/models/qwen2/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/models/qwen2/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/qwen2/__init__.py	2025-02-20 17:49:29.966024929 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/qwen2/test_lora_qwen2.py marc/third_party/torchtune/tests/torchtune/models/qwen2/test_lora_qwen2.py
--- marc_original/third_party/torchtune/tests/torchtune/models/qwen2/test_lora_qwen2.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/qwen2/test_lora_qwen2.py	2025-02-20 17:49:29.970024936 -0500
@@ -0,0 +1,186 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+
+from tests.test_utils import assert_expected, fixed_init_model
+from torch import nn
+from torchtune.models.qwen2 import lora_qwen2, qwen2
+from torchtune.models.qwen2._component_builders import lora_qwen2_self_attention
+from torchtune.training.seed import set_seed
+
+RANK = 4
+ALPHA = 1.0
+BSZ = 2
+SEQ_LEN = 32
+EMBED_DIM = 64
+INTERMEDIATE_DIM = 168
+NUM_HEADS = 4
+NUM_KV_HEADS = 2
+MAX_SEQ_LEN = 64
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestLoRAQwen2SelfAttention:
+    @pytest.fixture
+    def inputs(self) -> torch.Tensor:
+        inputs = torch.randn(BSZ, SEQ_LEN, EMBED_DIM)
+        return inputs
+
+    def get_lora_qwen2_self_attention(self, lora_modules):
+        lora_qwen2 = lora_qwen2_self_attention(
+            lora_modules=lora_modules,
+            embed_dim=EMBED_DIM,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            max_seq_len=MAX_SEQ_LEN,
+            lora_rank=RANK,
+            lora_alpha=ALPHA,
+        )
+        fixed_init_model(lora_qwen2)
+        return lora_qwen2
+
+    def test_empty_lora_modules(self):
+        with pytest.raises(ValueError, match="Must pass one or more of"):
+            _ = self.get_lora_qwen2_self_attention([])
+
+    @pytest.mark.parametrize(
+        "lora_modules, expected",
+        [
+            (["q_proj", "v_proj"], torch.tensor(83.6596)),
+            (["q_proj", "k_proj", "v_proj", "output_proj"], torch.tensor(129.4454)),
+            (["k_proj"], torch.tensor(69.3473)),
+        ],
+    )
+    def test_forward(self, inputs, lora_modules, expected):
+        lora_qwen2_sa = self.get_lora_qwen2_self_attention(lora_modules)
+        actual = lora_qwen2_sa(inputs, inputs)
+        assert_expected(actual.shape, (BSZ, SEQ_LEN, EMBED_DIM))
+        assert_expected(actual.mean(), expected, atol=1e-4, rtol=1e-6)
+
+
+class TestLoRAQwen2:
+    @pytest.fixture
+    def vocab_size(self):
+        return 50
+
+    @pytest.fixture
+    def inputs(self, vocab_size):
+        return torch.randint(low=0, high=vocab_size, size=(BSZ, SEQ_LEN))
+
+    def get_lora_qwen2(
+        self,
+        lora_modules,
+        apply_lora_to_mlp,
+        apply_lora_to_output,
+        vocab_size,
+        reset_norm=True,
+        quantize_base=False,
+        embed_dim=EMBED_DIM,
+        dtype=None,
+    ):
+        num_layers = 3
+        model = lora_qwen2(
+            lora_attn_modules=lora_modules,
+            apply_lora_to_mlp=apply_lora_to_mlp,
+            apply_lora_to_output=apply_lora_to_output,
+            vocab_size=vocab_size,
+            num_layers=num_layers,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=embed_dim,
+            intermediate_dim=INTERMEDIATE_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+            lora_rank=RANK,
+            lora_alpha=ALPHA,
+            quantize_base=quantize_base,
+        )
+        # To make final outputs less trivial
+        if reset_norm:
+            model.norm = nn.Identity()
+
+        # dtype=None means to just read dtype from parameters
+        # in the model. This dtype is set explicitly to bf16 currently
+        # when initializing QLoRA models, as ops such as `arange` aren't
+        # yet supported with the actual nf4 tensor dtype yet.
+        fixed_init_model(model, dtype=dtype)
+
+        return model
+
+    def get_ref_qwen2(self, vocab_size, embed_dim=EMBED_DIM):
+        num_layers = 3
+        model = qwen2(
+            vocab_size=vocab_size,
+            num_layers=num_layers,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=embed_dim,
+            intermediate_dim=INTERMEDIATE_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+        )
+        return model
+
+    @pytest.mark.parametrize(
+        "lora_modules, apply_lora_to_mlp, apply_lora_to_output, expected",
+        [
+            (["q_proj", "v_proj"], False, False, torch.tensor(3736558.0)),
+            (
+                ["q_proj", "k_proj", "v_proj", "output_proj"],
+                True,
+                False,
+                torch.tensor(13962364.0),
+            ),
+            (["k_proj"], True, True, torch.tensor(21335964.0)),
+        ],
+    )
+    def test_forward(
+        self,
+        vocab_size,
+        inputs,
+        lora_modules,
+        apply_lora_to_mlp,
+        apply_lora_to_output,
+        expected,
+    ):
+        model = self.get_lora_qwen2(
+            lora_modules, apply_lora_to_mlp, apply_lora_to_output, vocab_size
+        )
+        actual = model(inputs)
+        assert_expected(actual.shape, (BSZ, SEQ_LEN, vocab_size))
+        assert_expected(actual.mean(), expected, atol=1e-4, rtol=1e-6)
+
+    @pytest.mark.parametrize(
+        "lora_modules, apply_lora_to_mlp, apply_lora_to_output",
+        [
+            (["q_proj", "v_proj"], True, False),
+            (["q_proj", "k_proj", "v_proj", "output_proj"], False, False),
+            (["k_proj"], True, True),
+        ],
+    )
+    def test_lora_qwen2_state_dict_parity(
+        self, lora_modules, apply_lora_to_mlp, apply_lora_to_output, vocab_size
+    ):
+        lora_qwen2 = self.get_lora_qwen2(
+            lora_modules,
+            apply_lora_to_mlp,
+            apply_lora_to_output,
+            vocab_size,
+            reset_norm=False,
+        )
+        ref_qwen2 = self.get_ref_qwen2(vocab_size)
+        # Ensure ref_qwen2 state_dict can be loaded into lora_qwen2 with only "lora"
+        # keys missing.
+        ref_qwen2_state_dict = ref_qwen2.state_dict()
+        missing, unexpected = lora_qwen2.load_state_dict(
+            ref_qwen2_state_dict, strict=False
+        )
+        assert not unexpected
+        assert all(["lora" in key for key in missing])
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/qwen2/test_qwen2.py marc/third_party/torchtune/tests/torchtune/models/qwen2/test_qwen2.py
--- marc_original/third_party/torchtune/tests/torchtune/models/qwen2/test_qwen2.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/qwen2/test_qwen2.py	2025-02-20 17:49:29.974024943 -0500
@@ -0,0 +1,48 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from tests.test_utils import fixed_init_model
+from torchtune.models.qwen2 import qwen2
+from torchtune.training.seed import set_seed
+
+EMBED_DIM = 128
+INTER_DIM = 256
+NUM_LAYERS = 4
+NUM_HEADS = 16
+NUM_KV_HEADS = 8
+VOCAB_SIZE = 32000
+MAX_SEQ_LEN = 2048
+BSZ = 2
+SEQ_LEN = 100
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestQwen2:
+    @pytest.fixture
+    def inputs(self):
+        return torch.randint(0, VOCAB_SIZE, (BSZ, SEQ_LEN))
+
+    def test_forward(self, inputs):
+        model = qwen2(
+            vocab_size=VOCAB_SIZE,
+            num_layers=NUM_LAYERS,
+            num_heads=NUM_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=EMBED_DIM,
+            intermediate_dim=INTER_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+        )
+        fixed_init_model(model, min_val=-0.25, max_val=0.5)
+        actual = model(inputs)
+        expected = torch.tensor(3.9763)
+        assert actual.shape == (BSZ, SEQ_LEN, VOCAB_SIZE)
+        torch.testing.assert_close(actual.mean(), expected, atol=1e-4, rtol=1e-4)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/models/qwen2/test_qwen2_tokenizer.py marc/third_party/torchtune/tests/torchtune/models/qwen2/test_qwen2_tokenizer.py
--- marc_original/third_party/torchtune/tests/torchtune/models/qwen2/test_qwen2_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/models/qwen2/test_qwen2_tokenizer.py	2025-02-20 17:49:29.978024949 -0500
@@ -0,0 +1,637 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional
+
+import pytest
+
+from tests.common import ASSETS
+
+from torchtune.data import Message
+from torchtune.models.qwen2 import qwen2_tokenizer
+
+
+class TestQwen2Tokenizer:
+    def tokenizer(self, template: bool = False, max_seq_len: Optional[int] = None):
+        return qwen2_tokenizer(
+            path=str(ASSETS / "tiny_bpe_vocab.json"),
+            merges_file=str(ASSETS / "tiny_bpe_merges.txt"),
+            special_tokens_path=str(ASSETS / "tiny_bpe_tokenizer.json"),
+            prompt_template="torchtune.data.ChatMLTemplate" if template else None,
+            max_seq_len=max_seq_len,
+        )
+
+    @pytest.fixture
+    def messages(self):
+        return [
+            Message(
+                role="user",
+                content="Below is an instruction that describes a task. Write a response "
+                "that appropriately completes the request.\n\n### Instruction:\nGenerate "
+                "a realistic dating profile bio.\n\n### Response:\n",
+                masked=True,
+            ),
+            Message(
+                role="assistant",
+                content="I'm an outgoing and friendly person who loves spending time with "
+                "friends and family. I'm also a big-time foodie and love trying out new "
+                "restaurants and different cuisines. I'm a big fan of the arts and enjoy "
+                "going to museums and galleries. I'm looking for someone who shares my "
+                "interest in exploring new places, as well as someone who appreciates a "
+                "good conversation over coffee.",
+            ),
+        ]
+
+    def test_tokenize_messages_chat_template(self, messages):
+        tokenizer = self.tokenizer(template=True)
+        tokens, mask = tokenizer.tokenize_messages(messages)
+        expected_tokens = [
+            2001,
+            273,
+            105,
+            94,
+            33,
+            214,
+            174,
+            156,
+            194,
+            130,
+            197,
+            184,
+            446,
+            789,
+            113,
+            98,
+            1914,
+            13,
+            346,
+            788,
+            98,
+            706,
+            102,
+            182,
+            184,
+            1916,
+            176,
+            762,
+            83,
+            113,
+            103,
+            874,
+            269,
+            13,
+            94,
+            94,
+            2,
+            2,
+            2,
+            483,
+            197,
+            25,
+            94,
+            885,
+            98,
+            1226,
+            1960,
+            348,
+            114,
+            1123,
+            399,
+            1583,
+            78,
+            13,
+            94,
+            94,
+            2,
+            2,
+            2,
+            360,
+            1733,
+            102,
+            182,
+            25,
+            94,
+            2002,
+            94,
+            2001,
+            397,
+            251,
+            249,
+            94,
+            40,
+            1791,
+            194,
+            453,
+            70,
+            78,
+            114,
+            120,
+            967,
+            176,
+            618,
+            628,
+            1275,
+            794,
+            294,
+            1095,
+            445,
+            212,
+            1356,
+            120,
+            1299,
+            13,
+            223,
+            1791,
+            451,
+            98,
+            127,
+            181,
+            1047,
+            375,
+            915,
+            380,
+            120,
+            1448,
+            1732,
+            114,
+            453,
+            447,
+            1219,
+            64,
+            187,
+            921,
+            120,
+            742,
+            107,
+            84,
+            122,
+            893,
+            13,
+            223,
+            1791,
+            98,
+            127,
+            181,
+            123,
+            124,
+            131,
+            103,
+            744,
+            82,
+            120,
+            1506,
+            416,
+            114,
+            128,
+            1429,
+            182,
+            253,
+            82,
+            120,
+            163,
+            330,
+            105,
+            262,
+            13,
+            223,
+            1791,
+            155,
+            1551,
+            171,
+            1951,
+            628,
+            296,
+            64,
+            237,
+            886,
+            1390,
+            130,
+            883,
+            1678,
+            447,
+            306,
+            279,
+            113,
+            11,
+            215,
+            785,
+            215,
+            1951,
+            628,
+            378,
+            101,
+            66,
+            72,
+            593,
+            98,
+            984,
+            208,
+            1580,
+            167,
+            510,
+            737,
+            318,
+            1278,
+            13,
+            2002,
+            94,
+            2000,
+        ]
+        expected_mask = [True] * 67 + [False] * 123
+        assert expected_tokens == tokens
+        assert expected_mask == mask
+
+        formatted_messages = tokenizer.decode(tokens)
+        expected_formatted_messages = (
+            f"<|im_start|>user\n{messages[0].text_content}<|im_end|>\n"
+            f"<|im_start|>assistant\n{messages[1].text_content}<|im_end|>\n"
+            "<|endoftext|>"
+        )
+        assert expected_formatted_messages == formatted_messages
+
+    def test_tokenize_messages(self, messages):
+        tokenizer = self.tokenizer(template=False)
+        tokens, mask = tokenizer.tokenize_messages(messages)
+        expected_tokens = [
+            33,
+            214,
+            174,
+            156,
+            194,
+            130,
+            197,
+            184,
+            446,
+            789,
+            113,
+            98,
+            1914,
+            13,
+            346,
+            788,
+            98,
+            706,
+            102,
+            182,
+            184,
+            1916,
+            176,
+            762,
+            83,
+            113,
+            103,
+            874,
+            269,
+            13,
+            94,
+            94,
+            2,
+            2,
+            2,
+            483,
+            197,
+            25,
+            94,
+            885,
+            98,
+            1226,
+            1960,
+            348,
+            114,
+            1123,
+            399,
+            1583,
+            78,
+            13,
+            94,
+            94,
+            2,
+            2,
+            2,
+            360,
+            1733,
+            102,
+            182,
+            25,
+            94,
+            40,
+            1791,
+            194,
+            453,
+            70,
+            78,
+            114,
+            120,
+            967,
+            176,
+            618,
+            628,
+            1275,
+            794,
+            294,
+            1095,
+            445,
+            212,
+            1356,
+            120,
+            1299,
+            13,
+            223,
+            1791,
+            451,
+            98,
+            127,
+            181,
+            1047,
+            375,
+            915,
+            380,
+            120,
+            1448,
+            1732,
+            114,
+            453,
+            447,
+            1219,
+            64,
+            187,
+            921,
+            120,
+            742,
+            107,
+            84,
+            122,
+            893,
+            13,
+            223,
+            1791,
+            98,
+            127,
+            181,
+            123,
+            124,
+            131,
+            103,
+            744,
+            82,
+            120,
+            1506,
+            416,
+            114,
+            128,
+            1429,
+            182,
+            253,
+            82,
+            120,
+            163,
+            330,
+            105,
+            262,
+            13,
+            223,
+            1791,
+            155,
+            1551,
+            171,
+            1951,
+            628,
+            296,
+            64,
+            237,
+            886,
+            1390,
+            130,
+            883,
+            1678,
+            447,
+            306,
+            279,
+            113,
+            11,
+            215,
+            785,
+            215,
+            1951,
+            628,
+            378,
+            101,
+            66,
+            72,
+            593,
+            98,
+            984,
+            208,
+            1580,
+            167,
+            510,
+            737,
+            318,
+            1278,
+            13,
+            2000,
+        ]
+        expected_mask = [True] * 61 + [False] * 116
+        assert expected_tokens == tokens
+        assert expected_mask == mask
+
+    def test_tokenize_messages_gt_max_seq_len(self, messages):
+        # Super basic test to make sure max_seq_len is working properly
+        tokenizer = self.tokenizer(template=False, max_seq_len=10)
+        tokens, mask = tokenizer.tokenize_messages(messages)
+        assert len(tokens) == 10
+        assert len(mask) == 10
+
+    def test_tokenize_message_drop_eos(self, messages):
+        tokenizer = self.tokenizer(template=False)
+        expected_tokens = [
+            33,
+            214,
+            174,
+            156,
+            194,
+            130,
+            197,
+            184,
+            446,
+            789,
+            113,
+            98,
+            1914,
+            13,
+            346,
+            788,
+            98,
+            706,
+            102,
+            182,
+            184,
+            1916,
+            176,
+            762,
+            83,
+            113,
+            103,
+            874,
+            269,
+            13,
+            94,
+            94,
+            2,
+            2,
+            2,
+            483,
+            197,
+            25,
+            94,
+            885,
+            98,
+            1226,
+            1960,
+            348,
+            114,
+            1123,
+            399,
+            1583,
+            78,
+            13,
+            94,
+            94,
+            2,
+            2,
+            2,
+            360,
+            1733,
+            102,
+            182,
+            25,
+            94,
+            40,
+            1791,
+            194,
+            453,
+            70,
+            78,
+            114,
+            120,
+            967,
+            176,
+            618,
+            628,
+            1275,
+            794,
+            294,
+            1095,
+            445,
+            212,
+            1356,
+            120,
+            1299,
+            13,
+            223,
+            1791,
+            451,
+            98,
+            127,
+            181,
+            1047,
+            375,
+            915,
+            380,
+            120,
+            1448,
+            1732,
+            114,
+            453,
+            447,
+            1219,
+            64,
+            187,
+            921,
+            120,
+            742,
+            107,
+            84,
+            122,
+            893,
+            13,
+            223,
+            1791,
+            98,
+            127,
+            181,
+            123,
+            124,
+            131,
+            103,
+            744,
+            82,
+            120,
+            1506,
+            416,
+            114,
+            128,
+            1429,
+            182,
+            253,
+            82,
+            120,
+            163,
+            330,
+            105,
+            262,
+            13,
+            223,
+            1791,
+            155,
+            1551,
+            171,
+            1951,
+            628,
+            296,
+            64,
+            237,
+            886,
+            1390,
+            130,
+            883,
+            1678,
+            447,
+            306,
+            279,
+            113,
+            11,
+            215,
+            785,
+            215,
+            1951,
+            628,
+            378,
+            101,
+            66,
+            72,
+            593,
+            98,
+            984,
+            208,
+            1580,
+            167,
+            510,
+            737,
+            318,
+            1278,
+            13,
+            2000,
+        ]
+
+        # Remove the EOS token
+        expected_tokens = expected_tokens[:-1]
+        # On 1 less then with eos
+        expected_mask = [True] * 61 + [False] * 115
+
+        tokens, mask = tokenizer.tokenize_messages(messages, add_eos=False)
+        assert tokens == expected_tokens
+        assert mask == expected_mask
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/__init__.py marc/third_party/torchtune/tests/torchtune/modules/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/__init__.py	2025-02-20 17:49:29.982024956 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/loss/test_ce_chunked_output_loss.py marc/third_party/torchtune/tests/torchtune/modules/loss/test_ce_chunked_output_loss.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/loss/test_ce_chunked_output_loss.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/loss/test_ce_chunked_output_loss.py	2025-02-20 17:49:29.986024962 -0500
@@ -0,0 +1,50 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from tests.test_utils import assert_expected
+from torchtune.modules.loss import CEWithChunkedOutputLoss
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(42)
+
+
+class TestCEWithChunkedOutputLoss:
+    def test_chunked_cross_entropy_loss(self):
+        # Create a sample input and label
+        ignore_index = -100
+        batch_size = 3
+        num_tokens = 50
+        vocab_size = 50
+        logits = torch.randn(batch_size, num_tokens, vocab_size, dtype=torch.bfloat16)
+        labels = torch.randint(
+            0, vocab_size, (batch_size, num_tokens), dtype=torch.long
+        )
+
+        # add random ignore index to random tokens in the label
+        random_indices = torch.randint(0, num_tokens, (batch_size, num_tokens))
+        labels[random_indices < num_tokens // 5] = ignore_index
+
+        # chunked CE
+        ce_loss = CEWithChunkedOutputLoss(
+            num_output_chunks=8, ignore_index=ignore_index
+        )
+        logits_chunks = logits.chunk(ce_loss.num_output_chunks, dim=1)
+        chunked_loss = ce_loss(logits_chunks, labels)
+
+        # vanilla CE
+        logits = logits.reshape(-1, logits.size(-1))
+        labels = labels.reshape(-1)
+        standard_loss = torch.nn.functional.cross_entropy(
+            logits.float(), labels, reduction="mean", ignore_index=ignore_index
+        )
+
+        # Assert
+        assert_expected(chunked_loss, standard_loss, rtol=1e-2, atol=1e-2)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/loss/test_kd_losses.py marc/third_party/torchtune/tests/torchtune/modules/loss/test_kd_losses.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/loss/test_kd_losses.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/loss/test_kd_losses.py	2025-02-20 17:49:29.990024969 -0500
@@ -0,0 +1,116 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from tests.test_utils import assert_expected
+from torchtune.modules.loss import ForwardKLLoss, ForwardKLWithChunkedOutputLoss
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(42)
+
+
+class TestForwardKLWithChunkedOutputLoss:
+    def test_forward_kl_loss(self):
+        # Create a sample input and label
+        ignore_index = -100
+        batch_size = 3
+        num_tokens = 50
+        vocab_size = 50
+        logits = torch.randn(batch_size, num_tokens, vocab_size, dtype=torch.bfloat16)
+        teacher_logits = torch.randn(
+            batch_size, num_tokens, vocab_size, dtype=torch.bfloat16
+        )
+        labels = torch.randint(
+            0, vocab_size, (batch_size, num_tokens), dtype=torch.long
+        )
+
+        # add random ignore index to random tokens in the label
+        random_indices = torch.randint(0, num_tokens, (batch_size, num_tokens))
+        labels[random_indices < num_tokens // 5] = ignore_index
+
+        # chunked FKL
+        chunked_fkl_loss = ForwardKLWithChunkedOutputLoss(
+            num_output_chunks=8, ignore_index=ignore_index
+        )
+        logits_chunks = logits.chunk(chunked_fkl_loss.num_output_chunks, dim=1)
+        teacher_logits_chunks = teacher_logits.chunk(
+            chunked_fkl_loss.num_output_chunks, dim=1
+        )
+        chunked_loss = chunked_fkl_loss(logits_chunks, teacher_logits_chunks, labels)
+
+        # vanilla FKL
+        fkl_loss = ForwardKLLoss(ignore_index=ignore_index)
+        logits = logits.reshape(-1, logits.size(-1))
+        teacher_logits = teacher_logits.reshape(-1, teacher_logits.size(-1))
+        labels = labels.reshape(-1)
+        standard_loss = fkl_loss(logits, teacher_logits, labels)
+
+        # Assert
+        assert_expected(chunked_loss, standard_loss, rtol=1e-2, atol=1e-2)
+
+    def test_forward_kl_loss_expected(self):
+        student_logits = torch.tensor(
+            [
+                [
+                    [1.1250, -0.4102, -0.0879, -2.5000],
+                    [0.2676, 0.3535, 0.8711, -1.4688],
+                    [-0.1084, 1.6641, 0.0084, 0.1196],
+                    [0.5000, -0.6406, -0.2236, -1.5938],
+                ],
+                [
+                    [-1.5312, -1.9219, 0.0000, -0.5039],
+                    [-1.5391, 1.5312, 0.5820, 0.2695],
+                    [-0.3887, 1.2188, 0.0000, 0.6055],
+                    [0.5000, 1.3828, 0.1309, -1.0312],
+                ],
+            ],
+            dtype=torch.bfloat16,
+        )
+        teacher_logits = torch.tensor(
+            [
+                [
+                    [-0.0381, -1.2578, -1.2031, 0.0947],
+                    [-0.7852, 0.4492, 1.5547, 0.0972],
+                    [0.8203, 0.0012, 0.7656, 0.3477],
+                    [-1.5781, 0.4297, 0.5977, 0.3926],
+                ],
+                [
+                    [1.5156, 0.1641, 2.0781, -0.7734],
+                    [-0.5898, 0.4453, -0.7969, 0.6328],
+                    [0.6289, -0.8359, 0.9258, 0.2109],
+                    [0.0006, 0.5195, 3.2344, -1.5781],
+                ],
+            ],
+            dtype=torch.bfloat16,
+        )
+        labels = torch.tensor([[0, 3, 3, 1], [1, 1, 1, 1]])
+        expected_loss = torch.tensor(1.7209, dtype=torch.float32)
+
+        # chunked FKL loss
+        chunked_fkl_loss = ForwardKLWithChunkedOutputLoss(
+            num_output_chunks=2, ignore_index=-100
+        )
+        student_logits_chunks = student_logits.chunk(
+            chunked_fkl_loss.num_output_chunks, dim=1
+        )
+        teacher_logits_chunks = teacher_logits.chunk(
+            chunked_fkl_loss.num_output_chunks, dim=1
+        )
+        chunked_loss = chunked_fkl_loss(
+            student_logits_chunks, teacher_logits_chunks, labels
+        )
+
+        # vanilla FKL loss
+        fkl_loss = ForwardKLLoss(ignore_index=-100)
+        standard_loss = fkl_loss(student_logits, teacher_logits, labels)
+
+        # assert
+        assert_expected(chunked_loss, expected_loss, rtol=1e-2, atol=1e-2)
+        assert_expected(standard_loss, expected_loss, rtol=1e-2, atol=1e-2)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/low_precision/__init__.py marc/third_party/torchtune/tests/torchtune/modules/low_precision/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/low_precision/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/low_precision/__init__.py	2025-02-20 17:49:29.994024975 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/low_precision/test_nf4_dispatch_registration.py marc/third_party/torchtune/tests/torchtune/modules/low_precision/test_nf4_dispatch_registration.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/low_precision/test_nf4_dispatch_registration.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/low_precision/test_nf4_dispatch_registration.py	2025-02-20 17:49:29.998024982 -0500
@@ -0,0 +1,35 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+from torchao.dtypes import to_nf4
+
+
+class TestNF4DispatchRegistration:
+    """
+    Class for testing NF4Tensor dispatch ops.
+    """
+
+    def test_inplace_copy_copies_expected_attributes(self):
+        """
+        This test ensures that we're copying over all relevant attributes when implementing
+        torch.ops.aten.copy_.default. If this test fails, we would need to update our implementation
+        in _register_nf4_dispatch_ops to cover the newly added attributes.
+        """
+        expected_inplace_copy_attrs = [
+            "block_size",
+            "n_blocks",
+            "scaler_block_size",
+            "quantized_scalers",
+            "quantization_factor",
+            "scaler_mean",
+            "quantized_data",
+            "nf4",
+        ]
+
+        z = to_nf4(torch.rand(512, 512, dtype=torch.bfloat16))
+        inplace_copy_attr_set = set(z.__dict__.keys())
+        assert set(expected_inplace_copy_attrs) == inplace_copy_attr_set
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/low_precision/test_nf4_linear.py marc/third_party/torchtune/tests/torchtune/modules/low_precision/test_nf4_linear.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/low_precision/test_nf4_linear.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/low_precision/test_nf4_linear.py	2025-02-20 17:49:30.002024989 -0500
@@ -0,0 +1,123 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+try:
+    import bitsandbytes as bnb
+
+    bnb_installed = True
+except ImportError:
+    bnb_installed = False
+import pytest
+import torch
+from torchao.dtypes.nf4tensor import NF4Tensor
+from torchtune.modules.low_precision import FrozenNF4Linear
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(31)
+
+
+def _build_bnb_linear(input_weight):
+    """
+    Builds a bnb.nn.LinearNF4 from a given input weight
+    """
+    param = bnb.nn.Params4bit(input_weight, requires_grad=False, quant_type="nf4")
+    bnb_linear = bnb.nn.LinearNF4(
+        input_weight.size(0), input_weight.size(1), bias=False
+    )
+    bnb_linear.weight = param
+    bnb_linear.cuda()
+    return bnb_linear
+
+
+class TestNF4Linear:
+    """
+    Class for testing our NF4Linear implementation.
+    """
+
+    def test_bias_unsupported(self):
+        with pytest.raises(RuntimeError, match="does not currently support biases"):
+            _ = FrozenNF4Linear(1, 1, bias=True)
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_parameters(self, dtype):
+        nf4_linear = FrozenNF4Linear(512, 512, device="cpu", dtype=dtype)
+        params = list(nf4_linear.parameters())
+        assert len(params) == 1
+        assert isinstance(params[0], NF4Tensor)
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_state_dict(self, dtype):
+        nf4_linear = FrozenNF4Linear(512, 512, device="cpu", dtype=dtype)
+        state_dict = nf4_linear.state_dict()
+        assert len(state_dict) == 1
+        assert isinstance(state_dict["weight"], NF4Tensor)
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_output_dtype(self, dtype):
+        # Test to ensure W4 A16 produces A16 / W4A32 produces A32
+        nf4_linear = FrozenNF4Linear(512, 512, device="cpu", dtype=dtype)
+        inp = torch.randn(2, 512, dtype=dtype, requires_grad=True)
+        out = nf4_linear(inp)
+        assert out.dtype == dtype
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_backward_dtype(self, dtype):
+        # Test to ensure backward pass gives activation a bf16 gradient and no gradient
+        # to the linear's weight, as it is frozen.
+        nf4_linear = FrozenNF4Linear(512, 512, device="cpu", dtype=dtype)
+        inp = torch.randn(2, 512, dtype=dtype, requires_grad=True)
+        nf4_linear(inp).sum().backward()
+        assert inp.grad is not None and inp.grad.dtype == dtype
+        assert nf4_linear.weight.grad is None
+
+    @pytest.mark.skipif(not bnb_installed, reason="bitsandbytes is not installed")
+    @pytest.mark.skipif(not torch.cuda.is_available(), reason="Need CUDA available")
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_nf4_reconstruction_vs_bnb(self, dtype):
+        """
+        Ensures a BNB NF4 linear and our FrozenNF4Linear have low error when
+        reconstructing the respective original weights.
+        """
+        dim = 512
+        nf4_linear = FrozenNF4Linear(dim, dim, device="cuda", dtype=dtype)
+        orig_weight = nf4_linear.weight.get_original_weight().clone().detach()
+        bnb_nf4_linear = _build_bnb_linear(input_weight=orig_weight)
+
+        # From https://github.com/drisspg/transformer_nuggets/blob/f05afad68ad9086d342268f46a7f344617a02314/test/test_qlora.py#L65
+        bnb_reconstruction = bnb_nf4_linear(
+            torch.eye(dim, dim, dtype=dtype, device="cuda")
+        )
+        # Ensure nf4_linear and bnb reconstructions are close to each other.
+        assert torch.allclose(
+            bnb_reconstruction.T, nf4_linear.weight.get_original_weight(), 1e-2
+        )
+
+    @pytest.mark.skipif(not bnb_installed, reason="bitsandbytes is not installed")
+    @pytest.mark.skipif(not torch.cuda.is_available(), reason="Need CUDA available")
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_nf4_bnb_linear(self, dtype):
+        """
+        This test ensures that nf4_linear is "no worse" than BNB by ensuring the
+        error compared to a bf16 linear is not more than BNB's implementation.
+        """
+        dim = 512
+        nf4_linear = FrozenNF4Linear(dim, dim, device="cuda", dtype=dtype)
+        orig_weight = nf4_linear.weight.get_original_weight().clone().detach()
+        bnb_nf4_linear = _build_bnb_linear(input_weight=orig_weight)
+        bf16_linear = torch.nn.Linear(dim, dim, device="cuda", dtype=dtype)
+
+        inp = torch.randn(2, 512, dtype=dtype, device="cuda")
+
+        out_nf4 = nf4_linear(inp)
+        out_bnb = bnb_nf4_linear(inp)
+        out_ref = bf16_linear(inp)
+
+        err_bnb = out_bnb - out_ref
+        err_native = out_nf4 - out_ref
+        assert torch.allclose(err_bnb, err_native, 1.0e-2, 1.0e-2)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/__init__.py marc/third_party/torchtune/tests/torchtune/modules/model_fusion/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/model_fusion/__init__.py	2025-02-20 17:49:30.006024996 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_embed.py marc/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_embed.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_embed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_embed.py	2025-02-20 17:49:30.010025002 -0500
@@ -0,0 +1,90 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+from tests.test_utils import assert_expected, fixed_init_model
+from torchtune.modules.model_fusion import FusionEmbedding
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(1)
+
+
+class TestFusionEmbedding:
+    """
+    Class for testing our FusionEmbedding.
+    """
+
+    @pytest.fixture
+    def dim(self) -> int:
+        return 2
+
+    @pytest.fixture
+    def vocab_size(self) -> int:
+        return 10
+
+    @pytest.fixture
+    def fusion_vocab_size(self) -> int:
+        return 5
+
+    @pytest.fixture
+    def embed(self, dim, vocab_size, fusion_vocab_size) -> FusionEmbedding:
+        embeds = FusionEmbedding(
+            vocab_size=vocab_size, fusion_vocab_size=fusion_vocab_size, embed_dim=dim
+        )
+        fixed_init_model(embeds.embedding, min_val=0, max_val=0.5)
+        fixed_init_model(embeds.fusion_embedding, min_val=0.51, max_val=1)
+        return embeds
+
+    @torch.no_grad()
+    def test_forward(self, embed, vocab_size, fusion_vocab_size, dim):
+        """
+        Test that the forward pass of the FusionEmbedding works as expected.
+        """
+        tokens = torch.randint(0, vocab_size + fusion_vocab_size, (2, 10))
+        out = embed(tokens)
+
+        assert out.shape == (2, 10, dim)
+        assert_expected(out.mean(), torch.tensor(0.3409), atol=1e-3, rtol=1e-3)
+
+        # Only new tokens, embeddings should be > 0.5
+        tokens = torch.randint(vocab_size, vocab_size + fusion_vocab_size, (2, 10))
+        out = embed(tokens)
+
+        assert out.min() > 0.5
+
+        # Only old tokens, embeddings should be < 0.5
+        tokens = torch.randint(0, vocab_size, (2, 10))
+        out = embed(tokens)
+
+        assert out.max() < 0.5
+
+    def test_fusion_params(self, embed):
+        """
+        Test that the currect fusion params are returned.
+        """
+        fusion_params = set(embed.fusion_params())
+
+        assert fusion_params == {"fusion_embedding.weight"}
+
+    def test_get_and_load_state_dict(self, embed):
+        """
+        Test that the state dict hooks work in removing the "layer" variable
+        """
+        state_dict = embed.state_dict()
+        state_keys = set(state_dict.keys())
+
+        assert state_keys == {
+            "weight",
+            "fusion_embedding.weight",
+        }
+
+        # Check that the state_dict can be loaded back into the model
+        embed.load_state_dict(state_dict)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_layer.py marc/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_layer.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_layer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_layer.py	2025-02-20 17:49:30.014025008 -0500
@@ -0,0 +1,157 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+from tests.test_utils import assert_expected, fixed_init_model
+from torch import nn
+from torchtune.modules.model_fusion import FusionLayer
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(1)
+
+
+class DummyCrossAttentionLayer(nn.Module):
+    def __init__(self, dim):
+        super().__init__()
+        self.linear = nn.Linear(dim, dim)
+        self.cache_enabled = False
+        self.encoder_max_seq_len = None
+
+    def setup_cache(self, batch_size, dtype, encoder_max_seq_len, decoder_max_seq_len):
+        self.cache_enabled = True
+        self.encoder_max_seq_len = encoder_max_seq_len
+
+    def reset_cache(self):
+        self.cache_enabled = False
+
+    def forward(self, x):
+        return self.linear(x)
+
+
+class DummySelfAttentionLayer(nn.Module):
+    def __init__(self, dim):
+        super().__init__()
+        self.linear = nn.Linear(dim, dim)
+        self.cache_enabled = False
+        self.decoder_max_seq_len = None
+
+    def setup_cache(self, batch_size, dtype, encoder_max_seq_len, decoder_max_seq_len):
+        self.cache_enabled = True
+        self.decoder_max_seq_len = decoder_max_seq_len
+
+    def reset_cache(self):
+        self.cache_enabled = False
+
+    def forward(self, x):
+        return self.linear(x)
+
+
+class TestFusionLayer:
+    """
+    Class for testing our FusionLayer wrapper.
+    """
+
+    @pytest.fixture
+    def dim(self) -> int:
+        return 2
+
+    @pytest.fixture
+    def layer(self, dim) -> nn.Module:
+        layer = DummySelfAttentionLayer(dim)
+        fixed_init_model(layer, min_val=-0.1, max_val=0.1)
+        return layer
+
+    @pytest.fixture
+    def fusion_layer(self, dim) -> nn.Module:
+        layer = DummyCrossAttentionLayer(dim)
+        fixed_init_model(layer, min_val=-0.2, max_val=0.2)
+        return layer
+
+    @pytest.fixture
+    def fused_layer(self, layer, fusion_layer) -> FusionLayer:
+        return FusionLayer(layer, fusion_layer)
+
+    @torch.no_grad()
+    def test_forward(self, fused_layer, dim):
+        """
+        Test that the forward pass of the FusionLayer works as expected.
+        """
+        x = torch.rand((1, dim))
+        out = fused_layer(x)
+
+        assert out.shape == (1, dim)
+        assert_expected(out.mean(), torch.tensor(-0.0316), atol=1e-3, rtol=1e-3)
+
+    @torch.no_grad()
+    def test_fusion_last_forward(self, layer, fusion_layer, dim) -> nn.Module:
+        """
+        Test the forward method with fusion_first=False.
+        """
+        fused_layer = FusionLayer(layer, fusion_layer, fusion_first=False)
+
+        x = torch.rand((1, dim))
+        out = fused_layer(x)
+
+        assert out.shape == (1, dim)
+        assert_expected(out.mean(), torch.tensor(-0.0816), atol=1e-3, rtol=1e-3)
+
+    def test_get_and_load_state_dict(self, fused_layer):
+        """
+        Test that the state dict hooks work in removing the "layer" variable
+        """
+        state_dict = fused_layer.state_dict()
+        state_keys = set(state_dict.keys())
+
+        assert state_keys == {
+            "linear.weight",
+            "linear.bias",
+            "fusion_layer.linear.weight",
+            "fusion_layer.linear.bias",
+        }
+
+        # Check that the state_dict can be loaded back into the model
+        fused_layer.load_state_dict(state_dict)
+
+    def test_fusion_params(self, fused_layer):
+        """
+        Test that the currect fusion params are returned.
+        """
+        fusion_params = set(fused_layer.fusion_params())
+
+        assert fusion_params == {
+            "fusion_layer.linear.weight",
+            "fusion_layer.linear.bias",
+        }
+
+    def test_setup_cache(self, fused_layer):
+        """
+        Test that the cache methods works as expected.
+        """
+        fused_layer.setup_cache(
+            2, torch.float32, encoder_max_seq_len=10, decoder_max_seq_len=10
+        )
+        assert fused_layer.cache_enabled
+        fused_layer.reset_cache()
+        assert not fused_layer.cache_enabled
+
+    def test_setup_cache_different_cache_seq_len(self, fused_layer):
+        """
+        Test that the cache methods works as expected.
+        """
+        fused_layer.setup_cache(
+            2, torch.float32, encoder_max_seq_len=5, decoder_max_seq_len=10
+        )
+
+        assert fused_layer.layer.decoder_max_seq_len == 10
+        assert fused_layer.fusion_layer.encoder_max_seq_len == 5
+
+        assert not hasattr(fused_layer.layer, "encoder_max_seq_len")
+        assert not hasattr(fused_layer.fusion_layer, "decoder_max_seq_len")
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_models.py marc/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_models.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_models.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_models.py	2025-02-20 17:49:30.014025008 -0500
@@ -0,0 +1,187 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+from tests.test_utils import assert_expected, fixed_init_model
+from torch import nn
+from torchtune.modules.model_fusion import DeepFusionModel, register_fusion_module
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(1)
+
+
+class DummyModel(nn.Module):
+    def __init__(self, dim, vocab_size):
+        super().__init__()
+        self.cache_enabled = False
+        self.embed = nn.Embedding(vocab_size, dim)
+        self.q = nn.Linear(dim, dim)
+        self.k = nn.Linear(dim, dim)
+        self.v = nn.Linear(dim, dim)
+        self.output = nn.Linear(dim, vocab_size)
+        register_fusion_module(self.output)
+
+    def setup_caches(self, batch_size, dtype, *args, **kwargs):
+        self.cache_enabled = True
+
+    def caches_are_enabled(self):
+        return self.cache_enabled
+
+    def reset_caches(self):
+        self.cache_enabled = False
+
+    def forward(self, tokens, mask, encoder_input, encoder_mask, input_pos):
+        x = self.embed(tokens)
+        if encoder_input is not None:
+            q = self.q(x)
+            k = self.k(encoder_input)
+            v = self.v(encoder_input)
+            x += nn.functional.scaled_dot_product_attention(
+                q, k, v, attn_mask=encoder_mask
+            )
+        x = self.output(x)
+        return x
+
+
+class TestDeepFusionModel:
+    """
+    Class for testing our DeepFusionModel wrapper.
+    """
+
+    @pytest.fixture
+    def vocab_size(self) -> int:
+        return 100
+
+    @pytest.fixture
+    def dim(self) -> int:
+        return 64
+
+    @pytest.fixture
+    def encoder(self, dim, vocab_size) -> nn.Module:
+        encoder = nn.Embedding(vocab_size, dim)
+        fixed_init_model(encoder)
+        return encoder
+
+    @pytest.fixture
+    def decoder(self, dim, vocab_size) -> nn.Module:
+        decoder = DummyModel(dim, vocab_size)
+        fixed_init_model(decoder, max_val=0.1)
+        return decoder
+
+    @pytest.fixture
+    def fused_model(self, encoder, decoder) -> DeepFusionModel:
+        model = DeepFusionModel(
+            encoder=encoder,
+            decoder=decoder,
+        )
+        return model
+
+    @pytest.fixture
+    def inputs(self, dim, vocab_size):
+        batch_size = 2
+        seq_len = 10
+        tokens = torch.randint(0, vocab_size, (batch_size, seq_len))
+        encoder_input = {"input": torch.randint(0, vocab_size, (batch_size, seq_len))}
+        encoder_mask = torch.randint(0, 2, (batch_size, seq_len, seq_len)).bool()
+        input_pos = torch.Tensor([1]).int()
+        return tokens, encoder_input, encoder_mask, input_pos
+
+    @torch.no_grad()
+    def test_forward(self, fused_model, inputs, vocab_size):
+        """
+        Test that the forward pass of the DeepFusionModel works as expected.
+        """
+        tokens, encoder_input, encoder_mask, _ = inputs
+        batch_size, seq_len = tokens.shape
+        out = fused_model(
+            tokens, encoder_input=encoder_input, encoder_mask=encoder_mask
+        )
+
+        assert out.shape == (batch_size, seq_len, vocab_size)
+        assert_expected(out.mean(), torch.tensor(8.5584), atol=1e-3, rtol=1e-3)
+
+    @torch.no_grad()
+    def test_forward_no_encoding(self, fused_model, inputs, vocab_size):
+        """
+        Test that the forward pass of the DeepFusionModel with no encoder input.
+        """
+        tokens, *_ = inputs
+        batch_size, seq_len = tokens.shape
+        out = fused_model(tokens)
+
+        assert out.shape == (batch_size, seq_len, vocab_size)
+        assert_expected(out.mean(), torch.tensor(0.2271), atol=1e-3, rtol=1e-3)
+
+    @torch.no_grad()
+    def test_decoding_forward(self, fused_model, inputs, vocab_size):
+        """
+        Test that the forward pass of the DeepFusionModel works during decoding.
+        """
+        tokens, encoder_input, encoder_mask, input_pos = inputs
+        tokens = tokens[:, input_pos]
+        encoder_mask = encoder_mask[:, input_pos]
+        batch_size, seq_len = tokens.shape
+        out = fused_model(
+            tokens,
+            encoder_input=encoder_input,
+            encoder_mask=encoder_mask,
+            input_pos=input_pos,
+        )
+
+        assert out.shape == (batch_size, seq_len, vocab_size)
+        assert_expected(out.mean(), torch.tensor(9.0072), atol=1e-3, rtol=1e-3)
+
+    def test_setup_cache(self, fused_model):
+        """
+        Test that the cache methods works as expected.
+        """
+        fused_model.setup_caches(2, torch.float32)
+        assert fused_model.caches_are_enabled()
+        fused_model.reset_caches()
+        assert not fused_model.caches_are_enabled()
+
+    def test_set_trainable_params(self, fused_model, encoder, decoder):
+        """
+        Test that the trainable parameters are set correctly.
+        """
+        # Test default case
+        trainable_params = {
+            n for n, p in fused_model.named_parameters() if p.requires_grad
+        }
+        assert trainable_params == {"decoder.output.weight", "decoder.output.bias"}
+
+        # Test encoder only
+        model = DeepFusionModel(
+            encoder=encoder,
+            decoder=decoder,
+            encoder_trainable=True,
+            fusion_trainable=False,
+        )
+        trainable_params = {n for n, p in model.named_parameters() if p.requires_grad}
+        assert trainable_params == {"encoder.weight"}
+
+        # Test decoder only, and confirm fusion layers are removed independently
+        model = DeepFusionModel(
+            encoder=encoder,
+            decoder=decoder,
+            decoder_trainable=True,
+            fusion_trainable=False,
+        )
+        trainable_params = {n for n, p in model.named_parameters() if p.requires_grad}
+        assert trainable_params == {
+            "decoder.q.weight",
+            "decoder.q.bias",
+            "decoder.k.weight",
+            "decoder.k.bias",
+            "decoder.v.weight",
+            "decoder.v.bias",
+            "decoder.embed.weight",
+        }
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_utils.py marc/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_utils.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/model_fusion/test_fusion_utils.py	2025-02-20 17:49:30.018025015 -0500
@@ -0,0 +1,32 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from torch import nn
+from torchtune.modules.model_fusion import get_fusion_params, register_fusion_module
+
+
+def test_register_fusion_module():
+    """
+    Test that all parameters are returned as fusion_params.
+    """
+    model = nn.Linear(1, 1)
+    register_fusion_module(model)
+
+    fusion_params = set(model.fusion_params())
+    assert fusion_params == {"weight", "bias"}
+
+
+def test_get_fusion_params():
+    """
+    Test that the correct parameters are returned as fusion_params.
+    """
+    layer1 = nn.Linear(1, 1)
+    layer2 = nn.Linear(1, 1)
+    register_fusion_module(layer2)
+    model = nn.Sequential(layer1, layer2)
+
+    fusion_params = set(get_fusion_params(model))
+    assert fusion_params == {"1.weight", "1.bias"}
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/peft/__init__.py marc/third_party/torchtune/tests/torchtune/modules/peft/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/peft/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/peft/__init__.py	2025-02-20 17:49:30.022025022 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/peft/test_dora.py marc/third_party/torchtune/tests/torchtune/modules/peft/test_dora.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/peft/test_dora.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/peft/test_dora.py	2025-02-20 17:49:30.026025028 -0500
@@ -0,0 +1,192 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+
+import pytest
+
+import torch
+from tests.test_utils import fixed_init_model
+from torch import nn
+from torchao.dtypes.nf4tensor import NF4Tensor, to_nf4
+from torchtune import training
+from torchtune.modules.common_utils import reparametrize_as_dtype_state_dict_post_hook
+from torchtune.modules.peft import DoRALinear
+from torchtune.training.seed import set_seed
+
+RANK = 4
+ALPHA = 1.0
+BSZ = 2
+SEQ_LEN = 32
+EXPECTED_VAL = 0.05201
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestDoRALinear:
+    """
+    Class for testing our DoRALinear implementation. Expected values are computed
+    from the reference implementation and calculated in scripts/compare_lora.py.
+    """
+
+    @pytest.fixture
+    def in_dim(self) -> int:
+        return 64
+
+    @pytest.fixture
+    def out_dim(self) -> int:
+        return 128
+
+    @pytest.fixture
+    def inputs(self, in_dim) -> torch.Tensor:
+        inputs = torch.randn(BSZ, SEQ_LEN, in_dim)
+        return inputs
+
+    @pytest.fixture
+    def dora_linear(self, in_dim, out_dim) -> DoRALinear:
+        dora_linear = DoRALinear(
+            in_dim=in_dim,
+            out_dim=out_dim,
+            rank=RANK,
+            alpha=ALPHA,
+            use_bias=False,
+        )
+
+        fixed_init_model(dora_linear)
+        return dora_linear
+
+    @pytest.fixture
+    def qdora_linear(self, in_dim, out_dim) -> DoRALinear:
+        with training.set_default_dtype(torch.bfloat16):
+            qdora_linear = DoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=False,
+                quantize_base=True,
+            )
+            fixed_init_model(qdora_linear, dtype=torch.bfloat16)
+            return qdora_linear
+
+    def test_forward(self, inputs, dora_linear, out_dim) -> None:
+        expected = torch.tensor(EXPECTED_VAL)
+        actual = dora_linear(inputs)
+        assert actual.shape == (BSZ, SEQ_LEN, out_dim)
+        torch.testing.assert_close(actual.mean(), expected, atol=1e-4, rtol=1e-6)
+
+    def test_dora_weight_nf4_when_quantized(self, qdora_linear):
+        assert isinstance(qdora_linear.weight, NF4Tensor)
+
+    def test_bias_raises(self):
+        with pytest.raises(
+            NotImplementedError, match="DoRALinear does not support using bias"
+        ):
+            DoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=True,
+                quantize_base=False,
+            )
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_qdora_parity(self, dtype):
+        with training.set_default_dtype(dtype):
+            torch.manual_seed(0)
+            qdora_linear = DoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=False,
+                quantize_base=True,
+            )
+            torch.manual_seed(0)
+            dora_linear = DoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=False,
+                quantize_base=False,
+            )
+
+        # set weight of dora_linear to unquantized weight of qdora_linear and check
+        # parity.
+        dora_linear.weight.data = qdora_linear.weight.to(dtype)
+
+        qdora_linear.initialize_dora_magnitude()
+        dora_linear.initialize_dora_magnitude()
+
+        # Ensure forward passes are the same. This is because DoRALinear should use a special
+        # quantized linear operator that runs compute in higher prec (but only saves the 4 bit quantized tensor)
+        # for autograd.
+        inputs = torch.randn(BSZ, SEQ_LEN, 512, dtype=dtype)
+        torch.manual_seed(0)
+        dora_linear_out = dora_linear(inputs)
+        torch.manual_seed(0)
+        qdora_linear_out = qdora_linear(inputs)
+        torch.testing.assert_close(dora_linear_out, qdora_linear_out)
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_quantized_state_dict(self, dtype):
+        with training.set_default_dtype(dtype):
+            dora_linear = DoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=False,
+                quantize_base=True,
+            )
+
+        dora_linear._register_state_dict_hook(
+            partial(
+                reparametrize_as_dtype_state_dict_post_hook,
+                dtype=dtype,
+                offload_to_cpu=False,
+            )
+        )
+        sd = dora_linear.state_dict()
+        # No nf4 tensors, all have type dtype
+        for v in sd.values():
+            assert v.dtype == dtype
+            assert not isinstance(v, NF4Tensor)
+
+        # Load back in results in re-quant and creates the same nf4 tensor.
+        # This also ensures that DoRALinear can load a bf16 state_dict.
+        dora_linear_reload = DoRALinear(
+            in_dim=512,
+            out_dim=512,
+            rank=RANK,
+            alpha=ALPHA,
+            use_bias=False,
+            quantize_base=True,
+        )
+        # Zero out weight to verify reloading works
+        dora_linear_reload.weight = nn.Parameter(
+            to_nf4(
+                torch.zeros_like(
+                    dora_linear.weight.get_original_weight(),
+                    dtype=dtype,
+                    device=dora_linear.weight.device,
+                )
+            )
+        )
+        # nf4 tensors should be different
+        assert not torch.allclose(
+            dora_linear.weight.quantized_data, dora_linear_reload.weight.quantized_data
+        )
+        # but should be the same after loading
+        dora_linear_reload.load_state_dict(sd)
+        assert torch.allclose(
+            dora_linear.weight.quantized_data, dora_linear_reload.weight.quantized_data
+        )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/peft/test_lora.py marc/third_party/torchtune/tests/torchtune/modules/peft/test_lora.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/peft/test_lora.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/peft/test_lora.py	2025-02-20 17:49:30.030025035 -0500
@@ -0,0 +1,199 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+
+import pytest
+
+import torch
+from tests.test_utils import fixed_init_model
+from torch import nn
+from torchao.dtypes.nf4tensor import NF4Tensor, to_nf4
+from torchtune import training
+from torchtune.modules.common_utils import reparametrize_as_dtype_state_dict_post_hook
+from torchtune.modules.peft import LoRALinear
+from torchtune.training.seed import set_seed
+
+RANK = 4
+ALPHA = 1.0
+BSZ = 2
+SEQ_LEN = 32
+EXPECTED_VAL = 1.1252
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestLoRALinear:
+    """
+    Class for testing our LoRALinear implementation. Expected values are computed
+    from the reference implementation and calculated in scripts/compare_lora.py.
+    """
+
+    @pytest.fixture
+    def in_dim(self) -> int:
+        return 64
+
+    @pytest.fixture
+    def out_dim(self) -> int:
+        return 128
+
+    @pytest.fixture
+    def inputs(self, in_dim) -> torch.Tensor:
+        inputs = torch.randn(BSZ, SEQ_LEN, in_dim)
+        return inputs
+
+    @pytest.fixture
+    def lora_linear(self, in_dim, out_dim) -> LoRALinear:
+        lora_linear = LoRALinear(
+            in_dim=in_dim,
+            out_dim=out_dim,
+            rank=RANK,
+            alpha=ALPHA,
+            use_bias=True,
+        )
+        fixed_init_model(lora_linear)
+        return lora_linear
+
+    @pytest.fixture
+    def qlora_linear(self, in_dim, out_dim) -> LoRALinear:
+        with training.set_default_dtype(torch.bfloat16):
+            qlora_linear = LoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=False,
+                quantize_base=True,
+            )
+            fixed_init_model(qlora_linear, dtype=torch.bfloat16)
+            return qlora_linear
+
+    @torch.no_grad()
+    def set_dummy_weights_for_merge(self, lora_module):
+        lora_module.lora_a.weight = nn.Parameter(
+            torch.zeros_like(lora_module.lora_a.weight)
+        )
+        lora_module.lora_b.weight = nn.Parameter(
+            torch.zeros_like(lora_module.lora_b.weight)
+        )
+        lora_module.weight = nn.Parameter(torch.zeros_like(lora_module.weight))
+        lora_module.bias = nn.Parameter(torch.zeros_like(lora_module.bias))
+
+        # Hardcode some very specific nonzero values to make verification easy
+        lora_module.weight[4, 5] = 1
+        lora_module.bias[7] = 2
+        lora_module.lora_a.weight[1, 25] = 3
+        lora_module.lora_b.weight[32, 1] = 12
+
+    def test_forward(self, inputs, lora_linear, out_dim) -> None:
+        expected = torch.tensor(EXPECTED_VAL)
+        actual = lora_linear(inputs)
+        assert actual.shape == (BSZ, SEQ_LEN, out_dim)
+        torch.testing.assert_close(actual.mean(), expected, atol=1e-4, rtol=1e-6)
+
+    def test_lora_weight_nf4_when_quantized(self, qlora_linear):
+        assert isinstance(qlora_linear.weight, NF4Tensor)
+
+    def test_quantize_with_bias_raises(self):
+        with pytest.raises(NotImplementedError, match="does not support bias"):
+            LoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=True,
+                quantize_base=True,
+            )
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_qlora_parity(self, dtype):
+        with training.set_default_dtype(dtype):
+            qlora_linear = LoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=False,
+                quantize_base=True,
+            )
+            lora_linear = LoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=False,
+                quantize_base=False,
+            )
+
+        # set weight of lora_linear to unquantized weight of qlora_linear and check
+        # parity.
+        lora_linear.weight.data = qlora_linear.weight.to(dtype)
+
+        # Ensure forward passes are the same. This is because LoRALinear should use a special
+        # quantized linear operator that runs compute in higher prec (but only saves the 4 bit quantized tensor)
+        # for autograd.
+        inputs = torch.randn(BSZ, SEQ_LEN, 512, dtype=dtype)
+        lora_linear_out = lora_linear(inputs)
+        qlora_linear_out = qlora_linear(inputs)
+        torch.testing.assert_close(lora_linear_out, qlora_linear_out)
+
+    @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float32])
+    def test_quantized_state_dict(self, dtype):
+        with training.set_default_dtype(dtype):
+            lora_linear = LoRALinear(
+                in_dim=512,
+                out_dim=512,
+                rank=RANK,
+                alpha=ALPHA,
+                use_bias=False,
+                quantize_base=True,
+            )
+
+        lora_linear._register_state_dict_hook(
+            partial(
+                reparametrize_as_dtype_state_dict_post_hook,
+                dtype=dtype,
+                offload_to_cpu=False,
+            )
+        )
+        sd = lora_linear.state_dict()
+        # No nf4 tensors, all have type dtype
+        for v in sd.values():
+            assert v.dtype == dtype
+            assert not isinstance(v, NF4Tensor)
+
+        # Load back in results in re-quant and creates the same nf4 tensor.
+        # This also ensures that LoRALinear can load a bf16 state_dict.
+        lora_linear_reload = LoRALinear(
+            in_dim=512,
+            out_dim=512,
+            rank=RANK,
+            alpha=ALPHA,
+            use_bias=False,
+            quantize_base=True,
+        )
+        # Zero out weight to verify reloading works
+        lora_linear_reload.weight = nn.Parameter(
+            to_nf4(
+                torch.zeros_like(
+                    lora_linear.weight.get_original_weight(),
+                    dtype=dtype,
+                    device=lora_linear.weight.device,
+                )
+            )
+        )
+        # nf4 tensors should be different
+        assert not torch.allclose(
+            lora_linear.weight.quantized_data, lora_linear_reload.weight.quantized_data
+        )
+        # but should be the same after loading
+        lora_linear_reload.load_state_dict(sd)
+        assert torch.allclose(
+            lora_linear.weight.quantized_data, lora_linear_reload.weight.quantized_data
+        )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/peft/test_utils.py marc/third_party/torchtune/tests/torchtune/modules/peft/test_utils.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/peft/test_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/peft/test_utils.py	2025-02-20 17:49:30.034025042 -0500
@@ -0,0 +1,567 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from copy import deepcopy
+
+import pytest
+import torch
+
+from torch import nn
+from torchtune.models.llama2 import llama2, lora_llama2
+from torchtune.modules.peft import (
+    AdapterModule,
+    disable_adapter,
+    DoRALinear,
+    get_adapter_params,
+    get_merged_lora_ckpt,
+    LoRALinear,
+    set_trainable_params,
+    validate_missing_and_unexpected_for_lora,
+    validate_state_dict_for_lora,
+)
+
+N_LAYERS = 3
+IN_DIM = 5
+OUT_DIM = 10
+VOCAB_SIZE = 50
+NUM_HEADS = 4
+NUM_KV_HEADS = 2
+EMBED_DIM = 64
+MAX_SEQ_LEN = 64
+RANK = 2
+ALPHA = 1
+
+
+class DummyAdapterModule(nn.Module, AdapterModule):
+    def __init__(self, in_dim, out_dim):
+        super().__init__()
+        self.adapter = nn.Linear(in_dim, out_dim, bias=False)
+        self.linear = nn.Linear(in_dim, out_dim)
+
+    def adapter_params(self):
+        return ["adapter.weight"]
+
+    def forward(self, x):
+        return self.adapter(x) + self.non_adapter(x)
+
+
+class DummyAdapterParentModel(nn.Module, AdapterModule):
+    def __init__(self, in_dim, out_dim):
+        super().__init__()
+        self.dummy_adapter_module = DummyAdapterModule(in_dim, out_dim)
+        self.parent_adapter = nn.Linear(in_dim, out_dim)
+        self.parent_base_model = nn.Linear(in_dim, out_dim)
+
+    def adapter_params(self):
+        return ["parent_adapter.weight", "parent_adapter.bias"]
+
+    def forward(self, x):
+        return (
+            self.dummy_adapter_module(x)
+            + self.parent_adapter(x)
+            + self.parent_base_model(x)
+        )
+
+
+@pytest.fixture
+def dummy_adapter_parent_model():
+    return nn.ModuleList(
+        [DummyAdapterParentModel(IN_DIM, OUT_DIM) for _ in range(N_LAYERS)]
+    )
+
+
+@pytest.fixture
+def dummy_model_expected_adapter_keys():
+    keys = []
+    for i in range(N_LAYERS):
+        keys.extend(
+            [
+                f"{i}.parent_adapter.weight",
+                f"{i}.parent_adapter.bias",
+                f"{i}.dummy_adapter_module.adapter.weight",
+            ]
+        )
+    return keys
+
+
+@pytest.fixture
+def dummy_model_expected_base_model_keys():
+    keys = []
+    for i in range(N_LAYERS):
+        keys.extend(
+            [
+                f"{i}.parent_base_model.weight",
+                f"{i}.parent_base_model.bias",
+                f"{i}.dummy_adapter_module.linear.weight",
+                f"{i}.dummy_adapter_module.linear.bias",
+            ]
+        )
+    return keys
+
+
+@pytest.fixture
+def lora_llama2_model():
+    return lora_llama2(
+        lora_attn_modules=["q_proj", "v_proj"],
+        vocab_size=VOCAB_SIZE,
+        num_layers=N_LAYERS,
+        num_heads=NUM_HEADS,
+        num_kv_heads=NUM_KV_HEADS,
+        embed_dim=EMBED_DIM,
+        max_seq_len=MAX_SEQ_LEN,
+        lora_rank=4,
+        lora_alpha=1.0,
+    )
+
+
+@pytest.fixture
+def dora_llama2_model():
+    return lora_llama2(
+        lora_attn_modules=["q_proj", "v_proj"],
+        vocab_size=VOCAB_SIZE,
+        num_layers=N_LAYERS,
+        num_heads=NUM_HEADS,
+        num_kv_heads=NUM_KV_HEADS,
+        embed_dim=EMBED_DIM,
+        max_seq_len=MAX_SEQ_LEN,
+        lora_rank=4,
+        lora_alpha=1.0,
+        use_dora=True,
+    )
+
+
+@pytest.fixture
+def lora_llama2_model_all_keys(lora_llama2_model):
+    return lora_llama2_model.state_dict().keys()
+
+
+@pytest.fixture
+def dora_llama2_model_all_keys(dora_llama2_model):
+    return dora_llama2_model.state_dict().keys()
+
+
+@pytest.fixture
+def lora_llama2_expected_adapter_keys():
+    keys = []
+    for i in range(N_LAYERS):
+        keys.extend(
+            [
+                f"layers.{i}.attn.q_proj.lora_a.weight",
+                f"layers.{i}.attn.q_proj.lora_b.weight",
+                f"layers.{i}.attn.v_proj.lora_a.weight",
+                f"layers.{i}.attn.v_proj.lora_b.weight",
+            ]
+        )
+    return keys
+
+
+@pytest.fixture
+def dora_llama2_expected_adapter_keys():
+    keys = []
+    for i in range(N_LAYERS):
+        keys.extend(
+            [
+                f"layers.{i}.attn.q_proj.lora_a.weight",
+                f"layers.{i}.attn.q_proj.lora_b.weight",
+                f"layers.{i}.attn.v_proj.lora_a.weight",
+                f"layers.{i}.attn.v_proj.lora_b.weight",
+                f"layers.{i}.attn.q_proj.magnitude",
+                f"layers.{i}.attn.v_proj.magnitude",
+            ]
+        )
+    return keys
+
+
+@pytest.fixture
+def lora_llama2_expected_base_model_keys():
+
+    base_model = llama2(
+        vocab_size=VOCAB_SIZE,
+        num_layers=N_LAYERS,
+        num_heads=NUM_KV_HEADS,
+        num_kv_heads=NUM_KV_HEADS,
+        embed_dim=EMBED_DIM,
+        max_seq_len=MAX_SEQ_LEN,
+    )
+    return base_model.state_dict().keys()
+
+
+class TestPeftUtils:
+    @pytest.mark.parametrize(
+        "model_name, expected_keys",
+        [
+            ("dummy_adapter_parent_model", "dummy_model_expected_adapter_keys"),
+            ("lora_llama2_model", "lora_llama2_expected_adapter_keys"),
+            ("dora_llama2_model", "dora_llama2_expected_adapter_keys"),
+        ],
+    )
+    def test_get_adapter_params(self, request, model_name, expected_keys):
+        model = request.getfixturevalue(model_name)
+        adapter_params = get_adapter_params(model)
+        expected = request.getfixturevalue(expected_keys)
+        assert set(expected) == set(adapter_params.keys())
+
+    @pytest.mark.parametrize(
+        "model_name, expected_trainable_keys, expected_frozen_keys",
+        [
+            (
+                "dummy_adapter_parent_model",
+                "dummy_model_expected_adapter_keys",
+                "dummy_model_expected_base_model_keys",
+            ),
+            (
+                "lora_llama2_model",
+                "lora_llama2_expected_adapter_keys",
+                "lora_llama2_expected_base_model_keys",
+            ),
+            (
+                "dora_llama2_model",
+                "dora_llama2_expected_adapter_keys",
+                "lora_llama2_expected_base_model_keys",
+            ),
+        ],
+    )
+    def test_set_trainable_params(
+        self, request, model_name, expected_trainable_keys, expected_frozen_keys
+    ):
+        model = request.getfixturevalue(model_name)
+        adapter_params = get_adapter_params(model)
+        expected_trainable = request.getfixturevalue(expected_trainable_keys)
+        expected_frozen = request.getfixturevalue(expected_frozen_keys)
+        set_trainable_params(model, adapter_params)
+        for k, v in model.named_parameters():
+            if k in expected_trainable:
+                assert v.requires_grad
+            elif k in expected_frozen:
+                assert not v.requires_grad
+            else:
+                raise AssertionError(f"{k} not in expected keys")
+
+    @pytest.mark.parametrize(
+        (
+            """
+            lora_attn_modules,
+            apply_lora_to_mlp,
+            apply_lora_to_output,
+            full_model_state_dict_keys,
+            lora_state_dict_keys,
+            base_model_state_dict_keys,
+            expected
+            """
+        ),
+        [
+            (
+                ["q_proj", "k_proj"],
+                False,
+                False,
+                ["q_proj.lora_a.weight", "dummy_param.weight"],
+                ["q_proj.lora_a.weight"],
+                ["dummy_param.weight"],
+                "",
+            ),
+            (
+                ["v_proj"],
+                False,
+                False,
+                ["param_a", "param_b"],
+                None,
+                ["param_a", "param_b"],
+                "",
+            ),
+            (
+                ["output_proj"],
+                False,
+                True,
+                ["output_proj.weight", "output_proj.lora_a.weight"],
+                ["output_proj.lora_a.weight"],
+                ["output_proj.weight"],
+                "",
+            ),
+            (["q_proj"], False, False, ["param_a"], [], [], "Missing non-LoRA"),
+            (
+                ["k_proj", "output_proj"],
+                False,
+                True,
+                ["k_proj.lora_a.weight", "param_a"],
+                ["k_proj.lora_a.weight", "param_a"],
+                ["param_a"],
+                "found in LoRA",
+            ),
+            (
+                ["k_proj"],
+                False,
+                False,
+                ["k_proj.lora_a.weight"],
+                [],
+                ["k_proj.lora_a.weight"],
+                "found in base model",
+            ),
+            (
+                ["k_proj"],
+                False,
+                False,
+                ["k_proj.lora_a.weight"],
+                [],
+                None,
+                "Missing LoRA",
+            ),
+            (["q_proj"], False, False, [], ["a"], ["a"], "overlapping"),
+            (
+                ["v_proj"],
+                False,
+                False,
+                ["dummy_param.weight"],
+                ["v_proj.lora_a.weight"],
+                ["dummy_param.weight"],
+                "Extra",
+            ),
+            (
+                ["w1", "w2", "w3"],
+                True,
+                False,
+                ["w1.lora_a.weight", "w2.weight", "q_proj.weight"],
+                ["w1.lora_a.weight"],
+                ["q_proj.weight"],
+                "Missing non-LoRA key",
+            ),
+            (
+                ["q_proj", "output"],
+                False,
+                True,
+                [
+                    "q_proj.lora_a",
+                    "output.weight",
+                    "output.lora_a",
+                    "output_proj.lora_b",
+                ],
+                ["q_proj.lora_a", "output.lora_a", "output_proj.lora_b"],
+                ["output.weight"],
+                "Missing non-LoRA key",
+            ),
+            (
+                ["q_proj", "v_proj"],
+                False,
+                False,
+                "lora_llama2_model_all_keys",
+                "lora_llama2_expected_adapter_keys",
+                "lora_llama2_expected_base_model_keys",
+                "",
+            ),
+            (
+                ["q_proj", "v_proj"],
+                False,
+                False,
+                "dora_llama2_model_all_keys",
+                "dora_llama2_expected_adapter_keys",
+                "lora_llama2_expected_base_model_keys",
+                "",
+            ),
+        ],
+    )
+    def test_validate_lora_state_dict(
+        self,
+        request,
+        lora_attn_modules,
+        apply_lora_to_mlp,
+        apply_lora_to_output,
+        full_model_state_dict_keys,
+        lora_state_dict_keys,
+        base_model_state_dict_keys,
+        expected,
+    ):
+        if isinstance(full_model_state_dict_keys, str):
+            full_model_state_dict_keys = request.getfixturevalue(
+                full_model_state_dict_keys
+            )
+        if isinstance(lora_state_dict_keys, str):
+            lora_state_dict_keys = request.getfixturevalue(lora_state_dict_keys)
+        if isinstance(base_model_state_dict_keys, str):
+            base_model_state_dict_keys = request.getfixturevalue(
+                base_model_state_dict_keys
+            )
+        if expected:
+            with pytest.raises(AssertionError, match=expected):
+                validate_state_dict_for_lora(
+                    lora_attn_modules,
+                    apply_lora_to_mlp,
+                    apply_lora_to_output,
+                    full_model_state_dict_keys=full_model_state_dict_keys,
+                    lora_state_dict_keys=lora_state_dict_keys,
+                    base_model_state_dict_keys=base_model_state_dict_keys,
+                )
+        else:
+            validate_state_dict_for_lora(
+                lora_attn_modules,
+                apply_lora_to_mlp,
+                apply_lora_to_output,
+                full_model_state_dict_keys=full_model_state_dict_keys,
+                lora_state_dict_keys=lora_state_dict_keys,
+                base_model_state_dict_keys=base_model_state_dict_keys,
+            )
+
+    @pytest.mark.parametrize(
+        (
+            """
+            base_missing,
+            base_unexpected,
+            lora_missing,
+            lora_unexpected,
+            expected
+            """
+        ),
+        [
+            (["k_proj.lora"], [], ["q_proj.lora"], [], "Missing LoRA"),
+            (["k_proj.lora"], [], ["q_proj.magnitude"], [], "Missing LoRA"),
+            (["output_proj.lora"], [], ["q_proj.lora"], [], "Missing non-LoRA"),
+            (
+                ["k_proj.lora"],
+                ["output.weight"],
+                ["q_proj.base_weight"],
+                [],
+                "loading base model",
+            ),
+            (
+                ["k_proj.lora"],
+                [],
+                ["q_proj.base_weight"],
+                ["output.weight"],
+                "loading adapter",
+            ),
+            (["k_proj.lora"], [], ["q_proj.base_weight"], [], ""),
+        ],
+    )
+    def test_validate_missing_and_unexpected_for_lora(
+        self, base_missing, base_unexpected, lora_missing, lora_unexpected, expected
+    ):
+        lora_attn_modules = ["q_proj", "k_proj"]
+        apply_lora_to_mlp = True
+        apply_lora_to_output = False
+        if expected:
+            with pytest.raises(AssertionError, match=expected):
+                validate_missing_and_unexpected_for_lora(
+                    lora_attn_modules,
+                    apply_lora_to_mlp,
+                    apply_lora_to_output,
+                    base_missing,
+                    base_unexpected,
+                    lora_missing,
+                    lora_unexpected,
+                )
+        else:
+            validate_missing_and_unexpected_for_lora(
+                lora_attn_modules,
+                apply_lora_to_mlp,
+                apply_lora_to_output,
+                base_missing,
+                base_unexpected,
+                lora_missing,
+                lora_unexpected,
+            )
+
+
+class TestGetMergedLoRACkpt:
+    def dummy_lora_model(self):
+        model = nn.Sequential(
+            LoRALinear(in_dim=4, out_dim=6, rank=RANK, alpha=ALPHA),
+            nn.Linear(6, 3),
+        )
+        model[0].lora_a.weight = nn.Parameter(
+            torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8]])
+        )
+        model[0].lora_b.weight = nn.Parameter(
+            torch.Tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])
+        )
+        model[0].weight = nn.Parameter(3 * torch.ones((6, 4)))
+        return model
+
+    def dummy_dora_model(self):
+        model = nn.Sequential(
+            DoRALinear(in_dim=4, out_dim=6, rank=RANK, alpha=ALPHA),
+            nn.Linear(6, 3),
+        )
+        model[0].lora_a.weight = nn.Parameter(
+            torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8]])
+        )
+        model[0].lora_b.weight = nn.Parameter(
+            torch.Tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])
+        )
+        model[0].magnitude = nn.Parameter(torch.Tensor([1, 2, 3, 4, 5, 6]))
+        model[0].weight = nn.Parameter(3 * torch.ones((6, 4)))
+        return model
+
+    @pytest.mark.parametrize("use_dora", [True, False])
+    def test_get_merged_lora_ckpt(self, use_dora):
+        if use_dora:
+            dummy_model = self.dummy_dora_model()
+        else:
+            dummy_model = self.dummy_lora_model()
+        merged_sd = get_merged_lora_ckpt(
+            deepcopy(dummy_model.state_dict()), rank=RANK, alpha=ALPHA
+        )
+        if use_dora:
+            expected_merged_weight = torch.Tensor(
+                [
+                    [0.3906, 0.4596, 0.5285, 0.5974],
+                    [0.7202, 0.8940, 1.0671, 1.2417],
+                    [1.0459, 1.3265, 1.6071, 1.8877],
+                    [1.3706, 1.7585, 2.1464, 2.5343],
+                    [1.6948, 2.1902, 2.6856, 3.1810],
+                    [2.0188, 2.6218, 3.2248, 3.8278],
+                ]
+            )
+        else:
+            expected_merged_weight = torch.Tensor(
+                [
+                    [8.5, 10.0, 11.5, 13.0],
+                    [14.5, 18.0, 21.5, 25.0],
+                    [20.5, 26.0, 31.5, 37.0],
+                    [26.5, 34.0, 41.5, 49.0],
+                    [32.5, 42.0, 51.5, 61.0],
+                    [38.5, 50.0, 61.5, 73.0],
+                ]
+            )
+
+        print("dora", expected_merged_weight)
+        assert merged_sd.keys() == {"0.weight", "1.weight", "1.bias"}
+        torch.testing.assert_close(
+            merged_sd["0.weight"], expected_merged_weight, atol=1e-3, rtol=1e-3
+        )
+
+        merged_model = nn.Sequential(nn.Linear(4, 6, bias=False), nn.Linear(6, 3))
+        merged_model.load_state_dict(merged_sd, strict=True)
+
+        inputs = torch.randn(2, 8, 4)
+        torch.testing.assert_close(dummy_model(inputs), merged_model(inputs))
+
+
+class TestDisableAdapter:
+    def dummy_model(self):
+        model_ori = nn.Sequential(
+            nn.Linear(2, 6, bias=False),
+            nn.Linear(6, 3),
+        )
+        model_lora = nn.Sequential(
+            LoRALinear(in_dim=2, out_dim=6, rank=RANK, alpha=ALPHA),
+            nn.Linear(6, 3),
+        )
+        # TODO: fix weight initialization to use fixed_init_model
+        for p in model_ori.parameters():
+            nn.init.constant_(p, 1.0)
+        for p in model_lora.parameters():
+            nn.init.constant_(p, 1.0)
+        return model_ori, model_lora
+
+    def test_disable_adapter(self):
+        model_ori, model_lora = self.dummy_model()
+        inputs = torch.randn(2, 2)
+
+        ori_outputs = model_ori(inputs)
+
+        with disable_adapter(model_lora):
+            lora_outputs = model_lora(inputs)
+
+        assert model_lora[0].disabled is False
+        torch.testing.assert_close(ori_outputs, lora_outputs)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_attention.py marc/third_party/torchtune/tests/torchtune/modules/test_attention.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_attention.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_attention.py	2025-02-20 17:49:30.038025048 -0500
@@ -0,0 +1,347 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional, Tuple
+
+import pytest
+
+import torch
+
+from tests.test_utils import assert_expected, fixed_init_model
+from torch import nn
+
+from torchtune.modules import KVCache, MultiHeadAttention, RotaryPositionalEmbeddings
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestMultiHeadAttention:
+    """
+    Class for testing our MultiHeadAttention implementation.
+
+    The expected tensors are computed from the reference implementation
+    below by using the same seed, same params and same initialization used
+    in the fixtures below.
+    https://github.com/facebookresearch/llama/blob/main/llama/model.py#L450
+    """
+
+    @pytest.fixture
+    def input_params(self) -> Tuple[int, int, int]:
+        batch_size = 4
+        seq_len = 2048
+        embed_dim = 4096
+        return batch_size, seq_len, embed_dim
+
+    @pytest.fixture
+    def input(self, input_params: Tuple[int, int, int]) -> torch.Tensor:
+        batch_size, seq_len, embed_dim = input_params
+        x = torch.randn(batch_size, seq_len, embed_dim)
+        return x
+
+    @pytest.fixture
+    def attn_params_gqa(self) -> Tuple[int, int, int, int]:
+        num_heads = 32
+        num_kv_heads = 8
+        embed_dim = 4096
+        max_seq_len = 4096
+        return num_heads, num_kv_heads, embed_dim, max_seq_len
+
+    @pytest.fixture
+    def input_max_len_exceeded(
+        self,
+        input_params: Tuple[int, int, int],
+        attn_params_gqa: Tuple[int, int, int, int],
+    ) -> torch.Tensor:
+        batch_size, seq_len, embed_dim = input_params
+        _, _, _, max_seq_len = attn_params_gqa
+        seq_len = max_seq_len + 1
+        return torch.randn(batch_size, seq_len, embed_dim)
+
+    @pytest.fixture
+    def input_max_bs_exceeded(
+        self,
+        input_params: Tuple[int, int, int],
+        attn_params_gqa: Tuple[int, int, int, int],
+    ) -> torch.Tensor:
+        batch_size, seq_len, embed_dim = input_params
+        _, _, _, max_seq_len = attn_params_gqa
+        batch_size += 1
+        return torch.randn(batch_size, seq_len, embed_dim)
+
+    @pytest.fixture
+    def attn_params_mha(self) -> Tuple[int, Optional[int], int, int]:
+        num_heads = 32
+        embed_dim = 4096
+        max_seq_len = 4096
+        return num_heads, None, embed_dim, max_seq_len
+
+    @pytest.fixture
+    def attn_params_mqa(self) -> Tuple[int, int, int, int]:
+        num_heads = 32
+        num_kv_heads = 1
+        embed_dim = 4096
+        max_seq_len = 4096
+        return num_heads, num_kv_heads, embed_dim, max_seq_len
+
+    @pytest.fixture
+    def gqa(self, attn_params_gqa: Tuple[int, int, int, int]) -> MultiHeadAttention:
+        num_heads, num_kv_heads, embed_dim, max_seq_len = attn_params_gqa
+        head_dim = embed_dim // num_heads
+        num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+        rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+        attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            max_seq_len=max_seq_len,
+        )
+
+        fixed_init_model(attn)
+        attn.eval()
+        return attn
+
+    @pytest.fixture
+    def gqa_kv_cache(
+        self, attn_params_gqa: Tuple[int, int, int, int]
+    ) -> MultiHeadAttention:
+        num_heads, num_kv_heads, embed_dim, max_seq_len = attn_params_gqa
+        num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+        head_dim = embed_dim // num_heads
+        kv_cache = KVCache(
+            batch_size=4,
+            max_seq_len=max_seq_len,
+            num_heads=num_heads,
+            head_dim=head_dim,
+            dtype=torch.float32,
+        )
+        rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+        attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            kv_cache=kv_cache,
+            max_seq_len=max_seq_len,
+        )
+        fixed_init_model(attn)
+        attn.eval()
+        return attn
+
+    @pytest.fixture
+    def mha(self, attn_params_mha: Tuple[int, int, int, int]) -> MultiHeadAttention:
+        num_heads, num_kv_heads, embed_dim, max_seq_len = attn_params_mha
+        head_dim = embed_dim // num_heads
+        num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+        rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+        attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            max_seq_len=max_seq_len,
+        )
+        fixed_init_model(attn)
+        attn.eval()
+        return attn
+
+    @pytest.fixture
+    def mha_kv_cache(
+        self, attn_params_mha: Tuple[int, int, int, int]
+    ) -> MultiHeadAttention:
+        num_heads, num_kv_heads, embed_dim, max_seq_len = attn_params_mha
+        head_dim = embed_dim // num_heads
+        num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+        kv_cache = KVCache(
+            batch_size=4,
+            max_seq_len=max_seq_len,
+            num_heads=num_heads,
+            head_dim=head_dim,
+            dtype=torch.float32,
+        )
+        rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+        attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            kv_cache=kv_cache,
+            max_seq_len=max_seq_len,
+        )
+        fixed_init_model(attn)
+        attn.eval()
+        return attn
+
+    @pytest.fixture
+    def mqa(self, attn_params_mqa: Tuple[int, int, int, int]) -> MultiHeadAttention:
+        num_heads, num_kv_heads, embed_dim, max_seq_len = attn_params_mqa
+        head_dim = embed_dim // num_heads
+        num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+        rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+        attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            max_seq_len=max_seq_len,
+        )
+        fixed_init_model(attn)
+        attn.eval()
+        return attn
+
+    @pytest.fixture
+    def mqa_kv_cache(
+        self, attn_params_mqa: Tuple[int, int, int, int]
+    ) -> MultiHeadAttention:
+        num_heads, num_kv_heads, embed_dim, max_seq_len = attn_params_mqa
+        head_dim = embed_dim // num_heads
+        num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+        kv_cache = KVCache(
+            batch_size=4,
+            max_seq_len=max_seq_len,
+            num_heads=num_heads,
+            head_dim=head_dim,
+            dtype=torch.float32,
+        )
+        rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+        attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            kv_cache=kv_cache,
+            max_seq_len=max_seq_len,
+        )
+        fixed_init_model(attn)
+        attn.eval()
+        return attn
+
+    def test_forward_gqa(self, input: torch.Tensor, gqa: MultiHeadAttention) -> None:
+        with torch.no_grad():
+            output = gqa(input, input)
+        assert_expected(
+            output.mean(), torch.tensor(-2545.42236328125), atol=1e-8, rtol=1e-3
+        )
+        assert_expected(output.shape, input.shape)
+
+    def test_forward_gqa_kv_cache(
+        self, input: torch.Tensor, gqa_kv_cache: MultiHeadAttention, attn_params_gqa
+    ) -> None:
+
+        _, _, _, max_seq_len = attn_params_gqa
+        _, seq_len, _ = input.shape
+
+        causal_mask = torch.tril(torch.ones(max_seq_len, max_seq_len, dtype=torch.bool))
+        input_pos = torch.arange(seq_len)
+        mask = causal_mask[None, input_pos]
+
+        with torch.no_grad():
+            output = gqa_kv_cache(input, input, mask=mask, input_pos=input_pos)
+        assert_expected(
+            output.mean(), torch.tensor(-2545.42236328125), atol=1e-8, rtol=1e-3
+        )
+        assert_expected(output.shape, input.shape)
+
+    def test_forward_mha(self, input: torch.Tensor, mha: MultiHeadAttention) -> None:
+        with torch.no_grad():
+            output = mha(input, input)
+        assert_expected(
+            output.mean(), torch.tensor(-2597.248046875), atol=1e-8, rtol=1e-3
+        )
+        assert_expected(output.shape, input.shape)
+
+    def test_forward_mha_kv_cache(
+        self, input: torch.Tensor, mha_kv_cache: MultiHeadAttention, attn_params_mha
+    ) -> None:
+
+        _, _, _, max_seq_len = attn_params_mha
+        _, seq_len, _ = input.shape
+
+        causal_mask = torch.tril(torch.ones(max_seq_len, max_seq_len, dtype=torch.bool))
+        input_pos = torch.arange(seq_len)
+        mask = causal_mask[None, input_pos]
+
+        with torch.no_grad():
+            output = mha_kv_cache(input, input, mask=mask, input_pos=input_pos)
+        assert_expected(
+            output.mean(), torch.tensor(-2597.248046875), atol=1e-8, rtol=1e-3
+        )
+        assert_expected(output.shape, input.shape)
+
+    def test_forward_mqa(self, input: torch.Tensor, mqa: MultiHeadAttention) -> None:
+        with torch.no_grad():
+            output = mqa(input, input)
+        assert_expected(
+            output.mean(), torch.tensor(-2108.07666015625), atol=1e-8, rtol=1e-3
+        )
+        assert_expected(output.shape, input.shape)
+
+    def test_forward_mqa_kv_cache(
+        self, input: torch.Tensor, mqa_kv_cache: MultiHeadAttention, attn_params_mqa
+    ) -> None:
+        _, _, _, max_seq_len = attn_params_mqa
+        _, seq_len, _ = input.shape
+
+        causal_mask = torch.tril(torch.ones(max_seq_len, max_seq_len, dtype=torch.bool))
+        input_pos = torch.arange(seq_len)
+        mask = causal_mask[None, input_pos]
+
+        with torch.no_grad():
+            output = mqa_kv_cache(input, input, mask=mask, input_pos=input_pos)
+        assert_expected(
+            output.mean(), torch.tensor(-2108.076660156255), atol=1e-8, rtol=1e-3
+        )
+        assert_expected(output.shape, input.shape)
+
+    def test_max_seq_len_exceeded(
+        self,
+        input_max_len_exceeded: torch.Tensor,
+        gqa: MultiHeadAttention,
+    ) -> None:
+        with pytest.raises(Exception):
+            _ = gqa(input_max_len_exceeded)
+
+    def test_batch_size_exceeded(
+        self,
+        input_max_bs_exceeded: torch.Tensor,
+        gqa_kv_cache: MultiHeadAttention,
+    ) -> None:
+        with pytest.raises(Exception):
+            _ = gqa_kv_cache(input_max_bs_exceeded)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_attention_utils.py marc/third_party/torchtune/tests/torchtune/modules/test_attention_utils.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_attention_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_attention_utils.py	2025-02-20 17:49:30.042025054 -0500
@@ -0,0 +1,146 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.
+
+from unittest import mock
+
+import pytest
+import torch
+from tests.test_utils import gpu_test
+
+from torchtune.modules.attention_utils import (
+    _get_document_ids_from_seq_lens,
+    _sdpa_or_flex_attention,
+    _SUPPORTS_FLEX_ATTENTION,
+    create_block_causal_mask,
+    packed_block_causal_mask,
+)
+
+
+class TestBlockCausalMask:
+    @pytest.fixture
+    def seq_lens(self):
+        return [torch.tensor([2, 3, 1]), torch.tensor([2, 2, 2, 0])]
+
+    def test_get_document_ids_from_seq_lens(self, seq_lens):
+        actual = _get_document_ids_from_seq_lens(seq_lens)
+        expected = torch.tensor([[0, 0, 1, 1, 1, 2], [0, 0, 1, 1, 2, 2]])
+        torch.testing.assert_close(actual, expected)
+
+    def test_create_block_causal_mask(self, seq_lens):
+        actual = create_block_causal_mask(seq_lens)
+        expected = torch.tensor(
+            [
+                [
+                    [1, 0, 0, 0, 0, 0],
+                    [1, 1, 0, 0, 0, 0],
+                    [0, 0, 1, 0, 0, 0],
+                    [0, 0, 1, 1, 0, 0],
+                    [0, 0, 1, 1, 1, 0],
+                    [0, 0, 0, 0, 0, 1],
+                ],
+                [
+                    [1, 0, 0, 0, 0, 0],
+                    [1, 1, 0, 0, 0, 0],
+                    [0, 0, 1, 0, 0, 0],
+                    [0, 0, 1, 1, 0, 0],
+                    [0, 0, 0, 0, 1, 0],
+                    [0, 0, 0, 0, 1, 1],
+                ],
+            ],
+            dtype=torch.bool,
+        )
+        torch.testing.assert_close(actual, expected)
+
+    @mock.patch("torchtune.modules.attention_utils._SUPPORTS_FLEX_ATTENTION", False)
+    def test_packed_block_causal_mask_sdpa(self, seq_lens):
+        actual = packed_block_causal_mask(seq_lens)
+        expected = torch.tensor(
+            [
+                [
+                    [1, 0, 0, 0, 0, 0],
+                    [1, 1, 0, 0, 0, 0],
+                    [0, 0, 1, 0, 0, 0],
+                    [0, 0, 1, 1, 0, 0],
+                    [0, 0, 1, 1, 1, 0],
+                    [0, 0, 0, 0, 0, 1],
+                ],
+                [
+                    [1, 0, 0, 0, 0, 0],
+                    [1, 1, 0, 0, 0, 0],
+                    [0, 0, 1, 0, 0, 0],
+                    [0, 0, 1, 1, 0, 0],
+                    [0, 0, 0, 0, 1, 0],
+                    [0, 0, 0, 0, 1, 1],
+                ],
+            ],
+            dtype=torch.bool,
+        )
+        torch.testing.assert_close(actual, expected)
+
+    @pytest.mark.skipif(
+        not _SUPPORTS_FLEX_ATTENTION,
+        reason="Please install a nightly build of torch (>=2.5.0) to run this test.",
+    )
+    @gpu_test(gpu_count=1)
+    def test_packed_block_causal_mask_flex(self):
+        # create_block_mask requires that seq_len be divisible by 128, the default block size.
+        # see https://github.com/pytorch/pytorch/blob/3bf6be457d40034aa4b603b7ea1b8977051221ed/torch/nn/attention/flex_attention.py#L792  # noqa
+        actual = packed_block_causal_mask(
+            [torch.tensor([64, 64]), torch.tensor([64, 64])]
+        )
+        expected = torch.tensor([[[[1]]], [[[1]]]], device="cuda:0", dtype=torch.int32)
+        torch.testing.assert_close(actual.to_dense(), expected)
+
+
+class TestSDPAOrFlexAttention:
+    @pytest.mark.skipif(
+        not _SUPPORTS_FLEX_ATTENTION,
+        reason="Please install a nightly build of torch (>=2.5.0) to run this test.",
+    )
+    @mock.patch("torchtune.modules.attention_utils.compile_friendly_flex_attention")
+    @mock.patch(
+        "torchtune.modules.attention_utils.nn.functional.scaled_dot_product_attention"
+    )
+    def test_flex_attention(self, mock_sdpa, mock_flex):
+        # [b, n_h, s, h_d]
+        q = torch.ones(2, 1, 3, 4)
+        k = torch.ones(2, 1, 3, 4)
+        v = torch.ones(2, 1, 3, 4)
+        attn_mask = torch.ones(2, 3, 3)
+        dropout_p = 0.0
+        is_causal = False
+
+        # Pretend that mask is actually a BlockMask
+        with mock.patch(
+            "torchtune.modules.attention_utils.isinstance", return_value=True
+        ):
+            _attention_call = _sdpa_or_flex_attention()
+            _ = _attention_call(q, k, v, attn_mask, dropout_p, is_causal)
+            mock_sdpa.assert_not_called()
+            mock_flex.assert_called_with(q, k, v, block_mask=attn_mask)
+        # If mask is not a BlockMask, then we should call SDPA
+        _attention_call = _sdpa_or_flex_attention()
+        _ = _attention_call(q, k, v, attn_mask, dropout_p, is_causal)
+        mock_sdpa.assert_called_once()
+        assert mock_flex.call_count == 1
+
+    @mock.patch("torchtune.modules.attention_utils._SUPPORTS_FLEX_ATTENTION", False)
+    @mock.patch(
+        "torchtune.modules.attention_utils.nn.functional.scaled_dot_product_attention"
+    )
+    def test_sdpa_attention(self, mock_sdpa):
+        # [b, n_h, s, h_d]
+        q = torch.ones(2, 1, 3, 4)
+        k = torch.ones(2, 1, 3, 4)
+        v = torch.ones(2, 1, 3, 4)
+        attn_mask = torch.ones(2, 3, 3)
+        dropout_p = 0.0
+        is_causal = False
+        _attention_call = _sdpa_or_flex_attention()
+        _ = _attention_call(q, k, v, attn_mask, dropout_p, is_causal)
+        mock_sdpa.assert_called_once()
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_cosine_with_warmup.py marc/third_party/torchtune/tests/torchtune/modules/test_cosine_with_warmup.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_cosine_with_warmup.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_cosine_with_warmup.py	2025-02-20 17:49:30.046025061 -0500
@@ -0,0 +1,61 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.
+
+import pytest
+
+import torch
+import torch.optim as optim
+
+from tests.test_utils import assert_expected
+
+from torchtune.training.lr_schedulers import get_cosine_schedule_with_warmup
+
+
+class TestCosineLR:
+    @pytest.fixture
+    def scheduler(self):
+        optimizer = optim.SGD([torch.ones(1)], lr=0.2)
+        scheduler = get_cosine_schedule_with_warmup(
+            optimizer=optimizer,
+            num_warmup_steps=10,
+            num_training_steps=100,
+            num_cycles=1.0,
+        )
+        return scheduler
+
+    def test_cosine_schedule_init(self, scheduler):
+        optimizer = scheduler.optimizer
+        assert_expected(optimizer.param_groups[0]["lr"], 0.0)
+
+    def test_cosine_schedule_mid_warmup(self, scheduler):
+        optimizer = scheduler.optimizer
+        scheduler.last_epoch = 5 - 1
+        optimizer.step()
+        scheduler.step()
+        assert_expected(optimizer.param_groups[0]["lr"], 0.1)
+
+    def test_cosine_schedule_warmup(self, scheduler):
+        optimizer = scheduler.optimizer
+        scheduler.last_epoch = 10 - 1
+        optimizer.step()
+        scheduler.step()
+        assert_expected(optimizer.param_groups[0]["lr"], 0.2)
+
+    def test_cosine_schedule_minimum_value(self, scheduler):
+        optimizer = scheduler.optimizer
+        scheduler.last_epoch = 55 - 1
+        optimizer.step()
+        scheduler.step()
+        assert_expected(optimizer.param_groups[0]["lr"], 0.0)
+
+    def test_cosine_schedule_complete_cycle(self, scheduler):
+        optimizer = scheduler.optimizer
+        scheduler.last_epoch = 100 - 1
+        optimizer.step()
+        scheduler.step()
+        assert_expected(optimizer.param_groups[0]["lr"], 0.2)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_feed_forward.py marc/third_party/torchtune/tests/torchtune/modules/test_feed_forward.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_feed_forward.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_feed_forward.py	2025-02-20 17:49:30.046025061 -0500
@@ -0,0 +1,56 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Tuple
+
+import pytest
+
+import torch
+
+from tests.test_utils import assert_expected, fixed_init_model
+from torch import nn
+
+from torchtune.modules import FeedForward
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(0)
+
+
+class TestFeedForward:
+    """Class for testing FFN implementation."""
+
+    @pytest.fixture
+    def input_params(self) -> Tuple[int, int]:
+        dim = 4096
+        hidden_dim = 11008  # Scaled for SwiGLU
+        return dim, hidden_dim
+
+    @pytest.fixture
+    def input(self, input_params: Tuple[int, int]) -> torch.Tensor:
+        dim, _ = input_params
+        return torch.randn(1, dim)
+
+    @pytest.fixture
+    def ffn(self, input_params: Tuple[int, int]) -> FeedForward:
+        dim, hidden_dim = input_params
+        gate_proj = nn.Linear(dim, hidden_dim, bias=False)
+        down_proj = nn.Linear(hidden_dim, dim, bias=False)
+        up_proj = nn.Linear(dim, hidden_dim, bias=False)
+        ff = FeedForward(
+            gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj
+        ).eval()
+        fixed_init_model(ff)
+        ff.eval()
+        return ff
+
+    def test_forward(self, input: torch.Tensor, ffn: FeedForward) -> None:
+        with torch.no_grad():
+            x_out = ffn(input)
+        assert_expected(x_out.mean(), torch.tensor(251.5356), atol=1e-7, rtol=1e-3)
+        assert_expected(x_out.max(), torch.tensor(503.0614), atol=1e-7, rtol=1e-3)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_kv_cache.py marc/third_party/torchtune/tests/torchtune/modules/test_kv_cache.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_kv_cache.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_kv_cache.py	2025-02-20 17:49:30.050025068 -0500
@@ -0,0 +1,179 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+import torch._dynamo.testing
+from torchtune.modules import KVCache
+
+BSZ = 2
+MAX_SEQ_LEN = 16
+NUM_HEADS = 4
+HEAD_DIM = 256
+DTYPE = torch.float32
+
+
+class TestKVCache:
+    @pytest.fixture()
+    def k_vals_full(self):
+        return (
+            torch.tril(torch.ones(MAX_SEQ_LEN, HEAD_DIM))[
+                None,
+                None,
+                :,
+                :,
+            ]
+            .repeat(BSZ, NUM_HEADS, 1, 1)
+            .to(DTYPE)
+        )
+
+    @pytest.fixture()
+    def v_vals_full(self):
+        return (
+            torch.tril(torch.ones(MAX_SEQ_LEN, HEAD_DIM))[
+                None,
+                None,
+                :,
+                :,
+            ].repeat(BSZ, NUM_HEADS, 1, 1)
+            * 2
+        ).to(DTYPE)
+
+    @pytest.fixture()
+    def kv_cache(self):
+        return KVCache(BSZ, MAX_SEQ_LEN, NUM_HEADS, HEAD_DIM, DTYPE)
+
+    def test_kv_cache_init(self, kv_cache):
+        # kv cache should be init with zero
+        assert (kv_cache.k_cache == 0).all() and (kv_cache.v_cache == 0).all()
+
+    def test_kv_cache_reset(self, kv_cache, k_vals_full, v_vals_full):
+        kv_cache.update(k_vals_full, v_vals_full)
+        kv_cache.reset()
+
+        assert (kv_cache.k_cache == 0).all() and (kv_cache.v_cache == 0).all()
+        assert kv_cache.size == 0
+
+    def test_kv_cache_error_when_bsz_exceeded(self, kv_cache, k_vals_full, v_vals_full):
+        with pytest.raises(ValueError):
+            kv_cache.update(k_vals_full.repeat(4, 1, 1, 1), v_vals_full)
+
+    def test_kv_cache_error_when_seq_len_exceeded(
+        self, kv_cache, k_vals_full, v_vals_full
+    ):
+        with pytest.raises(AssertionError):
+            kv_cache.update(k_vals_full.repeat(1, 1, 4, 1), v_vals_full)
+
+    def test_kv_cache_error_when_seq_len_exceeded_after_update(
+        self, kv_cache, k_vals_full, v_vals_full
+    ):
+        # test that the cache position is correctly being used to check for seq len exceeded
+        # make a valid update filling half the cache
+        kv_cache.update(
+            k_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+            v_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+        )
+        with pytest.raises(
+            AssertionError,
+        ):
+            # now an invalid update exceeding the cache
+            kv_cache.update(k_vals_full, v_vals_full)
+
+    def test_kv_cache_size_update(self, kv_cache, k_vals_full, v_vals_full):
+        # tests that the kv_cache is correctly tracking the cache position
+
+        # make a valid update filling half the cache - like a prefill
+        kv_cache.update(
+            k_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+            v_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+        )
+        assert kv_cache.size == MAX_SEQ_LEN // 2
+        # now one update with the next key and value
+        kv_cache.update(
+            k_vals_full[:, :, (MAX_SEQ_LEN // 2) + 1].unsqueeze(-2),
+            v_vals_full[:, :, (MAX_SEQ_LEN // 2) + 1].unsqueeze(-2),
+        )
+        assert kv_cache.size == (MAX_SEQ_LEN // 2) + 1
+
+    def test_kv_cache_single_update(self, kv_cache, k_vals_full, v_vals_full):
+        # tests that the kv_cache is correctly returning the updated cache values
+        # after a single cache update
+
+        # make a valid update filling half the cache - like a prefill
+        k_out, v_out = kv_cache.update(
+            k_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+            v_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+        )
+
+        expected_k_out = torch.zeros_like(k_vals_full)
+        expected_v_out = torch.zeros_like(v_vals_full)
+
+        expected_k_out[:, :, torch.arange(0, (MAX_SEQ_LEN // 2))] = k_vals_full[
+            :, :, : (MAX_SEQ_LEN // 2)
+        ]
+        expected_v_out[:, :, torch.arange(0, (MAX_SEQ_LEN // 2))] = v_vals_full[
+            :, :, : (MAX_SEQ_LEN // 2)
+        ]
+
+        assert torch.equal(expected_k_out, k_out)
+        assert torch.equal(expected_v_out, v_out)
+
+    def test_kv_cache_multiple_updates(self, kv_cache, k_vals_full, v_vals_full):
+        # tests that the kv_cache is correctly returning the updated cache values
+        # after a single cache update, followed by another cache update with seq_len=1
+
+        # make an update filling half the cache - like a prefill
+        # fills position 0 through to (MAX_SEQ_LEN // 2) - 1
+        kv_cache.update(
+            k_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+            v_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+        )
+
+        # make an update for one more token, which is the value at
+        # (MAX_SEQ_LEN // 2)
+        k_out, v_out = kv_cache.update(
+            k_vals_full[:, :, (MAX_SEQ_LEN // 2)].unsqueeze(2),
+            v_vals_full[:, :, (MAX_SEQ_LEN // 2)].unsqueeze(2),
+        )
+
+        expected_k_out = torch.zeros_like(k_vals_full)
+        expected_v_out = torch.zeros_like(v_vals_full)
+
+        # cache should be incremented by one position
+        expected_k_out[:, :, torch.arange(0, ((MAX_SEQ_LEN // 2) + 1))] = k_vals_full[
+            :, :, : ((MAX_SEQ_LEN // 2) + 1)
+        ]
+        expected_v_out[:, :, torch.arange(0, ((MAX_SEQ_LEN // 2) + 1))] = v_vals_full[
+            :, :, : ((MAX_SEQ_LEN // 2) + 1)
+        ]
+
+        assert torch.equal(expected_k_out, k_out)
+        assert torch.equal(expected_v_out, v_out)
+
+    def test_kv_cache_no_recompiles(self, kv_cache, k_vals_full, v_vals_full):
+        def fn(k_val, v_val):
+            return kv_cache.update(k_val, v_val)
+
+        cnts = torch._dynamo.testing.CompileCounter()
+        # this effectively does torch.compile(fn)
+        fn = torch._dynamo.optimize(cnts, nopython=True)(fn)
+
+        # make an update filling half the cache - like a prefill
+        # fills position 0 through to (MAX_SEQ_LEN // 2) - 1
+        kv_cache.update(
+            k_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+            v_vals_full[:, :, : (MAX_SEQ_LEN // 2)],
+        )
+
+        # now make successive updates for one token position at a time
+        # and ensure there are no recompiles
+        for i in range(MAX_SEQ_LEN // 2):
+            fn(
+                k_vals_full[:, :, (MAX_SEQ_LEN // 2) + i].unsqueeze(2),
+                v_vals_full[:, :, (MAX_SEQ_LEN // 2) + i].unsqueeze(2),
+            )
+
+        assert cnts.frame_count == 1
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_layernorm.py marc/third_party/torchtune/tests/torchtune/modules/test_layernorm.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_layernorm.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_layernorm.py	2025-02-20 17:49:30.054025074 -0500
@@ -0,0 +1,57 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+
+from tests.test_utils import assert_expected
+
+from torchtune.modules.layer_norm import Fp32LayerNorm
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(0)
+
+
+class TestLayerNorm:
+    """
+    Class for testing our LayerNorm, which is just a wrapper around torch.nn.LayerNorm
+    to support fp16 training.
+    """
+
+    @pytest.fixture
+    def dim(self) -> int:
+        return 8
+
+    @pytest.fixture
+    def eps(self) -> float:
+        return 1e-6
+
+    @pytest.fixture
+    def input_random_fp16(self, dim) -> torch.Tensor:
+        return torch.randn(dim, dtype=torch.float16)
+
+    @pytest.fixture
+    def layer_norm(self, dim, eps) -> Fp32LayerNorm:
+        return Fp32LayerNorm(dim, eps=eps)
+
+    def test_forward_fp16(self, layer_norm, input_random_fp16, eps, dim) -> None:
+        output_fp16 = layer_norm(input_random_fp16)
+
+        # assert dtype as fp16
+        assert (
+            output_fp16.dtype == torch.float16
+        ), "Expected output to be fp16, but got {output_fp16.dtype=}"
+
+        # assert value as fp32
+        expected_output = torch.nn.LayerNorm(dim, eps=eps)(input_random_fp16.float())
+        output_fp32 = layer_norm(input_random_fp16.float())
+        assert_expected(
+            output_fp32.mean(), expected_output.mean(), atol=1e-8, rtol=1e-8
+        )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_position_embeddings.py marc/third_party/torchtune/tests/torchtune/modules/test_position_embeddings.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_position_embeddings.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_position_embeddings.py	2025-02-20 17:49:30.058025081 -0500
@@ -0,0 +1,179 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Tuple
+
+import pytest
+import torch
+
+from tests.test_utils import assert_expected, mps_ignored_test
+from torch import tensor
+from torchtune.models.phi3 import Phi3RotaryPositionalEmbeddings
+
+from torchtune.modules.position_embeddings import RotaryPositionalEmbeddings
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(0)
+
+
+class TestRotaryPositionEmbedding:
+    """
+    Class for testing our Rotary Positional Embeddings (RoPE)
+    implementation. The expected tensors are computed from the
+    reference implementation here:
+    https://github.com/facebookresearch/llama/blob/main/llama/model.py#L450
+    """
+
+    EXPECTED_X_OUT_MEAN = tensor(6.4543e-05)
+    EXPECTED_X_OUT_SUM = tensor(2165.7053)
+    EXPECTED_X_OUT_MAX = tensor(5.4546)
+
+    @pytest.fixture
+    def input_params(self) -> Tuple[int, int, int, int]:
+        bsz = 4
+        num_heads = 32
+        embed_dim = 4096
+        head_dim = embed_dim // num_heads
+        seq_len = 2048
+        max_seq_len = 4096
+        return bsz, num_heads, head_dim, seq_len, max_seq_len
+
+    @pytest.fixture
+    def input(self, input_params: Tuple[int, int, int, int]) -> tensor:
+        bsz, num_heads, head_dim, seq_len, _ = input_params
+        return torch.randn(bsz, seq_len, num_heads, head_dim)
+
+    @pytest.fixture
+    def rope(
+        self, input_params: Tuple[int, int, int, int]
+    ) -> RotaryPositionalEmbeddings:
+        _, _, head_dim, _, max_seq_len = input_params
+        return RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+
+    @mps_ignored_test()
+    def test_forward(self, input: tensor, rope: RotaryPositionalEmbeddings) -> None:
+        x_out = rope(input)
+
+        # check the numerics of the computed tensor
+        assert_expected(x_out.mean(), self.EXPECTED_X_OUT_MEAN)
+        assert_expected(x_out.sum(), self.EXPECTED_X_OUT_SUM)
+        assert_expected(x_out.max(), self.EXPECTED_X_OUT_MAX)
+
+        # check shapes
+        assert_expected(x_out.shape, input.shape)
+
+    @mps_ignored_test()
+    def test_forward_with_curr_pos(
+        self, input: tensor, rope: RotaryPositionalEmbeddings
+    ) -> None:
+        (
+            _,
+            seq_len,
+            _,
+            _,
+        ) = input.shape
+        x_out = rope(input, input_pos=torch.arange(seq_len))
+
+        # these values should be exactly the same as test_forward
+        # since in this case input_pos covers the entire input
+        # sequence. This tests that input_pos works as expected i.e.
+        # extracts the embeddings for the relevant positions
+        assert_expected(x_out.mean(), self.EXPECTED_X_OUT_MEAN, atol=1e-4)
+        assert_expected(x_out.sum(), self.EXPECTED_X_OUT_SUM)
+        assert_expected(x_out.max(), self.EXPECTED_X_OUT_MAX)
+
+        # check shapes
+        assert_expected(x_out.shape, input.shape)
+
+    @mps_ignored_test()
+    def test_forward_with_packed_pos(
+        self, input: tensor, rope: RotaryPositionalEmbeddings
+    ) -> None:
+        """
+        Use input_pos to indicate positions of each token relative to its sequence
+        when sample is packed.
+        """
+        (
+            bsz,
+            seq_len,
+            _,
+            _,
+        ) = input.shape
+        x_out = rope(
+            input, input_pos=torch.arange(seq_len).unsqueeze(0).expand(bsz, seq_len)
+        )
+
+        # these values should be exactly the same as test_forward
+        # AND test_forward_with_current_pos. In this case input_pos
+        # covers the entire batch dim and is defined for each sample separately.
+        # This tests that input_pos works as expected i.e.
+        # extracts the embeddings for the relevant positions for each sample
+        assert_expected(x_out.mean(), self.EXPECTED_X_OUT_MEAN, atol=1e-4)
+        assert_expected(x_out.sum(), self.EXPECTED_X_OUT_SUM)
+        assert_expected(x_out.max(), self.EXPECTED_X_OUT_MAX)
+
+        # check shapes
+        assert_expected(x_out.shape, input.shape)
+
+    def test_rope_init_meta_device(self, input_params):
+        _, _, head_dim, _, max_seq_len = input_params
+        rope_on_device = RotaryPositionalEmbeddings(
+            dim=head_dim, max_seq_len=max_seq_len
+        )
+        with torch.device("meta"):
+            meta_rope = RotaryPositionalEmbeddings(
+                dim=head_dim, max_seq_len=max_seq_len
+            )
+
+        meta_rope.rope_init()
+        for p1, p2 in zip(rope_on_device.buffers(), meta_rope.buffers()):
+            torch.testing.assert_close(p1, p2)
+
+
+class TestPhi3RotaryPositionalEmbeddings:
+    """
+    Class for testing the Phi3 models RoPE Embeddings. The expected tensors are
+    computed from the reference implementation here:
+    https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/modeling_phi3.py
+    """
+
+    @pytest.fixture
+    def input_params(self) -> Tuple[int, int, int, int]:
+        bsz = 4
+        num_heads = 32
+        embed_dim = 3072
+        seq_len = 60
+        max_seq_len = 4096
+        head_dim = embed_dim // num_heads
+        return bsz, num_heads, head_dim, seq_len, max_seq_len
+
+    @pytest.fixture
+    def input(self, input_params: Tuple[int, int, int, int]) -> tensor:
+        bsz, num_heads, head_dim, seq_len, _ = input_params
+        return torch.randn(bsz, seq_len, num_heads, head_dim)
+
+    @pytest.fixture
+    def rope_phi3(
+        self, input_params: Tuple[int, int, int, int]
+    ) -> Phi3RotaryPositionalEmbeddings:
+        _, _, head_dim, _, max_seq_len = input_params
+        return Phi3RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+
+    @mps_ignored_test()
+    def test_forward(
+        self, input: tensor, rope_phi3: Phi3RotaryPositionalEmbeddings
+    ) -> None:
+        x_out = rope_phi3(input)
+
+        # check the numerics of the computed tensor
+        assert_expected(x_out.mean(), tensor(-0.0005), atol=1e-4)
+        assert_expected(x_out.sum(), tensor(-381.0620))
+
+        # check shapes
+        assert_expected(x_out.shape, input.shape)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_rms_norm.py marc/third_party/torchtune/tests/torchtune/modules/test_rms_norm.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_rms_norm.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_rms_norm.py	2025-02-20 17:49:30.062025088 -0500
@@ -0,0 +1,71 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+
+from tests.test_utils import assert_expected
+from torch.nn.functional import normalize
+
+from torchtune.modules.rms_norm import RMSNorm
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(0)
+
+
+class TestRMSNorm:
+    """
+    Class for testing our RMSNorm implementation. Expected tensors
+    are generated using torch.nn.functional.normalization:
+
+    RMSNorm(x) = normalize(x, p=2, dim=-1) * (dim ** 0.5)
+    """
+
+    @pytest.fixture
+    def dim(self) -> int:
+        return 8
+
+    @pytest.fixture
+    def eps(self) -> float:
+        return 1e-6
+
+    @pytest.fixture
+    def input_ones(self, dim) -> torch.Tensor:
+        return torch.ones(dim, dtype=torch.float)
+
+    @pytest.fixture
+    def input_random(self, dim) -> torch.Tensor:
+        return torch.randn(dim, dtype=torch.float)
+
+    @pytest.fixture
+    def input_random_fp16(self, dim) -> torch.Tensor:
+        return torch.randn(dim, dtype=torch.float16)
+
+    @pytest.fixture
+    def rms_norm(self, dim, eps) -> RMSNorm:
+        return RMSNorm(dim, eps=eps)
+
+    def test_forward(self, rms_norm, input_ones, input_random, dim) -> None:
+        output_ones = rms_norm(input_ones)
+        output_random = rms_norm(input_random)
+
+        expected_random = normalize(input_random, p=2, dim=-1) * (dim**0.5)
+
+        assert_expected(output_ones, input_ones)
+        assert_expected(output_random, expected_random)
+
+    def test_forward_fp16(self, rms_norm, input_random_fp16, dim) -> None:
+        output_fp16 = rms_norm(input_random_fp16)
+
+        # convert input to float since rms_norm computes in fp32
+        expected_fp16 = normalize(input_random_fp16.float(), p=2, dim=-1) * (dim**0.5)
+
+        assert_expected(output_fp16, expected_fp16, atol=1e-7, rtol=1e-3)
+        assert output_fp16.dtype == torch.float32
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_transformer_decoder.py marc/third_party/torchtune/tests/torchtune/modules/test_transformer_decoder.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_transformer_decoder.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_transformer_decoder.py	2025-02-20 17:49:30.066025094 -0500
@@ -0,0 +1,468 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Tuple
+
+import pytest
+
+import torch
+from tests.test_utils import assert_expected, mps_ignored_test
+
+from torch import nn
+
+from torchtune.models.llama2 import llama2
+from torchtune.models.llama2._component_builders import llama2_mlp
+
+from torchtune.models.llama2._model_utils import scale_hidden_dim_for_mlp
+from torchtune.modules import (
+    FeedForward,
+    MultiHeadAttention,
+    RMSNorm,
+    RotaryPositionalEmbeddings,
+    TanhGate,
+    TransformerCrossAttentionLayer,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+from torchtune.training.seed import set_seed
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestTransformerSelfAttentionLayer:
+    """
+    Class for testing our TransformerSelfAttentionLayer implementation.
+
+    The expected tensors are computed from the reference implementation
+    below by using the same seed, same params and same initialization used
+    in the fixtures below.
+    https://github.com/facebookresearch/llama/blob/main/llama/model.py#L351
+    """
+
+    @pytest.fixture
+    def input_params(self) -> Tuple[int, int, int]:
+        batch_size = 4
+        seq_len = 2048
+        embed_dim = 4096
+        return batch_size, seq_len, embed_dim
+
+    @pytest.fixture
+    def input(self, input_params: Tuple[int, int, int]) -> torch.Tensor:
+        batch_size, seq_len, embed_dim = input_params
+        return torch.randn(batch_size, seq_len, embed_dim)
+
+    @pytest.fixture
+    def layer_params(self) -> Tuple[int, int, int, int]:
+        num_heads = 32
+        num_kv_heads = 8
+        embed_dim = 4096
+        max_seq_len = 4096
+        return num_heads, num_kv_heads, embed_dim, max_seq_len
+
+    @pytest.fixture
+    def transformer_layer(
+        self, layer_params: Tuple[int, int, int, int]
+    ) -> TransformerSelfAttentionLayer:
+        num_heads, num_kv_heads, embed_dim, max_seq_len = layer_params
+        head_dim = embed_dim // num_heads
+        rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+        self_attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            max_seq_len=max_seq_len,
+        )
+        hidden_dim = scale_hidden_dim_for_mlp(embed_dim)
+        mlp = llama2_mlp(dim=embed_dim, hidden_dim=hidden_dim)
+        transformer_layer = TransformerSelfAttentionLayer(
+            attn=self_attn,
+            mlp=mlp,
+            sa_norm=RMSNorm(dim=embed_dim),
+            mlp_norm=RMSNorm(dim=embed_dim),
+        )
+        # TODO: fix weight initialization to use fixed_init_model
+        for p in transformer_layer.parameters():
+            nn.init.constant_(p, 0.05)
+        transformer_layer.eval()
+        return transformer_layer
+
+    @mps_ignored_test()
+    def test_forward(
+        self, input: torch.Tensor, transformer_layer: TransformerSelfAttentionLayer
+    ) -> None:
+        with torch.no_grad():
+            output = transformer_layer(input)
+        assert_expected(output.mean(), torch.tensor(18261.0156), atol=1e-8, rtol=1e-3)
+        assert_expected(output.shape, input.shape)
+
+
+class TestTransformerCrossAttentionLayer:
+    """
+    Class for testing our TransformerCrossAttentionLayer implementation.
+    The expected tensors are computed from the reference implementation
+    below by using the same seed, same params and same initialization used
+    in the fixtures below.
+    """
+
+    @pytest.fixture
+    def input_params(self) -> Tuple[int, int, int, int]:
+        batch_size = 2
+        seq_len = 8
+        encoder_seq_len = 128
+        embed_dim = 4096
+        return batch_size, seq_len, encoder_seq_len, embed_dim
+
+    @pytest.fixture
+    def input(self, input_params: Tuple[int, int, int, int]) -> torch.Tensor:
+        batch_size, seq_len, encoder_seq_len, embed_dim = input_params
+        rand_x = torch.randn(batch_size, seq_len, embed_dim)
+        rand_y = torch.randn(batch_size, 128, embed_dim)
+        mask = torch.ones(batch_size, seq_len, 128, dtype=torch.bool)
+        mask[:, : seq_len // 2] = False
+        return rand_x, rand_y, mask
+
+    @pytest.fixture
+    def layer_params(self) -> Tuple[int, int, int, int]:
+        num_heads = 32
+        num_kv_heads = 8
+        embed_dim = 4096
+        max_seq_len = 4096
+        return num_heads, num_kv_heads, embed_dim, max_seq_len
+
+    @pytest.fixture
+    def transformer_layer(
+        self, layer_params: Tuple[int, int, int, int]
+    ) -> TransformerCrossAttentionLayer:
+        num_heads, num_kv_heads, embed_dim, max_seq_len = layer_params
+        head_dim = embed_dim // num_heads
+        attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            q_norm=RMSNorm(dim=head_dim, eps=1e-05),
+            k_norm=RMSNorm(dim=head_dim, eps=1e-05),
+            pos_embeddings=None,
+            max_seq_len=max_seq_len,
+            is_causal=False,
+            attn_dropout=0.0,
+        )
+        hidden_dim = scale_hidden_dim_for_mlp(embed_dim * 1.25, 1024)
+        gate_proj = nn.Linear(embed_dim, hidden_dim, bias=False)
+        down_proj = nn.Linear(hidden_dim, embed_dim, bias=False)
+        up_proj = nn.Linear(embed_dim, hidden_dim, bias=False)
+        mlp = FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj)
+
+        transformer_layer = TransformerCrossAttentionLayer(
+            attn=attn,
+            mlp=mlp,
+            ca_norm=RMSNorm(dim=embed_dim),
+            mlp_norm=RMSNorm(dim=embed_dim),
+            ca_scale=TanhGate(),
+            mlp_scale=TanhGate(),
+        )
+        # TODO: fix weight initialization to use fixed_init_model
+        for p in transformer_layer.parameters():
+            nn.init.constant_(p, 0.05)
+        transformer_layer.eval()
+        return transformer_layer
+
+    @mps_ignored_test()
+    def test_forward(
+        self,
+        input: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],
+        transformer_layer: TransformerSelfAttentionLayer,
+    ) -> None:
+        input_x, input_y, mask = input
+        with torch.no_grad():
+            output = transformer_layer(
+                input_x, encoder_input=input_y, encoder_mask=mask
+            )
+        assert_expected(output.mean(), torch.tensor(1.7762), atol=1e-8, rtol=1e-3)
+        assert_expected(output.shape, input_x.shape)
+
+
+class TestTransformerDecoder:
+    """
+    Class for testing our TransformerSelfAttentionLayer implementation.
+
+    The expected tensors are computed from the reference implementation
+    below by using the same seed, same params and same initialization used
+    in the fixtures below.
+    https://github.com/facebookresearch/llama/blob/main/llama/model.py#L413
+    """
+
+    @pytest.fixture
+    def input_params(self) -> Tuple[int, int, int]:
+        batch_size = 4
+        seq_len = 512
+        vocab_size = 256
+        return batch_size, seq_len, vocab_size
+
+    @pytest.fixture
+    def input(self, input_params: Tuple[int, int, int]) -> torch.Tensor:
+        batch_size, seq_len, vocab_size = input_params
+        return torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len))
+
+    @pytest.fixture
+    def causal_mask(self, input_params: Tuple[int, int, int]) -> torch.Tensor:
+        batch_size, seq_len, _ = input_params
+        return (
+            torch.tril(torch.ones((seq_len, seq_len)))
+            .unsqueeze(0)
+            .repeat(batch_size, 1, 1)
+        )
+
+    @pytest.fixture
+    def input_pos(self, input_params: Tuple[int, int, int]) -> torch.Tensor:
+        batch_size, seq_len, _ = input_params
+        return torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1)
+
+    @pytest.fixture
+    def decoder_params(self) -> Tuple[int, int, int, int, int, int]:
+        vocab_size = 256
+        embed_dim = 512
+        num_layers = 2
+        num_heads = 8
+        max_seq_len = 512
+        num_kv_heads = 8
+        return vocab_size, embed_dim, num_layers, num_heads, max_seq_len, num_kv_heads
+
+    @pytest.fixture
+    def input_max_len_exceeded(
+        self,
+        input_params: Tuple[int, int, int],
+        decoder_params: Tuple[int, int, int, int, int, int],
+    ) -> torch.Tensor:
+        batch_size, seq_len, vocab_size = input_params
+        _, _, _, _, max_seq_len, _ = decoder_params
+        seq_len = max_seq_len + 1
+        return torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len))
+
+    @pytest.fixture
+    def input_max_bs_exceeded(
+        self,
+        input_params: Tuple[int, int, int],
+        decoder_params: Tuple[int, int, int, int, int, int],
+    ) -> torch.Tensor:
+        batch_size, seq_len, vocab_size = input_params
+        _, _, _, _, max_seq_len, _ = decoder_params
+        batch_size = batch_size + 1
+        return torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len))
+
+    @pytest.fixture
+    def decoder(
+        self, decoder_params: Tuple[int, int, int, int, int, int]
+    ) -> TransformerDecoder:
+        (
+            vocab_size,
+            embed_dim,
+            num_layers,
+            num_heads,
+            max_seq_len,
+            num_kv_heads,
+        ) = decoder_params
+        decoder = llama2(
+            vocab_size=vocab_size,
+            num_layers=num_layers,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            embed_dim=embed_dim,
+            max_seq_len=max_seq_len,
+        )
+        # TODO: fix weight initialization to use fixed_init_model
+        for p in decoder.parameters():
+            nn.init.constant_(p, 0.2)
+        decoder.eval()
+        return decoder
+
+    @pytest.fixture
+    def decoder_with_kv_cache_enabled(
+        self, decoder_params: Tuple[int, int, int, int, int, int]
+    ) -> TransformerDecoder:
+        (
+            vocab_size,
+            embed_dim,
+            num_layers,
+            num_heads,
+            max_seq_len,
+            num_kv_heads,
+        ) = decoder_params
+        decoder = llama2(
+            vocab_size=vocab_size,
+            num_layers=num_layers,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            embed_dim=embed_dim,
+            max_seq_len=max_seq_len,
+        )
+        # TODO: fix weight initialization to use fixed_init_model
+        for p in decoder.parameters():
+            nn.init.constant_(p, 0.2)
+        decoder.eval()
+        decoder.setup_caches(batch_size=4, dtype=torch.float32)
+        return decoder
+
+    @mps_ignored_test()
+    def test_forward(
+        self,
+        input: torch.Tensor,
+        input_params: Tuple[int, int, int],
+        decoder: TransformerDecoder,
+    ) -> None:
+        batch_size, seq_len, vocab_size = input_params
+        with torch.no_grad():
+            output = decoder(input)
+        assert_expected(output.mean(), torch.tensor(20.4800), atol=1e-8, rtol=1e-6)
+        assert_expected(output.shape, torch.Size([batch_size, seq_len, vocab_size]))
+
+    def test_max_seq_len_exceeded(
+        self,
+        input_max_len_exceeded: torch.Tensor,
+        decoder: TransformerDecoder,
+    ) -> None:
+        with pytest.raises(Exception):
+            output = decoder(input_max_len_exceeded)
+
+    def test_kv_cache(
+        self,
+        input: torch.Tensor,
+        causal_mask: torch.Tensor,
+        input_pos: torch.Tensor,
+        decoder_with_kv_cache_enabled: TransformerDecoder,
+        decoder: TransformerDecoder,
+    ) -> None:
+        _, seq_len = input.shape
+        with torch.no_grad():
+            output_cache = decoder_with_kv_cache_enabled(
+                input, mask=causal_mask, input_pos=input_pos
+            )
+            output_no_cache = decoder(input)
+        assert_expected(output_cache.mean(), output_no_cache.mean())
+
+    def test_kv_cache_reset_values(
+        self,
+        input: torch.Tensor,
+        causal_mask: torch.Tensor,
+        input_pos: torch.Tensor,
+        decoder_with_kv_cache_enabled: TransformerDecoder,
+    ) -> None:
+
+        with torch.no_grad():
+            _ = decoder_with_kv_cache_enabled(
+                input, mask=causal_mask, input_pos=input_pos
+            )
+            kv_cache_k_val = decoder_with_kv_cache_enabled.layers[
+                0
+            ].attn.kv_cache.k_cache.clone()
+            kv_cache_v_val = decoder_with_kv_cache_enabled.layers[
+                0
+            ].attn.kv_cache.v_cache.clone()
+
+        decoder_with_kv_cache_enabled.reset_caches()
+        kv_cache_k_val_reset = decoder_with_kv_cache_enabled.layers[
+            0
+        ].attn.kv_cache.k_cache.clone()
+        kv_cache_v_val_reset = decoder_with_kv_cache_enabled.layers[
+            0
+        ].attn.kv_cache.v_cache.clone()
+
+        assert not torch.allclose(kv_cache_k_val, kv_cache_k_val_reset)
+        assert not torch.allclose(kv_cache_v_val, kv_cache_v_val_reset)
+
+    def test_kv_cache_reset_values_fails_when_not_enabled_first(
+        self,
+        decoder: TransformerDecoder,
+    ) -> None:
+        with pytest.raises(RuntimeError, match="Key value caches are not setup"):
+            decoder.reset_caches()
+
+    def test_kv_cache_batch_size_exceeded(
+        self,
+        input_max_bs_exceeded: torch.Tensor,
+        causal_mask: torch.Tensor,
+        input_pos: torch.Tensor,
+        decoder_with_kv_cache_enabled: TransformerDecoder,
+    ) -> None:
+
+        with pytest.raises(RuntimeError, match="The size of tensor a"):
+            decoder_with_kv_cache_enabled(
+                input_max_bs_exceeded, mask=causal_mask, input_pos=input_pos
+            )
+
+    def test_kv_cache_setup_no_mask_in_forward(
+        self,
+        input: torch.Tensor,
+        input_pos: torch.Tensor,
+        decoder_with_kv_cache_enabled: TransformerDecoder,
+    ) -> None:
+
+        with pytest.raises(ValueError, match="masks must be provided"):
+            decoder_with_kv_cache_enabled(input, input_pos=input_pos)
+
+    def test_kv_cache_setup_mask_no_input_pos_in_forward(
+        self,
+        input: torch.Tensor,
+        causal_mask: torch.Tensor,
+        decoder_with_kv_cache_enabled: TransformerDecoder,
+    ) -> None:
+
+        with pytest.raises(ValueError, match="input positions must be provided!"):
+            decoder_with_kv_cache_enabled(input, mask=causal_mask)
+
+    def test_kv_cache_setup_encoder_input_no_encoder_mask_in_forward(
+        self,
+        input: torch.Tensor,
+        causal_mask: torch.Tensor,
+        input_pos: torch.Tensor,
+        decoder_with_kv_cache_enabled: TransformerDecoder,
+    ) -> None:
+
+        with pytest.raises(
+            ValueError, match="Use the `encoder_mask` arg to provide a causal mask"
+        ):
+            decoder_with_kv_cache_enabled(
+                input, mask=causal_mask, input_pos=input_pos, encoder_input=input
+            )
+
+    def test_rms_norm_propagation(
+        self, decoder_params: Tuple[int, int, int, int, int, int]
+    ):
+        (
+            vocab_size,
+            embed_dim,
+            num_layers,
+            num_heads,
+            max_seq_len,
+            num_kv_heads,
+        ) = decoder_params
+        rms_norm_eps = 1e-2
+        decoder = llama2(
+            vocab_size=vocab_size,
+            num_layers=num_layers,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            embed_dim=embed_dim,
+            max_seq_len=max_seq_len,
+            norm_eps=rms_norm_eps,
+        )
+        rms_norms = [m for m in decoder.modules() if isinstance(m, RMSNorm)]
+        assert len(rms_norms) > 0
+        for rms_norm in rms_norms:
+            assert rms_norm.eps == rms_norm_eps
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/test_vision_transformer.py marc/third_party/torchtune/tests/torchtune/modules/test_vision_transformer.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/test_vision_transformer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/test_vision_transformer.py	2025-02-20 17:49:30.070025100 -0500
@@ -0,0 +1,212 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+
+from tests.test_utils import assert_expected, fixed_init_model, fixed_init_tensor
+from torchtune.models.clip._component_builders import clip_vision_encoder
+
+
+@pytest.fixture
+def transformer_config():
+    return {
+        "embed_dim": 32,
+        "cls_output_dim": 64,
+        "num_layers": 2,
+        "num_heads": 4,
+        "tile_size": 49,
+        "patch_size": 9,
+        "max_num_tiles": 4,
+        "in_channels": 3,
+        "output_cls_projection": False,
+        "out_indices": None,
+    }
+
+
+@pytest.fixture
+def vision_transformer(transformer_config):
+    vision_transformer = clip_vision_encoder(**transformer_config).eval()
+    fixed_init_model(vision_transformer, min_val=-1, max_val=1)
+    return vision_transformer
+
+
+class TestVisionTransformer:
+    @pytest.fixture(autouse=True)
+    def setup_class(self, transformer_config):
+        self.batch_size = 1
+        self.n_imgs = 2
+        num_channels = transformer_config["in_channels"]
+
+        # generate aspect ratios up to max_num_tiles, shape (bsz, num_conccurent_media, 2)
+        self.aspect_ratio = torch.tensor([[1, 3], [2, 2]]).reshape(
+            self.batch_size, self.n_imgs, 2
+        )
+
+        self.num_tiles = 4
+        assert (
+            self.num_tiles <= transformer_config["max_num_tiles"]
+        ), "For this test to be valid, num_tiles should be <= max_num_tiles"
+        assert (
+            torch.prod(self.aspect_ratio, dim=-1).max() <= self.num_tiles
+        ), "For this test to be vlaid, prod(aspect_ratio).max() should match num_tiles"
+
+        # generate image
+        image = torch.rand(
+            (
+                self.batch_size,
+                self.n_imgs,
+                self.num_tiles,
+                num_channels,
+                transformer_config["tile_size"],
+                transformer_config["tile_size"],
+            )
+        )
+        self.image = fixed_init_tensor(image.shape, min_val=-1, max_val=1)
+
+    @torch.no_grad()
+    def test_vision_transformer_without_hidden_layers(
+        self, vision_transformer, transformer_config
+    ):
+        # call model
+        output, _ = vision_transformer(self.image, self.aspect_ratio)
+
+        # assertion
+        expected_shape = (
+            self.batch_size,
+            self.n_imgs,
+            self.num_tiles,
+            vision_transformer.get_image_tokens_per_tile(),
+            transformer_config["embed_dim"],
+        )
+        assert (
+            output.shape == expected_shape
+        ), f"Expected shape {expected_shape}, but got {output.shape}"
+
+        assert_expected(output.mean(), torch.tensor(1.0172), atol=1e-3, rtol=1e-3)
+
+    def test_fails_if_ar_none_and_multiple_tiles(self, vision_transformer):
+        """
+        If aspect_ratio is none, then num_tiles shouldnt be greater than 1.
+        Here the test passes if something actually fails under these conditions.
+        """
+        assert self.image.shape[2] > 1, "This test is not valid for num_tiles=1"
+        try:
+            vision_transformer(self.image, aspect_ratio=None)
+            pytest.fail(
+                "Expected ValueError: If num_tiles>1, aspect_ratio should not be None"
+            )
+        except ValueError:
+            pass  # If ValueError is raised, the test passes
+
+    @torch.no_grad()
+    def test_vision_transformer_with_cls_projection(self, transformer_config):
+        transformer_config = transformer_config.copy()
+        transformer_config["output_cls_projection"] = True
+
+        # call model
+        model_with_cls = clip_vision_encoder(**transformer_config).eval()
+        fixed_init_model(model_with_cls, min_val=-1, max_val=1)
+        output, _ = model_with_cls(self.image, self.aspect_ratio)
+
+        # assertion
+        expected_shape = (
+            self.batch_size,
+            self.n_imgs,
+            self.num_tiles,
+            1,
+            transformer_config["cls_output_dim"],
+        )
+
+        assert (
+            output.shape == expected_shape
+        ), f"Expected shape {expected_shape}, but got {output.shape}"
+
+        assert_expected(output.mean(), torch.tensor(9.6240), atol=1e-3, rtol=1e-3)
+
+    @torch.no_grad()
+    def test_vision_transformer_return_hidden_layers(self, transformer_config):
+        transformer_config = transformer_config.copy()
+        transformer_config["out_indices"] = [
+            0,
+            1,
+        ]
+
+        # call model
+        model_with_hidden = clip_vision_encoder(**transformer_config)
+        fixed_init_model(model_with_hidden, min_val=-1, max_val=1)
+        x, hidden_layers = model_with_hidden(self.image, self.aspect_ratio)
+
+        # assertion x
+        expected_shape_x = (
+            self.batch_size,
+            self.n_imgs,
+            self.num_tiles,
+            model_with_hidden.get_image_tokens_per_tile(),
+            transformer_config["embed_dim"],
+        )
+
+        assert (
+            x.shape == expected_shape_x
+        ), f"Expected shape {expected_shape_x}, but got {x.shape=}"
+
+        assert_expected(x.mean(), torch.tensor(1.0172), atol=1e-3, rtol=1e-3)
+
+        # assertion hidden
+        num_hidden_layers_expected = len(transformer_config["out_indices"])
+
+        expected_shape_hidden_layers = (
+            self.batch_size,
+            self.n_imgs,
+            self.num_tiles,
+            model_with_hidden.get_image_tokens_per_tile(),
+            transformer_config["embed_dim"],
+        )
+
+        assert (
+            len(hidden_layers) == num_hidden_layers_expected
+        ), f"Expected {num_hidden_layers_expected} hidden layers, but got {len(hidden_layers)}"
+
+        for hidden_layer in hidden_layers:
+            assert (
+                hidden_layer.shape == expected_shape_hidden_layers
+            ), f"Expected shape {expected_shape_hidden_layers}, but got {hidden_layer.shape=}"
+
+        # Target based off of reference nn.TransformerEncoderLayer implementation
+        assert_expected(
+            torch.stack(hidden_layers, dim=-1).mean(),
+            torch.tensor(8.3112),
+            atol=1e-3,
+            rtol=1e-3,
+        )
+
+    @torch.no_grad()
+    def test_vision_transformer_single_tile(self, transformer_config):
+        transformer_config = transformer_config.copy()
+        transformer_config["max_num_tiles"] = 1
+
+        # get single tile: (bsz, n_imgs, 1, num_channels, tile_size, tile_size)
+        images = self.image[:, :, [0], :, :, :]
+
+        # call model
+        model_with_multiple_tiles = clip_vision_encoder(**transformer_config)
+        fixed_init_model(model_with_multiple_tiles, min_val=-1, max_val=1)
+        output, _ = model_with_multiple_tiles(images, aspect_ratio=None)
+
+        # assertion
+        expected_shape = (
+            self.batch_size,
+            self.n_imgs,
+            1,
+            model_with_multiple_tiles.get_image_tokens_per_tile(),
+            transformer_config["embed_dim"],
+        )
+        assert (
+            output.shape == expected_shape
+        ), f"Expected shape {expected_shape}, but got {output.shape}"
+
+        assert_expected(output.mean(), torch.tensor(0.5458), atol=1e-3, rtol=1e-3)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/tokenizers/test_sentencepiece.py marc/third_party/torchtune/tests/torchtune/modules/tokenizers/test_sentencepiece.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/tokenizers/test_sentencepiece.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/tokenizers/test_sentencepiece.py	2025-02-20 17:49:30.074025107 -0500
@@ -0,0 +1,75 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+from tests.common import ASSETS
+from torchtune.modules.tokenizers import SentencePieceBaseTokenizer
+
+
+class TestSentencePieceBaseTokenizer:
+    @pytest.fixture
+    def tokenizer(self):
+        # m.model is a pretrained Sentencepiece model using the following command:
+        # spm.SentencePieceTrainer.train('--input=<TRAIN_FILE> --model_prefix=m --vocab_size=2000')
+        sp_tokenizer = SentencePieceBaseTokenizer(str(ASSETS / "m.model"))
+        return sp_tokenizer
+
+    def test_encode(self, tokenizer):
+        assert tokenizer.encode("Hello world!") == [
+            tokenizer.bos_id,
+            12,
+            1803,
+            1024,
+            103,
+            tokenizer.eos_id,
+        ]
+        assert tokenizer.encode("Hello world!", add_eos=False) == [
+            tokenizer.bos_id,
+            12,
+            1803,
+            1024,
+            103,
+        ]
+        assert tokenizer.encode("Hello world!", add_bos=False) == [
+            12,
+            1803,
+            1024,
+            103,
+            tokenizer.eos_id,
+        ]
+        assert tokenizer.encode("Hello world!", add_eos=False, add_bos=False) == [
+            12,
+            1803,
+            1024,
+            103,
+        ]
+
+    def test_decode(self, tokenizer):
+        assert tokenizer.decode([1, 12, 1803, 1024, 103, 2]) == "Hello world!"
+
+    def test_token_ids(self, tokenizer):
+        assert tokenizer.eos_id == 2
+        assert tokenizer.pad_id == -1
+        assert tokenizer.bos_id == 1
+
+    def test_tokenizer_vocab_size(self, tokenizer):
+        assert tokenizer.vocab_size == 2000
+
+    def test_encode_without_leading_whitespace(self, tokenizer):
+        s1 = "Hello"
+        s2 = "I'm an outgoing and friendly person."
+        # TODO: investigate why test tokenizer model does not encode whitespace
+        tokenizer.encodes_whitespace = True
+        s1_tokens = tokenizer.encode(s1, add_bos=False, add_eos=False)
+        s2_tokens = tokenizer.encode(s2, add_bos=False, add_eos=False)
+        # Set prefix="pre" since "\n" is not in the test tokenizer's vocab
+        s2_tokens_no_whitespace = tokenizer.encode(
+            s2, add_bos=False, add_eos=False, trim_leading_whitespace=True, prefix="pre"
+        )
+        s1s2_tokens = tokenizer.encode(s1 + s2, add_bos=False, add_eos=False)
+        assert (s1_tokens + s2_tokens) != s1s2_tokens
+        assert (s1_tokens + s2_tokens_no_whitespace) == s1s2_tokens
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/tokenizers/test_tiktoken.py marc/third_party/torchtune/tests/torchtune/modules/tokenizers/test_tiktoken.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/tokenizers/test_tiktoken.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/tokenizers/test_tiktoken.py	2025-02-20 17:49:30.078025114 -0500
@@ -0,0 +1,120 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+from tests.common import ASSETS
+from torchtune.models.llama3._tokenizer import CL100K_PATTERN
+from torchtune.modules.tokenizers import TikTokenBaseTokenizer
+
+
+class TestTikTokenBaseTokenizer:
+    @pytest.fixture
+    def tokenizer(self):
+        # Pretrained tiktoken model generated via the script in
+        # https://gist.github.com/ebsmothers/54b133dd87db6679b14318545aaa2de4
+        return TikTokenBaseTokenizer(
+            path=str(ASSETS / "tiktoken_small.model"),
+            name="test_tiktoken",
+            pattern=CL100K_PATTERN,
+            bos_id=0,
+            eos_id=-1,
+            special_tokens={
+                "<|test_token_0|>": 2000,
+                "<|test_token_1|>": 2001,
+            },
+        )
+
+    @pytest.fixture
+    def texts(self):
+        return [
+            "I can see the sun. But even if I cannot see the sun, I know that it exists.",
+            "And to know that the sun is there - that is living.",
+        ]
+
+    @pytest.fixture
+    def token_ids(self):
+        return [
+            73,
+            503,
+            654,
+            262,
+            376,
+            110,
+            46,
+            690,
+            720,
+            428,
+            270,
+            1119,
+            654,
+            262,
+            376,
+            110,
+            44,
+            270,
+            686,
+            334,
+            312,
+            522,
+            511,
+            115,
+            46,
+        ]
+
+    def test_encode(self, tokenizer, texts, token_ids):
+        assert tokenizer.encode(texts[0], add_bos=True, add_eos=True) == [
+            0
+        ] + token_ids + [-1]
+
+    def test_decode(self, tokenizer, texts, token_ids):
+        assert tokenizer.decode(token_ids) == texts[0]
+
+    def test_encode_and_decode(self, tokenizer, texts):
+        token_ids = tokenizer.encode(texts[0], add_bos=False, add_eos=False)
+        decoded_text = tokenizer.decode(token_ids)
+        assert texts[0] == decoded_text
+
+    def test_tokenizer_vocab_size(self, tokenizer):
+        assert tokenizer.base_vocab_size == 2000
+        assert tokenizer.vocab_size == 2002
+
+    def test_split_long_repetitions(self, tokenizer):
+        normal_str = "Here is a normal string"
+        ten_spaces = "".join(10 * [" "])
+        space_str = ten_spaces.join(
+            ["Here", "is", "a", "string", "with", "long", "spaces"]
+        )
+        no_space_str = "".join(10 * ["ab"])
+
+        actual_split = tokenizer._split_long_repetitions(normal_str, 5)
+        expected_split = ["Here is a norma", "l strin", "g"]
+        for actual_substr, expected_substr in zip(actual_split, expected_split):
+            assert actual_substr == expected_substr
+        with pytest.raises(StopIteration):
+            next(actual_split)
+
+        actual_split = tokenizer._split_long_repetitions(space_str, 9)
+        expected_split = [
+            "Here" + ten_spaces[:-1],
+            " is" + ten_spaces[:-1],
+            " a" + ten_spaces[:-1],
+            " string" + ten_spaces[:-1],
+            " with" + ten_spaces[:-1],
+            " long" + ten_spaces[:-1],
+            " spaces",
+        ]
+        for actual_substr, expected_substr in zip(actual_split, expected_split):
+            assert actual_substr == expected_substr
+        with pytest.raises(StopIteration):
+            next(actual_split)
+
+        actual_split = tokenizer._split_long_repetitions(no_space_str, 4)
+        expected_split = ["abab"] * 5
+        for actual_substr, expected_substr in zip(actual_split, expected_split):
+            assert actual_substr == expected_substr
+        with pytest.raises(StopIteration):
+            next(actual_split)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/tokenizers/test_utils.py marc/third_party/torchtune/tests/torchtune/modules/tokenizers/test_utils.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/tokenizers/test_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/tokenizers/test_utils.py	2025-02-20 17:49:30.082025120 -0500
@@ -0,0 +1,57 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+from tests.test_utils import DummyTokenizer
+from torchtune.data import Message
+
+from torchtune.modules.tokenizers import tokenize_messages_no_special_tokens
+
+
+class TestTokenizerUtils:
+    @pytest.fixture
+    def tokenizer(self):
+        return DummyTokenizer(max_seq_len=100)
+
+    @pytest.fixture
+    def messages(self):
+        return [
+            Message(role="user", content="hello world!", masked=True),
+            Message(role="assistant", content="hello back!"),
+        ]
+
+    @pytest.mark.parametrize(
+        "add_bos, add_eos",
+        [
+            (True, True),
+            (False, False),
+        ],
+    )
+    def test_tokenize_no_special_tokens(self, tokenizer, messages, add_bos, add_eos):
+        tokens, mask = tokenize_messages_no_special_tokens(
+            tokenizer,
+            messages,
+            bos_id=tokenizer.bos_id if add_bos else None,
+            eos_id=tokenizer.eos_id if add_eos else None,
+        )
+
+        assert len(tokens) == len(mask)
+
+        # User message should be masked
+        assert mask[0] is True
+        # Assistant message should not be masked
+        assert mask[-1] is False
+
+        if add_bos:
+            assert tokens[0] == tokenizer.bos_id
+        else:
+            assert tokens[0] != tokenizer.bos_id
+
+        if add_eos:
+            assert tokens[-1] == tokenizer.eos_id
+        else:
+            assert tokens[-1] != tokenizer.eos_id
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_get_canvas_best_fit.py marc/third_party/torchtune/tests/torchtune/modules/transforms/test_get_canvas_best_fit.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_get_canvas_best_fit.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/transforms/test_get_canvas_best_fit.py	2025-02-20 17:49:30.086025127 -0500
@@ -0,0 +1,163 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+
+from torchtune.modules.transforms.vision_utils.get_canvas_best_fit import (
+    find_supported_resolutions,
+    get_canvas_best_fit,
+)
+
+
+class TestUtils:
+    @pytest.mark.parametrize(
+        "params",
+        [
+            {
+                "max_num_tiles": 1,
+                "tile_size": 224,
+                "expected_resolutions": [(224, 224)],
+            },
+            {
+                "max_num_tiles": 2,
+                "tile_size": 100,
+                "expected_resolutions": [(100, 200), (200, 100), (100, 100)],
+            },
+            {
+                "max_num_tiles": 3,
+                "tile_size": 50,
+                "expected_resolutions": [
+                    (50, 150),
+                    (150, 50),
+                    (50, 100),
+                    (100, 50),
+                    (50, 50),
+                ],
+            },
+            {
+                "max_num_tiles": 4,
+                "tile_size": 300,
+                "expected_resolutions": [
+                    (300, 1200),
+                    (600, 600),
+                    (300, 300),
+                    (1200, 300),
+                    (300, 900),
+                    (900, 300),
+                    (300, 600),
+                    (600, 300),
+                ],
+            },
+        ],
+    )
+    def test_find_supported_resolutions(self, params):
+        max_num_tiles = params["max_num_tiles"]
+        tile_size = params["tile_size"]
+        expected_resolutions = params["expected_resolutions"]
+        resolutions = find_supported_resolutions(max_num_tiles, tile_size)
+
+        assert len(set(resolutions)) == len(resolutions), "Resolutions should be unique"
+        assert set(resolutions) == set(
+            expected_resolutions
+        ), f"Expected resolutions {expected_resolutions} but got {resolutions}"
+
+    @pytest.mark.parametrize(
+        "params",
+        [
+            {
+                "image_size": (800, 600),
+                "possible_resolutions": [
+                    (224, 896),
+                    (448, 448),
+                    (224, 224),
+                    (896, 224),
+                    (224, 672),
+                    (672, 224),
+                    (224, 448),
+                    (448, 224),
+                ],
+                "resize_to_max_canvax": False,
+                "expected_best_resolution": (448, 448),
+            },
+            {
+                "image_size": (200, 300),
+                "possible_resolutions": [
+                    (224, 896),
+                    (448, 448),
+                    (224, 224),
+                    (896, 224),
+                    (224, 672),
+                    (672, 224),
+                    (224, 448),
+                    (448, 224),
+                ],
+                "resize_to_max_canvax": False,
+                "expected_best_resolution": (224, 448),
+            },
+            {
+                "image_size": (200, 500),
+                "possible_resolutions": [
+                    (224, 896),
+                    (448, 448),
+                    (224, 224),
+                    (896, 224),
+                    (224, 672),
+                    (672, 224),
+                    (224, 448),
+                    (448, 224),
+                ],
+                "resize_to_max_canvax": True,
+                "expected_best_resolution": (224, 672),
+            },
+            {
+                "image_size": (200, 200),
+                "possible_resolutions": [
+                    (224, 896),
+                    (448, 448),
+                    (224, 224),
+                    (896, 224),
+                    (224, 672),
+                    (672, 224),
+                    (224, 448),
+                    (448, 224),
+                ],
+                "resize_to_max_canvax": False,
+                "expected_best_resolution": (224, 224),
+            },
+            {
+                "image_size": (200, 100),
+                "possible_resolutions": [
+                    (224, 896),
+                    (448, 448),
+                    (224, 224),
+                    (896, 224),
+                    (224, 672),
+                    (672, 224),
+                    (224, 448),
+                    (448, 224),
+                ],
+                "resize_to_max_canvax": True,
+                "expected_best_resolution": (448, 224),
+            },
+        ],
+    )
+    def test_get_canvas_best_fit(self, params):
+        image_size = params["image_size"]
+        possible_resolutions = params["possible_resolutions"]
+        expected_best_resolution = params["expected_best_resolution"]
+        resize_to_max_canvax = params["resize_to_max_canvax"]
+
+        possible_resolutions = torch.tensor(possible_resolutions)
+
+        image = torch.rand(*image_size)
+        best_resolution = get_canvas_best_fit(
+            image, possible_resolutions, resize_to_max_canvax
+        )
+
+        assert (
+            tuple(best_resolution) == expected_best_resolution
+        ), f"Expected best resolution {expected_best_resolution} but got {best_resolution}"
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_get_inscribed_size.py marc/third_party/torchtune/tests/torchtune/modules/transforms/test_get_inscribed_size.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_get_inscribed_size.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/transforms/test_get_inscribed_size.py	2025-02-20 17:49:30.090025133 -0500
@@ -0,0 +1,62 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+from torchtune.modules.transforms.vision_utils.get_inscribed_size import (
+    get_inscribed_size,
+)
+
+
+class TestTransforms:
+    @pytest.mark.parametrize(
+        "params",
+        [
+            {
+                "image_size": (200, 100),
+                "target_size": (1000, 1200),
+                "max_size": 600,
+                "expected_inscribed_size": (600, 300),
+            },
+            {
+                "image_size": (2000, 200),
+                "target_size": (1000, 1200),
+                "max_size": 600,
+                "expected_inscribed_size": (1000, 100),
+            },
+            {
+                "image_size": (400, 200),
+                "target_size": (1000, 1200),
+                "max_size": 2000,
+                "expected_inscribed_size": (1000, 500),
+            },
+            {
+                "image_size": (400, 200),
+                "target_size": (1000, 1200),
+                "max_size": None,
+                "expected_inscribed_size": (1000, 500),
+            },
+            {
+                "image_size": (1000, 500),
+                "target_size": (400, 300),
+                "max_size": None,
+                "expected_inscribed_size": (400, 200),
+            },
+        ],
+    )
+    def test_get_inscribed_size(self, params):
+        image_size = params["image_size"]
+        target_size = params["target_size"]
+        max_size = params["max_size"]
+        expected_inscribed_size = params["expected_inscribed_size"]
+
+        inscribed_size = get_inscribed_size(
+            image_size=image_size,
+            target_size=target_size,
+            max_size=max_size,
+        )
+
+        assert inscribed_size == expected_inscribed_size
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_pad_dim_to_size.py marc/third_party/torchtune/tests/torchtune/modules/transforms/test_pad_dim_to_size.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_pad_dim_to_size.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/transforms/test_pad_dim_to_size.py	2025-02-20 17:49:30.090025133 -0500
@@ -0,0 +1,22 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+
+from torchtune.modules.transforms.vision_utils.pad_dim_to_size import pad_dim_to_size
+
+
+def test_pad_dim_to_size():
+    image = torch.ones(2, 2, 2, 2, dtype=torch.float16)
+    image = pad_dim_to_size(image, 4, 1)
+    assert image.shape == (2, 4, 2, 2)
+    assert image.mean() == 0.5, "Expected mean to be 0.5 after padding"
+    assert image.dtype == torch.float16, "Expected dtype to be float16 after padding"
+
+    with pytest.raises(Exception):
+        pad_dim_to_size(image, 2, 1)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_resize_with_pad.py marc/third_party/torchtune/tests/torchtune/modules/transforms/test_resize_with_pad.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_resize_with_pad.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/transforms/test_resize_with_pad.py	2025-02-20 17:49:30.094025140 -0500
@@ -0,0 +1,84 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+import torchvision
+
+from torchtune.modules.transforms.vision_utils.resize_with_pad import resize_with_pad
+
+
+class TestTransforms:
+    @pytest.mark.parametrize(
+        "params",
+        [
+            {
+                "image_size": (200, 100),
+                "target_size": (1000, 1200),
+                "max_size": 600,
+                "expected_resized_size": (600, 300),
+            },
+            {
+                "image_size": (2000, 200),
+                "target_size": (1000, 1200),
+                "max_size": 600,
+                "expected_resized_size": (1000, 100),
+            },
+            {
+                "image_size": (400, 200),
+                "target_size": (1000, 1200),
+                "max_size": 2000,
+                "expected_resized_size": (1000, 500),
+            },
+            {
+                "image_size": (400, 200),
+                "target_size": (1000, 1200),
+                "max_size": None,
+                "expected_resized_size": (1000, 500),
+            },
+            {
+                "image_size": (1000, 500),
+                "target_size": (400, 300),
+                "max_size": None,
+                "expected_resized_size": [400, 200],
+            },
+        ],
+    )
+    def test_resize_with_pad(self, params):
+
+        image_size = params["image_size"]
+        target_size = params["target_size"]
+        max_size = params["max_size"]
+        expected_resized_size = params["expected_resized_size"]
+
+        image = torch.rand(3, *image_size)  # Create a random image tensor
+
+        resized_image = resize_with_pad(
+            image=image,
+            target_size=target_size,
+            resample=torchvision.transforms.InterpolationMode["BILINEAR"],
+            max_size=max_size,
+        )
+
+        # assert everything beyond resize has value == 0
+        assert torch.all(
+            resized_image[:, (expected_resized_size[0] + 1) :, :] == 0
+        ), "Expected everything beyond resize to be pad with fill=0"
+
+        assert torch.all(
+            resized_image[:, :, (expected_resized_size[1] + 1) :] == 0
+        ), "Expected everything beyond resize to be pad with fill=0"
+
+        assert torch.all(
+            resized_image[:, : expected_resized_size[0], : expected_resized_size[1]]
+            != 0
+        ), "Expected no padding where the image is supposed to be"
+
+        # output should have shape target_size
+        assert (
+            resized_image.shape[-2:] == target_size
+        ), f"Expected output with shape {target_size} but got {resized_image.shape[-2:]}"
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_tile_crop.py marc/third_party/torchtune/tests/torchtune/modules/transforms/test_tile_crop.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_tile_crop.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/transforms/test_tile_crop.py	2025-02-20 17:49:30.098025146 -0500
@@ -0,0 +1,81 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+
+import torch
+
+from torchtune.modules.transforms.vision_utils.tile_crop import tile_crop
+
+
+class TestTransforms:
+    @pytest.mark.parametrize(
+        "params",
+        [
+            {
+                "expected_output_shape": torch.Size([24, 3, 50, 50]),
+                "image_size": (3, 200, 300),
+                "status": "Passed",
+                "tile_size": 50,
+            },
+            {
+                "expected_output_shape": torch.Size([6, 3, 200, 200]),
+                "image_size": (3, 400, 600),
+                "status": "Passed",
+                "tile_size": 200,
+            },
+            {
+                "expected_output_shape": torch.Size([1, 3, 250, 250]),
+                "image_size": (3, 250, 250),
+                "status": "Passed",
+                "tile_size": 250,
+            },
+            {
+                "error": "Image size 250x250 is not divisible by tile size 500",
+                "image_size": (3, 250, 250),
+                "status": "Failed",
+                "tile_size": 500,
+            },
+            {
+                "error": "Image size 250x250 is not divisible by tile size 80",
+                "image_size": (3, 250, 250),
+                "status": "Failed",
+                "tile_size": 80,
+            },
+        ],
+    )
+    def test_tile_crop(self, params):
+        image_size = params["image_size"]
+        tile_size = params["tile_size"]
+        status = params["status"]
+
+        image = torch.rand(*image_size)  # Create a random image tensor
+
+        if status == "Passed":
+            tiles = tile_crop(image, tile_size)
+            expected_output_shape = params["expected_output_shape"]
+            assert (
+                tiles.shape == expected_output_shape
+            ), f"Expected shape {expected_output_shape} but got {tiles.shape}"
+
+            # check if first and last tile matches the image
+            first_tile = image[..., :tile_size, :tile_size]
+            last_tile = image[..., -tile_size:, -tile_size:]
+            assert torch.equal(
+                tiles[0], first_tile
+            ), "Expected first tile to match the image"
+            assert torch.equal(
+                tiles[-1], last_tile
+            ), "Expected last tile to match the image"
+
+        elif status == "Failed":
+            with pytest.raises(Exception) as exc_info:
+                tile_crop(image, tile_size)
+            expected_error = params["error"]
+            actual_error = str(exc_info.value)
+            assert (
+                str(exc_info.value) == params["error"]
+            ), f"Expected error message '{expected_error}' but got '{actual_error}'"
diff -ruN marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_transforms.py marc/third_party/torchtune/tests/torchtune/modules/transforms/test_transforms.py
--- marc_original/third_party/torchtune/tests/torchtune/modules/transforms/test_transforms.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/modules/transforms/test_transforms.py	2025-02-20 17:49:30.102025153 -0500
@@ -0,0 +1,103 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from torchtune.modules.transforms import VisionCrossAttentionMask
+
+
+IMAGE_TOKEN_ID = 1
+MAX_NUM_TILES = 4
+
+
+class TestVisionCrossAttentionMask:
+    @pytest.fixture
+    def num_tiles(self):
+        return 2
+
+    @pytest.fixture
+    def tile_size(self):
+        return 4
+
+    @pytest.fixture
+    def patch_size(self):
+        return 2
+
+    @pytest.fixture
+    def image_num_tokens(self, num_tiles, tile_size, patch_size):
+        return ((tile_size // patch_size) ** 2 + 1) * num_tiles
+
+    @pytest.fixture
+    def tokens(self):
+        # This tests image tokens not at start, consecutive images, and image
+        # with text until end.
+        # text = 2, image = 1
+        return [2, 2, IMAGE_TOKEN_ID, IMAGE_TOKEN_ID, 2, 2, IMAGE_TOKEN_ID, 2, 2]
+
+    @pytest.fixture
+    def images(self, num_tiles, tokens):
+        n_img = len([token_id for token_id in tokens if token_id == IMAGE_TOKEN_ID])
+        n_channels = 3
+        tile_size = 2
+        return [
+            torch.ones(num_tiles, n_channels, tile_size, tile_size)
+            for _ in range(n_img)
+        ]
+
+    @pytest.fixture
+    def cross_attn_mask_transform(self, tile_size, patch_size):
+        # patches per tile = 4
+        return VisionCrossAttentionMask(
+            tile_size=tile_size,
+            patch_size=patch_size,
+            image_token_id=IMAGE_TOKEN_ID,
+            max_num_tiles=MAX_NUM_TILES,
+        )
+
+    def test_get_image_attention_intervals(self, cross_attn_mask_transform, tokens):
+        actual = cross_attn_mask_transform._get_image_attention_intervals(tokens)
+        expected = [[2, 6], [3, 6], [6, 9]]
+        assert actual == expected
+
+    def test_call(self, cross_attn_mask_transform, tokens, images, image_num_tokens):
+        sample = {"tokens": tokens, "encoder_input": {"images": images}}
+        dummy_kwargs = {"hello": 8}
+        sample.update(dummy_kwargs)
+        actual = cross_attn_mask_transform(sample)
+        expected = [
+            torch.zeros(len(tokens), image_num_tokens, dtype=torch.bool)
+            for _ in range(len(images))
+        ]
+        expected[0][2:6, :] = True
+        expected[1][3:6, :] = True
+        expected[2][6:9, :] = True
+        for i in range(len(images)):
+            torch.testing.assert_close(actual["encoder_mask"][i], expected[i])
+            torch.testing.assert_close(actual["encoder_input"]["images"][i], images[i])
+
+        assert actual["tokens"] == tokens
+        assert actual["hello"] == dummy_kwargs["hello"]
+
+    def test_inference_call(
+        self, cross_attn_mask_transform, tokens, images, image_num_tokens
+    ):
+        sample = {"tokens": tokens, "encoder_input": {"images": images}}
+        dummy_kwargs = {"hello": 8}
+        sample.update(dummy_kwargs)
+        actual = cross_attn_mask_transform(sample, inference=True)
+        expected = [
+            torch.zeros(len(tokens), image_num_tokens * 2, dtype=torch.bool)
+            for _ in range(len(images))
+        ]
+        expected[0][2:6, :image_num_tokens] = True
+        expected[1][3:6, :image_num_tokens] = True
+        expected[2][6:9, :image_num_tokens] = True
+        for i in range(len(images)):
+            torch.testing.assert_close(actual["encoder_mask"][i], expected[i])
+            torch.testing.assert_close(actual["encoder_input"]["images"][i], images[i])
+
+        assert actual["tokens"] == tokens
+        assert actual["hello"] == dummy_kwargs["hello"]
diff -ruN marc_original/third_party/torchtune/tests/torchtune/rlhf/__init__.py marc/third_party/torchtune/tests/torchtune/rlhf/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/rlhf/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/rlhf/__init__.py	2025-02-20 17:49:30.106025160 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/rlhf/loss/__init__.py marc/third_party/torchtune/tests/torchtune/rlhf/loss/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/rlhf/loss/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/rlhf/loss/__init__.py	2025-02-20 17:49:30.110025166 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/rlhf/loss/test_dpo_loss.py marc/third_party/torchtune/tests/torchtune/rlhf/loss/test_dpo_loss.py
--- marc_original/third_party/torchtune/tests/torchtune/rlhf/loss/test_dpo_loss.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/rlhf/loss/test_dpo_loss.py	2025-02-20 17:49:30.114025173 -0500
@@ -0,0 +1,125 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from torchtune.rlhf.loss import DPOLoss, RSOLoss, SimPOLoss
+
+
+@pytest.fixture(autouse=True)
+def random():
+    torch.manual_seed(16)
+
+
+class TestDPOLosses:
+    @pytest.fixture
+    def dpo_loss(self):
+        return DPOLoss(
+            beta=0.1,
+            label_smoothing=0.0,
+        )
+
+    @pytest.fixture
+    def rso_loss(self):
+        return RSOLoss(
+            gamma=0.1,
+        )
+
+    @pytest.fixture
+    def simpo_loss(self):
+        return SimPOLoss(
+            beta=2.0,
+            gamma=0.5,
+            label_smoothing=0.0,
+        )
+
+    @pytest.fixture
+    def loss_inputs(self):
+        """
+        kind-of-random inputs for testing the math out (below).
+        """
+        policy_chosen_logprobs = torch.tensor([-0.5, -10.0, -1.0])
+        policy_rejected_logprobs = torch.tensor([-0.1, -30.0, -21.0])
+
+        ref_chosen_logprobs = torch.tensor([-0.5, -10.1, -0.1])
+        ref_rejected_logprobs = torch.tensor([-0.1, -20.1, -0.1])
+
+        return (
+            policy_chosen_logprobs,
+            policy_rejected_logprobs,
+            ref_chosen_logprobs,
+            ref_rejected_logprobs,
+        )
+
+    def test_dpo_loss(self, dpo_loss, loss_inputs):
+        """
+        here's the maths (see `loss_inputs`):
+        ratios = torch.tensor([-0.4, 20.0, 20.0])
+        ref_ratios = torch.tensor([-0.4, 10, 0.0])
+
+            logits is ratios - ref_ratios
+
+        logits = torch.tensor([0.0, 10.0, 20.0])
+        scaled_logits = torch.tensor([0.0, 1.0, 2.0])
+
+        since label_smoothing is zero, loss is NLL with temperature scaled logits
+            logsigmoid is log(1/1+exp(-scaled_logits))
+            exp(-scaled_logits) is [1, 1/e, 1/e^2]
+            logsigmoid is -log([1 / 2, 1 / (1 + 1/e), 1 / (1 + 1/e^2)])
+
+        expected_losses = -torch.tensor(
+            [1 / 2, 1 / (1 + torch.exp(torch.tensor(-1.0))), 1 / (1 + torch.exp(torch.tensor(-2.0)))]
+        ).log()
+        expected_losses = -expected_logsigmoids
+        """
+        exp_scaled_logits = torch.exp(torch.tensor([0.0, -1.0, -2.0]))
+        expected_losses = -(1 / (1 + exp_scaled_logits)).log()
+        losses, *_ = dpo_loss(*loss_inputs)
+
+        torch.testing.assert_close(losses, expected_losses, atol=1e-4, rtol=1e-5)
+
+    def test_rso_loss(self, rso_loss, loss_inputs):
+        """
+        # maths:
+        ratios = torch.tensor([-0.4, 20.0, 20.0])
+        ref_ratios = torch.tensor([-0.4, 10, 0.0])
+
+        # logits is ratios - ref_ratios
+
+        logits = torch.tensor([0.0, 10.0, 20.0])
+        scaled_logits = torch.tensor([0.0, 1.0, 2.0])
+
+        # hinge loss doesn't use label smoothing
+        # loss = relu(1 - scaled_logits) = max(0, 1 - scaled_logits)
+        expected_losses = torch.tensor([1.0, 0.0, 0.0])
+        """
+
+        expected_losses = torch.tensor([1.0, 0.0, 0.0])
+
+        losses, *_ = rso_loss(*loss_inputs)
+
+        torch.testing.assert_close(losses, expected_losses, atol=1e-4, rtol=1e-5)
+
+    def test_simpo_loss(self, simpo_loss, loss_inputs):
+        """
+        here's the maths (see `loss_inputs`):
+        ratios = torch.tensor([-0.4, 20.0, 20.0])
+        gamma_logratios = 0.25
+
+            logits is ratios - gamma_logratios
+
+        logits = torch.tensor([-0.65, 19.75, 19.75])
+        scaled_logits = beta * logits = torch.tensor([-1.3,  39.5, 39.5])
+
+        since label_smoothing is zero, loss is NLL with temperature scaled logits
+        """
+        policy_chosen_logprobs, policy_rejected_logprobs, *_ = loss_inputs
+        exp_scaled_logits = torch.exp(torch.tensor([1.3, -39.5, -39.5]))
+
+        expected_losses = -(1 / (1 + exp_scaled_logits)).log()
+        losses, *_ = simpo_loss(policy_chosen_logprobs, policy_rejected_logprobs)
+
+        torch.testing.assert_close(losses, expected_losses, atol=1e-4, rtol=1e-5)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/rlhf/loss/test_ppo_loss.py marc/third_party/torchtune/tests/torchtune/rlhf/loss/test_ppo_loss.py
--- marc_original/third_party/torchtune/tests/torchtune/rlhf/loss/test_ppo_loss.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/rlhf/loss/test_ppo_loss.py	2025-02-20 17:49:30.118025179 -0500
@@ -0,0 +1,138 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+from torchtune.rlhf.loss import PPOLoss
+
+
+@pytest.fixture(autouse=True)
+def random():
+    torch.manual_seed(16)
+
+
+class TestPPOLoss:
+    @pytest.fixture
+    def loss_fn(self):
+        return PPOLoss(
+            value_clip_range=0.2,
+            value_coeff=0.1,
+            epsilon=0.2,
+        )
+
+    def test_policy_loss_clipped_for_high_logprobs(self, loss_fn):
+        # fixed old policy logprobs, advantages, returns
+        pi_old_logprobs = torch.tensor([0.5, 0.8, 1.2])
+        advantages = torch.tensor([1.0, 1.0, 1.0])
+        values = torch.tensor([1.0, 1.0, 1.0])
+        returns = torch.tensor([1.0, 1.0, 1.0])
+
+        pi_logprobs_high = torch.tensor([1.5, 1.8, 2.2])
+        # ratio will be [e, e, e]
+        # clipped ratio becomes [1.2, 1.2, 1.2] (1+epsilon)
+        # objective becomes max(-e, -1.2) since advantages is 1
+        expected_loss = torch.tensor(-1.2)
+        expected_ratios = torch.exp(torch.ones((3)))
+
+        _, policy_loss, _, ratios, _ = loss_fn(
+            pi_old_logprobs, pi_logprobs_high, advantages, values, values, returns
+        )
+
+        torch.testing.assert_close(
+            policy_loss.mean(), expected_loss, atol=1e-4, rtol=1e6
+        )
+        torch.testing.assert_close(ratios, expected_ratios.mean(), atol=1e-4, rtol=1e6)
+
+    def test_policy_loss_clipped_for_low_logprobs(self, loss_fn):
+        # fixed old policy logprobs, advantages, returns
+        pi_old_logprobs = torch.tensor([0.5, 0.8, 1.2])
+        advantages = torch.tensor([1.0, 1.0, 1.0])
+        values = torch.tensor([1.0, 1.0, 1.0])
+        returns = torch.tensor([1.0, 1.0, 1.0])
+
+        pi_logprobs_low = torch.tensor([-0.5, -0.2, 0.2])
+        # ratio will be [1/e, 1/e, 1/e] (~0.367)
+        # clipped ratio becomes [0.8, 0.8, 0.8] (1-epsilon)
+        # objective becomes max(1/e, 0.8) since advantages is 1
+        expected_loss = torch.tensor(0.8)
+        expected_ratios = 1 / torch.exp(torch.ones((3)))
+
+        _, policy_loss, _, ratios, _ = loss_fn(
+            pi_old_logprobs, pi_logprobs_low, advantages, values, values, returns
+        )
+
+        torch.testing.assert_close(
+            policy_loss.mean(), expected_loss, atol=1e-4, rtol=1e6
+        )
+        torch.testing.assert_close(ratios, expected_ratios.mean(), atol=1e-4, rtol=1e6)
+
+    def test_policy_loss_not_clipped(self, loss_fn):
+        # fixed old policy logprobs, advantages, returns
+        pi_old_logprobs = torch.tensor([0.5, 0.8, 1.2])
+        advantages = torch.tensor([1.0, 1.0, 1.0])
+        values = torch.tensor([1.0, 1.0, 1.0])
+        returns = torch.tensor([1.0, 1.0, 1.0])
+
+        pi_logprobs_unclipped = torch.tensor([0.6, 0.9, 1.3])
+        # ratio will be [e^0.1, e^0.1, e^0.1] (~1.1)
+        # ratio is not clipped since it is within [1-epsilon, 1+epsilon], [0.8, 1.2]
+        expected_loss = torch.tensor(0.1).exp()
+        expected_ratios = torch.exp(torch.ones(3) * 0.1)
+
+        _, policy_loss, _, ratios, _ = loss_fn(
+            pi_old_logprobs, pi_logprobs_unclipped, advantages, values, values, returns
+        )
+
+        torch.testing.assert_close(
+            policy_loss.mean(), expected_loss, atol=1e-4, rtol=1e6
+        )
+        torch.testing.assert_close(ratios, expected_ratios.mean(), atol=1e-4, rtol=1e6)
+
+    def test_policy_loss_lower_for_higher_advantages(self, loss_fn):
+        pi_logprobs = torch.tensor([-0.5, -0.8, -1.2])
+
+        advantages_high = torch.tensor([1.0, 2.0, 3.0])
+        advantages_low = torch.tensor([0.5, 1.0, 1.5])
+        values = torch.tensor([1.0, 1.0, 1.0])
+        returns = torch.tensor([1.0, 1.0, 1.0])
+
+        _, policy_loss_low, *_ = loss_fn(
+            pi_logprobs, pi_logprobs, advantages_high, values, values, returns
+        )
+        _, policy_loss_high, *_ = loss_fn(
+            pi_logprobs, pi_logprobs, advantages_low, values, values, returns
+        )
+
+        assert policy_loss_low.mean() < policy_loss_high.mean()
+
+    def test_value_loss_lower_for_values_similar_to_return(self, loss_fn):
+        # fix pi_logrobs, pi_old_logprobs, returns, advantages
+        pi_logprobs = torch.tensor([-0.5, -0.8, -1.2])
+        returns = torch.tensor([1.0, 1.0, 1.0])
+        advantages = torch.tensor([1.0, 1.0, 1.0])
+
+        # values estimates are similar to returns
+        values_similar = torch.tensor([0.9, 1.0, 1.1])
+        # value estimates are less similar to returns
+        values_less_similar = torch.tensor([0.5, 1.5, 2.0])
+
+        _, _, value_loss_lower, *_ = loss_fn(
+            pi_logprobs,
+            pi_logprobs,
+            advantages,
+            values_similar,
+            values_similar,
+            returns,
+        )
+        _, _, value_loss_higher, *_ = loss_fn(
+            pi_logprobs,
+            pi_logprobs,
+            advantages,
+            values_similar,
+            values_less_similar,
+            returns,
+        )
+        assert value_loss_lower.mean() < value_loss_higher.mean()
diff -ruN marc_original/third_party/torchtune/tests/torchtune/rlhf/test_rewards.py marc/third_party/torchtune/tests/torchtune/rlhf/test_rewards.py
--- marc_original/third_party/torchtune/tests/torchtune/rlhf/test_rewards.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/rlhf/test_rewards.py	2025-02-20 17:49:30.118025179 -0500
@@ -0,0 +1,222 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+from torchtune import rlhf
+
+
+class TestGetRewards:
+    def test_get_rewards(self):
+        scores = torch.tensor([1.0, 2.0, 3.0])
+        logprobs = torch.tensor(
+            [
+                [0.1, 0.2, 0.3],
+                [0.4, 0.5, 0.6],
+                [0.6, 0.7, 0.8],
+            ]
+        )
+        ref_logprobs = torch.tensor(
+            [
+                [0.2, 0.3, 0.4],
+                [0.6, 0.7, 0.8],
+                [0.9, 1.0, 1.1],
+            ]
+        )
+        kl_controller_value = 0.5
+
+        # expected kl is logprobs - ref_logprobs
+        expected_kl = torch.tensor(
+            [
+                [-0.1, -0.1, -0.1],
+                [-0.2, -0.2, -0.2],
+                [-0.3, -0.3, -0.3],
+            ]
+        )
+
+        # expected kl_rewards is -kl_controller_value * kl
+        expected_kl_rewards = torch.tensor(
+            [
+                [0.05, 0.05, 0.05],
+                [0.1, 0.1, 0.1],
+                [0.15, 0.15, 0.15],
+            ]
+        )
+
+        # expected rewards is kl_rewards[:, -1] + scores
+        expected_rewards = torch.tensor(
+            [
+                [0.05, 0.05, 1.05],
+                [0.1, 0.1, 2.1],
+                [0.15, 0.15, 3.15],
+            ]
+        )
+
+        rewards, kl, kl_rewards = rlhf.get_rewards_ppo(
+            scores, logprobs, ref_logprobs, kl_controller_value
+        )
+
+        torch.testing.assert_close(kl, expected_kl, rtol=1e-4, atol=1e-4)
+        torch.testing.assert_close(
+            kl_rewards, expected_kl_rewards, rtol=1e-4, atol=1e-4
+        )
+        torch.testing.assert_close(rewards, expected_rewards, rtol=1e-4, atol=1e-4)
+
+
+class TestWhiten:
+    def test_whiten_with_shift_mean(self):
+        x = torch.normal(1, 2, size=(100, 100))
+
+        expected_mean, expected_var = x.mean(), x.var()  # should be ~1.0, ~4.0
+        expected = (x - expected_mean) / (torch.sqrt(expected_var) + 1e-8)
+        expected += expected_mean
+        output = rlhf.whiten(x, shift_mean=True)
+
+        torch.testing.assert_close(output, expected, rtol=1e-4, atol=1e-4)
+
+    def test_whiten_without_shift_mean(self):
+        x = torch.normal(1, 2, size=(100, 100))
+
+        expected_mean, expected_var = x.mean(), x.var()  # should be ~1.0, ~4.0
+        expected = (x - expected_mean) / (torch.sqrt(expected_var) + 1e-8)
+        output = rlhf.whiten(x, shift_mean=False)
+
+        torch.testing.assert_close(output, expected, rtol=1e-4, atol=1e-4)
+
+    def test_masked_whiten(self):
+        x_mean_1 = torch.normal(1, 2, size=(50, 100))
+        x_mean_2 = torch.normal(2, 1, size=(50, 100))
+        x = torch.cat([x_mean_1, x_mean_2], dim=0)
+        mask = torch.ones_like(x, dtype=torch.bool)
+        mask[:50] = False
+
+        expected_mean, expected_var = (
+            x_mean_2.mean(),
+            x_mean_2.var(),
+        )  # should be ~2.0, ~1.0
+        expected = (x - expected_mean) / (torch.sqrt(expected_var) + 1e-8)
+        expected += expected_mean
+
+        output = rlhf.whiten(x, mask=mask)
+
+        torch.testing.assert_close(output, expected, rtol=1e-4, atol=1e-4)
+
+
+class TestMaskedMean:
+    def test_masked_single_batch_mean(self):
+        x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
+        mask = torch.tensor([True, True, True, False, False])
+
+        expected_mean = torch.tensor(2.0)
+        output = rlhf.masked_mean(x, mask)
+
+        torch.testing.assert_close(output, expected_mean, rtol=1e-4, atol=1e-4)
+
+    def test_masked_multi_batch_mean(self):
+        x = torch.tensor(
+            [
+                [1.0, 2.0, 3.0, 4.0, 5.0],
+                [2.0, 3.0, 4.0, 5.0, 6.0],
+            ]
+        )
+        mask = torch.tensor(
+            [[True, True, True, False, False], [False, False, True, True, True]]
+        )
+
+        expected_means = torch.tensor([2.0, 5.0])
+        output = rlhf.masked_mean(x, mask, dim=1)
+
+        torch.testing.assert_close(output, expected_means, rtol=1e-4, atol=1e-4)
+
+
+class TestMaskedVar:
+    def test_masked_var(self):
+        x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
+        mask = torch.tensor([True, True, True, False, False])
+
+        expected_var = torch.tensor(1.0)
+        output = rlhf.masked_var(x, mask)
+
+        torch.testing.assert_close(output, expected_var, rtol=1e-4, atol=1e-4)
+
+
+class TestEstimateAdvantages:
+    def test_estimate_returns(self):
+        values = torch.tensor([[0, 0, 0, 1]])
+        rewards = torch.tensor([[0, 0, 0, 1]])
+        gamma = 0.9
+        lmbda = 0.95
+
+        final_reward = 1.0
+        expected_returns = torch.tensor(
+            [
+                [
+                    final_reward * gamma * gamma * gamma * lmbda * lmbda,
+                    final_reward * gamma * gamma * lmbda,
+                    final_reward * gamma,
+                    final_reward,
+                ]
+            ]
+        )
+
+        _, returns = rlhf.estimate_advantages(values, rewards, gamma, lmbda)
+        torch.testing.assert_close(returns, expected_returns, rtol=1e-4, atol=1e-4)
+
+    def test_estimate_advantages_with_whitening(self):
+        values = torch.tensor([[0, 0, 0, 1]])
+        rewards = torch.tensor([[0, 0, 0, 1]])
+        gamma = 0.9
+        lmbda = 0.95
+
+        final_reward = 1.0
+        returns = torch.tensor(
+            [
+                [
+                    final_reward * gamma * gamma * gamma * lmbda * lmbda,
+                    final_reward * gamma * gamma * lmbda,
+                    final_reward * gamma,
+                    final_reward,
+                ]
+            ]
+        )
+
+        # see `torchtune.rlhf.estimate_advantages`
+        expected_advantages = returns - values
+        expected_whitened_advantages = rlhf.whiten(expected_advantages, shift_mean=True)
+        advantages, _ = rlhf.estimate_advantages(values, rewards, gamma, lmbda)
+        torch.testing.assert_close(
+            expected_whitened_advantages, advantages, rtol=1e-4, atol=1e-4
+        )
+
+    def test_estimate_advantages_with_masks(self):
+        values = torch.tensor([[0, 0, 0, 1]])
+        rewards = torch.tensor([[0, 0, 0, 1]])
+        masks = torch.tensor([[True, True, True, False]])
+        gamma = 0.9
+        lmbda = 0.95
+
+        final_reward = 1.0
+        returns = torch.tensor(
+            [
+                [
+                    final_reward * gamma * gamma * gamma * lmbda * lmbda,
+                    final_reward * gamma * gamma * lmbda,
+                    final_reward * gamma,
+                    final_reward,
+                ]
+            ]
+        )
+
+        # see `torchtune.rlhf.estimate_advantages`
+        expected_advantages = returns - values
+        expected_advantages = rlhf.whiten(expected_advantages, mask=masks)
+        expected_advantages[..., -1] = 0.0
+
+        advantages, _ = rlhf.estimate_advantages(
+            values, rewards, gamma, lmbda, masks=masks
+        )
+        torch.testing.assert_close(
+            advantages, expected_advantages, rtol=1e-4, atol=1e-4
+        )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/rlhf/test_sequence_processing.py marc/third_party/torchtune/tests/torchtune/rlhf/test_sequence_processing.py
--- marc_original/third_party/torchtune/tests/torchtune/rlhf/test_sequence_processing.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/rlhf/test_sequence_processing.py	2025-02-20 17:49:30.122025186 -0500
@@ -0,0 +1,61 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+from torchtune import rlhf
+
+
+class TestTruncateSequenceAtFirstStopToken:
+    def test_truncate_sequences(self):
+        stop_token_ids = torch.tensor([2, 869])
+        fill_value = 0
+        sequences = torch.tensor(
+            [
+                [869, 30, 869],
+                [2, 30, 869],
+                [869, 30, 2],
+                [50, 30, 869],
+                [13, 30, 2],
+                [13, 30, 5],
+                [13, 2, 20],
+                [13, 2, 2],
+                [2, 2, 2],
+            ]
+        )
+        eos_mask, truncated_sequences = rlhf.truncate_sequence_at_first_stop_token(
+            sequences, stop_token_ids, fill_value
+        )
+
+        expected_eos_mask = torch.tensor(
+            [
+                [False, True, True],
+                [False, True, True],
+                [False, True, True],
+                [False, False, False],
+                [False, False, False],
+                [False, False, False],
+                [False, False, True],
+                [False, False, True],
+                [False, True, True],
+            ]
+        )
+
+        expected_sequences = torch.tensor(
+            [
+                [869, fill_value, fill_value],
+                [2, fill_value, fill_value],
+                [869, fill_value, fill_value],
+                [50, 30, 869],
+                [13, 30, 2],
+                [13, 30, 5],
+                [13, 2, fill_value],
+                [13, 2, fill_value],
+                [2, fill_value, fill_value],
+            ]
+        )
+
+        assert expected_eos_mask.eq(eos_mask).all()
+        assert expected_sequences.eq(truncated_sequences).all()
diff -ruN marc_original/third_party/torchtune/tests/torchtune/training/checkpointing/test_checkpointer.py marc/third_party/torchtune/tests/torchtune/training/checkpointing/test_checkpointer.py
--- marc_original/third_party/torchtune/tests/torchtune/training/checkpointing/test_checkpointer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/training/checkpointing/test_checkpointer.py	2025-02-20 17:49:30.130025199 -0500
@@ -0,0 +1,766 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import json
+
+from pathlib import Path
+from typing import Tuple
+
+import pytest
+import torch
+from torch import randn
+
+from torchtune.models import gemma, llama2, mistral
+from torchtune.modules.peft import (
+    get_adapter_params,
+    get_lora_module_names,
+    validate_missing_and_unexpected_for_lora,
+)
+
+from torchtune.training.checkpointing import FullModelHFCheckpointer
+from torchtune.training.checkpointing._utils import (
+    ADAPTER_CONFIG,
+    ADAPTER_KEY,
+    safe_torch_load,
+)
+from torchtune.training.seed import set_seed
+
+_VOCAB_SIZE = 100
+_DIM = 64
+_HIDDEN_DIM = 256
+_NUM_HEADS = 4
+_NUM_KV_HEADS = 4
+_HEAD_DIM = 16
+
+
+@pytest.fixture(autouse=True)
+def random():
+    set_seed(16)
+
+
+class TestHFLlama2FullModelCheckpointer:
+    @pytest.fixture
+    def weight_dtype(self):
+        return torch.float16
+
+    @pytest.fixture
+    def state_dict_1(self, weight_dtype):
+        """
+        State dict for a HF format checkpoint. This state dict is "complete" and
+        can be loaded into a torchtune model once correctly converted.
+        """
+        state_dict = {
+            "model.embed_tokens.weight": randn(_VOCAB_SIZE, _DIM, dtype=weight_dtype),
+            "model.layers.0.input_layernorm.weight": randn(_DIM, dtype=weight_dtype),
+            "model.layers.0.self_attn.q_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.self_attn.k_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.self_attn.v_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.self_attn.o_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.post_attention_layernorm.weight": randn(
+                _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.self_attn.rotary_emb.inv_freq": randn(
+                _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.mlp.gate_proj.weight": randn(
+                _HIDDEN_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.mlp.down_proj.weight": randn(
+                _DIM, _HIDDEN_DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.mlp.up_proj.weight": randn(
+                _HIDDEN_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.norm.weight": torch.randn(_DIM, dtype=weight_dtype),
+            "lm_head.weight": torch.randn(_VOCAB_SIZE, _DIM, dtype=weight_dtype),
+        }
+        return state_dict
+
+    @pytest.fixture
+    def state_dict_2(self, weight_dtype):
+        """
+        State dict for a HF format checkpoint. This state dict is "incomplete" and
+        should be used along with ``state_dict_1`` to test multi-file checkpointing. Specifically
+        it's missing the embedding, norm and lm_head keys.
+        """
+        state_dict = {
+            "model.layers.1.input_layernorm.weight": randn(_DIM, dtype=weight_dtype),
+            "model.layers.1.self_attn.q_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.1.self_attn.k_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.1.self_attn.v_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.1.self_attn.o_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.1.post_attention_layernorm.weight": randn(
+                _DIM, dtype=weight_dtype
+            ),
+            "model.layers.1.self_attn.rotary_emb.inv_freq": randn(
+                _DIM, dtype=weight_dtype
+            ),
+            "model.layers.1.mlp.gate_proj.weight": randn(
+                _HIDDEN_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.1.mlp.down_proj.weight": randn(
+                _DIM, _HIDDEN_DIM, dtype=weight_dtype
+            ),
+            "model.layers.1.mlp.up_proj.weight": randn(
+                _HIDDEN_DIM, _DIM, dtype=weight_dtype
+            ),
+        }
+        return state_dict
+
+    @pytest.fixture
+    def llama2_hf_checkpoints(self, tmp_path, state_dict_1, state_dict_2):
+        """
+        Fixture which creates two checkpoint files for the Llama2 model. The
+        state dict follows the HF_FORMAT for the checkpoint format.
+
+        The state dicts are structured in such a way that both single file and
+        multiple file checkpoints can be tested.
+            * The first checkpoint contains layer0 + embed + norm + lm_head keys
+            and can be tested in isolation
+            * The second checkpoint contains all layer1 keys and should be tested
+            in the multiple file checkpoint test along with the first checkpoint
+
+        The model corresponds to the following config:
+            * vocab_size: 100
+            * num_layers: 1 for single checkpoint and 2 for multiple checkpoint
+            * num_heads: 4
+            * num_kv_heads: 4
+            * embed_dim: 64
+            * max_seq_len: 128
+        """
+        checkpoint_file_1 = tmp_path / "llama2_hf_checkpoint_01.pt"
+        checkpoint_file_2 = tmp_path / "llama2_hf_checkpoint_02.pt"
+
+        torch.save(state_dict_1, checkpoint_file_1)
+        torch.save(state_dict_2, checkpoint_file_2)
+
+        config = {
+            "hidden_size": 64,
+            "num_attention_heads": 4,
+            "num_key_value_heads": 4,
+        }
+        config_file = Path.joinpath(tmp_path, "config.json")
+        with config_file.open("w") as f:
+            json.dump(config, f)
+
+        return (checkpoint_file_1, checkpoint_file_2)
+
+    @pytest.fixture
+    def single_file_checkpointer(
+        self, llama2_hf_checkpoints, tmp_path
+    ) -> FullModelHFCheckpointer:
+        checkpoint_file, _ = llama2_hf_checkpoints
+        return FullModelHFCheckpointer(
+            checkpoint_dir=tmp_path,
+            checkpoint_files=[checkpoint_file],
+            model_type="LLAMA2",
+            output_dir=tmp_path,
+        )
+
+    @pytest.fixture
+    def multi_file_checkpointer(
+        self, llama2_hf_checkpoints, tmp_path
+    ) -> FullModelHFCheckpointer:
+        checkpoint_file_1, checkpoint_file_2 = llama2_hf_checkpoints
+        return FullModelHFCheckpointer(
+            checkpoint_dir=tmp_path,
+            checkpoint_files=[checkpoint_file_1, checkpoint_file_2],
+            model_type="LLAMA2",
+            output_dir=tmp_path,
+        )
+
+    def test_load_save_checkpoint_single_file(
+        self,
+        single_file_checkpointer: FullModelHFCheckpointer,
+        llama2_hf_checkpoints: Tuple[Path, Path],
+    ):
+        """
+        Test ``load_checkpoint`` and ``save_checkpoint`` method within the
+        FullModelHFCheckpointer for a single checkpoint file.
+
+        We test:
+        * ``load_checkpoint`` loads the right sets of keys
+        * Internal state of the checkpointer is correctly updated
+        * Converted checkpoint can be loaded into the llama2 torchtune implementation
+        * Saved checkpoint keys match the original checkpoint
+        """
+        # Read the state dict directly from file using torch.load. This will be the state
+        # dict we test against
+        checkpoint_file, _ = llama2_hf_checkpoints
+        orig_state_dict = safe_torch_load(checkpoint_file)
+
+        # Converted state dict from the checkpointer
+        state_dict = single_file_checkpointer.load_checkpoint()
+
+        # Check that we've loaded all the keys; We ignore inv_freq as is standard practice
+        assert len(state_dict["model"].keys()) + 1 == len(orig_state_dict.keys())
+
+        # the keys in original state dict should match up with the keys in the weight_map
+        for key in orig_state_dict.keys():
+            if "inv_freq" in key:
+                continue
+            assert key in single_file_checkpointer._weight_map
+
+        # loading the state dict into the model implementation should work correctly
+        model = llama2.llama2(
+            vocab_size=_VOCAB_SIZE,
+            num_layers=1,
+            num_heads=_NUM_HEADS,
+            num_kv_heads=_NUM_KV_HEADS,
+            embed_dim=_DIM,
+            max_seq_len=128,
+        )
+        model.load_state_dict(state_dict["model"])
+
+        single_file_checkpointer.save_checkpoint(state_dict, epoch=1)
+
+        # Reload the output checkpoint file and compare to the original checkpoint. This
+        # assumes we know what the name of the file is. This is fine, breaking this logic
+        # should be something we capture through this test
+        output_file = Path.joinpath(checkpoint_file.parent, "hf_model_0001_1.pt")
+        output_state_dict = safe_torch_load(output_file)
+
+        # We ignore inv_freq as is standard practice and so output dict will have one less key
+        assert len(output_state_dict.keys()) + 1 == len(orig_state_dict.keys())
+
+    def test_save_load_checkpoint_multiple_file(
+        self,
+        multi_file_checkpointer: FullModelHFCheckpointer,
+        llama2_hf_checkpoints: Tuple[Path, Path],
+    ):
+        """
+        Test ``load_checkpoint`` method within the FullModelCheckpointer for multiple
+        checkpoint file.
+
+        We test:
+        * ``load_checkpoint`` loads the right sets of keys
+        * Internal state of the checkpointer is correctly updated
+        * Converted checkpoint can be loaded into the llama2 torchtune implementation
+        """
+        # Read the state dict directly from files
+        checkpoint_file_1, checkpoint_file_2 = llama2_hf_checkpoints
+        orig_state_dict_1 = safe_torch_load(checkpoint_file_1)
+        orig_state_dict_2 = safe_torch_load(checkpoint_file_2)
+
+        # merged state dict from checkpointer
+        state_dict = multi_file_checkpointer.load_checkpoint()
+
+        # We ignore inv_freq as is standard practice
+        assert len(state_dict["model"].keys()) + 2 == len(
+            orig_state_dict_1.keys()
+        ) + len(orig_state_dict_2.keys())
+
+        # the keys in the weight_map should match up with the keys in the weight_map
+        for key in orig_state_dict_1.keys():
+            if "inv_freq" in key:
+                continue
+            assert key in multi_file_checkpointer._weight_map
+
+        for key in orig_state_dict_2.keys():
+            if "inv_freq" in key:
+                continue
+            assert key in multi_file_checkpointer._weight_map
+
+        # finally loading into the model should work
+        model = llama2.llama2(
+            vocab_size=_VOCAB_SIZE,
+            num_layers=2,
+            num_heads=_NUM_HEADS,
+            num_kv_heads=_NUM_KV_HEADS,
+            embed_dim=_DIM,
+            max_seq_len=128,
+        )
+        model.load_state_dict(state_dict["model"])
+
+        multi_file_checkpointer.save_checkpoint(state_dict, epoch=1)
+
+        # Reload the output checkpoint file and compare to the original checkpoint. This
+        # assumes we know what the name of the file is. This is fine, breaking this logic
+        # should be something we capture through this test
+        output_file_1 = Path.joinpath(checkpoint_file_1.parent, "hf_model_0001_1.pt")
+        output_file_2 = Path.joinpath(checkpoint_file_2.parent, "hf_model_0002_1.pt")
+        output_state_dict_1 = safe_torch_load(output_file_1)
+        output_state_dict_2 = safe_torch_load(output_file_2)
+
+        assert len(output_state_dict_1.keys()) + 1 == len(orig_state_dict_1.keys())
+        assert len(output_state_dict_2.keys()) + 1 == len(orig_state_dict_2.keys())
+
+    def test_load_save_adapter_only(
+        self, tmp_path, single_file_checkpointer, llama2_hf_checkpoints
+    ):
+        """ """
+        state_dict = single_file_checkpointer.load_checkpoint()
+
+        with pytest.raises(
+            ValueError, match="Adapter checkpoint not found in state_dict"
+        ):
+            single_file_checkpointer.save_checkpoint(
+                state_dict, epoch=2, adapter_only=True
+            )
+
+        state_dict[ADAPTER_KEY] = {}
+        single_file_checkpointer.save_checkpoint(state_dict, epoch=2, adapter_only=True)
+
+        output_file_1 = Path.joinpath(tmp_path, "hf_model_0001_2.pt")
+        output_file_2 = Path.joinpath(tmp_path, "adapter_2.pt")
+
+        with pytest.raises(ValueError, match="Unable to load checkpoint from"):
+            _ = safe_torch_load(output_file_1)
+
+        output_state_dict_2 = safe_torch_load(output_file_2)
+        # Check that the empty adapter we saved is the one loaded succesfully
+        assert len(output_state_dict_2.keys()) == 0
+
+    def test_save_checkpoint_in_peft_format(
+        self,
+        single_file_checkpointer: FullModelHFCheckpointer,
+        llama2_hf_checkpoints: Tuple[Path, Path],
+    ):
+        """
+        Test save_checkpoint method within the FullModelCheckpointer for
+        integration with HF PEFT (i.e. save_in_peft_format=True).
+
+        We test that:
+        * The file adapter_config.json contains the fields required by PEFT
+        and the correct values
+        * The state dict keys of the saved adapter checkpoint are remapped as expected
+        * The state dict values of the saved adapter checkpoint (after key remapping)
+        match those in torchtune for parameters that are not permuted by HF
+        # The state dict values of the saved adapter checkpoint (after key remapping)
+        do not match those in torchtune for parameters that are permuted by HF, but the
+        sums along the dimension of permutation match
+        """
+
+        # Define LoRA params for this test
+        lora_attn_modules = ["q_proj", "output_proj"]
+        apply_lora_to_mlp = True
+        apply_lora_to_output = True
+        lora_rank = 4
+        lora_alpha = 8
+
+        checkpoint_file, _ = llama2_hf_checkpoints
+        state_dict = single_file_checkpointer.load_checkpoint()
+
+        # Build LoRA Llama2 model and load in base model weights
+        model = llama2.lora_llama2(
+            lora_attn_modules=lora_attn_modules,
+            apply_lora_to_mlp=apply_lora_to_mlp,
+            apply_lora_to_output=apply_lora_to_output,
+            vocab_size=_VOCAB_SIZE,
+            num_layers=1,
+            num_heads=_NUM_HEADS,
+            num_kv_heads=_NUM_KV_HEADS,
+            embed_dim=_DIM,
+            max_seq_len=128,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+        )
+        missing, unexpected = model.load_state_dict(state_dict["model"], strict=False)
+        validate_missing_and_unexpected_for_lora(
+            lora_attn_modules=lora_attn_modules,
+            apply_lora_to_mlp=apply_lora_to_mlp,
+            apply_lora_to_output=apply_lora_to_output,
+            base_missing=missing,
+            base_unexpected=unexpected,
+        )
+
+        # LoRA B params are zero-initialized, randomly initialize them to make
+        # the test of their permutation on checkpoint save nontrivial
+        lora_b_sd = {
+            k: torch.randn_like(v)
+            for k, v in model.state_dict().items()
+            if "lora_b" in k
+        }
+        model.load_state_dict(lora_b_sd, strict=False)
+
+        # Construct the adapter weights and config and save using checkpointer
+        adapter_params = get_adapter_params(model)
+        adapter_key_filter = lambda x: x in adapter_params
+        expected_adapter_state_dict = {
+            k: v for k, v in model.state_dict().items() if adapter_key_filter(k)
+        }
+        adapter_config = {
+            "r": lora_rank,
+            "lora_alpha": lora_alpha,
+            "target_modules": get_lora_module_names(
+                lora_attn_modules,
+                apply_lora_to_mlp,
+                apply_lora_to_output,
+            ),
+            "peft_type": "LORA",
+        }
+        state_dict.update({ADAPTER_KEY: expected_adapter_state_dict})
+        state_dict.update({ADAPTER_CONFIG: adapter_config})
+        single_file_checkpointer.save_checkpoint(state_dict, epoch=1)
+
+        # Load saved adapter weights and config from file for comparison
+        adapter_weights_file = Path.joinpath(
+            checkpoint_file.parent, "adapter_model.bin"
+        )
+        actual_adapter_state_dict = safe_torch_load(adapter_weights_file)
+
+        adapter_config_file = Path.joinpath(
+            checkpoint_file.parent, "adapter_config.json"
+        )
+        with open(adapter_config_file, "r") as f:
+            adapter_config = json.load(f)
+
+        expected_target_modules = [
+            "down_proj",
+            "gate_proj",
+            "lm_head",
+            "o_proj",
+            "q_proj",
+            "up_proj",
+        ]
+        assert sorted(adapter_config["target_modules"]) == expected_target_modules
+
+        # Map PEFT keys back to torchtune keys
+        peft_to_tt = {
+            "o_proj": "output_proj",
+            "gate_proj": "w1",
+            "down_proj": "w2",
+            "up_proj": "w3",
+            "lm_head": "output",
+        }
+        for k, v in actual_adapter_state_dict.items():
+            new_k = k.replace("base_model.model.", "").replace("self_attn", "attn")
+            if "lm_head" not in new_k:
+                new_k = new_k.replace("model.", "")
+            for kk, vv in peft_to_tt.items():
+                if kk in k:
+                    new_k = new_k.replace(kk, vv)
+            new_k = new_k.replace("lora_A", "lora_a").replace("lora_B", "lora_b")
+
+            # LoRA B matrix for Q should not match due to Q and K permutation
+            # However, since they're permuted along embed dim, their sum along that axis should match
+            if "lora_b" in new_k and "q_proj" in new_k:
+                assert not torch.allclose(
+                    actual_adapter_state_dict[k], expected_adapter_state_dict[new_k]
+                )
+                torch.testing.assert_close(
+                    actual_adapter_state_dict[k].sum(dim=0),
+                    expected_adapter_state_dict[new_k].sum(dim=0),
+                )
+
+            # All other matrices should match exactly
+            if "lora_b" not in new_k:
+                torch.testing.assert_close(
+                    actual_adapter_state_dict[k], expected_adapter_state_dict[new_k]
+                )
+
+
+class TestHFMistralRewardModelFullModelCheckpointer:
+    @pytest.fixture
+    def weight_dtype(self):
+        return torch.float16
+
+    @pytest.fixture
+    def state_dict(self, weight_dtype):
+        """
+        State dict for a HF format mistral reward model checkpoint. This state dict is
+        "complete" and can be loaded into a torchtune model once correctly converted.
+        """
+        state_dict = {
+            "model.embed_tokens.weight": randn(_VOCAB_SIZE, _DIM, dtype=weight_dtype),
+            "model.layers.0.input_layernorm.weight": randn(_DIM, dtype=weight_dtype),
+            "model.layers.0.self_attn.q_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.self_attn.k_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.self_attn.v_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.self_attn.o_proj.weight": randn(
+                _DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.post_attention_layernorm.weight": randn(
+                _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.mlp.gate_proj.weight": randn(
+                _HIDDEN_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.mlp.down_proj.weight": randn(
+                _DIM, _HIDDEN_DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.mlp.up_proj.weight": randn(
+                _HIDDEN_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.norm.weight": randn(_DIM, dtype=weight_dtype),
+            "score.weight": randn(1, _DIM, dtype=weight_dtype),
+            # adding bias to ensure it doesn't cause an unexpected key
+            "score.bias": randn(1, _DIM, dtype=weight_dtype),
+        }
+        return state_dict
+
+    @pytest.fixture
+    def mistral_reward_model_hf_checkpoint(self, tmp_path, state_dict):
+        """
+        Fixture which creates a checkpoint file for the Mistral reward model. The
+        state dict follows the HF_FORMAT for the checkpoint format.
+
+        The state dicts supports testing for a single-file checkpoint.
+        Multiple file checkpoints are already tested for Llama2.
+            * The checkpoint contains layer0 + embed + norm + score keys
+            and can be tested in isolation
+
+        The model corresponds to the following config:
+            * num_layers: 1
+            * num_heads: 4
+            * num_kv_heads: 4
+            * embed_dim: 64
+            * max_seq_len: 128
+            * num_classes: 1
+            * intermediate_dim: 256
+
+        """
+        checkpoint_file = tmp_path / "mistral_reward_model_hf_checkpoint.pt"
+
+        torch.save(state_dict, checkpoint_file)
+
+        config = {
+            "hidden_size": 64,
+            "num_attention_heads": 4,
+            "num_key_value_heads": 4,
+            "num_classes": 1,
+        }
+        config_file = Path.joinpath(tmp_path, "config.json")
+        with config_file.open("w") as f:
+            json.dump(config, f)
+
+        return checkpoint_file
+
+    @pytest.fixture
+    def single_file_checkpointer(
+        self, mistral_reward_model_hf_checkpoint, tmp_path
+    ) -> FullModelHFCheckpointer:
+        checkpoint_file = mistral_reward_model_hf_checkpoint
+        return FullModelHFCheckpointer(
+            checkpoint_dir=tmp_path,
+            checkpoint_files=[checkpoint_file],
+            model_type="REWARD",
+            output_dir=tmp_path,
+        )
+
+    def test_load_save_checkpoint_single_file(
+        self,
+        single_file_checkpointer: FullModelHFCheckpointer,
+        mistral_reward_model_hf_checkpoint: Path,
+    ):
+        """
+        Test ``load_checkpoint`` and ``save_checkpoint`` method within the
+        FullModelHFCheckpointer for a single checkpoint file for a mistral reward model.
+
+        We test:
+        * ``load_checkpoint`` loads the right sets of keys
+        * Internal state of the checkpointer is correctly updated
+        * Converted checkpoint can be loaded into the `mistral_classifier` torchtune implementation
+        * Saved checkpoint keys match the original checkpoint
+        """
+        # Read the state dict directly from file using torch.load. This will be the state
+        # dict we test against
+        checkpoint_file = mistral_reward_model_hf_checkpoint
+        orig_state_dict = safe_torch_load(checkpoint_file)
+
+        # Converted state dict from the checkpointer
+        state_dict = single_file_checkpointer.load_checkpoint()
+        # Check that we've loaded all the keys minus the output bias
+        assert len(state_dict["model"].keys()) == len(orig_state_dict.keys()) - 1
+
+        # the keys in original state dict should match up with the keys in the weight_map
+        for key in orig_state_dict.keys():
+            if "inv_freq" in key or "output.bias" in key:
+                continue
+            assert key in single_file_checkpointer._weight_map
+
+        # loading the state dict into the model implementation should work correctly
+        model = mistral.mistral_classifier(
+            num_classes=1,
+            vocab_size=_VOCAB_SIZE,
+            num_layers=1,
+            num_heads=_NUM_HEADS,
+            num_kv_heads=_NUM_KV_HEADS,
+            embed_dim=_DIM,
+            intermediate_dim=_HIDDEN_DIM,
+            max_seq_len=128,
+        )
+        model.load_state_dict(state_dict["model"])
+
+        single_file_checkpointer.save_checkpoint(state_dict, epoch=1)
+
+        # Reload the output checkpoint file and compare to the original checkpoint. This
+        # assumes we know what the name of the file is. This is fine, breaking this logic
+        # should be something we capture through this test
+        output_file = Path.joinpath(checkpoint_file.parent, "hf_model_0001_1.pt")
+        output_state_dict = safe_torch_load(output_file)
+
+        assert len(output_state_dict.keys()) == len(orig_state_dict.keys()) - 1
+
+
+class TestHFGemmaFullModelCheckpointer:
+    @pytest.fixture
+    def weight_dtype(self):
+        return torch.float16
+
+    @pytest.fixture
+    def state_dict(self, weight_dtype):
+        """
+        State dict for a HF format Gemma checkpoint. This state dict is
+        "complete" and can be loaded into a TorchTune model once correctly converted.
+        """
+        state_dict = {
+            "model.embed_tokens.weight": randn(_VOCAB_SIZE, _DIM, dtype=weight_dtype),
+            "model.layers.0.input_layernorm.weight": randn(_DIM, dtype=weight_dtype),
+            "model.layers.0.self_attn.q_proj.weight": randn(
+                _DIM, _NUM_HEADS * _HEAD_DIM, dtype=weight_dtype
+            ),
+            # setting num_kv_heads to 1
+            "model.layers.0.self_attn.k_proj.weight": randn(
+                _HEAD_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.self_attn.v_proj.weight": randn(
+                _HEAD_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.self_attn.o_proj.weight": randn(
+                _NUM_HEADS * _HEAD_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.post_attention_layernorm.weight": randn(
+                _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.mlp.gate_proj.weight": randn(
+                _HIDDEN_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.mlp.down_proj.weight": randn(
+                _DIM, _HIDDEN_DIM, dtype=weight_dtype
+            ),
+            "model.layers.0.mlp.up_proj.weight": randn(
+                _HIDDEN_DIM, _DIM, dtype=weight_dtype
+            ),
+            "model.norm.weight": randn(_DIM, dtype=weight_dtype),
+        }
+        return state_dict
+
+    @pytest.fixture
+    def gemma_hf_checkpoint(self, tmp_path, state_dict):
+        """
+        Fixture which creates a checkpoint file for Gemma. The
+        state dict follows the HF_FORMAT for the checkpoint format.
+
+        The state dicts supports testing for a single-file checkpoint.
+        Multiple file checkpoints are already tested for Llama2.
+
+        The model corresponds to the following config:
+            * num_layers: 1
+            * num_heads: 4
+            * num_kv_heads: 1
+            * embed_dim: 64
+            * max_seq_len: 128
+            * num_classes: 1
+            * intermediate_dim: 256
+            * head_dim : 16
+
+        """
+        checkpoint_file = tmp_path / "gemma_hf_checkpoint.pt"
+
+        torch.save(state_dict, checkpoint_file)
+
+        config = {
+            "hidden_size": _DIM,
+            "num_attention_heads": _NUM_HEADS,
+            "num_key_value_heads": 1,
+            "head_dim": _HEAD_DIM,
+            "intermediate_size": _HIDDEN_DIM,
+        }
+        config_file = Path.joinpath(tmp_path, "config.json")
+        with config_file.open("w") as f:
+            json.dump(config, f)
+
+        return checkpoint_file
+
+    @pytest.fixture
+    def single_file_checkpointer(
+        self, gemma_hf_checkpoint, tmp_path
+    ) -> FullModelHFCheckpointer:
+        checkpoint_file = gemma_hf_checkpoint
+        return FullModelHFCheckpointer(
+            checkpoint_dir=tmp_path,
+            checkpoint_files=[checkpoint_file],
+            model_type="GEMMA",
+            output_dir=tmp_path,
+        )
+
+    def test_load_save_checkpoint_single_file(
+        self,
+        single_file_checkpointer: FullModelHFCheckpointer,
+        gemma_hf_checkpoint: Path,
+    ):
+        """
+        Test ``load_checkpoint`` and ``save_checkpoint`` method within the
+        FullModelHFCheckpointer for a single checkpoint file for Gemma.
+
+        We test:
+        * ``load_checkpoint`` loads the right sets of keys
+        * Internal state of the checkpointer is correctly updated
+        * Converted checkpoint can be loaded into the `gemma` TorchTune implementation
+        * lm_head weights are tied to the embed_tokens weights during saving
+        * lmhead weights are popped during loading
+        """
+        # Read the state dict directly from file using torch.load. This will be the state
+        # dict we test against
+        checkpoint_file = gemma_hf_checkpoint
+        orig_state_dict = safe_torch_load(checkpoint_file)
+
+        # Converted state dict from the checkpointer
+
+        state_dict = single_file_checkpointer.load_checkpoint()
+        assert len(state_dict["model"].keys()) == len(orig_state_dict.keys())
+
+        # the keys in original state dict should match up with the keys in the weight_map
+        for key in orig_state_dict.keys():
+            if "inv_freq" in key:
+                continue
+            assert key in single_file_checkpointer._weight_map
+
+        # loading the state dict into the model implementation should work correctly
+        model = gemma.gemma(
+            vocab_size=_VOCAB_SIZE,
+            num_layers=1,
+            num_heads=_NUM_HEADS,
+            head_dim=_HEAD_DIM,
+            num_kv_heads=1,
+            embed_dim=_DIM,
+            intermediate_dim=_HIDDEN_DIM,
+            max_seq_len=128,
+        )
+        model.load_state_dict(state_dict["model"])
+
+        single_file_checkpointer.save_checkpoint(state_dict, epoch=1)
+
+        # Reload the output checkpoint file and compare to the original checkpoint. This
+        # assumes we know what the name of the file is. This is fine, breaking this logic
+        # should be something we capture through this test
+        output_file = Path.joinpath(checkpoint_file.parent, "hf_model_0001_1.pt")
+        output_state_dict = safe_torch_load(output_file)
+
+        assert len(output_state_dict.keys()) == len(orig_state_dict.keys())
diff -ruN marc_original/third_party/torchtune/tests/torchtune/training/checkpointing/test_checkpointer_utils.py marc/third_party/torchtune/tests/torchtune/training/checkpointing/test_checkpointer_utils.py
--- marc_original/third_party/torchtune/tests/torchtune/training/checkpointing/test_checkpointer_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/training/checkpointing/test_checkpointer_utils.py	2025-02-20 17:49:30.134025206 -0500
@@ -0,0 +1,220 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from copy import deepcopy
+from pathlib import Path
+
+import pytest
+import torch
+from torchtune.models.llama2 import llama2, llama2_classifier
+from torchtune.training.checkpointing._utils import (
+    FormattedCheckpointFiles,
+    safe_torch_load,
+    update_state_dict_for_classifier,
+)
+
+N_LAYERS = 3
+IN_DIM = 5
+OUT_DIM = 10
+VOCAB_SIZE = 50
+NUM_HEADS = 4
+NUM_KV_HEADS = 2
+EMBED_DIM = 64
+MAX_SEQ_LEN = 64
+NUM_CLASSES = 6
+
+
+class TestCheckpointerUtils:
+    @pytest.fixture
+    def model_checkpoint(self, tmp_path):
+        """
+        Fixture which creates a checkpoint file for testing checkpointer utils.
+        """
+        checkpoint_file = tmp_path / "model_checkpoint_01.pt"
+
+        state_dict = {
+            "token_embeddings.weight": torch.ones(1, 10),
+            "output.weight": torch.ones(1, 10),
+        }
+
+        torch.save(state_dict, checkpoint_file)
+
+        return checkpoint_file
+
+    @pytest.mark.parametrize("weights_only", [True, False])
+    def test_safe_torch_load(self, model_checkpoint, weights_only):
+        state_dict = safe_torch_load(Path(model_checkpoint), weights_only)
+
+        assert "token_embeddings.weight" in state_dict
+        assert "output.weight" in state_dict
+
+        assert state_dict["token_embeddings.weight"].shape[1] == 10
+        assert state_dict["output.weight"].shape[0] == 1
+
+
+class TestUpdateStateDictForClassifer:
+    @pytest.fixture()
+    def llama2_state_dict(self):
+        model = llama2(
+            vocab_size=VOCAB_SIZE,
+            num_layers=N_LAYERS,
+            num_heads=NUM_KV_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=EMBED_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+        )
+        return model.state_dict()
+
+    @pytest.fixture()
+    def llama2_classifier_model(self):
+        return llama2_classifier(
+            num_classes=NUM_CLASSES,
+            vocab_size=VOCAB_SIZE,
+            num_layers=N_LAYERS,
+            num_heads=NUM_KV_HEADS,
+            num_kv_heads=NUM_KV_HEADS,
+            embed_dim=EMBED_DIM,
+            max_seq_len=MAX_SEQ_LEN,
+        )
+
+    def test_bias_in_classifier_checkpoint_is_removed(self, llama2_classifier_model):
+        # construct bogus state dict with output.bias included
+        state_dict_with_bias = llama2_classifier_model.state_dict().copy()
+        state_dict_with_bias["output.bias"] = torch.tensor([NUM_CLASSES])
+
+        # function should remove output.bias
+        update_state_dict_for_classifier(
+            state_dict_with_bias, llama2_classifier_model.named_parameters()
+        )
+
+        assert "output.bias" not in state_dict_with_bias
+
+    def test_loading_base_checkpoint_into_classifier(
+        self, llama2_state_dict, llama2_classifier_model
+    ):
+        # grabbing the expected output.weight - the correct outcome here
+        # is for all weights aside from output.weight to be loaded in
+        # from the base model, so output.weight will remain in its rand init state
+        expected_output_weight = llama2_classifier_model.state_dict()[
+            "output.weight"
+        ].clone()
+
+        # update the state dict to load with the classifier's output.weight
+        update_state_dict_for_classifier(
+            llama2_state_dict, llama2_classifier_model.named_parameters()
+        )
+
+        # load in all the base params
+        llama2_classifier_model.load_state_dict(llama2_state_dict)
+
+        # now we can assert that output.weight was unchanged
+        output_weight = llama2_classifier_model.state_dict()["output.weight"]
+        assert torch.equal(expected_output_weight, output_weight)
+
+    def test_assertion_error_when_missing_output_in_state_dict(
+        self, llama2_state_dict, llama2_classifier_model
+    ):
+        llama2_state_dict.pop("output.weight")
+        with pytest.raises(
+            AssertionError, match="Expected output.weight in state_dict"
+        ):
+            update_state_dict_for_classifier(
+                llama2_state_dict, llama2_classifier_model.named_parameters()
+            )
+
+    def test_assertion_error_when_missing_output_in_model_named_parameters(
+        self, llama2_state_dict, llama2_classifier_model
+    ):
+        named_params = [
+            (k, v)
+            for (k, v) in llama2_classifier_model.named_parameters()
+            if k != "output.weight"
+        ]
+        with pytest.raises(
+            AssertionError, match="Expected output.weight in model_named_parameters"
+        ):
+            update_state_dict_for_classifier(llama2_state_dict, named_params)
+
+    def test_loading_classifier_weights(self, llama2_classifier_model):
+        state_dict_to_load = deepcopy(llama2_classifier_model.state_dict())
+        state_dict_to_load["output.weight"] = torch.ones_like(
+            state_dict_to_load["output.weight"]
+        )
+
+        update_state_dict_for_classifier(
+            state_dict_to_load, llama2_classifier_model.named_parameters()
+        )
+        llama2_classifier_model.load_state_dict(state_dict_to_load)
+
+        model_state_dict = llama2_classifier_model.state_dict()
+
+        assert set(model_state_dict.keys()) == set(state_dict_to_load.keys())
+        assert torch.equal(
+            model_state_dict["output.weight"],
+            torch.ones_like(model_state_dict["output.weight"]),
+        )
+
+    def test_loading_classifier_weights_force_override(self, llama2_classifier_model):
+        state_dict_to_load = deepcopy(llama2_classifier_model.state_dict())
+        state_dict_to_load["output.weight"] = torch.ones_like(
+            state_dict_to_load["output.weight"]
+        )
+
+        expected_output_weight = llama2_classifier_model.state_dict()[
+            "output.weight"
+        ].clone()
+
+        update_state_dict_for_classifier(
+            state_dict_to_load, llama2_classifier_model.named_parameters(), True
+        )
+        llama2_classifier_model.load_state_dict(state_dict_to_load)
+
+        model_state_dict = llama2_classifier_model.state_dict()
+
+        assert set(model_state_dict.keys()) == set(state_dict_to_load.keys())
+        assert torch.equal(model_state_dict["output.weight"], expected_output_weight)
+
+
+class TestFormattedCheckpointFiles:
+    @pytest.fixture
+    def expected_filenames(self):
+        return [
+            "model_0001_of_0012.pt",
+            "model_0002_of_0012.pt",
+            "model_0003_of_0012.pt",
+            "model_0004_of_0012.pt",
+            "model_0005_of_0012.pt",
+            "model_0006_of_0012.pt",
+            "model_0007_of_0012.pt",
+            "model_0008_of_0012.pt",
+            "model_0009_of_0012.pt",
+            "model_0010_of_0012.pt",
+            "model_0011_of_0012.pt",
+            "model_0012_of_0012.pt",
+        ]
+
+    def test_invalid_to_dict(self):
+        invalid_dict = {"bad_key": "model_{}_of_{}.pt", "max_filename": "0005"}
+        with pytest.raises(ValueError, match="Must pass 'filename_format'"):
+            _ = FormattedCheckpointFiles.from_dict(invalid_dict)
+
+    def test_invalid_filename_format(self):
+        formatted_string = "invalid_format_{}.pt"
+        formatted_file_dict = {
+            "filename_format": formatted_string,
+            "max_filename": "0005",
+        }
+        with pytest.raises(ValueError, match="must have exactly two placeholders"):
+            FormattedCheckpointFiles.from_dict(formatted_file_dict)
+
+    def test_build_checkpoint_filenames(self, expected_filenames):
+        formatted_file_dict = {
+            "filename_format": "model_{}_of_{}.pt",
+            "max_filename": "0012",
+        }
+        formatted_files = FormattedCheckpointFiles.from_dict(formatted_file_dict)
+        actual_filenames = formatted_files.build_checkpoint_filenames()
+        assert actual_filenames == expected_filenames
diff -ruN marc_original/third_party/torchtune/tests/torchtune/training/test_distributed.py marc/third_party/torchtune/tests/torchtune/training/test_distributed.py
--- marc_original/third_party/torchtune/tests/torchtune/training/test_distributed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/training/test_distributed.py	2025-02-20 17:49:30.134025206 -0500
@@ -0,0 +1,525 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.
+
+import copy
+from itertools import chain
+
+import pytest
+import torch
+import torch.nn as nn
+from packaging import version
+from tests.test_utils import gpu_test, single_box_init
+from torch.distributed import launcher
+
+from torch.distributed._composable.fsdp import fully_shard
+from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
+    CheckpointWrapper,
+)
+from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
+
+from torch.testing._internal.common_fsdp import FSDPTest, MLP
+from torchao.dtypes.nf4tensor import NF4Tensor
+from torchtune import modules, training
+from torchtune.models.llama2._component_builders import llama2, lora_llama2
+from torchtune.models.llama3._component_builders import llama3
+from torchtune.modules import TransformerSelfAttentionLayer
+from torchtune.modules.peft import (
+    DoRALinear,
+    get_adapter_params,
+    LoRALinear,
+    set_trainable_params,
+)
+
+
+class TestDistributed:
+    def test_init_distributed(self) -> None:
+        """Integration test to confirm consistency across device initialization utilities."""
+        distributed = training.init_distributed()
+        assert (
+            not distributed
+        ), "Should return False as there are no distributed environment variables"
+
+    @staticmethod
+    def _test_worker_fn(init_pg_explicit: bool) -> None:
+        """
+        Integration test to confirm distributed initialization and consistency with process group backend utilities.
+        """
+        if init_pg_explicit:
+            torch.distributed.init_process_group(backend="gloo")
+        if not torch.distributed.is_initialized():
+            training.init_distributed(backend="gloo")
+        if not torch.distributed.is_initialized():
+            raise AssertionError("Expected torch.distributed to be initialized")
+        pg_backend = torch.distributed.get_backend()
+        assert (
+            pg_backend == "gloo"
+        ), f"Expected 'gloo' backend, but received {pg_backend}"
+
+    @staticmethod
+    def _test_world_size_with_cpu_device(expected_world_size: int) -> None:
+        training.init_distributed(backend="gloo")
+        world_size, _ = training.get_world_size_and_rank()
+        if world_size != expected_world_size:
+            raise AssertionError(
+                f"Expected different world size: received {world_size}, expected {expected_world_size}"
+            )
+
+    def _test_launch_worker(
+        self,
+        get_pet_launch_config,
+        num_processes: int,
+        init_pg_explicit: bool,
+    ) -> None:
+        lc = get_pet_launch_config(num_processes)
+        launcher.elastic_launch(lc, entrypoint=self._test_worker_fn)(init_pg_explicit)
+
+    def test_init_from_env_no_dup(self, get_pet_launch_config) -> None:
+        self._test_launch_worker(get_pet_launch_config, 2, init_pg_explicit=False)
+        # trivial test case to ensure test passes with no exceptions
+        assert True
+
+    def test_init_from_env_dup(self, get_pet_launch_config) -> None:
+        self._test_launch_worker(get_pet_launch_config, 2, init_pg_explicit=True)
+        # trivial test case to ensure test passes with no exceptions
+        assert True
+
+    def test_world_size_with_cpu(self, get_pet_launch_config) -> None:
+        desired_world_size = 4
+        lc = get_pet_launch_config(desired_world_size)
+        launcher.elastic_launch(lc, entrypoint=self._test_world_size_with_cpu_device)(
+            desired_world_size
+        )
+
+    def test_validate_no_params_on_meta_device(self) -> None:
+        with torch.device("meta"):
+            model = torch.nn.Linear(3, 3)
+
+        with pytest.raises(RuntimeError, match="Unexpected param or buffer"):
+            training.validate_no_params_on_meta_device(model)
+
+        # Test model with only buffer
+        model = torch.nn.Linear(3, 3)
+        buffer = torch.ones(1, device="meta")
+        model.register_buffer("buffer", buffer)
+
+        with pytest.raises(RuntimeError, match="Unexpected param or buffer"):
+            training.validate_no_params_on_meta_device(model)
+
+    def test_get_fsdp_wrap_policies(self) -> None:
+        with single_box_init():
+            llama3_policy = training.get_full_finetune_fsdp_wrap_policy(
+                memory_efficient_fsdp_wrap=True,
+                modules_to_wrap={modules.TransformerSelfAttentionLayer},
+            )
+            l3 = llama3(
+                vocab_size=64,
+                num_layers=1,
+                num_heads=4,
+                num_kv_heads=4,
+                embed_dim=64,
+                max_seq_len=128,
+            )
+            wrapped_l3 = FSDP(
+                l3, auto_wrap_policy=llama3_policy, device_id=torch.device("cpu")
+            )
+            # Ensure embedding, output proj, and transformer decoder blocks are wrapped
+            assert isinstance(wrapped_l3.tok_embeddings, FSDP)
+            assert isinstance(wrapped_l3.output, FSDP)
+            for layer in wrapped_l3.layers:
+                assert isinstance(layer, FSDP)
+
+            llama2_policy = training.get_full_finetune_fsdp_wrap_policy(
+                memory_efficient_fsdp_wrap=False,
+                modules_to_wrap={modules.TransformerSelfAttentionLayer},
+            )
+            l2 = llama2(
+                vocab_size=64,
+                num_layers=1,
+                num_heads=4,
+                num_kv_heads=4,
+                embed_dim=64,
+                max_seq_len=128,
+            )
+            wrapped_l2 = FSDP(
+                l2, auto_wrap_policy=llama2_policy, device_id=torch.device("cpu")
+            )
+            # Ensure embedding, output proj, and transformer decoder blocks are not wrapped
+            assert not isinstance(wrapped_l2.tok_embeddings, FSDP)
+            assert not isinstance(wrapped_l2.output, FSDP)
+            # Ensure transformer decoder blocks are wrapped
+            for layer in wrapped_l2.layers:
+                assert isinstance(layer, FSDP)
+
+
+N_LAYERS = 3
+IN_DIM = 5
+OUT_DIM = 10
+VOCAB_SIZE = 50
+NUM_HEADS = 4
+NUM_KV_HEADS = 2
+EMBED_DIM = 64
+MAX_SEQ_LEN = 64
+
+
+def _get_n_lora_and_tformer_layers(model):
+    num_lora_ab = 0
+    num_transformer_layers = 0
+    for module in model.modules():
+        if isinstance(module, LoRALinear) or isinstance(module, DoRALinear):
+            num_nested_linears = len(
+                [m for m in module.modules() if isinstance(m, nn.Linear)]
+            )
+            num_lora_ab += num_nested_linears
+        if isinstance(module, TransformerSelfAttentionLayer):
+            num_transformer_layers += 1
+
+    return num_lora_ab, num_transformer_layers
+
+
+# TODO: figure out a permanent home for FSDP + LoRA code
+class TestLoRAFSDP:
+    def test_lora_fsdp_wrap(self):
+        with torch.device("meta"):
+            model = lora_llama2(
+                lora_attn_modules=["q_proj", "v_proj"],
+                vocab_size=VOCAB_SIZE,
+                num_layers=N_LAYERS,
+                num_heads=NUM_HEADS,
+                num_kv_heads=NUM_KV_HEADS,
+                embed_dim=EMBED_DIM,
+                max_seq_len=MAX_SEQ_LEN,
+                lora_rank=4,
+                lora_alpha=1.0,
+            )
+
+        adapter_params = get_adapter_params(model)
+        set_trainable_params(model, adapter_params)
+        num_lora_ab, num_transformer_layers = _get_n_lora_and_tformer_layers(model)
+        with single_box_init():
+            lora_wrap_policy = training.lora_fsdp_wrap_policy(
+                modules_to_wrap={TransformerSelfAttentionLayer}
+            )
+            training.prepare_model_for_fsdp_with_meta_device(model)
+            wrapped_lora = FSDP(
+                model,
+                auto_wrap_policy=lora_wrap_policy,
+                device_id=torch.device("cpu"),
+            )
+
+            # After FSDP wrap, nothing should be left on meta device, and LoRA params
+            # should be initialized.
+            for p in chain(wrapped_lora.parameters(), wrapped_lora.buffers()):
+                assert not p.is_meta
+
+            for m in wrapped_lora.modules():
+                if isinstance(m, LoRALinear) or isinstance(m, DoRALinear):
+                    torch.testing.assert_close(
+                        m.lora_b.weight, torch.zeros_like(m.lora_b.weight)
+                    )
+            # Total # FSDP modules should be num_transformer + num_lora_ab + 1
+            total_fsdp_submodules = len([m for m in FSDP.fsdp_modules(wrapped_lora)])
+            assert total_fsdp_submodules == (num_lora_ab + num_transformer_layers + 1)
+            # LoRA a & b linears should be individually wrapped.
+            # And TransformerSelfAttentionLayers should be individually wrapped.
+            for fsdp_submodule in FSDP.fsdp_modules(wrapped_lora):
+                if isinstance(fsdp_submodule.module, nn.Linear):
+                    num_lora_ab -= 1
+                elif isinstance(fsdp_submodule.module, TransformerSelfAttentionLayer):
+                    num_transformer_layers -= 1
+            assert num_lora_ab == 0
+            assert num_transformer_layers == 0
+
+    def test_lora_meta_device_init_fsdp(self):
+        with torch.device("meta"):
+            lora = lora_llama2(
+                lora_attn_modules=["q_proj", "v_proj"],
+                vocab_size=VOCAB_SIZE,
+                num_layers=N_LAYERS,
+                num_heads=NUM_HEADS,
+                num_kv_heads=NUM_KV_HEADS,
+                embed_dim=EMBED_DIM,
+                max_seq_len=MAX_SEQ_LEN,
+                lora_rank=4,
+                lora_alpha=8,
+            )
+        training.prepare_model_for_fsdp_with_meta_device(lora)
+        for m in lora.modules():
+            m.to_empty(device=torch.device("cpu"), recurse=False)
+            m.reset_parameters()
+        # No params should be left on meta device
+        for n, p in lora.named_parameters():
+            assert not p.is_meta, f"parameter {n} is still on meta device!"
+        # Neither should buffers
+        for n, b in lora.named_buffers():
+            assert not b.is_meta, f"buffer {n} is still on meta device!"
+
+
+class TestFullyShardState(FSDPTest):
+    @property
+    def world_size(self) -> int:
+        return 2
+
+    @gpu_test(gpu_count=2)
+    @pytest.mark.skipif(
+        version.parse(torch.__version__).base_version < "2.4.0",
+        reason="torch >= 2.4 required",
+    )
+    def test_lora_state_dict(self):
+        rank = self.rank
+        is_rank_zero = rank == 0
+        mlp_dim = 4
+        epochs = 5
+        torch.manual_seed(42)
+        # base_model is simple DDP
+        with torch.device("cuda"):
+            base_model = nn.Sequential(
+                MLP(mlp_dim),
+                nn.Sequential(MLP(mlp_dim), nn.Linear(mlp_dim, mlp_dim)),
+                MLP(mlp_dim),
+            )
+            base_optim = torch.optim.Adam(
+                base_model.parameters(), weight_decay=0.01, lr=0.01
+            )
+
+        fsdp_model_to_save = copy.deepcopy(base_model)
+        for module in fsdp_model_to_save:
+            fully_shard(module)
+        fully_shard(fsdp_model_to_save)
+        fsdp_optim_to_save = torch.optim.Adam(
+            fsdp_model_to_save.parameters(), weight_decay=0.01, lr=0.01
+        )
+
+        # inp is different for each rank
+        torch.manual_seed(42 + rank)
+
+        # test get full state dict
+        for _ in range(epochs):
+            inp = torch.randn((2, mlp_dim), device="cuda")
+            base_model(inp).sum().backward()
+            for param in base_model.parameters():
+                torch.distributed.all_reduce(
+                    param.grad, op=torch.distributed.ReduceOp.AVG
+                )
+            base_optim.step()
+            base_optim.zero_grad()
+            fsdp_model_to_save(inp).sum().backward()
+            fsdp_optim_to_save.step()
+            fsdp_optim_to_save.zero_grad()
+        expected_model_sd = base_model.state_dict()
+        expected_optim_sd = base_optim.state_dict()
+        model_full_sd = training.get_full_model_state_dict(
+            fsdp_model_to_save, is_rank_zero
+        )
+        optim_full_sd = training.get_full_optimizer_state_dict(
+            fsdp_optim_to_save,
+            is_rank_zero,
+        )
+        if is_rank_zero:
+            self.assertEqual(set(model_full_sd.keys()), set(expected_model_sd.keys()))
+            for key, value in model_full_sd.items():
+                self.assertEqual(value, expected_model_sd[key])
+            self.assertEqual(len(optim_full_sd["param_groups"]), 1)
+            self.assertEqual(
+                len(optim_full_sd["param_groups"]),
+                len(expected_optim_sd["param_groups"]),
+            )
+            self.assertEqual(
+                len(optim_full_sd["param_groups"][0].keys()),
+                len(expected_optim_sd["param_groups"][0].keys()),
+            )
+            for key, value in optim_full_sd["param_groups"][0].items():
+                if key == "params":
+                    self.assertEqual(
+                        len(value), len(expected_optim_sd["param_groups"][0][key])
+                    )
+                else:
+                    self.assertEqual(value, expected_optim_sd["param_groups"][0][key])
+            self.assertEqual(
+                len(optim_full_sd["state"].keys()),
+                len(expected_optim_sd["state"].keys()),
+            )
+            for actual, expected in zip(
+                optim_full_sd["state"].values(), expected_optim_sd["state"].values()
+            ):
+                self.assertEqual(actual, expected)
+        else:
+            self.assertEqual(len(model_full_sd), 0)
+            self.assertEqual(len(optim_full_sd), 0)
+
+        # test set full state dict
+        with torch.device("meta"):
+            fsdp_model_to_load = nn.Sequential(
+                MLP(mlp_dim),
+                nn.Sequential(MLP(mlp_dim), nn.Linear(mlp_dim, mlp_dim)),
+                MLP(mlp_dim),
+            )
+        for module in fsdp_model_to_load:
+            fully_shard(module)
+        fully_shard(fsdp_model_to_load)
+        training.load_from_full_model_state_dict(
+            fsdp_model_to_load,
+            copy.deepcopy(base_model.state_dict()),
+            torch.device("cuda"),
+            is_rank_zero,
+        )
+        fsdp_optim_to_load = torch.optim.Adam(
+            fsdp_model_to_load.parameters(), weight_decay=0.01, lr=0.01
+        )
+        training.load_from_full_optimizer_state_dict(
+            fsdp_optim_to_load,
+            # mimic mmap=True where every rank see full SD
+            copy.deepcopy(self._broadcast_full_state_dict(optim_full_sd)),
+            torch.device("cuda"),
+        )
+        for _ in range(epochs):
+            inp = torch.randn((2, mlp_dim), device="cuda")
+            fsdp_model_to_load(inp).sum().backward()
+            fsdp_model_to_save(inp).sum().backward()
+            fsdp_optim_to_load.step()
+            fsdp_optim_to_save.step()
+            fsdp_optim_to_load.zero_grad()
+            fsdp_optim_to_save.zero_grad()
+        sharded_optim_sd = fsdp_optim_to_load.state_dict()
+        expected_sharded_optim_sd = fsdp_optim_to_save.state_dict()
+        self.assertEqual(
+            sharded_optim_sd["param_groups"],
+            expected_sharded_optim_sd["param_groups"],
+        )
+        self.assertEqual(
+            set(sharded_optim_sd["state"].keys()),
+            set(expected_sharded_optim_sd["state"].keys()),
+        )
+        for key, value in sharded_optim_sd["state"].items():
+            self.assertEqual(value, expected_sharded_optim_sd["state"][key])
+
+        sharded_model_sd = fsdp_model_to_load.state_dict()
+        expected_sharded_model_sd = fsdp_model_to_save.state_dict()
+        self.assertEqual(
+            set(sharded_model_sd.keys()), set(expected_sharded_model_sd.keys())
+        )
+        for key, value in sharded_model_sd.items():
+            self.assertEqual(value, expected_sharded_model_sd[key])
+
+    @pytest.mark.skipif(
+        version.parse(torch.__version__).base_version < "2.4.0",
+        reason="torch >= 2.4 required",
+    )
+    @gpu_test(gpu_count=2)
+    def test_qlora_state_dict(self):
+        self.run_subtests(
+            {
+                "enable_activation_checkpointing": [False, True],
+            },
+            self._test_qlora_state_dict,
+        )
+
+    def _test_qlora_state_dict(self, enable_activation_checkpointing: bool):
+        is_rank_zero = self.rank == 0
+        torch.manual_seed(42)
+        kwargs = {
+            "lora_attn_modules": ["q_proj", "v_proj", "k_proj", "output_proj"],
+            "apply_lora_to_mlp": True,
+            "apply_lora_to_output": False,
+            "vocab_size": 1024,
+            "num_layers": 3,
+            "num_heads": 4,
+            "num_kv_heads": 2,
+            "embed_dim": 1024,
+            "max_seq_len": 64,
+            "lora_rank": 4,
+            "lora_alpha": 1.0,
+            "quantize_base": True,
+        }
+        # single-device model as groundtruth
+        with torch.device("cuda"):
+            base_model = lora_llama2(**kwargs)
+        set_trainable_params(base_model, get_adapter_params(base_model))
+        if enable_activation_checkpointing:
+            training.set_activation_checkpointing(
+                base_model, auto_wrap_policy={modules.TransformerSelfAttentionLayer}
+            )
+
+        # fsdp model for saving state dict
+        fsdp_model_to_save = copy.deepcopy(base_model)
+        for m in fsdp_model_to_save.modules():
+            if enable_activation_checkpointing:
+                if isinstance(m, CheckpointWrapper):
+                    fully_shard(m)
+            else:
+                if isinstance(m, modules.TransformerSelfAttentionLayer):
+                    fully_shard(m)
+        fully_shard(fsdp_model_to_save)
+
+        # one forward pass for lazy init
+        torch.manual_seed(42 + self.rank)
+        inp = torch.randint(
+            low=0,
+            high=kwargs["vocab_size"],
+            size=(2, kwargs["max_seq_len"]),
+            device="cuda",
+        )
+        base_model(inp)
+        fsdp_model_to_save(inp)
+
+        expected_model_sd = {k: v.cpu() for k, v in base_model.state_dict().items()}
+        model_full_sd = training.get_full_model_state_dict(
+            fsdp_model_to_save, is_rank_zero
+        )
+        if is_rank_zero:
+            self.assertEqual(set(model_full_sd.keys()), set(expected_model_sd.keys()))
+            for key, value in model_full_sd.items():
+                self.assertEqual(value, expected_model_sd[key])
+
+        # fsdp model for loading tate dict
+        torch.manual_seed(42)
+        with torch.device("meta"):
+            fsdp_model_to_load = lora_llama2(**kwargs)
+        set_trainable_params(fsdp_model_to_load, get_adapter_params(fsdp_model_to_load))
+        if enable_activation_checkpointing:
+            training.set_activation_checkpointing(
+                fsdp_model_to_load,
+                auto_wrap_policy={modules.TransformerSelfAttentionLayer},
+            )
+        # init rope since it's not covered in state dict
+        for m in fsdp_model_to_load.modules():
+            if isinstance(m, modules.RotaryPositionalEmbeddings):
+                m.reset_parameters()
+        for m in fsdp_model_to_load.modules():
+            if enable_activation_checkpointing:
+                if isinstance(m, CheckpointWrapper):
+                    fully_shard(m)
+            else:
+                if isinstance(m, modules.TransformerSelfAttentionLayer):
+                    fully_shard(m)
+        fully_shard(fsdp_model_to_load)
+        training.load_from_full_model_state_dict(
+            fsdp_model_to_load, expected_model_sd, torch.device("cuda"), is_rank_zero
+        )
+        fsdp_model_to_load(inp)
+        sharded_model_sd = fsdp_model_to_load.state_dict()
+        expected_sharded_model_sd = fsdp_model_to_save.state_dict()
+        self.assertEqual(
+            set(sharded_model_sd.keys()), set(expected_sharded_model_sd.keys())
+        )
+        for key, value in sharded_model_sd.items():
+            if isinstance(value._local_tensor, NF4Tensor):
+                self.assertEqual(
+                    value._local_tensor.get_original_weight(),
+                    expected_sharded_model_sd[key]._local_tensor.get_original_weight(),
+                )
+            else:
+                self.assertEqual(value, expected_sharded_model_sd[key])
+
+    def _broadcast_full_state_dict(self, full_sd):
+        result = []
+        if torch.distributed.get_rank() == 0:
+            result.append(full_sd)
+        else:
+            result.append(None)
+        torch.distributed.broadcast_object_list(result, src=0)
+        return result[0]
diff -ruN marc_original/third_party/torchtune/tests/torchtune/training/test_memory.py marc/third_party/torchtune/tests/torchtune/training/test_memory.py
--- marc_original/third_party/torchtune/tests/torchtune/training/test_memory.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/training/test_memory.py	2025-02-20 17:49:30.138025213 -0500
@@ -0,0 +1,129 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import pytest
+import torch
+import torch.nn as nn
+from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
+    CheckpointWrapper,
+)
+from torchtune.training import (
+    create_optim_in_bwd_wrapper,
+    register_optim_in_bwd_hooks,
+    set_activation_checkpointing,
+)
+
+
+class TestSetActivationCheckpointing:
+    @pytest.fixture
+    def model(self) -> int:
+        return nn.Sequential(
+            nn.Linear(10, 10),
+            nn.Linear(10, 10),
+            nn.Linear(10, 10),
+            nn.ReLU(),
+            nn.Dropout(0.5),
+        )
+
+    def _verify(self, model):
+        for submodule in model.modules():
+            if isinstance(submodule, CheckpointWrapper):
+                assert isinstance(submodule._checkpoint_wrapped_module, nn.Linear)
+
+    def test_activation_checkpoint_set_policy(self, model):
+        set_activation_checkpointing(model=model, auto_wrap_policy={nn.Linear})
+        self._verify(model)
+
+    def test_activation_checkpoint_custom_policy(self, model):
+        def custom_policy(module: nn.Module, recurse: bool, **kwargs) -> bool:
+            if recurse:
+                return True
+            return isinstance(module, nn.Linear)
+
+        set_activation_checkpointing(model=model, auto_wrap_policy=custom_policy)
+        self._verify(model)
+
+
+def _run_dummy_step(model, wrapper):
+    with torch.no_grad():
+        for p in model.parameters():
+            p.grad = torch.rand_like(p)
+    for v in wrapper.optim_map.values():
+        v.step()
+        v.zero_grad()
+
+
+def _validate_dicts(d1, d2):
+    if len(d1) != len(d2):
+        return False
+    for k, v in d1.items():
+        if k not in d2:
+            return False
+        if isinstance(v, dict):
+            return _validate_dicts(v, d2[k])
+        else:
+            if isinstance(v, torch.Tensor):
+                if not torch.allclose(v, d2[k]):
+                    return False
+            elif v != d2[k]:
+                return False
+    return True
+
+
+@pytest.fixture
+def model():
+    return torch.nn.Linear(10, 1)
+
+
+@pytest.fixture
+def optim_dict(model):
+    return {p: torch.optim.AdamW([p], lr=0.01) for p in model.parameters()}
+
+
+@pytest.fixture
+def wrapper(model, optim_dict):
+    return create_optim_in_bwd_wrapper(model, optim_dict)
+
+
+class TestOptimInBackward:
+    def test_state_dict_save_load(self, model, wrapper):
+        # Run a dummy step to create optimizer states
+        _run_dummy_step(model, wrapper)
+
+        sd = wrapper.state_dict()
+        new_optim_dict = create_optim_in_bwd_wrapper(
+            model, {p: torch.optim.AdamW([p], lr=0.01) for p in model.parameters()}
+        )
+        assert not _validate_dicts(sd, new_optim_dict.state_dict())
+        new_optim_dict.load_state_dict(sd)
+        assert _validate_dicts(sd, new_optim_dict.state_dict())
+
+    def test_missing_unexpected_param_load_raises(self, model, wrapper):
+        # Run a dummy step to create optimizer states
+        _run_dummy_step(model, wrapper)
+        sd = wrapper.state_dict()
+        new_optim_dict = create_optim_in_bwd_wrapper(
+            model, {p: torch.optim.AdamW([p], lr=0.01) for p in model.parameters()}
+        )
+        with pytest.raises(RuntimeError, match="Expected to load optimizer state"):
+            sd.pop(next(iter(sd.keys())))
+            new_optim_dict.load_state_dict(sd)
+
+        sd = wrapper.state_dict()
+        sd["new_key"] = 1234
+        with pytest.raises(RuntimeError, match="unexpected param"):
+            new_optim_dict.load_state_dict(sd)
+
+
+class TestRegisterOptimHooks:
+    def test_register_optim_in_bwd_hooks(self, model, optim_dict):
+        register_optim_in_bwd_hooks(model, optim_dict)
+        # Ensure backward() updates the parameters and sets grads to None
+        orig_params = [p.clone().detach() for p in model.parameters()]
+        model(torch.rand(2, 10)).sum().backward()
+        for p, orig_p in zip(model.parameters(), orig_params):
+            assert not p.grad
+            assert not torch.allclose(p, orig_p)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/training/test_metric_logging.py marc/third_party/torchtune/tests/torchtune/training/test_metric_logging.py
--- marc_original/third_party/torchtune/tests/torchtune/training/test_metric_logging.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/training/test_metric_logging.py	2025-02-20 17:49:30.142025219 -0500
@@ -0,0 +1,201 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+import tempfile
+from io import StringIO
+from typing import cast
+from unittest.mock import patch
+
+import pytest
+from omegaconf import OmegaConf
+from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
+
+from tests.test_utils import assert_expected, captured_output
+
+from torchtune.training.metric_logging import (
+    CometLogger,
+    DiskLogger,
+    StdoutLogger,
+    TensorBoardLogger,
+    WandBLogger,
+)
+
+
+class TestDiskLogger:
+    def test_log(self) -> None:
+        with tempfile.TemporaryDirectory() as log_dir:
+            logger = DiskLogger(log_dir=log_dir)
+            for i in range(5):
+                logger.log("test_log", float(i) ** 2, i)
+            logger.close()
+
+            log_path = logger.path_to_log_file()
+            assert log_path.exists()
+            values = open(log_path).readlines()
+            assert_expected(len(values), 5)
+            for i in range(5):
+                assert values[i] == f"Step {i} | test_log:{float(i) ** 2}\n"
+
+    def test_log_dict(self) -> None:
+        with tempfile.TemporaryDirectory() as log_dir:
+            logger = DiskLogger(log_dir=log_dir)
+            for i in range(5):
+                logger.log_dict(step=i, payload={"metric_1": i, "metric_2": i**2})
+            logger.close()
+
+            log_path = logger.path_to_log_file()
+            assert log_path.exists()
+            values = open(log_path).readlines()
+            assert_expected(len(values), 5)
+            for i in range(5):
+                assert values[i] == f"Step {i} | metric_1:{i} metric_2:{i ** 2} \n"
+
+
+class TestStdoutLogger:
+    def test_stdout_log(self) -> None:
+        logger = StdoutLogger()
+        with captured_output() as (out, _):
+            logger.log(step=0, name="metric_1", data=1.1)
+            out = cast(StringIO, out)
+            assert (
+                out.getvalue() == "Step 0 | metric_1:1.1\n"
+            ), f"Actual output: {out.getvalue()}"
+
+            logger.log(step=1, name="metric_1", data=2.1)
+            assert (
+                out.getvalue() == "Step 0 | metric_1:1.1\nStep 1 | metric_1:2.1\n"
+            ), f"Actual output: {out.getvalue()}"
+
+            logger.close()
+            assert (
+                out.getvalue() == "Step 0 | metric_1:1.1\nStep 1 | metric_1:2.1\n"
+            ), f"Actual output: {out.getvalue()}"
+
+    def test_stdout_log_dict(self) -> None:
+        logger = StdoutLogger()
+        with captured_output() as (out, _):
+            logger.log_dict(step=0, payload={"metric_1": 1, "metric_2": 1})
+            out = cast(StringIO, out)
+            assert (
+                out.getvalue() == "Step 0 | metric_1:1 metric_2:1 \n"
+            ), f"Actual output: {out.getvalue()}"
+
+            logger.log_dict(
+                step=1, payload={"metric_1": 2, "metric_2": 2.2, "metric_3": 2.2344}
+            )
+            assert (
+                out.getvalue()
+                == "Step 0 | metric_1:1 metric_2:1 \nStep 1 | metric_1:2 metric_2:2.2 metric_3:2.2344 \n"
+            ), f"Actual output: {out.getvalue()}"
+
+            logger.close()
+            assert (
+                out.getvalue()
+                == "Step 0 | metric_1:1 metric_2:1 \nStep 1 | metric_1:2 metric_2:2.2 metric_3:2.2344 \n"
+            ), f"Actual output: {out.getvalue()}"
+
+
+class TestTensorBoardLogger:
+    def test_log(self) -> None:
+        with tempfile.TemporaryDirectory() as log_dir:
+            logger = TensorBoardLogger(log_dir=log_dir)
+            for i in range(5):
+                logger.log("test_log", float(i) ** 2, i)
+            logger.close()
+
+            acc = EventAccumulator(logger.log_dir)
+            acc.Reload()
+            for i, event in enumerate(acc.Tensors("test_log")):
+                assert_expected(event.tensor_proto.float_val[0], float(i) ** 2)
+                assert_expected(event.step, i)
+
+    def test_log_dict(self) -> None:
+        with tempfile.TemporaryDirectory() as log_dir:
+            logger = TensorBoardLogger(log_dir=log_dir)
+            metric_dict = {f"log_dict_{i}": float(i) ** 2 for i in range(5)}
+            logger.log_dict(metric_dict, 1)
+            logger.close()
+
+            acc = EventAccumulator(logger.log_dir)
+            acc.Reload()
+            for i in range(5):
+                tensor_tag = acc.Tensors(f"log_dict_{i}")[0]
+                assert_expected(tensor_tag.tensor_proto.float_val[0], float(i) ** 2)
+                assert_expected(tensor_tag.step, 1)
+
+
+@pytest.mark.skip(reason="This was never running and needs to be fixed")
+class TestWandBLogger:
+    def test_log(self) -> None:
+        with patch("wandb.init") as mock_init, patch("wandb.log") as mock_log:
+            logger = WandBLogger(project="test_project")
+            for i in range(5):
+                logger.log("test_log", float(i) ** 2, i)
+            logger.close()
+
+            assert mock_log.call_count == 5
+            for i in range(5):
+                mock_log.assert_any_call({"test_log": float(i) ** 2}, step=i)
+
+    def test_log_dict(self) -> None:
+        with patch("wandb.init") as mock_init, patch("wandb.log") as mock_log:
+            logger = WandBLogger(project="test_project")
+            metric_dict = {f"log_dict_{i}": float(i) ** 2 for i in range(5)}
+            logger.log_dict(metric_dict, 1)
+            logger.close()
+
+            mock_log.assert_called_with(metric_dict, step=1)
+
+    def test_save_config(self) -> None:
+        with patch("wandb.init") as mock_init, patch(
+            "wandb.run", create=True
+        ) as mock_run, patch("OmegaConf.save") as mock_save, patch(
+            "wandb.save"
+        ) as mock_wandb_save:
+
+            logger = WandBLogger(project="test_project")
+            cfg = OmegaConf.create({"a": 1, "b": 2})
+
+            with patch.object(logger, "_wandb", mock_run):
+                logger.save_config(cfg)
+
+            expected_config_path = "torchtune_config.yaml"
+            mock_save.assert_called_once_with(cfg, expected_config_path)
+            mock_wandb_save.assert_called_once_with(expected_config_path)
+
+
+class TestCometLogger:
+    def test_log(self) -> None:
+        with patch("comet_ml.start") as mock_experiment:
+            logger = CometLogger(project="test_project")
+            for i in range(5):
+                logger.log("test_log", float(i) ** 2, i)
+            logger.close()
+
+            assert mock_experiment.return_value.log_metric.call_count == 5
+            for i in range(5):
+                mock_experiment.return_value.log_metric.assert_any_call(
+                    "test_log", float(i) ** 2, step=i
+                )
+
+    def test_log_dict(self) -> None:
+        with patch("comet_ml.start") as mock_experiment:
+            logger = CometLogger(project="test_project")
+            metric_dict = {f"log_dict_{i}": float(i) ** 2 for i in range(5)}
+            logger.log_dict(metric_dict, 1)
+            logger.close()
+
+            mock_experiment.return_value.log_metrics.assert_called_with(
+                metric_dict, step=1
+            )
+
+    def test_log_config(self) -> None:
+        with patch("comet_ml.start") as mock_experiment:
+            logger = CometLogger(project="test_project")
+            cfg = OmegaConf.create({"a": 1, "b": 2})
+            logger.log_config(cfg)
+            mock_experiment.return_value.log_parameters.assert_called_with(cfg)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/training/test_pooling.py marc/third_party/torchtune/tests/torchtune/training/test_pooling.py
--- marc_original/third_party/torchtune/tests/torchtune/training/test_pooling.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/training/test_pooling.py	2025-02-20 17:49:30.146025225 -0500
@@ -0,0 +1,56 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import torch
+from torchtune.training.pooling import get_unmasked_sequence_lengths
+
+
+class TestGetLastUnmaskedTokenIdx:
+    def test_get_last_unmasked_token_idx_multi_batch(self):
+        """
+        Tests that the last non-padding tokens are correctly selected for a multi-batch input.
+        """
+        padding_token_idx = 0
+        tokens = torch.tensor([[1, 3, 4, 9], [4, 5, 6, 0], [1, 0, 0, 0], [0, 0, 0, 0]])
+        expected_output = torch.tensor([3, 2, 0, 0])
+        idxs = get_unmasked_sequence_lengths(tokens == padding_token_idx)
+        torch.testing.assert_close(idxs, expected_output)
+
+    def test_get_last_unmasked_token_idx_single_batch(self):
+        """
+        Tests that the last non-padding tokens are correctly selected for a single-batch input.
+        """
+        padding_token_idx = 0
+        tokens = torch.tensor([[1, 3, 4, 9, 0]])
+        expected_output = torch.tensor([3])
+        idxs = get_unmasked_sequence_lengths(tokens == padding_token_idx)
+
+        torch.testing.assert_close(idxs, expected_output)
+
+    def test_get_last_unmasked_token_idx_multi_batch_all_full(self):
+        """
+        Tests that the last non-padding tokens are correctly selected for multi-batch input,
+        where none of the sequences have padding tokens.
+        """
+        padding_token_idx = 0
+        tokens = torch.tensor(
+            [[1, 3, 4, 9], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
+        )
+        expected_output = torch.tensor([3, 3, 3, 3])
+        idxs = get_unmasked_sequence_lengths(tokens == padding_token_idx)
+
+        torch.testing.assert_close(idxs, expected_output)
+
+    def test_get_last_unmasked_token_idx_multi_batch_all_empty(self):
+        """
+        Tests that the last non-padding tokens are correctly selected for multi-batch input,
+        where none of the sequences have any non-padding tokens.
+        """
+        padding_token_idx = 0
+        tokens = torch.zeros((4, 4), dtype=torch.long)
+        expected_output = torch.tensor([0, 0, 0, 0])
+        idxs = get_unmasked_sequence_lengths(tokens == padding_token_idx)
+
+        torch.testing.assert_close(idxs, expected_output)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/training/test_precision.py marc/third_party/torchtune/tests/torchtune/training/test_precision.py
--- marc_original/third_party/torchtune/tests/torchtune/training/test_precision.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/training/test_precision.py	2025-02-20 17:49:30.150025232 -0500
@@ -0,0 +1,91 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.
+
+
+from unittest import mock
+
+import pytest
+import torch
+
+from torchtune.training.precision import (
+    _set_float32_precision,
+    get_dtype,
+    PRECISION_STR_TO_DTYPE,
+    set_default_dtype,
+    validate_expected_param_dtype,
+    verify_bf16_support,
+)
+
+
+class TestPrecisionUtils:
+
+    cuda_available: bool = torch.cuda.is_available()
+
+    def test_get_dtype(self):
+        """
+        Tests that the correct dtype is returned based on the input string.
+        """
+        dtypes = [None, torch.half] + list(PRECISION_STR_TO_DTYPE.keys())
+        expected_dtypes = [
+            torch.float32,
+            torch.float16,
+            torch.float16,
+            torch.bfloat16,
+            torch.float32,
+            torch.float64,
+        ]
+        for dtype, expected_dtype in zip(dtypes, expected_dtypes):
+            if dtype == "bf16" and not verify_bf16_support():
+                continue  # skip bf16 tests if not supported.
+            assert (
+                get_dtype(dtype) == expected_dtype
+            ), f"{dtype} should return {expected_dtype}"
+
+    @mock.patch("torchtune.training.precision.verify_bf16_support", return_value=False)
+    def test_error_bf16_unsupported(self, mock_verify):
+        """
+        Tests that an error is raised if bf16 is specified but not supported.
+        """
+        with pytest.raises(
+            RuntimeError, match="bf16 precision was requested but not available"
+        ):
+            get_dtype(torch.bfloat16)
+
+    @pytest.mark.skipif(not cuda_available, reason="The test requires GPUs to run.")
+    def test_set_float32_precision(self) -> None:
+        setattr(  # noqa: B010
+            torch.backends, "__allow_nonbracketed_mutation_flag", True
+        )
+        _set_float32_precision("highest")
+        assert torch.get_float32_matmul_precision() == "highest"
+        assert not torch.backends.cudnn.allow_tf32
+        assert not torch.backends.cuda.matmul.allow_tf32
+
+        _set_float32_precision("high")
+        setattr(  # noqa: B010
+            torch.backends, "__allow_nonbracketed_mutation_flag", False
+        )
+        assert torch.get_float32_matmul_precision() == "high"
+        assert torch.backends.cudnn.allow_tf32
+        assert torch.backends.cuda.matmul.allow_tf32
+
+    def test_set_default_dtype(self):
+        dtype = torch.bfloat16
+        prev_dtype = torch.get_default_dtype()
+        with set_default_dtype(dtype):
+            assert torch.get_default_dtype() == dtype
+
+        assert torch.get_default_dtype() == prev_dtype
+
+    def test_validate_expected_param_dtype(self):
+        """
+        Tests that we raise if any model param has a different dtype than the expected dtype.
+        """
+        m = torch.nn.Linear(10, 10)
+        with pytest.raises(ValueError, match=f"has dtype {next(m.parameters()).dtype}"):
+            validate_expected_param_dtype(m.named_parameters(), dtype=torch.float16)
diff -ruN marc_original/third_party/torchtune/tests/torchtune/training/test_profiler.py marc/third_party/torchtune/tests/torchtune/training/test_profiler.py
--- marc_original/third_party/torchtune/tests/torchtune/training/test_profiler.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/training/test_profiler.py	2025-02-20 17:49:30.154025239 -0500
@@ -0,0 +1,253 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+
+import pytest
+import torch
+from omegaconf import DictConfig, OmegaConf
+from torch._C._profiler import _ExperimentalConfig
+from torchtune import config
+from torchtune.training import (
+    DEFAULT_PROFILE_DIR,
+    DEFAULT_PROFILER_ACTIVITIES,
+    DEFAULT_SCHEDULE,
+    DEFAULT_TRACE_OPTS,
+    DummyProfiler,
+    PROFILER_KEY,
+)
+
+# Disable logging otherwise output will be very verbose
+logging.basicConfig(level=logging.ERROR)
+
+PROFILER_ATTRS = [
+    "activities",
+    "profile_memory",
+    "with_stack",
+    "record_shapes",
+    "with_flops",
+]
+
+
+@pytest.fixture
+def profiler_cfg():
+    return """
+profiler:
+ enabled: True
+ cpu: True
+ cuda: True
+ profile_memory: False
+ with_stack: False
+ record_shapes: True
+ with_flops: True
+ wait_steps: 3
+ warmup_steps: 1
+ active_steps: 1
+ num_cycles: 0
+"""
+
+
+# This is a reference implementation of a profiler setup method to be defined within a `recipe`.
+# A version of this lives in `torch.utils._profiler` but is not exported as the public API.
+# Rather, the user is expected to define their own high-level setup function that parses the `cfg`
+# and call a user-facing profiler setup function (e.g. `setup_torch_profiler`).
+def _setup_profiler(
+    cfg_profiler: DictConfig, return_cfg: bool = False
+) -> torch.profiler.profile:
+    """
+    Parses the `profiler` section of top-level `cfg` and sets up profiler
+
+    Args:
+        cfg_profiler (DictConfig): `profiler` section of the top-level `cfg` (the main config passed to `recipe.main`)
+        return_cfg (bool): Doesn't seem to be used. Default False.
+
+    Returns:
+        profiler: torch.profiler.profile | DummyProfiler - DummyProfiler is a nullcontext with no-op methods
+        for `start`, `stop`, and `step` that can be used in place of `torch.profiler.profile` if profiler is not enabled such
+        that the instrumented training loop does not need to be changed profiling is disabled.
+    """
+    # Missing profiler section in config, assume disabled
+    if cfg_profiler is None:
+        cfg_profiler = DictConfig({"enabled": False})
+
+    # Check that component is included and set correctly
+    if cfg_profiler.get("_component_", None) is None:
+        cfg_profiler["_component_"] = "torchtune.training.setup_torch_profiler"
+    else:
+        assert (
+            cfg_profiler.get("_component_") == "torchtune.training.setup_torch_profiler"
+        ), "Only torch profiler supported currently: component must be `torchtune.training.setup_torch_profiler`"
+
+    profiler, profiler_cfg = config.instantiate(cfg_profiler)
+
+    return profiler, profiler_cfg
+
+
+@pytest.fixture
+def reference_profiler_basic():
+    return torch.profiler.profile(
+        activities=[
+            torch.profiler.ProfilerActivity.CPU,
+            torch.profiler.ProfilerActivity.CUDA,
+        ],
+        schedule=torch.profiler.schedule(wait=3, warmup=1, active=1, repeat=0),
+        profile_memory=False,
+        with_stack=False,
+        record_shapes=True,
+        with_flops=True,
+    )
+
+
+@pytest.fixture
+def reference_profiler_full():
+    return torch.profiler.profile(
+        activities=[
+            torch.profiler.ProfilerActivity.CPU,
+            torch.profiler.ProfilerActivity.CUDA,
+        ],
+        schedule=torch.profiler.schedule(wait=3, warmup=1, active=1, repeat=0),
+        profile_memory=True,
+        with_stack=True,
+        record_shapes=True,
+        with_flops=True,
+        experimental_config=_ExperimentalConfig(verbose=True),
+    )
+
+
+def check_profiler_attrs(profiler, ref_profiler):
+    for attr in PROFILER_ATTRS:
+        assert getattr(profiler, attr) == getattr(ref_profiler, attr)
+
+
+def check_schedule(schedule, ref_schedule, num_steps=10):
+    ref_steps = [ref_schedule(i) for i in range(num_steps)]
+    test_steps = [schedule(i) for i in range(num_steps)]
+    assert ref_steps == test_steps
+
+
+def test_instantiate_basic(profiler_cfg, reference_profiler_basic):
+    cfg = OmegaConf.create(profiler_cfg)[PROFILER_KEY]
+
+    profiler, updated_cfg = _setup_profiler(cfg)
+
+    check_profiler_attrs(profiler, reference_profiler_basic)
+
+    ref_schedule = torch.profiler.schedule(
+        wait=updated_cfg["wait_steps"],
+        warmup=updated_cfg["warmup_steps"],
+        active=updated_cfg["active_steps"],
+        repeat=updated_cfg["num_cycles"],
+    )
+    check_schedule(profiler.schedule, ref_schedule)
+
+
+def test_instantiate_full(profiler_cfg, reference_profiler_full):
+    cfg = OmegaConf.create(profiler_cfg)[PROFILER_KEY]
+
+    # Check `setup` automatically overrides `with_stack` and `record_shapes` when profile_memory is True and adds
+    # experimental_config, which is needed for stack exporting (see comments in `setup_torch_profiler`)
+    cfg.profile_memory = True
+    cfg.with_stack = False
+    cfg.record_shapes = False
+    profiler, updated_cfg = _setup_profiler(cfg)
+
+    check_profiler_attrs(profiler, reference_profiler_full)
+    assert profiler.experimental_config is not None
+    assert updated_cfg.with_stack is True
+    assert updated_cfg.record_shapes is True
+
+
+def test_schedule_setup(profiler_cfg, reference_profiler_basic):
+
+    cfg = OmegaConf.create(profiler_cfg)[PROFILER_KEY]
+
+    # Test that after removing schedule, setup method will implement default schedule
+    _ = [cfg.pop(k) for k in DEFAULT_SCHEDULE.keys()]
+    profiler, updated_cfg = _setup_profiler(cfg)
+    test_schedule = profiler.schedule
+    ref_schedule = torch.profiler.schedule(
+        wait=DEFAULT_SCHEDULE["wait_steps"],
+        warmup=DEFAULT_SCHEDULE["warmup_steps"],
+        active=DEFAULT_SCHEDULE["active_steps"],
+        repeat=DEFAULT_SCHEDULE["num_cycles"],
+    )
+    check_schedule(ref_schedule, test_schedule)
+
+    # Check cfg is updated correctly
+    for k in DEFAULT_SCHEDULE.keys():
+        assert updated_cfg[k] == DEFAULT_SCHEDULE[k]
+
+    # Test missing key is automatically set to default
+    for k in DEFAULT_SCHEDULE.keys():
+        cfg = OmegaConf.create(profiler_cfg)[PROFILER_KEY]
+        cfg.pop(k)
+        profiler, updated_cfg = _setup_profiler(cfg)
+        assert updated_cfg[k] == DEFAULT_SCHEDULE[k]
+
+
+def test_default_activities(profiler_cfg):
+    cfg = OmegaConf.create(profiler_cfg)[PROFILER_KEY]
+
+    # Test setup automatically adds CPU + CUDA tracing if neither CPU nor CUDA is specified
+    cfg.pop("cpu")
+    cfg.pop("cuda")
+    profiler, updated_cfg = _setup_profiler(cfg)
+    assert profiler.activities == DEFAULT_PROFILER_ACTIVITIES
+    assert updated_cfg.cpu is True
+    assert updated_cfg.cuda is True
+
+
+def test_default_output_dir(profiler_cfg):
+    cfg = OmegaConf.create(profiler_cfg)[PROFILER_KEY]
+
+    # Test cfg output_dir is set correctly
+    if cfg.get("output_dir", None) is not None:
+        cfg.pop("output_dir")
+    _, updated_cfg = _setup_profiler(cfg, return_cfg=True)
+    assert updated_cfg.output_dir == DEFAULT_PROFILE_DIR
+
+
+def test_default_trace_opts(profiler_cfg):
+    cfg = OmegaConf.create(profiler_cfg)[PROFILER_KEY]
+
+    # Test missing profiler options are set to defaults
+    cfg.pop("profile_memory")
+    cfg.pop("with_stack")
+    cfg.pop("record_shapes")
+    cfg.pop("with_flops")
+    profiler, updated_cfg = _setup_profiler(cfg)
+    check_profiler_attrs(
+        profiler,
+        torch.profiler.profile(
+            activities=DEFAULT_PROFILER_ACTIVITIES, **DEFAULT_TRACE_OPTS
+        ),
+    )
+    for k in ["profile_memory", "with_stack", "record_shapes", "with_flops"]:
+        assert updated_cfg[k] == DEFAULT_TRACE_OPTS[k]
+
+
+def test_dummy_profiler(profiler_cfg):
+
+    # Test missing `profile` key returns fake profiler
+    cfg = OmegaConf.create(profiler_cfg)
+    cfg.pop(PROFILER_KEY)
+    profiler, _ = _setup_profiler(cfg)
+    assert isinstance(profiler, DummyProfiler)
+
+    # Test that disabled profiler creates fake profiler
+    cfg = OmegaConf.create(profiler_cfg)[PROFILER_KEY]
+    cfg.enabled = False
+    profiler, _ = _setup_profiler(cfg)
+    assert isinstance(profiler, DummyProfiler)
+
+    # Test that fake_profiler.step() does nothing both when used as context manager and as standalone object
+    with profiler as prof:
+        prof.step()
+
+    # Additional DummyProfiler no-ops when used as object and not context
+    assert profiler.step() is None
+    assert profiler.start() is None
+    assert profiler.stop() is None
diff -ruN marc_original/third_party/torchtune/tests/torchtune/training/test_seed.py marc/third_party/torchtune/tests/torchtune/training/test_seed.py
--- marc_original/third_party/torchtune/tests/torchtune/training/test_seed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/training/test_seed.py	2025-02-20 17:49:30.158025245 -0500
@@ -0,0 +1,85 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.
+
+import os
+
+import numpy as np
+import pytest
+import torch
+from torchtune.training.seed import set_seed
+
+
+class TestSeed:
+    def test_seed_range(self) -> None:
+        """
+        Verify that exceptions are raised on input values
+        """
+        with pytest.raises(ValueError, match="Invalid seed value provided"):
+            set_seed(-1)
+
+        invalid_max = np.iinfo(np.uint64).max
+        with pytest.raises(ValueError, match="Invalid seed value provided"):
+            set_seed(invalid_max)
+
+        # should not raise any exceptions
+        set_seed(42)
+
+    def test_deterministic_true(self) -> None:
+        for det_debug_mode, det_debug_mode_str in [(1, "warn"), (2, "error")]:
+            warn_only = det_debug_mode == 1
+            for debug_mode in (det_debug_mode, det_debug_mode_str):
+                # torch/testing/_internal/common_utils.py calls `disable_global_flags()`
+                # workaround RuntimeError: not allowed to set ... after disable_global_flags
+                setattr(  # noqa: B010
+                    torch.backends, "__allow_nonbracketed_mutation_flag", True
+                )
+                set_seed(42, debug_mode=debug_mode)
+                setattr(  # noqa: B010
+                    torch.backends, "__allow_nonbracketed_mutation_flag", False
+                )
+                assert torch.backends.cudnn.deterministic
+                assert not torch.backends.cudnn.benchmark
+                assert det_debug_mode == torch.get_deterministic_debug_mode()
+                assert torch.are_deterministic_algorithms_enabled()
+                assert (
+                    warn_only == torch.is_deterministic_algorithms_warn_only_enabled()
+                )
+                assert os.environ["CUBLAS_WORKSPACE_CONFIG"] == ":4096:8"
+
+    def test_deterministic_false(self) -> None:
+        for debug_mode in ("default", 0):
+            setattr(  # noqa: B010
+                torch.backends, "__allow_nonbracketed_mutation_flag", True
+            )
+            set_seed(42, debug_mode=debug_mode)
+            setattr(  # noqa: B010
+                torch.backends, "__allow_nonbracketed_mutation_flag", False
+            )
+            assert not torch.backends.cudnn.deterministic
+            assert torch.backends.cudnn.benchmark
+            assert 0 == torch.get_deterministic_debug_mode()
+            assert not torch.are_deterministic_algorithms_enabled()
+            assert not torch.is_deterministic_algorithms_warn_only_enabled()
+
+    def test_deterministic_unset(self) -> None:
+        det = torch.backends.cudnn.deterministic
+        benchmark = torch.backends.cudnn.benchmark
+        det_debug_mode = torch.get_deterministic_debug_mode()
+        det_algo_enabled = torch.are_deterministic_algorithms_enabled()
+        det_algo_warn_only_enabled = (
+            torch.is_deterministic_algorithms_warn_only_enabled()
+        )
+        set_seed(42, debug_mode=None)
+        assert det == torch.backends.cudnn.deterministic
+        assert benchmark == torch.backends.cudnn.benchmark
+        assert det_debug_mode == torch.get_deterministic_debug_mode()
+        assert det_algo_enabled == torch.are_deterministic_algorithms_enabled()
+        assert (
+            det_algo_warn_only_enabled
+            == torch.is_deterministic_algorithms_warn_only_enabled()
+        )
diff -ruN marc_original/third_party/torchtune/tests/torchtune/utils/__init__.py marc/third_party/torchtune/tests/torchtune/utils/__init__.py
--- marc_original/third_party/torchtune/tests/torchtune/utils/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/utils/__init__.py	2025-02-20 17:49:30.162025252 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/tests/torchtune/utils/test_device.py marc/third_party/torchtune/tests/torchtune/utils/test_device.py
--- marc_original/third_party/torchtune/tests/torchtune/utils/test_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/utils/test_device.py	2025-02-20 17:49:30.166025259 -0500
@@ -0,0 +1,89 @@
+#!/usr/bin/env python3
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+from unittest import mock
+from unittest.mock import patch
+
+import pytest
+
+import torch
+from torchtune.utils._device import (
+    _get_device_type_from_env,
+    _setup_cuda_device,
+    batch_to_device,
+    get_device,
+)
+
+
+class TestDevice:
+
+    cuda_available: bool = torch.cuda.is_available()
+
+    @patch("torch.cuda.is_available", return_value=False)
+    def test_get_cpu_device(self, mock_cuda):
+        devices = [None, "cpu", "meta"]
+        expected_devices = [
+            torch.device("cpu"),
+            torch.device("cpu"),
+            torch.device("meta"),
+        ]
+        for device, expected_device in zip(devices, expected_devices):
+            device = get_device(device)
+            assert device == expected_device
+            assert device.index is None
+
+    def test_batch_to_device(self):
+        batch = {
+            "a": torch.ones(1),
+            "b": {
+                "c": torch.ones(1),
+                "d": torch.ones(1),
+            },
+        }
+        device = torch.device("meta")
+        batch_to_device(batch, device)
+        assert batch["a"].device == device
+        assert batch["b"]["c"].device == device
+        assert batch["b"]["d"].device == device
+
+        batch["e"] = 0
+        with pytest.raises(ValueError):
+            batch_to_device(batch, device)
+
+    @pytest.mark.skipif(not cuda_available, reason="The test requires GPUs to run.")
+    def test_get_gpu_device(self) -> None:
+        device_idx = torch.cuda.device_count() - 1
+        assert device_idx >= 0
+        with mock.patch.dict(os.environ, {"LOCAL_RANK": str(device_idx)}, clear=True):
+            device = get_device()
+            assert device.type == "cuda"
+            assert device.index == device_idx
+            assert device.index == torch.cuda.current_device()
+
+            # Test that we raise an error if the device index is specified on distributed runs
+            if device_idx > 0:
+                with pytest.raises(
+                    RuntimeError,
+                    match=f"Device specified is cuda:0 but was assigned cuda:{device_idx}",
+                ):
+                    device = get_device("cuda:0")
+
+        invalid_device_idx = device_idx + 10
+        with mock.patch.dict(os.environ, {"LOCAL_RANK": str(invalid_device_idx)}):
+            with pytest.raises(
+                RuntimeError,
+                match="The local rank is larger than the number of available GPUs",
+            ):
+                device = get_device("cuda")
+
+        # Test that we fall back to 0 if LOCAL_RANK is not specified
+        device = torch.device(_get_device_type_from_env())
+        device = _setup_cuda_device(device)
+        assert device.type == "cuda"
+        assert device.index == 0
+        assert device.index == torch.cuda.current_device()
diff -ruN marc_original/third_party/torchtune/tests/torchtune/utils/test_logging.py marc/third_party/torchtune/tests/torchtune/utils/test_logging.py
--- marc_original/third_party/torchtune/tests/torchtune/utils/test_logging.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/tests/torchtune/utils/test_logging.py	2025-02-20 17:49:30.170025265 -0500
@@ -0,0 +1,74 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+from io import StringIO
+from unittest import mock
+
+import pytest
+from torchtune.utils._logging import deprecated, log_rank_zero
+
+
+def test_deprecated():
+    @deprecated(msg="Please use `TotallyAwesomeClass` instead.")
+    class DummyClass:
+        pass
+
+    with pytest.warns(
+        FutureWarning,
+        match="DummyClass is deprecated and will be removed in future versions. Please use `TotallyAwesomeClass` instead.",
+    ):
+        DummyClass()
+
+    with pytest.warns(None) as record:
+        DummyClass()
+
+    assert len(record) == 0, "Warning raised twice when it should only be raised once."
+
+    @deprecated(msg="Please use `totally_awesome_func` instead.")
+    def dummy_func():
+        pass
+
+    with pytest.warns(
+        FutureWarning,
+        match="dummy_func is deprecated and will be removed in future versions. Please use `totally_awesome_func` instead.",
+    ):
+        dummy_func()
+
+
+def test_log_rank_zero(capsys):
+    # Create a logger and add a StreamHandler to it so we can
+    # assert on logged strings
+    logger = logging.getLogger(__name__)
+    logger.setLevel("DEBUG")
+    stream = StringIO()
+    handler = logging.StreamHandler(stream)
+    logger.addHandler(handler)
+
+    with mock.patch(
+        "torchtune.utils._logging.dist.is_available", return_value=True
+    ), mock.patch("torchtune.utils._logging.dist.is_initialized", return_value=True):
+        # Make sure rank 0 logs as expected
+        with mock.patch(
+            "torchtune.utils._logging.dist.get_rank",
+            return_value=0,
+        ):
+            log_rank_zero(logger, "this is a test", level=logging.DEBUG)
+            output = stream.getvalue().strip()
+            assert "this is a test" in output
+
+        # Clear the stream
+        stream.truncate(0)
+        stream.seek(0)
+
+        # Make sure all other ranks do not log anything
+        with mock.patch(
+            "torchtune.utils._logging.dist.get_rank",
+            return_value=1,
+        ):
+            log_rank_zero(logger, "this is a test", level=logging.DEBUG)
+            output = stream.getvalue().strip()
+            assert not output
diff -ruN marc_original/third_party/torchtune/torchtune/_cli/cp.py marc/third_party/torchtune/torchtune/_cli/cp.py
--- marc_original/third_party/torchtune/torchtune/_cli/cp.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/_cli/cp.py	2025-02-20 17:49:30.178025278 -0500
@@ -0,0 +1,116 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import argparse
+import shutil
+import textwrap
+from pathlib import Path
+
+import torchtune
+from torchtune._cli.subcommand import Subcommand
+from torchtune._recipe_registry import get_all_recipes
+
+ROOT = Path(torchtune.__file__).parent.parent
+
+
+class Copy(Subcommand):
+    """Holds all the logic for the `tune cp` subcommand."""
+
+    def __init__(self, subparsers):
+        super().__init__()
+        self._parser = subparsers.add_parser(
+            "cp",
+            prog="tune cp",
+            usage="tune cp <recipe|config> destination [OPTIONS]",
+            help="Copy a built-in recipe or config to a local path.",
+            description="Copy a built-in recipe or config to a local path.",
+            epilog=textwrap.dedent(
+                """\
+            examples:
+                $ tune cp lora_finetune_distributed .
+                Copied file to ./lora_finetune_distributed.py
+
+                $ tune cp llama2/7B_full ./new_dir/my_custom_lora.yaml --make-parents
+                Copyied file to ./new_dir/my_custom_lora.yaml
+
+            Need to see all possible recipes/configs to copy? Try running `tune ls`.
+            """
+            ),
+            formatter_class=argparse.RawTextHelpFormatter,
+        )
+        self._add_arguments()
+        self._parser.set_defaults(func=self._cp_cmd)
+
+    def _add_arguments(self) -> None:
+        """Add arguments to the parser."""
+        self._parser.add_argument(
+            "file",
+            type=str,
+            help="Recipe/config to copy. For a list of all possible options, run `tune ls`",
+        )
+        self._parser.add_argument(
+            "destination",
+            type=Path,
+            help="Location to copy the file to",
+        )
+        self._parser.add_argument(
+            "-n",
+            "--no-clobber",
+            action="store_true",
+            help="Do not overwrite destination if it already exists",
+            default=False,
+        )
+        self._parser.add_argument(
+            "--make-parents",
+            action="store_true",
+            help="Create parent directories for destination if they do not exist. "
+            "If not set to True, will error if parent directories do not exist",
+            default=False,
+        )
+
+    def _cp_cmd(self, args: argparse.Namespace):
+        """Copy a recipe or config to a new location."""
+        destination: Path = args.destination
+        src = None
+
+        # Iterate through all recipes and configs
+        for recipe in get_all_recipes():
+            if recipe.name == args.file:
+                src = ROOT / "recipes" / recipe.file_path
+                proper_suffix = ".py"
+                break
+            for config in recipe.configs:
+                if config.name == args.file:
+                    src = ROOT / "recipes" / "configs" / config.file_path
+                    proper_suffix = ".yaml"
+                    break
+
+        # Fail if no file exists
+        if src is None:
+            self._parser.error(
+                f"Invalid file name: {args.file}. Try `tune ls` to see all available files to copy."
+            )
+
+        # Attach proper suffix if needed
+        if destination.name != "" and destination.suffix != proper_suffix:
+            destination = destination.with_suffix(proper_suffix)
+
+        # Copy file
+        try:
+            if args.no_clobber and destination.exists():
+                print(
+                    f"File already exists at {destination.absolute()}, not overwriting."
+                )
+            else:
+                if args.make_parents:
+                    destination.parent.mkdir(parents=True, exist_ok=True)
+                output = shutil.copy(src, destination)
+                print(f"Copied file to {output}")
+        except FileNotFoundError:
+            self._parser.error(
+                f"Cannot create regular file: '{destination}'. No such file or directory. "
+                "If the specified destination's parent directory does not exist and you would "
+                "like to create it on-the-fly, use the --make-parents flag."
+            )
diff -ruN marc_original/third_party/torchtune/torchtune/_cli/download.py marc/third_party/torchtune/torchtune/_cli/download.py
--- marc_original/third_party/torchtune/torchtune/_cli/download.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/_cli/download.py	2025-02-20 17:49:30.182025285 -0500
@@ -0,0 +1,161 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import argparse
+import os
+import textwrap
+
+from pathlib import Path
+from typing import Literal, Union
+
+from huggingface_hub import snapshot_download
+from huggingface_hub.utils import GatedRepoError, RepositoryNotFoundError
+from torchtune._cli.subcommand import Subcommand
+
+
+class Download(Subcommand):
+    """Holds all the logic for the `tune download` subcommand."""
+
+    def __init__(self, subparsers: argparse._SubParsersAction):
+        super().__init__()
+        self._parser = subparsers.add_parser(
+            "download",
+            prog="tune download",
+            usage="tune download <repo-id> [OPTIONS]",
+            help="Download a model from the Hugging Face Hub.",
+            description="Download a model from the Hugging Face Hub.",
+            epilog=textwrap.dedent(
+                """\
+            examples:
+                # Download a model from the Hugging Face Hub with a Hugging Face API token
+                $ tune download meta-llama/Llama-2-7b-hf --hf-token <TOKEN>
+                Successfully downloaded model repo and wrote to the following locations:
+                /tmp/Llama-2-7b-hf/config.json
+                /tmp/Llama-2-7b-hf/README.md
+                /tmp/Llama-2-7b-hf/consolidated.00.pth
+                ...
+
+                # Download an ungated model from the Hugging Face Hub
+                $ tune download mistralai/Mistral-7B-Instruct-v0.2 --output-dir /tmp/model
+                Successfully downloaded model repo and wrote to the following locations:
+                /tmp/model/config.json
+                /tmp/model/README.md
+                /tmp/model/model-00001-of-00002.bin
+                ...
+
+            For a list of all models, visit the Hugging Face Hub https://huggingface.co/models.
+            """
+            ),
+            formatter_class=argparse.RawTextHelpFormatter,
+        )
+        self._add_arguments()
+        self._parser.set_defaults(func=self._download_cmd)
+
+    def _add_arguments(self) -> None:
+        """Add arguments to the parser."""
+        self._parser.add_argument(
+            "repo_id",
+            type=str,
+            help="Name of the repository on Hugging Face Hub.",
+        )
+        self._parser.add_argument(
+            "--output-dir",
+            type=Path,
+            required=False,
+            default=None,
+            help="Directory in which to save the model. Defaults to `/tmp/<model_name>`.",
+        )
+        self._parser.add_argument(
+            "--output-dir-use-symlinks",
+            type=str,
+            required=False,
+            default="auto",
+            help=(
+                "To be used with `output-dir`. If set to 'auto', the cache directory will be used and the file will be"
+                " either duplicated or symlinked to the local directory depending on its size. It set to `True`, a"
+                " symlink will be created, no matter the file size. If set to `False`, the file will either be"
+                " duplicated from cache (if already exists) or downloaded from the Hub and not cached."
+            ),
+        )
+        self._parser.add_argument(
+            "--hf-token",
+            type=str,
+            required=False,
+            default=os.getenv("HF_TOKEN", None),
+            help="Hugging Face API token. Needed for gated models like Llama2.",
+        )
+        self._parser.add_argument(
+            "--ignore-patterns",
+            type=str,
+            required=False,
+            default="*.safetensors",
+            help="If provided, files matching any of the patterns are not downloaded. Defaults to ignoring "
+            "safetensors files to avoid downloading duplicate weights.",
+        )
+
+    def _download_cmd(self, args: argparse.Namespace) -> None:
+        """Downloads a model from the Hugging Face Hub."""
+        # Download the tokenizer and PyTorch model files
+
+        # Default output_dir is `/tmp/<model_name>`
+        output_dir = args.output_dir
+        if output_dir is None:
+            model_name = args.repo_id.split("/")[-1]
+            output_dir = Path("/tmp") / model_name
+
+        # Raise if local_dir_use_symlinks is invalid
+        output_dir_use_symlinks: Union[Literal["auto"], bool]
+        use_symlinks_lowercase = args.output_dir_use_symlinks.lower()
+        if use_symlinks_lowercase == "true":
+            output_dir_use_symlinks = True
+        elif use_symlinks_lowercase == "false":
+            output_dir_use_symlinks = False
+        elif use_symlinks_lowercase == "auto":
+            output_dir_use_symlinks = "auto"
+        else:
+            self._parser.error(
+                f"'{args.output_dir_use_symlinks}' is not a valid value for `--output-dir-use-symlinks`. It must be either"
+                " 'auto', 'True' or 'False'."
+            )
+
+        print(f"Ignoring files matching the following patterns: {args.ignore_patterns}")
+        try:
+            true_output_dir = snapshot_download(
+                args.repo_id,
+                local_dir=output_dir,
+                local_dir_use_symlinks=output_dir_use_symlinks,
+                ignore_patterns=args.ignore_patterns,
+                token=args.hf_token,
+            )
+        except GatedRepoError:
+            if args.hf_token:
+                self._parser.error(
+                    "It looks like you are trying to access a gated repository. Please ensure you "
+                    "have access to the repository."
+                )
+            else:
+                self._parser.error(
+                    "It looks like you are trying to access a gated repository. Please ensure you "
+                    "have access to the repository and have provided the proper Hugging Face API token "
+                    "using the option `--hf-token` or by running `huggingface-cli login`."
+                    "You can find your token by visiting https://huggingface.co/settings/tokens"
+                )
+        except RepositoryNotFoundError:
+            self._parser.error(
+                f"Repository '{args.repo_id}' not found on the Hugging Face Hub."
+            )
+        except Exception as e:
+            import traceback
+
+            tb = traceback.format_exc()
+            msg = f"Failed to download {args.repo_id} with error: '{e}' and traceback: {tb}"
+            self._parser.error(msg)
+
+        print(
+            "Successfully downloaded model repo and wrote to the following locations:",
+            *list(Path(true_output_dir).iterdir()),
+            sep="\n",
+        )
diff -ruN marc_original/third_party/torchtune/torchtune/_cli/__init__.py marc/third_party/torchtune/torchtune/_cli/__init__.py
--- marc_original/third_party/torchtune/torchtune/_cli/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/_cli/__init__.py	2025-02-20 17:49:30.178025278 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/torchtune/_cli/ls.py marc/third_party/torchtune/torchtune/_cli/ls.py
--- marc_original/third_party/torchtune/torchtune/_cli/ls.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/_cli/ls.py	2025-02-20 17:49:30.186025291 -0500
@@ -0,0 +1,64 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import argparse
+import textwrap
+
+from torchtune._cli.subcommand import Subcommand
+
+from torchtune._recipe_registry import get_all_recipes
+
+
+class List(Subcommand):
+    """Holds all the logic for the `tune ls` subcommand."""
+
+    NULL_VALUE = "<>"
+
+    def __init__(self, subparsers: argparse._SubParsersAction):
+        super().__init__()
+        self._parser = subparsers.add_parser(
+            "ls",
+            prog="tune ls",
+            help="List all built-in recipes and configs",
+            description="List all built-in recipes and configs",
+            epilog=textwrap.dedent(
+                """\
+            examples:
+                $ tune ls
+                RECIPE                                   CONFIG
+                full_finetune_single_device              llama2/7B_full_single_device
+                full_finetune_distributed                llama2/7B_full
+                                                         llama2/13B_full
+                ...
+
+            To run one of these recipes:
+                $ tune run full_finetune_single_device --config full_finetune_single_device
+            """
+            ),
+            formatter_class=argparse.RawTextHelpFormatter,
+        )
+        self._parser.set_defaults(func=self._ls_cmd)
+
+    def _ls_cmd(self, args: argparse.Namespace) -> None:
+        """List all available recipes and configs."""
+        # Print table header
+        header = f"{'RECIPE':<40} {'CONFIG':<40}"
+        print(header)
+
+        # Print recipe/config pairs
+        for recipe in get_all_recipes():
+            # If there are no configs for a recipe, print a blank config
+            recipe_str = recipe.name
+            if len(recipe.configs) == 0:
+                row = f"{recipe_str:<40} {self.NULL_VALUE:<40}"
+                print(row)
+            for i, config in enumerate(recipe.configs):
+                # If there are multiple configs for a single recipe, omit the recipe name
+                # on latter configs
+                if i > 0:
+                    recipe_str = ""
+                row = f"{recipe_str:<40} {config.name:<40}"
+                print(row)
Binary files marc_original/third_party/torchtune/torchtune/_cli/__pycache__/cp.cpython-312.pyc and marc/third_party/torchtune/torchtune/_cli/__pycache__/cp.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/_cli/__pycache__/download.cpython-312.pyc and marc/third_party/torchtune/torchtune/_cli/__pycache__/download.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/_cli/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/_cli/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/_cli/__pycache__/ls.cpython-312.pyc and marc/third_party/torchtune/torchtune/_cli/__pycache__/ls.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/_cli/__pycache__/run.cpython-312.pyc and marc/third_party/torchtune/torchtune/_cli/__pycache__/run.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/_cli/__pycache__/subcommand.cpython-312.pyc and marc/third_party/torchtune/torchtune/_cli/__pycache__/subcommand.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/_cli/__pycache__/tune.cpython-312.pyc and marc/third_party/torchtune/torchtune/_cli/__pycache__/tune.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/_cli/__pycache__/validate.cpython-312.pyc and marc/third_party/torchtune/torchtune/_cli/__pycache__/validate.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/_cli/run.py marc/third_party/torchtune/torchtune/_cli/run.py
--- marc_original/third_party/torchtune/torchtune/_cli/run.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/_cli/run.py	2025-02-20 17:49:30.190025298 -0500
@@ -0,0 +1,208 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import argparse
+import os
+import runpy
+import sys
+import textwrap
+
+from pathlib import Path
+from typing import Optional
+
+import torchtune
+
+from torch.distributed.elastic.multiprocessing.errors import record
+from torch.distributed.run import get_args_parser as get_torchrun_args_parser, run
+from torchtune._cli.subcommand import Subcommand
+from torchtune._recipe_registry import Config, get_all_recipes, Recipe
+
+ROOT = Path(torchtune.__file__).parent.parent
+
+
+class Run(Subcommand):
+    """Holds all the logic for the `tune run` subcommand."""
+
+    def __init__(self, subparsers):
+        super().__init__()
+        self._parser = subparsers.add_parser(
+            "run",
+            prog="tune run",
+            help="Run a recipe. For distributed recipes, this supports all torchrun arguments.",
+            description="Run a recipe. For distributed recipes, this supports all torchrun arguments.",
+            usage="tune run [TORCHRUN-OPTIONS] <recipe> --config <config> [RECIPE-OPTIONS]",
+            epilog=textwrap.dedent(
+                """\
+                examples:
+
+                    # Run a finetuning recipe on a single device w/ default values
+                    $ tune run lora_finetune_single_device --config llama2/7B_lora_single_device
+
+                    # Run a finetuning recipe in a distributed fashion using torchrun w/ default values
+                    $ tune run --nproc_per_node 4 full_finetune_distributed --config llama2/7B_full_finetune_distributed
+
+                    # Override a parameter in the config file and specify a number of GPUs for torchrun
+                    $ tune run --nproc_per_node 2 \
+                        lora_finetune_single_device \
+                        --config llama2/7B_lora_single_device \
+                        model.lora_rank=16 \
+
+                Remember, you can use `tune cp` to copy a default recipe/config to your local dir and modify the values.
+                """
+            ),
+            formatter_class=argparse.RawTextHelpFormatter,
+        )
+        self._add_arguments()
+        self._parser.set_defaults(func=self._run_cmd)
+
+    def _add_arguments(self) -> None:
+        """Add arguments to the parser.
+
+        This is a bit hacky since we need to add the torchrun arguments to our parser.
+        This grabs the argparser from torchrun, iterates over it's actions, and adds them
+        to our parser. We rename the training_script and training_script_args to recipe and recipe_args
+        respectively. In addition, we leave out the help argument since we add it manually to ours.
+        """
+        torchrun_argparser = get_torchrun_args_parser()
+        for action in torchrun_argparser._actions:
+            if action.dest == "training_script":
+                action.dest = "recipe"
+                action.help = """Name or path to recipe to be launched followed by args.
+For a list of all possible recipes, run `tune ls`."""
+            elif action.dest == "training_script_args":
+                action.dest = "recipe_args"
+                action.help = "Args to be passed to the recipe."
+            elif action.dest == "help":
+                continue
+            self._parser._add_action(action)
+
+    @record
+    def _run_distributed(self, args: argparse.Namespace, is_builtin: bool):
+        """Run a recipe with torchrun."""
+        # TODO (rohan-varma): Add check that nproc_per_node <= cuda device count. Currently,
+        # we don't do this since we test on CPUs for distributed. Will update once multi GPU CI is supported.
+        print("Running with torchrun...")
+        # Have to reset the argv so that the recipe can be run with the correct arguments
+        args.training_script = args.recipe
+        args.training_script_args = args.recipe_args
+        # torchtune built-in recipes are specified with an absolute posix path, but
+        # custom recipes are specified as a relative module dot path and need to be
+        # run with python -m
+        args.module = not is_builtin
+        run(args)
+
+    def _run_single_device(self, args: argparse.Namespace, is_builtin: bool):
+        """Run a recipe on a single device."""
+        sys.argv = [str(args.recipe)] + args.recipe_args
+        if is_builtin:
+            # torchtune built-in recipes are specified with an absolute posix path
+            runpy.run_path(str(args.recipe), run_name="__main__")
+        else:
+            # custom recipes are specified as a relative module dot path
+            runpy.run_module(str(args.recipe), run_name="__main__")
+
+    def _is_distributed_args(self, args: argparse.Namespace):
+        """Check if the user is trying to run a distributed recipe."""
+        total = len(sys.argv) - 2  # total args minus "tune run"
+        script_args = len(args.recipe_args) + 1  # script args + 1 for script name
+        return total > script_args
+
+    def _get_recipe(self, recipe_str: str) -> Optional[Recipe]:
+        """Get a recipe from the name or path.
+
+        Args:
+            recipe_str (str): The name or path of the recipe.
+
+        Returns:
+            The recipe if it's found in built-in recipes, otherwise None.
+        """
+        for recipe in get_all_recipes():
+            if recipe.name == recipe_str:
+                return recipe
+
+    def _convert_to_dotpath(self, recipe_path: str) -> str:
+        """Convert a custom recipe path to a dot path that can be run as a module.
+
+        Args:
+            recipe_path (str): The path of the recipe.
+
+        Returns:
+            The dot path of the recipe.
+        """
+        filepath, _ = os.path.splitext(recipe_path)
+        return filepath.replace("/", ".")
+
+    def _get_config(
+        self, config_str: str, specific_recipe: Optional[Recipe]
+    ) -> Optional[Config]:
+        """Get a config from the name or path.
+
+        Args:
+            config_str (str): The name or path of the config.
+            specific_recipe (Optional[Recipe]): The specific recipe to search through.
+
+        Returns:
+            The config if it's found in built-in configs, otherwise None.
+        """
+        # If a specific recipe is provided, search through it
+        if specific_recipe is not None:
+            for config in specific_recipe.configs:
+                if config.name == config_str:
+                    return config
+
+        # If not, search through all recipes
+        for recipe in get_all_recipes():
+            for config in recipe.configs:
+                if config.name == config_str:
+                    return config
+
+    def _run_cmd(self, args: argparse.Namespace):
+        """Run a recipe."""
+        # We have to assume that the recipe supports distributed training
+        supports_distributed = True
+        recipe_path, config_path = None, None
+
+        # Try to find config string in args
+        try:
+            config_idx = args.recipe_args.index("--config") + 1
+            config_str = args.recipe_args[config_idx]
+        except ValueError:
+            self._parser.error("The '--config' argument is required.")
+
+        # Get recipe path
+        recipe = self._get_recipe(args.recipe)
+        if recipe is None:
+            recipe_path = self._convert_to_dotpath(args.recipe)
+            is_builtin = False
+        else:
+            recipe_path = str(ROOT / "recipes" / recipe.file_path)
+            supports_distributed = recipe.supports_distributed
+            is_builtin = True
+
+        # Get config path
+        config = self._get_config(config_str, recipe)
+        if config is None:
+            config_path = config_str
+        else:
+            config_path = str(ROOT / "recipes" / "configs" / config.file_path)
+
+        # Prepare args
+        args.recipe = recipe_path
+        args.recipe_args[config_idx] = config_path
+
+        # Make sure user code in current directory is importable
+        sys.path.append(os.getcwd())
+
+        # Execute recipe
+        if self._is_distributed_args(args):
+            if not supports_distributed:
+                self._parser.error(
+                    f"Recipe {recipe.name} does not support distributed training."
+                    "Please run without torchrun commands."
+                )
+            self._run_distributed(args, is_builtin=is_builtin)
+        else:
+            self._run_single_device(args, is_builtin=is_builtin)
diff -ruN marc_original/third_party/torchtune/torchtune/_cli/subcommand.py marc/third_party/torchtune/torchtune/_cli/subcommand.py
--- marc_original/third_party/torchtune/torchtune/_cli/subcommand.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/_cli/subcommand.py	2025-02-20 17:49:30.194025304 -0500
@@ -0,0 +1,17 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+class Subcommand:
+    def __init__(self, *args, **kwargs):
+        pass
+
+    @classmethod
+    def create(cls, *args, **kwargs):
+        return cls(*args, **kwargs)
+
+    def _add_arguments(self):
+        pass
diff -ruN marc_original/third_party/torchtune/torchtune/_cli/tune.py marc/third_party/torchtune/torchtune/_cli/tune.py
--- marc_original/third_party/torchtune/torchtune/_cli/tune.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/_cli/tune.py	2025-02-20 17:49:30.198025311 -0500
@@ -0,0 +1,53 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import argparse
+
+from torchtune._cli.cp import Copy
+from torchtune._cli.download import Download
+from torchtune._cli.ls import List
+from torchtune._cli.run import Run
+from torchtune._cli.validate import Validate
+
+
+class TuneCLIParser:
+    """Holds all information related to running the CLI"""
+
+    def __init__(self):
+        # Initialize the top-level parser
+        self._parser = argparse.ArgumentParser(
+            prog="tune",
+            description="Welcome to the torchtune CLI!",
+            add_help=True,
+        )
+        # Default command is to print help
+        self._parser.set_defaults(func=lambda args: self._parser.print_help())
+
+        # Add subcommands
+        subparsers = self._parser.add_subparsers(title="subcommands")
+        Download.create(subparsers)
+        List.create(subparsers)
+        Copy.create(subparsers)
+        Run.create(subparsers)
+        Validate.create(subparsers)
+
+    def parse_args(self) -> argparse.Namespace:
+        """Parse CLI arguments"""
+        return self._parser.parse_args()
+
+    def run(self, args: argparse.Namespace) -> None:
+        """Execute CLI"""
+        args.func(args)
+
+
+def main():
+    parser = TuneCLIParser()
+    args = parser.parse_args()
+    parser.run(args)
+
+
+if __name__ == "__main__":
+    main()
diff -ruN marc_original/third_party/torchtune/torchtune/_cli/validate.py marc/third_party/torchtune/torchtune/_cli/validate.py
--- marc_original/third_party/torchtune/torchtune/_cli/validate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/_cli/validate.py	2025-02-20 17:49:30.202025317 -0500
@@ -0,0 +1,59 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import argparse
+import textwrap
+from pathlib import Path
+
+from omegaconf import OmegaConf
+
+from torchtune import config
+from torchtune._cli.subcommand import Subcommand
+from torchtune.config._errors import ConfigError
+
+
+class Validate(Subcommand):
+    """Holds all the logic for the `tune validate` subcommand."""
+
+    def __init__(self, subparsers: argparse._SubParsersAction):
+        super().__init__()
+        self._parser = subparsers.add_parser(
+            "validate",
+            prog="tune validate",
+            help="Validate a config and ensure that it is well-formed.",
+            description="Validate a config and ensure that it is well-formed.",
+            usage="tune validate <config>",
+            epilog=textwrap.dedent(
+                """\
+                examples:
+
+                    $ tune validate recipes/configs/full_finetune_distributed.yaml
+                    Config is well-formed!
+                """
+            ),
+            formatter_class=argparse.RawTextHelpFormatter,
+        )
+        self._add_arguments()
+        self._parser.set_defaults(func=self._validate_cmd)
+
+    def _add_arguments(self) -> None:
+        """Add arguments to the parser."""
+        self._parser.add_argument(
+            "config",
+            type=Path,
+            help="Path to a config to validate.",
+        )
+
+    def _validate_cmd(self, args: argparse.Namespace):
+        """Validate a config file."""
+        cfg = OmegaConf.load(args.config)
+
+        try:
+            config.validate(cfg)
+        except ConfigError as e:
+            self._parser.error(str(e))
+
+        print("Config is well-formed!")
diff -ruN marc_original/third_party/torchtune/torchtune/config/_errors.py marc/third_party/torchtune/torchtune/config/_errors.py
--- marc_original/third_party/torchtune/torchtune/config/_errors.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/config/_errors.py	2025-02-20 17:49:30.214025337 -0500
@@ -0,0 +1,35 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List
+
+
+class InstantiationError(Exception):
+    """
+    Raised when a `_component_` field in a config is unable to be instantiated.
+    """
+
+    pass
+
+
+class ConfigError(Exception):
+    """
+    Raised when the yaml config is not well-formed. Prints all the collected
+    errors at once.
+
+    Args:
+        errors (List[Exception]): exceptions found when validating `_component_`
+            fields in the config
+    """
+
+    def __init__(self, errors: List[Exception]):
+        self.errors = errors
+
+    def __str__(self):
+        error_messages = [f"{type(e).__name__}: {str(e)}" for e in self.errors]
+        return "Config is not well-formed, found the following errors: \n" + "\n".join(
+            error_messages
+        )
diff -ruN marc_original/third_party/torchtune/torchtune/config/__init__.py marc/third_party/torchtune/torchtune/config/__init__.py
--- marc_original/third_party/torchtune/torchtune/config/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/config/__init__.py	2025-02-20 17:49:30.210025331 -0500
@@ -0,0 +1,17 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._instantiate import instantiate
+from ._parse import parse
+from ._utils import log_config
+from ._validate import validate
+
+__all__ = [
+    "instantiate",
+    "parse",
+    "log_config",
+    "validate",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/config/_instantiate.py marc/third_party/torchtune/torchtune/config/_instantiate.py
--- marc_original/third_party/torchtune/torchtune/config/_instantiate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/config/_instantiate.py	2025-02-20 17:49:30.218025344 -0500
@@ -0,0 +1,112 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import copy
+import os
+import sys
+from typing import Any, Callable, Dict, Tuple
+
+from omegaconf import DictConfig, OmegaConf
+from torchtune.config._errors import InstantiationError
+from torchtune.config._utils import _get_component_from_path, _has_component
+
+
+def _create_component(
+    _component_: Callable[..., Any],
+    args: Tuple[Any, ...],
+    kwargs: Dict[str, Any],
+):
+    return _component_(*args, **kwargs)
+
+
+def _instantiate_node(node: Dict[str, Any], *args: Tuple[Any, ...]):
+    """
+    Creates the object specified in _component_ field with provided positional args
+    and kwargs already merged. Raises an InstantiationError if _component_ is not specified.
+    """
+    if _has_component(node):
+        _component_ = _get_component_from_path(node.get("_component_"))
+        kwargs = {k: v for k, v in node.items() if k != "_component_"}
+        return _create_component(_component_, args, kwargs)
+    else:
+        raise InstantiationError(
+            "Cannot instantiate specified object."
+            + "\nMake sure you've specified a _component_ field with a valid dotpath."
+        )
+
+
+def instantiate(
+    config: DictConfig,
+    *args: Tuple[Any, ...],
+    **kwargs: Dict[str, Any],
+) -> Any:
+    """
+    Given a DictConfig with a _component_ field specifying the object to instantiate and
+    additional fields for keyword arguments, create an instance of the specified object.
+    You can use this function to create the exact instance of a torchtune object you want
+    to use in your recipe using the specification from the config.
+
+    This function also supports passing in positional args and keyword args within the
+    function call. These are automatically merged with the provided config, with keyword
+    args taking precedence.
+
+    Based on Hydra's `instantiate` utility from Facebook Research:
+    https://github.com/facebookresearch/hydra/blob/main/hydra/_internal/instantiate/_instantiate2.py#L148
+
+    Args:
+        config (DictConfig): a single field in the OmegaConf object parsed from the yaml file.
+            This is expected to have a _component_ field specifying the path of the object
+            to instantiate.
+        *args (Tuple[Any, ...]): positional arguments to pass to the object to instantiate.
+        **kwargs (Dict[str, Any]): keyword arguments to pass to the object to instantiate.
+
+    Examples:
+        >>> config.yaml:
+        >>>     model:
+        >>>       _component_: torchtune.models.llama2
+        >>>       num_layers: 32
+        >>>       num_heads: 32
+        >>>       num_kv_heads: 32
+
+        >>> from torchtune import config
+        >>> vocab_size = 32000
+        >>> # Pass in vocab size as positional argument. Since it is positioned first
+        >>> # in llama2(), it must be specified first. Pass in other arguments as kwargs.
+        >>> # This will return an nn.Module directly for llama2 with specified args.
+        >>> model = config.instantiate(parsed_yaml.model, vocab_size, max_seq_len=4096, embed_dim=4096)
+
+    Returns:
+        Any: the instantiated object.
+
+    Raises:
+        ValueError: if config is not a DictConfig.
+    """
+
+    # Return None if config is None
+    if config is None:
+        return None
+    if not OmegaConf.is_dict(config):
+        raise ValueError(f"instantiate only supports DictConfigs, got {type(config)}")
+
+    # Ensure local imports are able to be instantiated
+    if os.getcwd() not in sys.path:
+        sys.path.append(os.getcwd())
+
+    config_copy = copy.deepcopy(config)
+    config_copy._set_flag(
+        flags=["allow_objects", "struct", "readonly"], values=[True, False, False]
+    )
+    config_copy._set_parent(config._get_parent())
+    config = config_copy
+
+    if kwargs:
+        # This overwrites any repeated fields in the config with kwargs
+        config = OmegaConf.merge(config, kwargs)
+
+    # Resolve all interpolations, or references to other fields within the same config
+    OmegaConf.resolve(config)
+
+    return _instantiate_node(OmegaConf.to_object(config), *args)
diff -ruN marc_original/third_party/torchtune/torchtune/config/_parse.py marc/third_party/torchtune/torchtune/config/_parse.py
--- marc_original/third_party/torchtune/torchtune/config/_parse.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/config/_parse.py	2025-02-20 17:49:30.222025350 -0500
@@ -0,0 +1,101 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import argparse
+import functools
+import sys
+from argparse import Namespace
+from typing import Any, Callable, List, Tuple
+
+from omegaconf import DictConfig, OmegaConf
+
+from torchtune.config._utils import _merge_yaml_and_cli_args
+
+
+Recipe = Callable[[DictConfig], Any]
+
+
+class TuneRecipeArgumentParser(argparse.ArgumentParser):
+    """
+    A helpful utility subclass of the ``argparse.ArgumentParser`` that
+    adds a builtin argument "config". The config argument takes a file path to a YAML file
+    and loads in argument defaults from said file. The YAML file must only contain
+    argument names and their values and nothing more, it does not have to include all of the
+    arguments. These values will be treated as defaults and can still be overridden from the
+    command line. Everything else works the same as the base ArgumentParser and you should
+    consult the docs for more info: https://docs.python.org/3/library/argparse.html.
+
+    Note:
+        This class uses "config" as a builtin argument so it is not available to use.
+    """
+
+    def __init__(self, *args, **kwargs) -> None:
+        super().__init__(*args, **kwargs)
+        super().add_argument(
+            "--config",
+            type=str,
+            help="Path/name of a yaml file with recipe args",
+            required=True,
+        )
+
+    def parse_known_args(self, *args, **kwargs) -> Tuple[Namespace, List[str]]:
+        """This acts the same as the base parse_known_args but will first load in defaults from
+        from the config yaml file if it is provided. The command line args will always take
+        precident over the values in the config file. All other parsing method, such as parse_args,
+        internally call this method so they will inherit this property too. For more info see
+        the docs for the base method: https://docs.python.org/3/library/argparse.html#the-parse-args-method.
+        """
+        namespace, unknown_args = super().parse_known_args(*args, **kwargs)
+
+        unknown_flag_args = [arg for arg in unknown_args if arg.startswith("--")]
+        if unknown_flag_args:
+            raise ValueError(
+                f"Additional flag arguments not supported: {unknown_flag_args}. Please use --config or key=value overrides"
+            )
+
+        config = OmegaConf.load(namespace.config)
+        assert "config" not in config, "Cannot use 'config' within a config file"
+        self.set_defaults(**config)
+
+        namespace, unknown_args = super().parse_known_args(*args, **kwargs)
+        del namespace.config
+
+        return namespace, unknown_args
+
+
+def parse(recipe_main: Recipe) -> Callable[[Recipe], Any]:
+    """
+    Decorator that handles parsing the config file and CLI overrides
+    for a recipe. Use it on the recipe's main function.
+
+    Args:
+        recipe_main (Recipe): The main method that initializes
+            and runs the recipe
+
+    Examples:
+        >>> @parse
+        >>> def main(cfg: DictConfig):
+        >>>     ...
+
+        >>> # With the decorator, the parameters will be parsed into cfg when run as:
+        >>> tune my_recipe --config config.yaml foo=bar
+
+    Returns:
+        Callable[[Recipe], Any]: the decorated main
+    """
+
+    @functools.wraps(recipe_main)
+    def wrapper(*args: Any, **kwargs: Any) -> Any:
+        parser = TuneRecipeArgumentParser(
+            description=recipe_main.__doc__,
+            formatter_class=argparse.RawDescriptionHelpFormatter,
+        )
+        # Get user-specified args from config and CLI and create params for recipe
+        yaml_args, cli_args = parser.parse_known_args()
+        conf = _merge_yaml_and_cli_args(yaml_args, cli_args)
+
+        sys.exit(recipe_main(conf))
+
+    return wrapper
Binary files marc_original/third_party/torchtune/torchtune/config/__pycache__/_errors.cpython-312.pyc and marc/third_party/torchtune/torchtune/config/__pycache__/_errors.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/config/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/config/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/config/__pycache__/_instantiate.cpython-312.pyc and marc/third_party/torchtune/torchtune/config/__pycache__/_instantiate.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/config/__pycache__/_parse.cpython-312.pyc and marc/third_party/torchtune/torchtune/config/__pycache__/_parse.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/config/__pycache__/_utils.cpython-312.pyc and marc/third_party/torchtune/torchtune/config/__pycache__/_utils.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/config/__pycache__/_validate.cpython-312.pyc and marc/third_party/torchtune/torchtune/config/__pycache__/_validate.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/config/_utils.py marc/third_party/torchtune/torchtune/config/_utils.py
--- marc_original/third_party/torchtune/torchtune/config/_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/config/_utils.py	2025-02-20 17:49:30.226025357 -0500
@@ -0,0 +1,217 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from argparse import Namespace
+from importlib import import_module
+from types import ModuleType
+from typing import Any, Dict, List, Union
+
+from omegaconf import DictConfig, OmegaConf
+
+from torchtune.config._errors import InstantiationError
+from torchtune.utils._logging import get_logger, log_rank_zero
+
+
+def log_config(recipe_name: str, cfg: DictConfig) -> None:
+    """
+    Logs the resolved config (merged YAML file and CLI overrides) to rank zero.
+
+    Args:
+        recipe_name (str): name of the recipe to display
+        cfg (DictConfig): parsed config object
+    """
+    logger = get_logger("DEBUG")
+    cfg_str = OmegaConf.to_yaml(cfg, resolve=True, sort_keys=True)
+    log_rank_zero(
+        logger=logger, msg=f"Running {recipe_name} with resolved config:\n\n{cfg_str}"
+    )
+
+
+def _has_component(node: Union[Dict[str, Any], DictConfig]) -> bool:
+    return (OmegaConf.is_dict(node) or isinstance(node, dict)) and "_component_" in node
+
+
+def _get_component_from_path(path: str) -> Any:
+    """
+    Return an object by name or dotted path, importing as necessary.
+    The base functionality relies on ``getattr()`` and handles all
+    possible exceptions accordingly.
+
+    Based on Hydra's `_locate` from Facebook Research:
+    https://github.com/facebookresearch/hydra/blob/main/hydra/_internal/utils.py#L614
+
+    Args:
+        path (str): Dotted path of the object
+
+    Returns:
+        Any: The object
+
+    Raises:
+        InstantiationError: If there is an exception loading the
+            object from the provided path
+        ValueError: If a relative or invalid dotpath is passed in
+    """
+    if path == "":
+        raise ValueError("Empty path")
+
+    parts = [part for part in path.split(".")]
+    for part in parts:
+        # If a relative path is passed in, the first part will be empty
+        if not len(part):
+            raise ValueError(
+                f"Error loading '{path}': invalid dotstring."
+                + "\nRelative imports are not supported."
+            )
+    # First module requires trying to import to validate
+    part0 = parts[0]
+    try:
+        obj = import_module(part0)
+    except ImportError as exc_import:
+        raise InstantiationError(
+            f"Error loading '{path}':\n{repr(exc_import)}"
+            + f"\nAre you sure that module '{part0}' is installed?"
+        ) from exc_import
+    # Subsequent components can be checked via getattr() on first module
+    # It can either be an attribute that we can return or a submodule that we
+    # can import and continue searching
+    for m in range(1, len(parts)):
+        part = parts[m]
+        try:
+            obj = getattr(obj, part)
+        # If getattr fails, check to see if it's a module we can import and
+        # continue down the path
+        except AttributeError as exc_attr:
+            parent_dotpath = ".".join(parts[:m])
+            if isinstance(obj, ModuleType):
+                mod = ".".join(parts[: m + 1])
+                try:
+                    obj = import_module(mod)
+                    continue
+                except ModuleNotFoundError as exc_import:
+                    raise InstantiationError(
+                        f"Error loading '{path}':\n{repr(exc_import)}"
+                        + f"\nAre you sure that '{part}' is importable from module '{parent_dotpath}'?"
+                    ) from exc_import
+                # Any other error trying to import module can be raised as
+                # InstantiationError
+                except Exception as exc_import:
+                    raise InstantiationError(
+                        f"Error loading '{path}':\n{repr(exc_import)}"
+                    ) from exc_import
+            # If the component is not an attribute nor a module, it doesn't exist
+            raise InstantiationError(
+                f"Error loading '{path}':\n{repr(exc_attr)}"
+                + f"\nAre you sure that '{part}' is an attribute of '{parent_dotpath}'?"
+            ) from exc_attr
+    return obj
+
+
+def _merge_yaml_and_cli_args(yaml_args: Namespace, cli_args: List[str]) -> DictConfig:
+    """
+    Takes the direct output of argparse's parse_known_args which returns known
+    args as a Namespace and unknown args as a dotlist (in our case, yaml args and
+    cli args, respectively) and merges them into a single OmegaConf DictConfig.
+
+    If a cli arg overrides a yaml arg with a _component_ field, the cli arg can
+    be specified with the parent field directly, e.g., model=torchtune.models.lora_llama2_7b
+    instead of model._component_=torchtune.models.lora_llama2_7b. Nested fields within the
+    component should be specified with dot notation, e.g., model.lora_rank=16.
+
+    Example:
+        >>> config.yaml:
+        >>>     a: 1
+        >>>     b:
+        >>>       _component_: torchtune.models.my_model
+        >>>       c: 3
+
+        >>> tune full_finetune --config config.yaml b=torchtune.models.other_model b.c=4
+        >>> yaml_args, cli_args = parser.parse_known_args()
+        >>> conf = _merge_yaml_and_cli_args(yaml_args, cli_args)
+        >>> print(conf)
+        >>> {"a": 1, "b": {"_component_": "torchtune.models.other_model", "c": 4}}
+
+    Args:
+        yaml_args (Namespace): Namespace containing args from yaml file, components
+            should have _component_ fields
+        cli_args (List[str]): List of key=value strings
+
+    Returns:
+        DictConfig: OmegaConf DictConfig containing merged args
+
+    Raises:
+        ValueError: If a cli override is not in the form of key=value
+    """
+    # Convert Namespace to simple dict
+    yaml_kwargs = vars(yaml_args)
+    cli_dotlist = []
+    for arg in cli_args:
+        # If CLI override uses the remove flag (~), remove the key from the yaml config
+        if arg.startswith("~"):
+            dotpath = arg[1:].split("=")[0]
+            if "_component_" in dotpath:
+                raise ValueError(
+                    f"Removing components from CLI is not supported: ~{dotpath}"
+                )
+            try:
+                _remove_key_by_dotpath(yaml_kwargs, dotpath)
+            except (KeyError, ValueError):
+                raise ValueError(
+                    f"Could not find key {dotpath} in yaml config to remove"
+                ) from None
+            continue
+        # Get other overrides that should be specified as key=value
+        try:
+            k, v = arg.split("=")
+        except ValueError:
+            raise ValueError(
+                f"Command-line overrides must be in the form of key=value, got {arg}"
+            ) from None
+        # If a cli arg overrides a yaml arg with a _component_ field, update the
+        # key string to reflect this
+        if k in yaml_kwargs and _has_component(yaml_kwargs[k]):
+            k += "._component_"
+        # TODO: this is a hack but otherwise we can't pass strings with leading zeroes
+        # to define the checkpoint file format. We manually override OmegaConf behavior
+        # by prepending the value with !!str to force a string type
+        if "max_filename" in k:
+            v = "!!str " + v
+        cli_dotlist.append(f"{k}={v}")
+
+    # Merge the args
+    cli_conf = OmegaConf.from_dotlist(cli_dotlist)
+    yaml_conf = OmegaConf.create(yaml_kwargs)
+
+    # CLI takes precedence over yaml args
+    return OmegaConf.merge(yaml_conf, cli_conf)
+
+
+def _remove_key_by_dotpath(nested_dict: Dict[str, Any], dotpath: str) -> None:
+    """
+    Removes a key specified by dotpath from a nested dict. Errors should handled by
+    the calling function.
+
+    Args:
+        nested_dict (Dict[str, Any]): Dict to remove key from
+        dotpath (str): dotpath of key to remove, e.g., "a.b.c"
+    """
+    path = dotpath.split(".")
+
+    def delete_non_component(d: Dict[str, Any], key: str) -> None:
+        if _has_component(d[key]):
+            raise ValueError(
+                f"Removing components from CLI is not supported: ~{dotpath}"
+            )
+        del d[key]
+
+    def recurse_and_delete(d: Dict[str, Any], path: List[str]) -> None:
+        if len(path) == 1:
+            delete_non_component(d, path[0])
+        else:
+            recurse_and_delete(d[path[0]], path[1:])
+            if not d[path[0]]:
+                delete_non_component(d, path[0])
+
+    recurse_and_delete(nested_dict, path)
diff -ruN marc_original/third_party/torchtune/torchtune/config/_validate.py marc/third_party/torchtune/torchtune/config/_validate.py
--- marc_original/third_party/torchtune/torchtune/config/_validate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/config/_validate.py	2025-02-20 17:49:30.230025363 -0500
@@ -0,0 +1,45 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import inspect
+
+from omegaconf import DictConfig
+from torchtune.config._errors import ConfigError
+from torchtune.config._utils import _get_component_from_path, _has_component
+
+
+def validate(cfg: DictConfig) -> None:
+    """
+    Ensure that all components in the config can be instantiated correctly
+
+    Args:
+        cfg (DictConfig): The config to validate
+
+    Raises:
+        ConfigError: If any component cannot be instantiated
+    """
+
+    errors = []
+    for node, nodedict in cfg.items():
+        if _has_component(nodedict):
+            try:
+                _component_ = _get_component_from_path(nodedict.get("_component_"))
+                kwargs = {k: v for k, v in nodedict.items() if k != "_component_"}
+                sig = inspect.signature(_component_)
+                sig.bind(**kwargs)
+            # Some objects require other objects as arguments, like optimizers,
+            # lr_schedulers, datasets, etc. Try doing partial instantiation
+            except TypeError as e:
+                if "missing a required argument" in str(e):
+                    sig.bind_partial(**kwargs)
+                else:
+                    # inspect.signature does not retain the function name in the
+                    # exception, so we manually add it back in
+                    e = TypeError(f"{_component_.__name__} {str(e)}")
+                    errors.append(e)
+
+    if errors:
+        raise ConfigError(errors)
diff -ruN marc_original/third_party/torchtune/torchtune/data/_chat_formats.py marc/third_party/torchtune/torchtune/data/_chat_formats.py
--- marc_original/third_party/torchtune/torchtune/data/_chat_formats.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/data/_chat_formats.py	2025-02-20 17:49:30.238025377 -0500
@@ -0,0 +1,44 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from abc import ABC, abstractmethod
+from typing import Dict, List, Tuple
+
+from torchtune.data._messages import Message, Role
+
+
+class ChatFormat(ABC):
+    """
+    Warning:
+        This class is deprecated and will be removed in a future release. Please use
+        :class:`~torchtune.data.PromptTemplate` for custom chat formats.
+
+    Interface for chat formats. Each chat format should include tags for system,
+    user, and assistant roles that are prepended or appended to the message
+    content.
+    """
+
+    # Template should map role to a tuple containing the tag to prepend to the text
+    # and tag to append to the text. Leave as empty strings to not prepend or append
+    template: Dict[Role, Tuple[str, str]]
+
+    @classmethod
+    @abstractmethod
+    def format(
+        cls,
+        sample: List[Message],
+    ) -> List[Message]:
+        """
+        Format each role's message(s) according to the chat format
+
+        Args:
+            sample (List[Message]): a single conversation, structured as a list
+                of `Message` objects
+
+        Returns:
+            The formatted list of messages
+        """
+        pass
diff -ruN marc_original/third_party/torchtune/torchtune/data/_collate.py marc/third_party/torchtune/torchtune/data/_collate.py
--- marc_original/third_party/torchtune/torchtune/data/_collate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/data/_collate.py	2025-02-20 17:49:30.242025384 -0500
@@ -0,0 +1,551 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+import torch
+import torch.nn.functional as F
+from torch.nn.utils.rnn import pad_sequence
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX, PACK_TYPE
+from torchtune.modules.attention_utils import packed_block_causal_mask
+
+
+def left_pad_sequence(
+    sequences: List[torch.Tensor],
+    batch_first: bool = False,
+    padding_value: float = 0,
+) -> torch.Tensor:
+    """
+    This function is identical to :func:`torch.nn.utils.rnn.pad_sequence`, but
+    instead pads a list of variable length Tensors from the left to the length
+    of the longest sequence.
+
+    Note:
+        This function returns a Tensor of size ``T x B x *`` or ``B x T x *``
+        where `T` is the length of the longest sequence. This function assumes
+        trailing dimensions and type of all the Tensors in sequences are same.
+
+    Args:
+        sequences (List[torch.Tensor]): list of variable length sequences.
+        batch_first (bool): if ``True``, the output will be in ``B x T x *``
+            format, ``T x B x *`` otherwise. Default False.
+        padding_value (float): value for padded elements. Default: 0.
+
+    Returns:
+        Tensor of size ``T x B x *`` if :attr:`batch_first` is ``False``.
+        Tensor of size ``B x T x *`` otherwise
+
+    Example:
+        >>> a = torch.tensor([1, 2, 3])
+        >>> b = torch.tensor([4, 5, 6, 7])
+        >>> c = torch.tensor([8, 9, 10, 11, 12])
+        >>> left_pad_sequence([a, b, c], batch_first=True, padding_value=0)
+        tensor([[ 0,  0,  1,  2,  3],
+                [ 0,  4,  5,  6,  7],
+                [ 8,  9, 10, 11, 12]])
+    """
+    return pad_sequence(
+        map(lambda x: torch.flip(x, dims=[0]), sequences),
+        batch_first=batch_first,
+        padding_value=padding_value,
+    ).flip(dims=[int(batch_first)])
+
+
+def padded_collate(
+    batch: List[Dict[str, List[int]]],
+    *,
+    pad_direction: str,
+    keys_to_pad: List[str],
+    padding_idx: Union[int, Dict[str, int]],
+):
+    """
+    A generic padding collation function which pads ``keys_to_pad`` entries in a
+    batch of sequences from the given ``pad_direction`` to the maximum sequence length for
+    each entry in the batch.
+
+    Note:
+        This function assumes all batch elements which are not in ``keys_to_pad`` do not require
+        any collation (see example below).
+
+    Args:
+        batch (List[Dict[str, List[int]]]): A list of dictionaries containing inputs.
+        pad_direction (str): whether to pad entries from the left, or right. If ``pad_direction="right"``, we use
+            :func:`torch.nn.utils.rnn.pad_sequence`, otherwise if ``pad_direction="left"``,
+            we use :func:`torchtune.data.left_pad_sequence`.
+        keys_to_pad (List[str]): Batch element keys to apply padding to. Should be a subset
+            of keys in the batch.
+        padding_idx (Union[int, Dict[str, int]]): Either a single integer padding value to apply to all
+            ``keys_to_pad`` elements, or a mapping with keys identical to ``keys_to_pad`` with per-key
+            padding values.
+
+    Returns:
+        torch.Tensor: The padded tensor of input ids with shape [batch_size, max_seq_len].
+
+    Raises:
+        ValueError: if ``pad_direction`` is not one of "left" or "right".
+        ValueError: if ``keys_to_pad`` is empty, or is not a list, or is not a subset of keys in the batch.
+        ValueError: if ``padding_idx`` is provided as a dictionary, but the keys are not identical to
+            ``keys_to_pad``.
+
+    Example:
+        >>> a = [1, 2, 3]
+        >>> b = [4, 5, 6, 7]
+        >>> c = [8, 9, 10, 11, 12]
+        >>> batch = [
+        >>>     {"tokens": a, "labels": 1},
+        >>>     {"tokens": b, "labels": 3},
+        >>>     {"tokens": c, "labels": 0},
+        >>> ]
+        >>> padded_collate(
+        >>>     batch,
+        >>>     pad_direction="left",
+        >>>     keys_to_pad=["tokens"],
+        >>>     padding_idx=-10
+        >>> )
+        {
+            'labels': tensor([1, 3, 0]),
+            'tokens': tensor([[-10, -10,   1,   2,   3],
+                              [-10,   4,   5,   6,   7],
+                              [  8,   9,  10,  11,  12]])
+        }
+    """
+    if pad_direction not in ["left", "right"]:
+        raise ValueError(
+            f"pad_direction should be one of 'left' or 'right' but found {pad_direction}"
+        )
+
+    if not isinstance(keys_to_pad, list) or not keys_to_pad:
+        raise ValueError(
+            f"keys_to_pad should be a list of strings with at least one element, but found {keys_to_pad}!"
+        )
+
+    keys_to_pad = set(keys_to_pad)
+    if isinstance(padding_idx, dict):
+        if not set(padding_idx.keys()) == keys_to_pad:
+            raise ValueError(
+                f"padding_idx was provided as a dictionary, but the keys ({padding_idx.keys()}) "
+                f"are not the same as keys_to_pad ({keys_to_pad})"
+            )
+        if not keys_to_pad <= set(batch[0].keys()):
+            raise ValueError(
+                "keys_to_pad should be a subset of keys in the batch, but found "
+                f"{keys_to_pad} and {set(batch[0].keys())}, respectively."
+            )
+
+    # let's pull out any batch elements which don't need any padding
+    # and convert to tensors
+    batch_keys = [k for k in batch[0].keys() if k not in keys_to_pad]
+    output_dict = {k: torch.tensor([x[k] for x in batch]) for k in batch_keys}
+
+    # now pad the remaining keys
+    pad_fn = (
+        torch.nn.utils.rnn.pad_sequence
+        if pad_direction == "right"
+        else left_pad_sequence
+    )
+    for k in keys_to_pad:
+        output_dict[k] = pad_fn(
+            [torch.tensor(x[k]) for x in batch],
+            batch_first=True,
+            padding_value=padding_idx[k]
+            if isinstance(padding_idx, dict)
+            else padding_idx,
+        )
+    return output_dict
+
+
+def padded_collate_sft(
+    batch: List[Dict[str, List[int]]],
+    padding_idx: int = 0,
+    ignore_idx: int = CROSS_ENTROPY_IGNORE_IDX,
+) -> Dict[str, torch.Tensor]:
+    """Pad a batch of sequences to the longest sequence length in the batch, and
+    convert integer lists to tensors.
+
+    Args:
+        batch (List[Dict[str, List[int]]]): A list of dictionaries containing input, label pairs.
+        padding_idx (int): Padding index for input ids. Defaults to 0.
+        ignore_idx (int): Padding index for labels. Defaults to -100.
+
+    Returns:
+        Dict[str, torch.Tensor]: Collated input and label tensors.
+
+    Example:
+        >>> token_pairs = [
+        >>>    {"tokens": [1, 2, 3], "labels": [4, 5, 6]},
+        >>>    {"tokens": [7,], "labels": [10,]},
+        >>> ]
+        >>> collated = padded_collate(
+        >>>    batch=token_pairs,
+        >>>    padding_idx=padding_idx,
+        >>>    ignore_idx=ignore_idx,
+        >>> )
+        >>> collated["tokens"]
+        >>> tensor([[1, 2, 3], [7, 0, 0]])
+        >>> collated["labels"]
+        >>> tensor([[4, 5, 6], [10, -100, -100]])
+    """
+    input_ids = pad_sequence(
+        [torch.tensor(x["tokens"]) for x in batch],
+        batch_first=True,
+        padding_value=padding_idx,
+    )
+    labels = pad_sequence(
+        [torch.tensor(x["labels"]) for x in batch],
+        batch_first=True,
+        padding_value=ignore_idx,
+    )
+
+    input_ids_seq_len = input_ids.shape[-1]
+    labels_seq_len = labels.shape[-1]
+
+    # Hack to pad correctly and not use max_seq_len, which is costly
+    if input_ids_seq_len > labels_seq_len:
+        labels = F.pad(
+            labels, (0, input_ids_seq_len - labels_seq_len), value=ignore_idx
+        )
+    elif labels_seq_len > input_ids_seq_len:
+        input_ids = F.pad(
+            input_ids,
+            (0, labels_seq_len - input_ids_seq_len),
+            value=padding_idx,
+        )
+    return {"tokens": input_ids.long(), "labels": labels.long()}
+
+
+# TODO: Generalize this to support any type of encoder input, right now this assumes
+# a specific encoder_input signature
+def padded_collate_tiled_images_and_mask(
+    batch: List[Dict[str, Any]],
+    padding_idx: int = 0,
+    ignore_idx: int = CROSS_ENTROPY_IGNORE_IDX,
+    pad_direction: str = "right",
+    pad_max_images: Optional[int] = None,
+) -> Dict[str, torch.Tensor]:
+    """Pad a batch of text sequences, tiled image tensors, aspect ratios,
+    and cross attention masks. This can be used for both training and inference.
+
+    ``batch`` is expected to be a list of sample dicts containing the following::
+        - "tokens": List[int] of length text_seq_len, varies across samples
+        - "labels": List[int] of length text_seq_len, varies across samples
+        - "encoder_input": Dict[str, List[torch.Tensor]]
+            - "images": List[torch.Tensor], each with shape (n_tiles, c, h, w)
+            - "aspect_ratio": List[torch.Tensor], each with shape (2, ) to indicate h_ratio, w_ratio
+        - "encoder_mask": List[Tensor], each with shape (text_seq_len, image_seq_len)
+
+    Shape notation:
+        - c = channel dim
+        - h = height dim
+        - w = weight dim
+
+    Note:
+        For each element in the batch, ``len(images) == len(encoder_mask) == len(aspect_ratio)``.
+
+    This collater does the following:
+        (1) Pad text sequence and encoder mask to the longest sequence length in the batch
+        (2) Pad image tensors in the tile dimension with zeros to the largest number
+            of tiles in the batch
+        (3) Add empty images of zeros to samples up to max number of images in the batch
+        (4) Pad aspect ratios with (1,1) for all added padding images
+
+    Args:
+        batch (List[Dict[str, Any]]): A list of sample dicts containing tokens,
+            labels, images, encoder_mask, and aspect_ratio.
+        padding_idx (int): Padding index for input token ids. Defaults to 0.
+        ignore_idx (int): Padding index for labels. Defaults to -100.
+        pad_direction (str): whether to pad entries from the left, or right. If ``pad_direction="right"``, we use
+            :func:`torch.nn.utils.rnn.pad_sequence`, otherwise if ``pad_direction="left"``,
+            we use :func:`torchtune.data.left_pad_sequence`. For training, we typically want to pad from the right.
+            For inference, we typically want to pad from the left. Defaults to "right".
+        pad_max_images (Optional[int]): Maximum number of images to pad to. If None, will pad to the largest number of images
+            in the batch. Defaults to None.
+
+    Returns:
+        Dict[str, Tensor]: Collated tokens, labels, images, encoder_mask, aspect_ratio tensors.
+            - tokens: Tensor of shape (bsz, max_seq_len)
+            - labels: Tensor of shape (bsz, max_seq_len)
+            - images: Tensor of shape (bsz, max_num_images, max_num_tiles, c, h, w)
+            - encoder_mask: Tensor of shape (bsz, max_seq_len, tokens_per_tile * max_num_tiles * max_num_images)
+            - aspect_ratio: Tensor of shape (bsz, max_num_images, 2)
+
+    Raises:
+        ValueError: if ``pad_direction`` is not one of "left" or "right".
+
+    Example:
+        >>> image_id = 1
+        >>> tokens_per_tile = 5
+        >>> c, h, w = 1, 1, 1
+        >>> batch = [
+        ...     {
+        ...         "tokens": [1, 2, 1, 3], "labels": [4, 5, 6, 7],
+        ...         "encoder_input": {
+        ...             # One image with two tiles, one image with three tiles
+        ...             "images": [torch.ones(2, c, h, w), torch.ones(3, c, h, w)],
+        ...             "aspect_ratio": [torch.tensor([1, 2]), torch.tensor([1, 3])],
+        ...         },
+        ...         # Mask is shape (text_seq_len, tokens_per_tile * n_tiles)
+        ...         "encoder_mask": [torch.ones(4, 5 * 2), torch.ones(4, 5 * 3)],
+        ...     },
+        ...     {
+        ...         "tokens": [1, 4], "labels": [8, 9],
+        ...         "encoder_input": {
+        ...             # One image with four tiles
+        ...             "images": [torch.ones(4, c, h, w)],
+        ...             "aspect_ratio": [torch.tensor([2, 2])],
+        ...         },
+        ...         # Mask is shape (text_seq_len, tokens_per_tile * n_tiles)
+        ...         "encoder_mask": [torch.ones(2, 5 * 4)],
+        ...     },
+        ... ]
+        >>> model_inputs = padded_collate_tiled_images_and_mask(batch=batch)
+        >>> print(model_inputs["tokens"])
+        tensor([[1, 2, 1, 3],
+                [1, 4, 0, 0]])
+        >>> print(model_inputs["labels"])
+        tensor([[4, 5, 6, 7],
+                [8, 9, -100, -100]])
+        >>> print(model_inputs["encoder_input"]["images"].shape)  # (bsz, max_num_images, max_num_tiles, c, h, w)
+        torch.Size([2, 2, 4, 1, 1, 1])
+        >>> print(model_inputs["encoder_mask"].shape)  # (bsz, max_text_seq_len, tokens_per_tile * max_num_tiles * max_num_images)
+        torch.Size([2, 4, 40])
+        >>> print(model_inputs["encoder_input"]["aspect_ratio"].shape)  # (bsz, max_num_images, 2)
+        torch.Size([2, 2, 2])
+        >>> print(model_inputs["encoder_input"]["images"][0, 0, ...])  # Image with two tiles got padded to four
+        tensor([[[[1.]]], [[[1.]]], [[[0.]]], [[[0.]]]])
+        >>> print(model_inputs["encoder_input"]["images"][0, 1, ...])  # Image with three tiles got padded to four
+        tensor([[[[1.]]], [[[1.]]], [[[1.]]], [[[0.]]]])
+        >>> print(model_inputs["encoder_input"]["images"][1, 0, ...])  # Image with four tiles did not get padded
+        tensor([[[[1.]]], [[[1.]]], [[[1.]]], [[[1.]]]])
+        >>> print(model_inputs["encoder_input"]["images"][1, 1, ...])  # Extra padding image was added to second sample
+        tensor([[[[0.]]], [[[0.]]], [[[0.]]], [[[0.]]]])
+    """
+    if pad_direction not in ["left", "right"]:
+        raise ValueError(
+            f"pad_direction should be one of 'left' or 'right' but found {pad_direction}"
+        )
+
+    # Text tokens can be handled independently by existing collaters
+    if pad_direction == "right":
+        text_only = [
+            {"tokens": sample["tokens"], "labels": sample["labels"]} for sample in batch
+        ]
+        collated_text = padded_collate_sft(text_only, padding_idx, ignore_idx)
+    # For inference, we don't need to handle labels
+    elif pad_direction == "left":
+        collated_text = {
+            "tokens": left_pad_sequence(
+                [torch.tensor(x["tokens"]) for x in batch],
+                batch_first=True,
+                padding_value=padding_idx,
+            )
+        }
+
+    max_seq_len = collated_text["tokens"].shape[-1]
+    bsz = len(batch)
+
+    # TODO: Figure out how to make this more efficient or vectorized. Setting
+    # max_num_tiles beforehand will save one nested for loop but may incur more
+    # memory and compute costs in attention if max_num_tiles > batch_max_num_tiles
+
+    # First loop: get max number of tiles in batch
+    max_num_tiles = max(
+        image.shape[0]
+        for sample in batch
+        for image in sample["encoder_input"]["images"]
+    )
+    # Second loop: pad images and masks to max number of tiles, max text seq len in batch
+    batch_images = []
+    batch_masks = []
+    batch_aspect_ratios = []
+    for sample in batch:
+        sample_images = []
+        sample_masks = []
+        for image, mask in zip(
+            sample["encoder_input"]["images"], sample["encoder_mask"]
+        ):
+            # Single image in each sample has shape (n_tiles, c, h, w)
+            n_tiles = image.shape[0]
+            # Single mask in each sample corresponds to a single image and has shape (text_seq_len, image_seq_len)
+            # where image_seq_len = n_tiles * tokens_per_tile
+            text_seq_len, image_seq_len = mask.shape
+            tokens_per_tile = image_seq_len // n_tiles
+            padding_tiles = max_num_tiles - n_tiles
+            right_padding_text = (
+                max_seq_len - text_seq_len if pad_direction == "right" else 0
+            )
+            left_padding_text = (
+                max_seq_len - text_seq_len if pad_direction == "left" else 0
+            )
+
+            # Image should now have shape (max_num_tiles, c, h, w)
+            padded_image = F.pad(image, (0, 0, 0, 0, 0, 0, 0, padding_tiles), value=0)
+            # Mask should now have shape (max_seq_len, max_image_seq_len), where
+            # max_image_seq_len = max_num_tiles * tokens_per_tile
+            padded_mask = F.pad(
+                mask,
+                (
+                    0,
+                    padding_tiles * tokens_per_tile,
+                    left_padding_text,
+                    right_padding_text,
+                ),
+                value=0,
+            )
+
+            sample_images.append(padded_image)
+            sample_masks.append(padded_mask)
+        # Stack multiple images and masks per sample in num_images dimension
+        batch_images.append(torch.stack(sample_images))
+        batch_masks.append(torch.stack(sample_masks))
+        batch_aspect_ratios.append(torch.stack(sample["encoder_input"]["aspect_ratio"]))
+    # Finally, pad images, masks, aspect ratios to max number of images in batch
+    # (bsz, max_num_images, max_num_tiles, c, h, w)
+    collated_images = pad_sequence(batch_images, batch_first=True, padding_value=0)
+    # (bsz, max_num_images, max_seq_len, max_image_seq_len)
+    collated_masks = pad_sequence(batch_masks, batch_first=True, padding_value=0)
+    # (bsz, max_num_images, 2)
+    collated_aspect_ratios = pad_sequence(
+        batch_aspect_ratios, batch_first=True, padding_value=1
+    )
+
+    # Concatenate masks for multiple images across image_seq_len dimension
+    concat_masks = collated_masks.view(bsz, max_seq_len, -1)
+    if pad_max_images is not None:
+        _, _, img_seq = concat_masks.shape
+        concat_masks = F.pad(
+            concat_masks, (0, pad_max_images * image_seq_len - img_seq)
+        )
+
+    batch_dict = {
+        "tokens": collated_text["tokens"],
+        "encoder_input": {
+            "images": collated_images,
+            "aspect_ratio": collated_aspect_ratios,
+        },
+        "encoder_mask": concat_masks,
+    }
+
+    if "labels" in collated_text:
+        batch_dict["labels"] = collated_text["labels"]
+
+    return batch_dict
+
+
+def padded_collate_packed(
+    batch: List[PACK_TYPE],
+) -> Dict[str, torch.Tensor]:
+    """Collate packed sequences into a batch. Only convert the seq lens into
+    a block mask for use with attention. Tokens, labels, and input_pos are
+    already padded to the same length within :class:`~torchtune.datasets.PackedDataset`.
+
+    Args:
+        batch (List[PACK_TYPE]): A list of pack dictionaries containing the following keys:
+            - tokens: input token ids
+            - labels: label token ids
+            - input_pos: relative position ids for each sequence in pack
+            - seq_lens: lengths of each sample within the pack
+
+    Returns:
+        Dict[str, torch.Tensor]: Collated input, label, input_pos, mask tensors.
+
+    Example:
+        >>> token_pairs = [
+        >>>    {"tokens": [1, 2, 3, 4, 5, 6], "labels": [7, 8, 9, 10, 11, 12],
+        >>>     "input_pos": [0, 1, 2, 0, 1, 0], "seq_lens": [3, 2, 1]},
+        >>>    {"tokens": [13, 14, 15, 16, 17, 18], "labels": [19, 20, 21, 22, 23, 24],
+        >>>     "input_pos": [0, 1, 0, 1, 0, 1], "seq_lens": [2, 2, 2]},
+        >>> ]
+        >>> collated = padded_collate_packed(
+        >>>    batch=token_pairs,
+        >>>    device=device,
+        >>> )
+        >>> collated["mask"]
+        >>> tensor([
+        >>> [[1, 0, 0, 0, 0, 0],
+        >>>  [1, 1, 0, 0, 0, 0],
+        >>>  [1, 1, 1, 0, 0, 0],
+        >>>  [0, 0, 0, 1, 0, 0],
+        >>>  [0, 0, 0, 1, 1, 0],
+        >>>  [0, 0, 0, 0, 0, 1]],
+        >>> [[1, 0, 0, 0, 0, 0],
+        >>>  [1, 1, 0, 0, 0, 0],
+        >>>  [0, 0, 1, 0, 0, 0],
+        >>>  [0, 0, 1, 1, 0, 0],
+        >>>  [0, 0, 0, 0, 1, 0],
+        >>>  [0, 0, 0, 0, 1, 1]])
+    """
+
+    tokens = torch.stack([x["tokens"] for x in batch])
+    labels = torch.stack([x["labels"] for x in batch])
+    input_pos = torch.stack([x["input_pos"] for x in batch])
+    seq_lens = [x["seq_lens"] for x in batch]
+
+    block_mask = packed_block_causal_mask(
+        seq_lens=seq_lens,
+    )
+
+    return {
+        "tokens": tokens,
+        "labels": labels,
+        "input_pos": input_pos,
+        "mask": block_mask,
+    }
+
+
+def padded_collate_dpo(
+    batch: List[Dict[str, List[int]]],
+    padding_idx: int = 0,
+    ignore_idx: int = CROSS_ENTROPY_IGNORE_IDX,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """Pad a batch of sequences for Direct Preference Optimization (DPO).
+
+    This function takes a batch of sequences, where each sequence is represented
+    as a dictionary with multiple key-value pairs. Each key corresponds to a different
+    sequence component, such as input_ids or labels.
+
+    Args:
+        batch (List[Dict[str, List[int]]]): A list of dictionaries, where each dictionary
+            represents a sequence with multiple components, 'chosen_input_ids',
+            'chosen_labels', 'rejected_input_ids', and 'rejected_labels' are required.
+        padding_idx (int): Padding index for input ids. Defaults to 0.
+        ignore_idx (int): Padding index for labels. Defaults to -100.
+
+    Returns:
+        Tuple[torch.Tensor, torch.Tensor]: A tuple containing concatenated and padded
+        input ids and labels.
+
+    Example:
+        >>> batch = [
+        >>>    {'chosen_input_ids': [1, 2, 3], 'rejected_input_ids': [4, 5],
+        >>>      'chosen_labels': [6, 7, 8], 'rejected_labels': [9, 10]},
+        >>>    {'chosen_input_ids': [11, 12], 'rejected_input_ids': [13, 14, 15],
+        >>>      'chosen_labels': [16, 17], 'rejected_labels': [18, 19, 20]},
+        >>> ]
+        >>> padded_collate_dpo(batch)
+        >>> (tensor([[ 1,  2,  3],
+        >>>          [11, 12,  0],
+        >>>          [ 4,  5,  0],
+        >>>          [13, 14, 15]]),
+        >>>  tensor([[ 6,  7,  8],
+        >>>          [16, 17, -100],
+        >>>          [ 9, 10, -100],
+        >>>          [18, 19, 20]]))
+    """
+    chosen_input_ids = [torch.tensor(ex["chosen_input_ids"]) for ex in batch]
+    rejected_input_ids = [torch.tensor(ex["rejected_input_ids"]) for ex in batch]
+    chosen_labels = [torch.tensor(ex["chosen_labels"]) for ex in batch]
+    rejected_labels = [torch.tensor(ex["rejected_labels"]) for ex in batch]
+
+    to_pad_input_ids = chosen_input_ids + rejected_input_ids
+    to_pad_labels = chosen_labels + rejected_labels
+
+    concatenated_input_ids = pad_sequence(
+        to_pad_input_ids, batch_first=True, padding_value=padding_idx
+    )
+    concatenated_labels = pad_sequence(
+        to_pad_labels, batch_first=True, padding_value=ignore_idx
+    )
+
+    return concatenated_input_ids, concatenated_labels
diff -ruN marc_original/third_party/torchtune/torchtune/data/_common.py marc/third_party/torchtune/torchtune/data/_common.py
--- marc_original/third_party/torchtune/torchtune/data/_common.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/data/_common.py	2025-02-20 17:49:30.242025384 -0500
@@ -0,0 +1,11 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import Dict, List, Union
+
+import torch
+
+CROSS_ENTROPY_IGNORE_IDX = -100
+PACK_TYPE = Dict[str, Union[torch.Tensor, List[int]]]
diff -ruN marc_original/third_party/torchtune/torchtune/data/_converters.py marc/third_party/torchtune/torchtune/data/_converters.py
--- marc_original/third_party/torchtune/torchtune/data/_converters.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/data/_converters.py	2025-02-20 17:49:30.246025389 -0500
@@ -0,0 +1,174 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, List, Mapping
+
+from torchtune.data._messages import Message
+from torchtune.utils._logging import deprecated
+
+
+@deprecated(
+    msg="Please use an instance of `torchtune.data.ShareGPTToMessages` as the "
+    "`message_transform` argument for `torchtune.datasets.SFTDataset` instead."
+)
+def get_sharegpt_messages(
+    sample: Mapping[str, Any], train_on_input: bool = False
+) -> List[Message]:
+    """
+    Warning:
+        This class is deprecated and will be removed in a future release. Please use
+        :class:`~torchtune.data.ShareGPTToMessages` instead. The following are equivalent:
+
+        .. code-block:: python
+
+            # Deprecated
+            transformed_sample = get_sharegpt_messages(sample, train_on_input=True)
+
+            # New
+            transformed_sample = ShareGPTToMessages(train_on_input=True)(sample)
+
+    Convert a chat sample adhering to the ShareGPT json structure to torchtune's :class:`~torchtune.data.Message`
+    structure.
+
+    ShareGPT follows::
+
+        {
+            "conversations": [
+                {
+                    "from": <system|human|gpt>,
+                    "value": <message>,
+                },
+                ...
+            ]
+        }
+
+    :class:`~torchtune.data.Message` follows::
+
+        [
+            {
+                "role": <system|user|assistant>,
+                "content": <message>,
+            },
+            ...
+        ]
+
+    Args:
+        sample (Mapping[str, Any]): a single data sample with "conversations" field pointing
+            to a list of dict messages.
+        train_on_input (bool): whether the prompt should remain unmasked. Default: False
+
+    Returns:
+        List[Message]: A list of messages with "role" and "content" fields.
+    """
+    role_map = {"system": "system", "human": "user", "gpt": "assistant"}
+    conversations = sample["conversations"]
+
+    messages = []
+    for message in conversations:
+        role = role_map[message["from"]]
+        content = message["value"]
+        masked = (role != "assistant") and (not train_on_input)
+        messages.append(
+            Message(
+                role=role, content=[{"type": "text", "content": content}], masked=masked
+            )
+        )
+    return messages
+
+
+@deprecated(
+    msg="Please use an instance of `torchtune.data.OpenAIToMessages` as the "
+    "`message_transform` argument for `torchtune.datasets.SFTDataset` instead."
+)
+def get_openai_messages(
+    sample: Mapping[str, Any],
+    train_on_input: bool = False,
+) -> List[Message]:
+    """
+    Warning:
+        This class is deprecated and will be removed in a future release. Please use
+        :class:`~torchtune.data.OpenAIToMessages` instead. The following are equivalent:
+
+        .. code-block:: python
+
+            # Deprecated
+            transformed_sample = get_openai_messages(sample, train_on_input=True)
+
+            # New
+            transformed_sample = OpenAIToMessages(train_on_input=True)(sample)
+
+    Convert a chat sample adhering to the OpenAI API json structure to torchtune's :class:`~torchtune.data.Message`
+    structure.
+
+    OpenAI API `standard chat format <https://platform.openai.com/docs/guides/text-generation/chat-completions-api>`_ follows::
+
+        {
+            # key could be "messages" OR "conversations"
+            "messages": [
+                {
+                    "role": <system|user|assistant>,
+                    "content": <message>,
+                },
+                ...
+            ]
+        }
+
+    :class:`~torchtune.data.Message` follows::
+
+        [
+            {
+                "role": <system|user|assistant>,
+                "content": <message>,
+            },
+            ...
+        ]
+
+    Args:
+        sample (Mapping[str, Any]): a single data sample with "conversations" field pointing
+            to a list of dict messages.
+        train_on_input (bool): whether the prompt should remain unmasked. Default: False
+
+    Raises:
+        ValueError: If the sample does not contain "messages" or "conversations" key.
+
+    Returns:
+        List[Message]: A list of messages with "role" and "content" fields.
+    """
+    if "messages" in sample:
+        messages_key = "messages"
+    elif "conversations" in sample:
+        messages_key = "conversations"
+    else:
+        raise ValueError(
+            f"Sample does not contain 'messages' or 'conversations' key. Existing keys: {sample.keys()}"
+        )
+    conversations = sample[messages_key]
+
+    messages = []
+    for message in conversations:
+        message["masked"] = (message["role"] != "assistant") and (not train_on_input)
+        messages.append(Message.from_dict(message))
+    return messages
+
+
+def arc_to_messages(
+    sample: Mapping[str, Any], train_on_input: bool = False,
+) -> List[Message]:
+    """
+    Convert a chat sample adhering to the ARC format to the chat format.
+    """
+
+    input = sample["input"]
+    output = sample["output"]
+
+    messages = []
+
+    for message in input:
+        messages.append(Message(role=message["role"], content=message["content"], masked=(not train_on_input)))
+
+    messages.append(Message(role="assistant", content=output["content"], masked=False))
+
+    return messages
\ No newline at end of file
diff -ruN marc_original/third_party/torchtune/torchtune/data/__init__.py marc/third_party/torchtune/torchtune/data/__init__.py
--- marc_original/third_party/torchtune/torchtune/data/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/data/__init__.py	2025-02-20 17:49:30.234025370 -0500
@@ -0,0 +1,73 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from torchtune.data._chat_formats import ChatFormat
+from torchtune.data._collate import (
+    left_pad_sequence,
+    padded_collate,
+    padded_collate_dpo,
+    padded_collate_packed,
+    padded_collate_sft,
+    padded_collate_tiled_images_and_mask,
+)
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+from torchtune.data._converters import get_openai_messages, get_sharegpt_messages, arc_to_messages
+from torchtune.data._instruct_templates import InstructTemplate
+from torchtune.data._messages import (
+    ARCToMessages,
+    AlpacaToMessages,
+    ChosenRejectedToMessages,
+    InputOutputToMessages,
+    Message,
+    OpenAIToMessages,
+    Role,
+    ShareGPTToMessages,
+    ARCMultiModalToMessages,
+    validate_messages,
+)
+from torchtune.data._prompt_templates import (
+    ChatMLTemplate,
+    GrammarErrorCorrectionTemplate,
+    PromptTemplate,
+    PromptTemplateInterface,
+    QuestionAnswerTemplate,
+    SummarizeTemplate,
+)
+from torchtune.data._utils import format_content_with_images, load_image, truncate
+
+__all__ = [
+    "ChatFormat",
+    "CROSS_ENTROPY_IGNORE_IDX",
+    "GrammarErrorCorrectionTemplate",
+    "InstructTemplate",
+    "SummarizeTemplate",
+    "OpenAIToMessages",
+    "ShareGPTToMessages",
+    "AlpacaToMessages",
+    "ARCToMessages",
+    "ARCMultiModalToMessages",
+    "truncate",
+    "Message",
+    "validate_messages",
+    "Role",
+    "format_content_with_images",
+    "PromptTemplateInterface",
+    "PromptTemplate",
+    "InputOutputToMessages",
+    "ChosenRejectedToMessages",
+    "QuestionAnswerTemplate",
+    "ChatMLTemplate",
+    "get_openai_messages",
+    "get_sharegpt_messages",
+    "arc_to_messages",
+    "padded_collate_sft",
+    "padded_collate_dpo",
+    "left_pad_sequence",
+    "padded_collate",
+    "padded_collate_tiled_images_and_mask",
+    "padded_collate_packed",
+    "load_image",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/data/_instruct_templates.py marc/third_party/torchtune/torchtune/data/_instruct_templates.py
--- marc_original/third_party/torchtune/torchtune/data/_instruct_templates.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/data/_instruct_templates.py	2025-02-20 17:49:30.250025396 -0500
@@ -0,0 +1,41 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from abc import ABC, abstractmethod
+from typing import Any, Dict, Mapping, Optional
+
+
+class InstructTemplate(ABC):
+    """
+    Warning:
+        This class is deprecated and will be removed in a future release. Please use
+        :class:`~torchtune.data.PromptTemplate` for custom instruct templates.
+
+    Interface for instruction templates. Each template should include the template
+    prompt with placeholders for the data inputs.
+    """
+
+    template = ""
+
+    @classmethod
+    @abstractmethod
+    def format(
+        cls, sample: Mapping[str, Any], column_map: Optional[Dict[str, str]] = None
+    ) -> str:
+        """
+        Format the prompt template with the given arguments.
+
+        Args:
+            sample (Mapping[str, Any]): a single data sample with various fields
+            column_map (Optional[Dict[str, str]]): a mapping from the expected
+                placeholder names in the template to the column names in the sample.
+                If None, assume these are identical. Note: if the sample output is not named
+                as "output" in the dataset, you always need to map it to "output" in column_map.
+
+        Returns:
+            The formatted prompt
+        """
+        pass
diff -ruN marc_original/third_party/torchtune/torchtune/data/_messages.py marc/third_party/torchtune/torchtune/data/_messages.py
--- marc_original/third_party/torchtune/torchtune/data/_messages.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/data/_messages.py	2025-02-20 17:49:30.254025403 -0500
@@ -0,0 +1,776 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from pathlib import Path
+from typing import Any, Dict, List, Literal, Mapping, Optional, Union
+
+from torchtune.data._utils import format_content_with_images, load_image
+
+from torchtune.modules.transforms import Transform
+
+Role = Literal[
+    "system",  # Origin is system prompt
+    "user",  # Origin is user
+    "assistant",  # Origin is the model output
+    "ipython",  # Origin is return from a tool call
+]
+
+
+class Message:
+    """
+    This class represents individual messages in a fine-tuning dataset. It supports
+    text-only content, text with interleaved images, and tool calls. The :class:`~torchtune.modules.tokenizers.ModelTokenizer`
+    will tokenize the content of the message using ``tokenize_messages`` and attach
+    the appropriate special tokens based on the flags set in this class.
+
+    Args:
+        role (Role): role of the message writer. Can be "system" for system prompts,
+            "user" for human prompts, "assistant" for model responses, or "ipython"
+            for tool call returns.
+        content (Union[str, List[Dict[str, Any]]]): content of the message. If it is text only content,
+            you can pass in a string. If it is multimodal content, pass in a list of dictionaries formatted
+            as follows::
+
+                [
+                    {"type": "image", "content": <PIL.Image.Image>},
+                    {"type": "text", "content": "What is in this image?"},
+                ]
+
+        masked (bool): whether the message is masked in the sample. If True, do not use
+            in loss calculation. Default: False
+        ipython (bool): whether the message is a tool call. Default: False
+        eot (bool): whether the message corresponds to the end of a turn, where control is handed over
+            to the assistant from the user or the user from the assistant. Default: True. Should be true
+            in most cases except for:
+
+            - For multiple consecutive assistant messages (i.e., tool calls
+              by assistant), only the last assistant message will have ``eot=True``
+            - All ipython messages (tool call returns) should set ``eot=False``.
+
+    Note:
+        Message class expects any image content to be in
+        `PIL Image format <https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image>`_.
+    """
+
+    def __init__(
+        self,
+        role: Role,
+        content: Union[str, List[Dict[str, Any]]],
+        masked: bool = False,
+        ipython: bool = False,
+        eot: bool = True,
+    ):
+        self.role = role
+        self.content = self._convert_to_list_of_dict(content)
+        self.masked = masked
+        self.ipython = ipython
+        self.eot = eot
+
+        self._validate_message()
+
+    def _convert_to_list_of_dict(self, content) -> List[Dict[str, Any]]:
+        """User is currently allowed to pass in a string for text-only content.
+        This ensures that the content is formatted as a list of dictionaries."""
+        if isinstance(content, str):
+            return [{"type": "text", "content": content}]
+
+        assert isinstance(
+            content, list
+        ), f"content must be of type List[Dict[str, Any]], got {content}"
+
+        return content
+
+    @classmethod
+    def from_dict(cls, d: dict) -> "Message":
+        """
+        Construct a Message from a dictionary.
+
+        Args:
+            d (dict): dictionary containing the fields of the Message.
+
+        Returns:
+            Message: constructed Message.
+        """
+        return cls(
+            role=d["role"],
+            content=d["content"],
+            masked=d.get("masked", False),
+            ipython=d.get("ipython", False),
+            eot=d.get("eot", True),
+        )
+
+    def get_media(self) -> List["PIL.Image.Image"]:
+        """
+        Returns media content of the message.
+        """
+        return [
+            content["content"] for content in self.content if content["type"] == "image"
+        ]
+
+    @property
+    def contains_media(self) -> bool:
+        """
+        Returns whether the message contains media.
+        """
+        return any(content["type"] == "image" for content in self.content)
+
+    @property
+    def text_content(self) -> str:
+        """
+        Returns text-only content of the message.
+        """
+        return "".join(
+            content["content"] for content in self.content if content["type"] == "text"
+        )
+
+    def _validate_message(self) -> None:
+        if self.ipython and self.contains_media:
+            raise ValueError(
+                f"Media tokens in tool calls are not supported. Both are set in message: {self.text_content}"
+            )
+        if self.ipython and self.role != "assistant":
+            raise ValueError(
+                f"Only assistant messages can be tool calls. Found role {self.role} in message: {self.text_content}"
+            )
+
+    def __repr__(self) -> str:
+        content_only = [content["content"] for content in self.content]
+        return f"Message(role='{self.role}', content={content_only!r})"
+
+
+class InputOutputToMessages(Transform):
+    """
+    Message transform class that converts a single sample with "input" and "output" fields,
+    (or equivalent fields specified in column_map) to user and assistant messages,
+    respectively. This is useful for datasets that have two columns, one containing
+    the user prompt string and the other containing the model response string::
+
+        |  input          |  output          |
+        |-----------------|------------------|
+        | "user prompt"   | "model response" |
+
+    Args:
+        train_on_input (bool): Whether the model is trained on the user prompt or not.
+            Default is False.
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected "input"
+            and "output" column names to the actual column names in the dataset. Keys should
+            be "input" and "output" and values should be the actual column names. Default is None,
+            keeping the default "input" and "output" column names.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Default is None.
+
+    Raises:
+        ValueError: If ``column_map`` is provided and ``input`` not in ``column_map``, or
+            ``output`` not in ``column_map``.
+    """
+
+    def __init__(
+        self,
+        train_on_input: bool = False,
+        column_map: Optional[Dict[str, str]] = None,
+        new_system_prompt: Optional[str] = None,
+    ):
+        self.train_on_input = train_on_input
+        self.new_system_prompt = new_system_prompt
+        if column_map:
+            if "input" not in column_map:
+                raise ValueError(
+                    f"Expected a key of 'input' in column_map but found {column_map.keys()}."
+                )
+            if "output" not in column_map:
+                raise ValueError(
+                    f"Expected a key of 'output' in column_map but found {column_map.keys()}."
+                )
+            self._column_map = column_map
+        else:
+            self._column_map = {"input": "input", "output": "output"}
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        messages = [
+            Message(
+                role="user",
+                content=sample[self._column_map["input"]],
+                masked=not self.train_on_input,
+                eot=True,
+            ),
+            Message(
+                role="assistant",
+                content=sample[self._column_map["output"]],
+                masked=False,
+                eot=True,
+            ),
+        ]
+        if self.new_system_prompt is not None:
+            messages = [
+                Message(
+                    role="system", content=self.new_system_prompt, masked=True, eot=True
+                )
+            ] + messages
+        return {"messages": messages}
+
+
+class ChosenRejectedToMessages(Transform):
+    """
+    Transform for converting a single sample from datasets with "chosen" and "rejected" columns
+    containing conversations to a list of chosen and rejected messages. For example::
+
+        |  chosen                                |  rejected                              |
+        |----------------------------------------|----------------------------------------|
+        | [{"role": "user", "content": Q1},      | [{"role": "user", "content": Q1},      |
+        |  {"role": "assistant", "content": A1}] |  {"role": "assistant", "content": A2}] |
+
+    will be converted to:
+
+    .. code-block:: python
+
+        chosen = [
+            Message(role="user", content="Q1"),
+            Message(role="assistant", content="A1"),
+        ]
+        rejected = [
+            Message(role="user", content="Q1"),
+            Message(role="assistant", content="A2"),
+        ]
+
+    A single sample typically consists of a single optional system prompt and one or multiple
+    turns of user and assistant messages.
+
+    Args:
+        train_on_input (bool): Whether the model is trained on the user prompt or not.
+            Default is False.
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected
+            "chosen" and "rejected" column names to the actual column names in the dataset.
+            Keys should be "chosen" and "rejected" and values should be the actual column names.
+            Default is None, keeping the default column names.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+
+    Raises:
+        ValueError: If ``column_map`` is provided and ``chosen`` not in ``column_map``, or
+            ``rejected`` not in ``column_map``.
+    """
+
+    def __init__(
+        self,
+        train_on_input: bool = False,
+        column_map: Optional[Dict[str, str]] = None,
+        new_system_prompt: Optional[str] = None,
+    ):
+        self.train_on_input = train_on_input
+        self.new_system_prompt = new_system_prompt
+        if column_map:
+            if "chosen" not in column_map:
+                raise ValueError(
+                    f"Expected a key of 'chosen' in column_map but found {column_map.keys()}."
+                )
+            if "rejected" not in column_map:
+                raise ValueError(
+                    f"Expected a key of 'rejected' in column_map but found {column_map.keys()}."
+                )
+            self._column_map = column_map
+        else:
+            self._column_map = {"chosen": "chosen", "rejected": "rejected"}
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        chosen_messages = []
+        for message in sample[self._column_map["chosen"]]:
+            if message["role"] == "system" and self.new_system_prompt is not None:
+                continue
+            message["masked"] = (message["role"] != "assistant") and (
+                not self.train_on_input
+            )
+            chosen_messages.append(Message.from_dict(message))
+
+        rejected_messages = []
+        for message in sample[self._column_map["rejected"]]:
+            if message["role"] == "system" and self.new_system_prompt is not None:
+                continue
+            message["masked"] = (message["role"] != "assistant") and (
+                not self.train_on_input
+            )
+            rejected_messages.append(Message.from_dict(message))
+
+        if self.new_system_prompt is not None:
+            chosen_messages = [
+                Message(
+                    role="system", content=self.new_system_prompt, masked=True, eot=True
+                )
+            ] + chosen_messages
+            rejected_messages = [
+                Message(
+                    role="system", content=self.new_system_prompt, masked=True, eot=True
+                )
+            ] + rejected_messages
+
+        return {"chosen": chosen_messages, "rejected": rejected_messages}
+
+
+class ShareGPTToMessages(Transform):
+    """
+    Convert a single chat sample adhering to the ShareGPT JSON structure to torchtune's :class:`~torchtune.data.Message`
+    structure.
+
+    A single sample typically consists of a single optional system prompt and one or multiple
+    turns of user and assistant messages.
+
+    ShareGPT follows::
+
+        {
+            "conversations": [
+                {
+                    "from": <system|human|gpt>,
+                    "value": <message>,
+                },
+                ...
+            ]
+        }
+
+    :class:`~torchtune.data.Message` follows::
+
+        [
+            {
+                "role": <system|user|assistant>,
+                "content": <message>,
+            },
+            ...
+        ]
+
+    Args:
+        train_on_input (bool): whether the prompt should remain unmasked. For multimodal datasets, ``train_on_input``
+            is always False and this value is ignored. Default: False
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns ("conversations")
+            to the new column names in the dataset. Key should be "conversations" and value should
+            be the new column name. If None, keep the default "conversations".
+            Default is None.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+        image_dir (Optional[Path]): path to the directory containing the images that is prepended to all image
+            paths in the dataset. For example, if ``image_dir="/home/user/dataset/"` and the sample image path
+            was ``"images/1.jpg"``, the final image path that will be loaded is ``"/home/user/dataset/images/1.jpg"``.
+            If None, assume images are available in current working directory or are located
+            on a remote url. For text-only, leave as None. Default is None.
+        image_tag (Optional[str]): placeholder tags in the text content of each message to be replaced by image
+            special tokens. If images are present and this is None, then will prepend image tokens to the first
+            user message in the sample by default. If text-only, this field is ignored. Default is ``"<image>"``.
+
+    Raises:
+        ValueError: If ``column_map`` is provided and ``conversations`` not in ``column_map``.
+    """
+
+    def __init__(
+        self,
+        train_on_input: bool = False,
+        column_map: Optional[Dict[str, str]] = None,
+        new_system_prompt: Optional[str] = None,
+        image_dir: Optional[Path] = None,
+        image_tag: Optional[str] = "<image>",
+    ):
+        self.train_on_input = train_on_input
+        self.new_system_prompt = new_system_prompt
+        if column_map:
+            if "conversations" not in column_map:
+                raise ValueError(
+                    f"Expected a key of 'conversations' in column_map but found {column_map.keys()}."
+                )
+            self._column_map = column_map
+        else:
+            self._column_map = {"conversations": "conversations", "image": "image"}
+        self.image_dir = image_dir
+        self.image_tag = image_tag
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        """
+        Return a list of Message objects from the provided sample dict.
+
+        Args:
+            sample (Mapping[str, Any]): a single data sample with "messages" field pointing
+                to a list of dict messages.
+
+        Returns:
+            List[Message]: A list of messages with "role" and "content" fields.
+        """
+        role_map = {"system": "system", "human": "user", "gpt": "assistant"}
+        messages = []
+        if self.new_system_prompt is not None:
+            messages.append(
+                Message(
+                    role="system", content=self.new_system_prompt, masked=True, eot=True
+                )
+            )
+
+        is_multimodal = "image" in sample or (
+            "image" in self._column_map and self._column_map["image"] in sample
+        )
+
+        # Gate variable to ensure that we only prepend image tokens to the first user message
+        image_loaded = False
+        for message in sample[self._column_map["conversations"]]:
+            role = role_map[message["from"]]
+            content = message["value"]
+            if role == "system" and self.new_system_prompt is not None:
+                continue
+            if role == "user":
+                if is_multimodal and not image_loaded:
+                    image_path = sample[self._column_map["image"]]
+                    if self.image_dir is not None:
+                        image_path = self.image_dir / image_path
+                    pil_image = load_image(image_path)
+                    # If image tag is not specified, prepend by default
+                    if self.image_tag is None:
+                        content = [
+                            {"type": "image", "content": pil_image},
+                            {"type": "text", "content": content},
+                        ]
+                    else:
+                        content = format_content_with_images(
+                            content,
+                            image_tag=self.image_tag,
+                            images=[pil_image],
+                        )
+                    image_loaded = True
+
+            # If multimodal and user message, always mask
+            # Otherwise, if user message, mask if train_on_input is False
+            masked = (role != "assistant") and (
+                not self.train_on_input or is_multimodal
+            )
+            messages.append(Message(role=role, content=content, masked=masked))
+
+        return {"messages": messages}
+
+
+class OpenAIToMessages(Transform):
+    """
+    Convert a single chat sample adhering to the `OpenAI chat completion <https://platform.openai.com/docs/api-reference/chat>`_
+    JSON structure to torchtune's :class:`~torchtune.data.Message` structure. This supports both
+    text and image messages.
+
+    A single sample typically consists of a single optional system prompt and one or multiple
+    turns of user and assistant messages.
+
+    For example::
+
+        {
+            "messages": [
+                {
+                    "role": <system|user|assistant>,
+                    "content": [
+                        {
+                            "type": "text",
+                            "text": "What'\''s in this image?",
+                        },
+                        {
+                            "type": "image_url",
+                            "image_url": {
+                                "url": <url>,
+                            },
+                        },
+                },
+                ...
+            ]
+        }
+
+    :class:`~torchtune.data.Message` follows::
+
+        [
+            {
+                "role": <system|user|assistant>,
+                "content": [
+                    {
+                        "type": "text",
+                        "content": "What'\''s in this image?",
+                    },
+                    {
+                        "type": "image",
+                        "content": <PIL.Image.Image>,
+                    },
+                ],
+            },
+            ...
+        ]
+
+    Args:
+        train_on_input (bool): whether the prompt should remain unmasked. Default: False
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns ("messages")
+            to the new column names in the dataset. Key should be "messages" and value should be
+            the new column name. If None, keep the default "messages".
+            Default is None.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+
+    Raises:
+        ValueError: If ``column_map`` is provided and ``messages`` not in ``column_map``.
+    """
+
+    def __init__(
+        self,
+        train_on_input: bool = False,
+        column_map: Optional[Dict[str, str]] = None,
+        new_system_prompt: Optional[str] = None,
+    ):
+        self.train_on_input = train_on_input
+        self.new_system_prompt = new_system_prompt
+        if column_map:
+            if "messages" not in column_map:
+                raise ValueError(
+                    f"Expected a key of 'messages' in column_map but found {column_map.keys()}."
+                )
+            self._column_map = column_map
+        else:
+            self._column_map = {"messages": "messages"}
+
+    def _convert_from_openai_content(
+        self, content: List[Dict[str, Any]]
+    ) -> List[Dict[str, Any]]:
+        """Converts a list of content dicts from the OpenAI format to the torchtune format."""
+        converted_content = []
+        for content_dict in content:
+            if content_dict["type"] == "text":
+                converted_content.append(
+                    {"type": "text", "content": content_dict["text"]}
+                )
+            elif content_dict["type"] == "image_url":
+                converted_content.append(
+                    {
+                        "type": "image",
+                        "content": load_image(content_dict["image_url"]["url"]),
+                    }
+                )
+        return converted_content
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        """
+        Return a list of Message objects from the provided sample dict.
+
+        Args:
+            sample (Mapping[str, Any]): a single data sample with "messages" field pointing
+                to a list of dict messages.
+
+        Returns:
+            List[Message]: A list of messages with "role" and "content" fields.
+        """
+        updated_messages = []
+        if self.new_system_prompt is not None:
+            updated_messages.append(
+                Message(
+                    role="system", content=self.new_system_prompt, masked=True, eot=True
+                )
+            )
+        for message in sample[self._column_map["messages"]]:
+            if message["role"] == "system" and self.new_system_prompt is not None:
+                continue
+            masked = (message["role"] != "assistant") and (not self.train_on_input)
+            if isinstance(message["content"], list):
+                content = self._convert_from_openai_content(message["content"])
+            elif isinstance(message["content"], str):
+                content = message["content"]
+            updated_messages.append(
+                Message(
+                    role=message["role"],
+                    content=content,
+                    masked=masked,
+                ),
+            )
+
+        return {"messages": updated_messages}
+
+
+def validate_messages(
+    messages: List[Message],
+) -> None:
+    """
+    Given a list of messages, ensure that messages form a valid
+    back-and-forth conversation. An error will be raised if:
+
+    - There is a system message that's not the first message
+    - There are two consecutive user messages
+    - An assistant message comes before the first user message
+    - The message is empty
+    - Messages are shorter than length of 2 (min. one user-assistant turn)
+
+
+    Args:
+        messages (List[Message]): the messages to validate.
+
+    Raises:
+        ValueError: If the messages are invalid.
+    """
+    if len(messages) < 2:
+        raise ValueError(
+            f"Messages must be at least length 2, but got {len(messages)} messages"
+        )
+
+    last_turn = "assistant"
+    for i, message in enumerate(messages):
+        if message.role == "assistant" and last_turn != "user":
+            raise ValueError(
+                f"Assistant message before expected user message at index {i} in messages"
+            )
+        if message.role == "user" and last_turn == "user":
+            raise ValueError(
+                f"Two consecutive user messages at index {i} and {i - 1} in messages"
+            )
+        if message.role == "system" and i > 0:
+            raise ValueError(
+                f"System message at index {i} in messages, but system messages must come first"
+            )
+        last_turn = message.role
+
+
+class AlpacaToMessages(Transform):
+    """
+    Message transform class for Alpaca-style datasets with "instruction", "input", and "output"
+    (or equivalent fields specified in column_map) columns. User messages are formed from the
+    instruction + input columns and assistant messages are formed from the output column. Prompt
+    templating is conditional on the presence of the "input" column, and thus is handled directly
+    in this transform class instead of a dedicated :class:`~torchtune.data.PromptTemplate` class
+    due to this custom logic.
+
+    Args:
+        train_on_input (bool): Whether the model is trained on the user prompt or not.
+            Default is True.
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected "instruction", "input",
+            and "output" column names to the actual column names in the dataset. Default is None,
+            keeping the default column names.
+    """
+
+    def __init__(
+        self, train_on_input: bool = True, column_map: Optional[Dict[str, str]] = None
+    ):
+        self.train_on_input = train_on_input
+        self.column_map = column_map
+        self.template = {
+            "prompt_input": (
+                "Below is an instruction that describes a task, paired with an input that provides further context. "
+                "Write a response that appropriately completes the request.\n\n"
+                "### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n"
+            ),
+            "prompt_no_input": (
+                "Below is an instruction that describes a task. "
+                "Write a response that appropriately completes the request.\n\n"
+                "### Instruction:\n{instruction}\n\n### Response:\n"
+            ),
+        }
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        column_map = self.column_map or {}
+        key_input = column_map.get("input", "input")
+        key_instruction = column_map.get("instruction", "instruction")
+        key_output = column_map.get("output", "output")
+
+        if key_input in sample and sample[key_input]:
+            prompt = self.template["prompt_input"].format(
+                instruction=sample[key_instruction], input=sample[key_input]
+            )
+        else:
+            prompt = self.template["prompt_no_input"].format(
+                instruction=sample[key_instruction]
+            )
+
+        messages = [
+            Message(
+                role="user",
+                content=prompt,
+                masked=not self.train_on_input,
+                eot=True,
+            ),
+            Message(
+                role="assistant",
+                content=sample[key_output],
+                masked=False,
+                eot=True,
+            ),
+        ]
+        return {"messages": messages}
+
+
+
+
+class ARCMultiModalToMessages(Transform):
+    """
+    Args:
+        train_on_input (bool): Whether the model is trained on the user prompt or not.
+            Default is False.
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected "instruction", "input",
+            and "output" column names to the actual column names in the dataset. Default is None,
+            keeping the default column names.
+    """
+
+    def __init__(
+        self, train_on_input: bool = False, column_map: Optional[Dict[str, str]] = None, image_factor: int = 4,
+    ):
+        self.train_on_input = train_on_input
+        self.column_map = column_map
+        self.template = None
+        self.image_factor = 4
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        column_map = self.column_map or {}
+        key_input = column_map.get("input", "input")
+        key_output = column_map.get("output", "output")
+
+        input = sample["input"]
+        output = sample["output"]
+
+        messages = []
+
+        for message in input + [output]:
+            new_content = []
+            for content in message["content"]:
+                if content["type"] == "text":
+                    new_content.append({"type": "text", "content": content["text"]})
+                elif content["type"] == "image_url":
+                    image_numpy = content["image_url"]["url"]
+                    # increase resolution 4x4 times
+                    image_numpy = np.repeat(image_numpy, self.image_factor, axis=0)
+                    image_numpy = np.repeat(image_numpy, self.image_factor, axis=1)
+                    image = Image.fromarray(image_numpy)
+                    new_content.append({"type": "image", "content": image})
+            new_message = Message(role=message["role"], content=new_content, masked=not (message["role"] == "assistant" or self.train_on_input))
+            messages.append(new_message)
+
+
+        # messages.append(Message(role="assistant", content=output["content"], masked=False))
+        # breakpoint()
+        return {"messages": messages}
+
+
+class ARCToMessages(Transform):
+    """
+    Args:
+        train_on_input (bool): Whether the model is trained on the user prompt or not.
+            Default is False.
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected "instruction", "input",
+            and "output" column names to the actual column names in the dataset. Default is None,
+            keeping the default column names.
+    """
+
+    def __init__(
+        self, train_on_input: bool = False, column_map: Optional[Dict[str, str]] = None
+    ):
+        self.train_on_input = train_on_input
+        self.column_map = column_map
+        self.template = None
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        column_map = self.column_map or {}
+        key_input = column_map.get("input", "input")
+        key_output = column_map.get("output", "output")
+
+        input = sample["input"]
+        output = sample["output"]
+
+        messages = []
+
+        for message in input:
+            messages.append(Message(role=message["role"], content=message["content"], masked=not self.train_on_input))
+
+        messages.append(Message(role="assistant", content=output["content"], masked=False))
+
+        return {"messages": messages}
diff -ruN marc_original/third_party/torchtune/torchtune/data/_prompt_templates.py marc/third_party/torchtune/torchtune/data/_prompt_templates.py
--- marc_original/third_party/torchtune/torchtune/data/_prompt_templates.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/data/_prompt_templates.py	2025-02-20 17:49:30.258025410 -0500
@@ -0,0 +1,288 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from functools import partial
+from typing import Dict, List, Protocol, Tuple, Union
+
+from torchtune.config._utils import _get_component_from_path
+
+from torchtune.data._messages import Message, Role
+
+_TemplateType = Union[str, Dict[Role, Tuple[str, str]]]
+
+
+class PromptTemplateInterface(Protocol):
+    """
+    Interface for prompt templates. Each prompt template can include structured
+    text for system, user, and assistant roles that are prepended or appended to
+    the message content.
+    """
+
+    # Template should map role to a tuple containing the tag to prepend to the text
+    # and tag to append to the text. Leave as empty strings to not prepend or append
+    template: Dict[Role, Tuple[str, str]]
+
+    def __call__(
+        self,
+        messages: List[Message],
+        inference: bool = False,
+    ) -> List[Message]:
+        """
+        Format each role's message(s) according to the prompt template
+
+        Args:
+            messages (List[Message]): a single conversation, structured as a list
+                of :class:`~torchtune.data.Message` objects
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            The formatted list of messages
+        """
+        pass
+
+
+class PromptTemplate(PromptTemplateInterface):
+    """
+    Quickly define a custom prompt template by passing in a dictionary mapping role to
+    the prepend and append tags. For example, to achieve the following prompt
+    template::
+
+        System: {content}\\n
+        User: {content}\\n
+        Assistant: {content}\\n
+        Tool: {content}\\n
+
+    You need to pass in a tuple for each role, where ``PREPEND_TAG`` is the string
+    added before the text content and ``APPEND_TAG`` is the string added after::
+
+        template = {role: (PREPEND_TAG, APPEND_TAG)}
+
+    Thus, the template would be defined as follows::
+
+        template = {
+            "system": ("System: ", "\\n"),
+            "user": ("User: ", "\\n"),
+            "assistant": ("Assistant: ", "\\n"),
+            "ipython": ("Tool: ", "\\n"),
+        }
+
+    Once instantiated, you must call the prompt template on a list of messages. It
+    will return the same list of messages updated with the template.
+
+    Note:
+        Any tags prepended/appended to the assistant message will be included
+        in the loss calculation. All other prepend/append tags for other roles
+        (system, user, ipython) are, in most cases, not included in loss. Consider using
+        the append tags for user messages for tags that need to come before the
+        assistant message but should not be included in loss. For more custom masking
+        and prompt templating, you can create your own class based off the
+        :class:`~torchtune.data.PromptTemplate` interface.
+
+    Args:
+        template (Dict[Role, Tuple[str, str]]): a dictionary mapping role to the
+            prepend and append tags
+    """
+
+    def __init__(
+        self,
+        template: Dict[Role, Tuple[str, str]],
+    ):
+        self.template = template
+
+    def __call__(
+        self, messages: List[Message], inference: bool = False
+    ) -> List[Message]:
+        """
+        Format each role's message(s) according to the prompt template by prepending
+        and appending the defined tags.
+
+        Args:
+            messages (List[Message]): list of messages to apply the template to
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            List[Message]: The formatted list of messages
+        """
+        formatted_dialogue = []
+        for message in messages:
+            content = message.content
+            if message.role in self.template:
+                prepend_tag = self.template[message.role][0]
+                append_tag = self.template[message.role][1]
+                content = message.content
+
+                if isinstance(prepend_tag, str) and len(prepend_tag) > 0:
+                    content = [{"type": "text", "content": prepend_tag}] + content
+
+                if isinstance(append_tag, str) and len(append_tag) > 0:
+                    content = content + [{"type": "text", "content": append_tag}]
+            formatted_dialogue.append(
+                Message(
+                    role=message.role,
+                    content=content,
+                    masked=message.masked,
+                    ipython=message.ipython,
+                    eot=message.eot,
+                ),
+            )
+        return formatted_dialogue
+
+
+class ChatMLTemplate(PromptTemplateInterface):
+    """
+    OpenAI's `Chat Markup Language
+    <https://github.com/MicrosoftDocs/azure-docs/blob/772c14eeabfa0c0c561d5c2d34ef19341f528b7b/articles/ai-services/openai/how-to/chat-markup-language.md>`_
+    used by their chat models.
+
+    It is the default chat template used by Hugging Face models.
+
+    .. code-block:: text
+
+        <|im_start|>system
+        Provide some context and/or instructions to the model.<|im_end|>
+        <|im_start|>user
+        The users message goes here<|im_end|>
+        <|im_start|>assistant
+        The assistants response goes here<|im_end|>
+
+    """
+
+    template = {
+        "system": ("<|im_start|>system\n", "<|im_end|>\n"),
+        "user": ("<|im_start|>user\n", "<|im_end|>\n"),
+        "assistant": ("<|im_start|>assistant\n", "<|im_end|>\n"),
+        "ipython": ("", ""),
+    }
+
+    def __call__(
+        self,
+        messages: List[Message],
+        inference: bool = False,
+    ) -> List[Message]:
+        """
+        Format user, assistant, and system messages with appropriate tags.
+
+        Args:
+            messages (List[Message]): a single conversation, structured as a list
+                of `Message` objects
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            The formatted list of messages
+        """
+        formatted_dialogue = []
+        for index, message in enumerate(messages):
+            prepend_tag = self.template[message.role][0]
+            append_tag = self.template[message.role][1]
+            # If empty assistant message at the end, we are expecting the model
+            # to generate the response continuing from the assistant prepend tag,
+            # so do not add the append tag.
+            if (
+                message.role == "assistant"
+                and index == len(messages) - 1
+                and len(message.text_content) == 0
+            ):
+                content = message.content
+                if isinstance(prepend_tag, str) and len(prepend_tag) > 0:
+                    content = [
+                        {"type": "text", "content": prepend_tag}
+                    ] + message.content
+            else:
+                content = message.content
+
+                if isinstance(prepend_tag, str) and len(prepend_tag) > 0:
+                    content = [{"type": "text", "content": prepend_tag}] + content
+
+                if isinstance(append_tag, str) and len(append_tag) > 0:
+                    content = content + [{"type": "text", "content": append_tag}]
+
+            formatted_dialogue.append(
+                Message(
+                    role=message.role,
+                    content=content,
+                    masked=message.masked,
+                    ipython=message.ipython,
+                    eot=message.eot,
+                ),
+            )
+        return formatted_dialogue
+
+
+GrammarErrorCorrectionTemplate = partial(
+    PromptTemplate,
+    template={
+        "user": ("Correct this to standard English: ", "\n---\nCorrected: "),
+    },
+)
+GrammarErrorCorrectionTemplate.__doc__ = """
+A prompt template for grammar error correction tasks::
+
+    Correct this to standard English: {user_message}
+    ---
+    Corrected: {assistant_message}
+
+Please see :class:`~torchtune.data.PromptTemplate` for full API arguments.
+"""
+SummarizeTemplate = partial(
+    PromptTemplate,
+    template={
+        "user": ("Summarize this dialogue:\n", "\n---\nSummary:\n"),
+    },
+)
+SummarizeTemplate.__doc__ = """
+A prompt template for summarization tasks::
+
+    Summarize this dialogue:
+    {user_message}
+    ---
+    Summary:
+    {assistant_message}
+
+Please see :class:`~torchtune.data.PromptTemplate` for full API arguments.
+"""
+QuestionAnswerTemplate = partial(
+    PromptTemplate,
+    template={
+        "user": ("Question: ", "\n\nAnswer: "),
+    },
+)
+QuestionAnswerTemplate.__doc__ = """
+A prompt template for question answering tasks::
+
+    Question: {user_message}
+
+    Answer: {assistant_message}
+
+Please see :class:`~torchtune.data.PromptTemplate` for full API arguments.
+"""
+
+
+def _get_prompt_template(
+    prompt_template: _TemplateType,
+) -> PromptTemplateInterface:
+    """
+    Retrieve prompt template from import dotpath or create a custom one with provided
+    template dictionary.
+
+    Args:
+        prompt_template (_TemplateType): optional specified prompt template.
+            If a string, it is assumed to be the dotpath of a :class:`~torchtune.data.PromptTemplateInterface`
+            class. If a dictionary, it is assumed to be a custom prompt template mapping role to the
+            prepend/append tags.
+
+    Returns:
+        PromptTemplateInterface: the specified prompt template
+
+    Raises:
+        ValueError: If a string or dictionary is not passed in
+    """
+    if isinstance(prompt_template, str):
+        return _get_component_from_path(prompt_template)()
+    elif isinstance(prompt_template, dict):
+        return PromptTemplate(prompt_template)
+    else:
+        raise ValueError(
+            f"Prompt template must be a dotpath string or dictionary with custom template, got {type(prompt_template)}"
+        )
Binary files marc_original/third_party/torchtune/torchtune/data/__pycache__/_chat_formats.cpython-312.pyc and marc/third_party/torchtune/torchtune/data/__pycache__/_chat_formats.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/data/__pycache__/_collate.cpython-312.pyc and marc/third_party/torchtune/torchtune/data/__pycache__/_collate.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/data/__pycache__/_common.cpython-312.pyc and marc/third_party/torchtune/torchtune/data/__pycache__/_common.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/data/__pycache__/_converters.cpython-312.pyc and marc/third_party/torchtune/torchtune/data/__pycache__/_converters.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/data/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/data/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/data/__pycache__/_instruct_templates.cpython-312.pyc and marc/third_party/torchtune/torchtune/data/__pycache__/_instruct_templates.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/data/__pycache__/_messages.cpython-312.pyc and marc/third_party/torchtune/torchtune/data/__pycache__/_messages.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/data/__pycache__/_prompt_templates.cpython-312.pyc and marc/third_party/torchtune/torchtune/data/__pycache__/_prompt_templates.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/data/__pycache__/_utils.cpython-312.pyc and marc/third_party/torchtune/torchtune/data/__pycache__/_utils.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/data/_utils.py marc/third_party/torchtune/torchtune/data/_utils.py
--- marc_original/third_party/torchtune/torchtune/data/_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/data/_utils.py	2025-02-20 17:49:30.262025416 -0500
@@ -0,0 +1,144 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from pathlib import Path
+from typing import Any, Dict, List, Optional, TypeVar, Union
+from urllib import request
+
+T = TypeVar("T", bound=type)
+
+
+def truncate(
+    tokens: List[Any],
+    max_seq_len: int,
+    eos_id: Optional[Any] = None,
+) -> List[Any]:
+    """
+    Truncate a list of tokens to a maximum length. If eos_id is provided, the last
+    token will be replaced with eos_id.
+
+    Args:
+        tokens (List[Any]): list of tokens to truncate
+        max_seq_len (int): maximum length of the list
+        eos_id (Optional[Any]): token to replace the last token with. If None, the
+            last token will not be replaced. Default is None.
+
+    Returns:
+        List[Any]: truncated list of tokens
+    """
+    tokens_truncated = tokens[:max_seq_len]
+    if eos_id is not None and tokens_truncated[-1] != eos_id:
+        tokens_truncated[-1] = eos_id
+    return tokens_truncated
+
+
+def load_image(image_loc: Union[Path, str]) -> "PIL.Image.Image":
+    """
+    Convenience method to load an image in PIL format from a local file path or remote source.
+
+    Args:
+        image_loc (Union[Path, str]): Local file path or remote source pointing to the image
+            which will be loaded in PIL format.
+
+    Note:
+        If loading an image from a remote source, the function expects the URL provided in ``image_loc``
+        to start with "http" or "https" e.g. "https://www.wikipedia.org/en/bird.jpg".
+
+    Raises:
+        ValueError: If the image cannot be loaded from remote source.
+        ValueError: If the image cannot be opened as a :class:`~PIL.Image.Image`.
+
+    Examples:
+        >>> # Load from remote source
+        >>> image = load_image("https://www.wikipedia.org/en/bird.jpg")
+
+        >>> # Load from local file path
+        >>> image = load_image(Path("/home/user/bird.jpg"))
+
+    Returns:
+        PIL.Image.Image: The loaded image.
+    """
+    # Hackily import PIL to avoid burdensome import in the main module
+    # TODO: Fix this
+    from PIL import Image
+
+    # If pointing to remote source, try to load to local
+    if isinstance(image_loc, str) and image_loc.startswith("http"):
+        try:
+            image_loc = request.urlopen(image_loc)
+        except Exception as e:
+            raise ValueError(f"Failed to load image from {image_loc}") from e
+
+    # Open the local image as a PIL image
+    try:
+        image = Image.open(image_loc)
+    except Exception as e:
+        raise ValueError(f"Failed to open image as PIL Image from {image_loc}") from e
+
+    return image
+
+
+def format_content_with_images(
+    content: str, *, image_tag: str, images: List["PIL.Image.Image"]
+) -> List[Dict[str, Any]]:
+    """
+    Given a raw text string, split by the specified ``image_tag``
+    and form into list of dictionaries to be used in the :class:`~torchtune.data.Message` content
+    field::
+
+        [
+            {
+                "role": "system" | "user" | "assistant",
+                "content":
+                    [
+                        {"type": "image", "content": <PIL.Image.Image>},
+                        {"type": "text", "content": "This is a sample image."},
+                    ],
+            },
+            ...
+        ]
+
+    Args:
+        content (str): raw message text
+        image_tag (str): string to split the text by
+        images (List["PIL.Image.Image"]): list of images to be used in the content
+
+    Raises:
+        ValueError: If the number of images does not match the number of image tags in the content
+
+    Examples:
+        >>> content = format_content_with_images(
+        ...     "<|image|>hello <|image|>world",
+        ...     image_tag="<|image|>",
+        ...     images=[<PIL.Image.Image>, <PIL.Image.Image>]
+        ... )
+        >>> print(content)
+        [
+            {"type": "image", "content": <PIL.Image.Image>},
+            {"type": "text", "content": "hello "},
+            {"type": "image", "content": <PIL.Image.Image>},
+            {"type": "text", "content": "world"}
+        ]
+
+    Returns:
+        List[Dict[str, Any]]: list of dictionaries to be used in the :class:`~torchtune.data.Message` content field
+    """
+    num_image_tags_in_content = content.count(image_tag)
+    if len(images) != num_image_tags_in_content:
+        raise ValueError(
+            f"Number of images ({len(images)}) does not match number of image tags "
+            f"({num_image_tags_in_content}) in content: {content}"
+        )
+
+    split_content = content.split(image_tag)
+    final_content_list = []
+    for i, substr in enumerate(split_content):
+        if len(substr) > 0:
+            final_content_list.append({"type": "text", "content": substr})
+        if i < len(split_content) - 1:
+            final_content_list.append({"type": "image", "content": images.pop(0)})
+
+    return final_content_list
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_alpaca.py marc/third_party/torchtune/torchtune/datasets/_alpaca.py
--- marc_original/third_party/torchtune/torchtune/datasets/_alpaca.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_alpaca.py	2025-02-20 17:49:30.270025430 -0500
@@ -0,0 +1,103 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+
+from typing import Any, Callable, Dict, Optional, Union
+
+from torchtune.data._messages import AlpacaToMessages
+
+from torchtune.datasets._packed import PackedDataset
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def alpaca_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str = "tatsu-lab/alpaca",
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = True,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> Union[SFTDataset, PackedDataset]:
+    """
+    Support for family of Alpaca-style datasets from Hugging Face Datasets using
+    the `data input format <https://huggingface.co/datasets/tatsu-lab/alpaca#data-instances>`_
+    and `prompt template <https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py#L31>`_
+    from the original alpaca codebase, where ``instruction``, ``input``, and ``output``
+    are fields from the dataset. This template is automatically applied independent
+    of any prompt template configured in the tokenizer.
+
+    Masking of the prompt during training is controlled by the ``train_on_input`` flag, which is
+    set to ``True`` by `default <https://github.com/tloen/alpaca-lora/blob/main/finetune.py#L49>`_
+    - If ``train_on_input`` is True, the prompt is used during training and
+    contributes to the loss.
+    - If ``train_on_input`` is False, the prompt is masked out (tokens replaced with -100)
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details. Default is ``tatsu-lab/alpaca``.
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns in the message transform
+            :class:`~torchtune.data.AlpacaToMessages` to the new column names in the dataset. Keys should be
+            "instruction", "input", and "output" and values should be the actual column names. If None, uses
+            the default column names ``"instruction``, ``"input"``, and ``"output"`` in ``tatsu-lab/alpaca``.
+        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
+        packed (bool): Whether or not to pack the dataset to ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``. See Hugging
+            Face's `API ref <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset>`_
+            for more details.
+
+    Returns:
+        Union[SFTDataset, PackedDataset]: dataset configured with source data and transform
+
+    Raises:
+        ValueError: If ``packed`` is True and ``max_seq_len`` is not set on the tokenizer.
+
+    Example:
+        >>> alpaca_ds = alpaca_dataset(tokenizer=tokenizer)
+        >>> for batch in Dataloader(alpaca_ds, batch_size=8):
+        >>>     print(f"Batch size: {len(batch)}")
+        >>> Batch size: 8
+    """
+
+    message_transform = AlpacaToMessages(
+        train_on_input=train_on_input, column_map=column_map
+    )
+    ds = SFTDataset(
+        source=source,
+        message_transform=message_transform,
+        model_transform=tokenizer,
+        filter_fn=filter_fn,
+        split=split,
+        **load_dataset_kwargs,
+    )
+    if packed:
+        if tokenizer.max_seq_len is None:
+            raise ValueError(
+                "PackedDataset requires a max_seq_len to be set on the tokenizer."
+            )
+        return PackedDataset(ds, max_seq_len=tokenizer.max_seq_len)
+    return ds
+
+
+alpaca_cleaned_dataset = partial(alpaca_dataset, source="yahma/alpaca-cleaned")
+alpaca_cleaned_dataset.__doc__ = """
+Builder for a variant of Alpaca-style datasets with the cleaned version of the
+original Alpaca dataset, `yahma/alpaca-cleaned <https://huggingface.co/datasets/yahma/alpaca-cleaned>`_.
+See the dataset page and :func:`~torchtune.datasets.alpaca_dataset` for more details.
+"""
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_arc.py marc/third_party/torchtune/torchtune/datasets/_arc.py
--- marc_original/third_party/torchtune/torchtune/datasets/_arc.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_arc.py	2025-02-20 17:49:30.274025436 -0500
@@ -0,0 +1,111 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+
+from typing import Any, Callable, Dict, Optional, Union
+
+from torchtune.data._messages import ARCToMessages
+
+from torchtune.datasets._packed import PackedDataset
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def arc_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str = "tatsu-lab/arc",
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = False,
+    unmask_outputs: bool = True,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> Union[SFTDataset, PackedDataset]:
+    """
+    Masking of the prompt during training is controlled by the ``train_on_input`` flag, which is
+    set to ``True`` by `default <https://github.com/tloen/arc-lora/blob/main/finetune.py#L49>`_
+    - If ``train_on_input`` is True, the prompt is used during training and
+    contributes to the loss.
+    - If ``train_on_input`` is False, the prompt is masked out (tokens replaced with -100)
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details. Default is ``tatsu-lab/arc``.
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns in the message transform
+            :class:`~torchtune.data.ARCToMessages` to the new column names in the dataset. Keys should be
+            "instruction", "input", and "output" and values should be the actual column names. If None, uses
+            the default column names ``"instruction``, ``"input"``, and ``"output"`` in ``tatsu-lab/arc``.
+        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
+        packed (bool): Whether or not to pack the dataset to ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``. See Hugging
+            Face's `API ref <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset>`_
+            for more details.
+
+    Returns:
+        Union[SFTDataset, PackedDataset]: dataset configured with source data and transform
+
+    Raises:
+        ValueError: If ``packed`` is True and ``max_seq_len`` is not set on the tokenizer.
+
+    Example:
+        >>> arc_ds = arc_dataset(tokenizer=tokenizer)
+        >>> for batch in Dataloader(arc_ds, batch_size=8):
+        >>>     print(f"Batch size: {len(batch)}")
+        >>> Batch size: 8
+    """
+
+    message_transform = ARCToMessages(
+        train_on_input=train_on_input, column_map=column_map
+    )
+    ds = SFTDataset(
+        source=source,
+        message_transform=message_transform,
+        model_transform=tokenizer,
+        filter_fn=filter_fn,
+        unmask_outputs=unmask_outputs,
+        split=split,
+        data_files = {"train": "td_False_ttd_False_ttdwa_False_ad_True_trd_False.jsonl",
+                      "test": "td_True_ttd_False_ttdwa_False_ad_True_trd_False.jsonl",},
+        **load_dataset_kwargs,
+    )
+    if packed:
+        if tokenizer.max_seq_len is None:
+            raise ValueError(
+                "PackedDataset requires a max_seq_len to be set on the tokenizer."
+            )
+        return PackedDataset(ds, max_seq_len=tokenizer.max_seq_len)
+    return ds
+
+
+arc_cleaned_dataset = partial(arc_dataset, source="yahma/arc-cleaned")
+arc_cleaned_dataset.__doc__ = """
+Builder for a variant of ARC-style datasets with the cleaned version of the
+original ARC dataset, `yahma/arc-cleaned <https://huggingface.co/datasets/yahma/arc-cleaned>`_.
+See the dataset page and :func:`~torchtune.datasets.arc_dataset` for more details.
+"""
+
+if __name__ == "__main__":
+    from torchtune.models.llama3 import llama3_tokenizer
+    from torch.utils.data import DataLoader
+
+    tokenizer = llama3_tokenizer("/raid/lingo//models/Meta-Llama-3-8B-Instruct/original/tokenizer.model")
+    arc_ds = arc_dataset(source="/raid/lingo/akyurek/git/arc/data/tasks/all_in_pm_fix_30/", tokenizer=tokenizer)
+    print(len(arc_ds))
+    for batch in DataLoader(arc_ds, batch_size=8):
+        print(f"Batch size: {len(batch)}")
+
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_chat.py marc/third_party/torchtune/torchtune/datasets/_chat.py
--- marc_original/third_party/torchtune/torchtune/datasets/_chat.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_chat.py	2025-02-20 17:49:30.274025436 -0500
@@ -0,0 +1,187 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, Optional, Union
+
+from torchtune.data._messages import OpenAIToMessages, ShareGPTToMessages
+from torchtune.datasets._packed import PackedDataset
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def chat_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str,
+    conversation_column: str,
+    conversation_style: str,
+    train_on_input: bool = False,
+    new_system_prompt: Optional[str] = None,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> Union[SFTDataset, PackedDataset]:
+    """
+    Configure a custom dataset with conversations between user and model assistant.
+
+    This builder function can be used to configure a custom chat dataset directly from the yaml config
+    as an alternative to :class:`~torchtune.datasets.SFTDataset`, as it is made to be config friendly.
+
+    The dataset is expected to contain a single column with the conversations:
+
+    .. code-block:: text
+
+        |  conversations                         |
+        |----------------------------------------|
+        | [{"role": "user", "content": Q1},      |
+        |  {"role": "assistant", "content": A1}] |
+
+    This will be converted to:
+
+    .. code-block:: python
+
+        messages = [
+            Message(role="user", content="Q1"),
+            Message(role="assistant", content="A1"),
+        ]
+
+    This list of messages is then tokenized for model training.
+
+    You may have a different structure for your conversations, such as different role names or
+    different keys in the json structure. You can use the ``conversation_style`` parameter
+    to choose from standard formats such as "sharegpt" (see :class:`~torchtune.data.ShareGPTToMessages`)
+    or "openai" (see :class:`~torchtune.data.OpenAIToMessages`). If your dataset is not in one of these
+    formats, we recommend creating a custom message transform and using it in a custom dataset
+    builder function similar to :class:`~torchtune.datasets.chat_dataset`.
+
+    If your column names are different, use the ``conversation_column`` parameter to point
+    towards the column with the conversations.
+
+    Masking of the prompt during training is controlled by the ``train_on_input`` flag, which is
+    set to ``False`` by default.
+
+    - If ``train_on_input`` is True, the prompt is used during training and
+      contributes to the loss.
+    - If ``train_on_input`` is False, the prompt is masked out (tokens replaced with -100).
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text"), pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details.
+        conversation_column (str): name of column containing the conversations.
+        conversation_style (str): string specifying expected style of conversations in the dataset
+            for automatic conversion to the :class:`~torchtune.data.Message` structure.
+            Supported styles are: "sharegpt", "openai"
+        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Default is None.
+        packed (bool): Whether or not to pack the dataset to ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``,
+            such as ``data_files`` or ``split``.
+
+    Examples:
+
+    ::
+
+        my_dataset.json
+        [
+            {
+                "conversations": [
+                    {
+                        "from": "human",
+                        "value": "What time is it in London?",
+                    },
+                    {
+                        "from": "gpt",
+                        "value": "It is 10:00 AM in London.",
+                    },
+                ],
+            },
+            {
+                "conversations": [
+                    ...
+                ],
+            },
+            ...,
+        ]
+
+    ::
+
+        >>> from torchtune.datasets import chat_dataset
+        >>> dataset = chat_dataset(
+        ...     tokenizer=tokenizer,
+        ...     source="json",
+        ...     data_files="my_dataset.json",
+        ...     conversation_column="conversations",
+        ...     conversation_style="sharegpt",
+        ...     train_on_input=False,
+        ...     packed=False,
+        ...     split="train",
+        ... )
+        >>> tokens = dataset[0]["tokens"]
+        >>> tokenizer.decode(tokens)
+        "What time is it in London?It is 10:00 AM in London."
+
+    This can also be accomplished via the yaml config:
+
+    .. code-block:: yaml
+
+        dataset:
+          _component_: torchtune.datasets.chat_dataset
+          source: json
+          data_files: my_dataset.json
+          conversation_column: conversations
+          conversation_style: sharegpt
+          train_on_input: False
+          packed: False
+          split: train
+
+    Returns:
+        Union[SFTDataset, PackedDataset]: the configured :class:`~torchtune.datasets.SFTDataset`
+            or :class:`~torchtune.datasets.PackedDataset` if ``packed=True``
+
+    Raises:
+        ValueError: if the conversation format is not supported
+    """
+    if conversation_style == "sharegpt":
+        message_transform = ShareGPTToMessages(
+            train_on_input=train_on_input,
+            column_map={"conversations": conversation_column},
+            new_system_prompt=new_system_prompt,
+        )
+    elif conversation_style == "openai":
+        message_transform = OpenAIToMessages(
+            train_on_input=train_on_input,
+            column_map={"messages": conversation_column},
+            new_system_prompt=new_system_prompt,
+        )
+    else:
+        raise ValueError(f"Unsupported conversation style: {conversation_style}")
+
+    ds = SFTDataset(
+        source=source,
+        message_transform=message_transform,
+        model_transform=tokenizer,
+        split=split,
+        filter_fn=filter_fn,
+        **load_dataset_kwargs,
+    )
+    if packed:
+        if tokenizer.max_seq_len is None:
+            raise ValueError(
+                "PackedDataset requires a max_seq_len to be set on the tokenizer."
+            )
+        return PackedDataset(ds, max_seq_len=tokenizer.max_seq_len)
+    return ds
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_cnn_dailymail.py marc/third_party/torchtune/torchtune/datasets/_cnn_dailymail.py
--- marc_original/third_party/torchtune/torchtune/datasets/_cnn_dailymail.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_cnn_dailymail.py	2025-02-20 17:49:30.278025442 -0500
@@ -0,0 +1,57 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, Optional
+
+from torchtune.datasets._text_completion import TextCompletionDataset
+
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def cnn_dailymail_articles_dataset(
+    tokenizer: ModelTokenizer,
+    source: str = "ccdv/cnn_dailymail",
+    max_seq_len: Optional[int] = None,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> TextCompletionDataset:
+    """
+    Support for family of datasets similar to `CNN / DailyMail <https://huggingface.co/datasets/ccdv/cnn_dailymail>`_,
+    a corpus of news articles. This builder only extracts the articles and not the highlights for
+    general text completion tasks.
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path string of dataset, anything supported by Hugging Face's ``load_dataset``
+            (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)
+        max_seq_len (Optional[int]): Maximum number of tokens in the returned input and label token id lists.
+            Default is None, disabling truncation. We recommend setting this to the highest you can fit in memory
+            and is supported by the model. For example, llama2-7B supports up to 4096 for sequence length.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
+
+    Returns:
+        TextCompletionDataset: the configured TextCompletionDataset
+    """
+
+    return TextCompletionDataset(
+        tokenizer=tokenizer,
+        source=source,
+        column="article",
+        max_seq_len=max_seq_len,
+        filter_fn=filter_fn,
+        split=split,
+        # This is used to specify the version of the dataset, a required argument
+        # by the cnn_dailymail dataset builder:
+        # https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_dailymail.py#L80
+        name="3.0.0",
+        **load_dataset_kwargs,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_concat.py marc/third_party/torchtune/torchtune/datasets/_concat.py
--- marc_original/third_party/torchtune/torchtune/datasets/_concat.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_concat.py	2025-02-20 17:49:30.282025449 -0500
@@ -0,0 +1,96 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Tuple
+
+from torch.utils.data import Dataset
+
+from torchtune import utils
+
+from torchtune.datasets._packed import PackedDataset
+
+log = utils.get_logger("DEBUG")
+
+
+class ConcatDataset(Dataset):
+    """
+    A dataset class for concatenating multiple sub-datasets into a single dataset. This class enables the
+    unified handling of different datasets as if they were a single dataset, simplifying tasks such as
+    training models on multiple sources of data simultaneously.
+
+    The class internally manages the aggregation of different datasets and allows transparent indexing across them.
+    However, it requires all constituent datasets to be fully loaded into memory, which might not be optimal for
+    very large datasets.
+
+    Upon initialization, this class computes the cumulative length of all datasets and maintains an internal mapping
+    of indices to the respective datasets. This approach allows the :class:`~torchtune.datasets.ConcatDataset`
+    to delegate data retrieval to the appropriate sub-dataset transparently when a particular index is accessed.
+
+    Note:
+        Using this class with very large datasets can lead to high memory consumption, as it requires all datasets to
+        be loaded into memory. For large-scale scenarios, consider other strategies that might stream data on demand.
+
+    Args:
+        datasets (List[Dataset]): A list of datasets to concatenate. Each dataset must be an instance of a class
+            derived from :class:`~torch.utils.data.Dataset`.
+
+    Raises:
+        ValueError: if instanse of `PackedDataset` is in `datasets`
+
+    Examples:
+        >>> dataset1 = MyCustomDataset(params1)
+        >>> dataset2 = MyCustomDataset(params2)
+        >>> concat_dataset = ConcatDataset([dataset1, dataset2])
+        >>> print(len(concat_dataset))  # Total length of both datasets
+        >>> data_point = concat_dataset[1500]  # Accesses an element from the appropriate dataset
+
+    This can also be accomplished by passing in a list of datasets to the YAML config::
+
+        dataset:
+          - _component_: torchtune.datasets.instruct_dataset
+            source: vicgalle/alpaca-gpt4
+            template: torchtune.data.AlpacaInstructTemplate
+            split: train
+            train_on_input: True
+          - _component_: torchtune.datasets.instruct_dataset
+            source: samsum
+            template: torchtune.data.SummarizeTemplate
+            column_map: {"output": "summary"}
+            output: summary
+            split: train
+            train_on_input: False
+
+    This class primarily focuses on providing a unified interface to access elements from multiple datasets,
+    enhancing the flexibility in handling diverse data sources for training machine learning models.
+    """
+
+    def __init__(self, datasets: List[Dataset]):
+        self._datasets: List[Dataset] = datasets
+
+        for dataset in self._datasets:
+            if isinstance(dataset, PackedDataset):
+                raise ValueError(
+                    "ConcatDataset can't process instances of PackedDataset."
+                )
+
+        self._len: int = sum(len(dataset) for dataset in datasets)
+        self._indexes: List[Tuple[int, int, int]] = []
+
+        # Calculate distribution of indexes in all datasets
+        cumulative_index = 0
+        for idx, dataset in enumerate(datasets):
+            next_cumulative_index = cumulative_index + len(dataset)
+            self._indexes.append((cumulative_index, next_cumulative_index, idx))
+            cumulative_index = next_cumulative_index
+
+    def __getitem__(self, index: int) -> Tuple[List[int], List[int]]:
+        for start, stop, dataset_index in self._indexes:
+            if start <= index < stop:
+                dataset = self._datasets[dataset_index]
+                return dataset[index - start]
+
+    def __len__(self) -> int:
+        return self._len
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_grammar.py marc/third_party/torchtune/torchtune/datasets/_grammar.py
--- marc_original/third_party/torchtune/torchtune/datasets/_grammar.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_grammar.py	2025-02-20 17:49:30.286025456 -0500
@@ -0,0 +1,96 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+from typing import Any, Callable, Dict, Optional, Union
+
+from torchtune.data import InputOutputToMessages
+from torchtune.datasets._packed import PackedDataset
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def grammar_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str = "liweili/c4_200m",
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = False,
+    new_system_prompt: Optional[str] = None,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> Union[SFTDataset, PackedDataset]:
+    """
+    Support for grammar correction datasets and their variants from Hugging Face Datasets.
+    Here is an `example <https://huggingface.co/datasets/liweili/c4_200m>`_ of a grammar correction dataset.
+
+    It is recommended to configure the tokenizer with the :class:`~torchtune.data.GrammarErrorCorrectionTemplate`
+    in conjunction with this dataset.
+
+    Masking of the prompt during training is controlled by the ``train_on_input`` flag, which is
+    set to ``False`` by default
+    - If ``train_on_input`` is True, the prompt is used during training and
+    contributes to the loss.
+    - If ``train_on_input`` is False, the prompt is masked out (tokens replaced with -100)
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text"), pass
+            in the filepath in ``data_files``, and set ``split="train"``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details. Default is ``liweili/c4_200m``.
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns in the message transform
+            :class:`~torchtune.data.InputOutputToMessages` to the new column names in the dataset. Keys should be
+            "input" and "output" and values should be the actual column names. If None, use
+            the default column names ``"input"`` and ``"output"``in ``liweili/c4_200m``.
+        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
+        new_system_prompt (Optional[str]): if specified, prepend a system message to every sample. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+        packed (bool): Whether or not to pack the dataset to tokenizer's ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
+
+    Returns:
+        Union[SFTDataset, PackedDataset]: dataset configured with source data and template
+
+    Raises:
+        ValueError: If ``packed=True`` and ``tokenizer.max_seq_len`` is not set.
+
+    Example:
+        >>> grammar_ds = grammar_dataset(model_transform=tokenizer)
+        >>> for batch in Dataloader(grammar_ds, batch_size=8):
+        >>>     print(f"Batch size: {len(batch)}")
+        >>> Batch size: 8
+    """
+
+    message_transform = InputOutputToMessages(
+        train_on_input=train_on_input,
+        column_map=column_map,
+        new_system_prompt=new_system_prompt,
+    )
+    ds = SFTDataset(
+        source=source,
+        message_transform=message_transform,
+        model_transform=tokenizer,
+        filter_fn=filter_fn,
+        split=split,
+        **load_dataset_kwargs,
+    )
+    if packed:
+        if tokenizer.max_seq_len is None:
+            raise ValueError(
+                "PackedDataset requires a max_seq_len to be set on the tokenizer."
+            )
+        return PackedDataset(ds, max_seq_len=tokenizer.max_seq_len)
+    return ds
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_hh_rlhf_helpful.py marc/third_party/torchtune/torchtune/datasets/_hh_rlhf_helpful.py
--- marc_original/third_party/torchtune/torchtune/datasets/_hh_rlhf_helpful.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_hh_rlhf_helpful.py	2025-02-20 17:49:30.290025462 -0500
@@ -0,0 +1,70 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, Optional
+
+from torchtune.data import ChosenRejectedToMessages
+from torchtune.datasets._preference import PreferenceDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def hh_rlhf_helpful_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str = "RLHFlow/HH-RLHF-Helpful-standard",
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = False,
+    new_system_prompt: Optional[str] = None,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> PreferenceDataset:
+    """
+    Constructs preference datasets similar to `Anthropic's helpful/harmless RLHF
+    data
+    <https://huggingface.co/datasets/RLHFlow/HH-RLHF-Helpful-standard>`_. This is
+    the processed helpful subset of the original dataset in a standardized format.
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See Hugging Face's ``load_dataset``
+            (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)
+            for more details. Default is ``RLHFlow/HH-RLHF-Helpful-standard``.
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns "chosen" and "rejected"
+            in the message transform :class:`~torchtune.data.ChosenRejectedToMessages` to the new column names in
+            the dataset. Keys should be "chosen" and "rejected" and values should be the actual column names.
+            If None, keep the default columns "chosen" and "rejected".
+        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
+        new_system_prompt (Optional[str]): if specified, prepend a system message to every sample for both chosen
+            and rejected. This can serve as instructions to guide the model response. Setting this will OVERRIDE
+            any system messages already present in the dataset. Default is None.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
+
+    Returns:
+        PreferenceDataset: The preference dataset built from source paired data.
+    """
+
+    message_transform = ChosenRejectedToMessages(
+        train_on_input=train_on_input,
+        column_map=column_map,
+        new_system_prompt=new_system_prompt,
+    )
+
+    return PreferenceDataset(
+        source=source,
+        message_transform=message_transform,
+        tokenizer=tokenizer,
+        filter_fn=filter_fn,
+        split=split,
+        **load_dataset_kwargs,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/__init__.py marc/third_party/torchtune/torchtune/datasets/__init__.py
--- marc_original/third_party/torchtune/torchtune/datasets/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/__init__.py	2025-02-20 17:49:30.266025423 -0500
@@ -0,0 +1,49 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from torchtune.datasets import multimodal
+from torchtune.datasets._arc import arc_dataset
+from torchtune.datasets._alpaca import alpaca_cleaned_dataset, alpaca_dataset
+from torchtune.datasets._chat import chat_dataset
+from torchtune.datasets._cnn_dailymail import cnn_dailymail_articles_dataset
+from torchtune.datasets._concat import ConcatDataset
+from torchtune.datasets._grammar import grammar_dataset
+from torchtune.datasets._hh_rlhf_helpful import hh_rlhf_helpful_dataset
+from torchtune.datasets._instruct import instruct_dataset
+from torchtune.datasets._packed import PackedDataset
+from torchtune.datasets._preference import preference_dataset, PreferenceDataset
+from torchtune.datasets._samsum import samsum_dataset
+from torchtune.datasets._sft import SFTDataset
+from torchtune.datasets._slimorca import slimorca_dataset
+from torchtune.datasets._stack_exchange_paired import stack_exchange_paired_dataset
+from torchtune.datasets._text_completion import (
+    text_completion_dataset,
+    TextCompletionDataset,
+)
+from torchtune.datasets._wikitext import wikitext_dataset
+
+__all__ = [
+    "arc_dataset",
+    "alpaca_dataset",
+    "alpaca_cleaned_dataset",
+    "grammar_dataset",
+    "samsum_dataset",
+    "stack_exchange_paired_dataset",
+    "slimorca_dataset",
+    "instruct_dataset",
+    "preference_dataset",
+    "chat_dataset",
+    "text_completion_dataset",
+    "TextCompletionDataset",
+    "cnn_dailymail_articles_dataset",
+    "PackedDataset",
+    "ConcatDataset",
+    "wikitext_dataset",
+    "PreferenceDataset",
+    "SFTDataset",
+    "hh_rlhf_helpful_dataset",
+    "multimodal",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_instruct.py marc/third_party/torchtune/torchtune/datasets/_instruct.py
--- marc_original/third_party/torchtune/torchtune/datasets/_instruct.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_instruct.py	2025-02-20 17:49:30.294025469 -0500
@@ -0,0 +1,155 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, Optional, Union
+
+from torchtune.data import InputOutputToMessages
+from torchtune.datasets._packed import PackedDataset
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def instruct_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str,
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = False,
+    new_system_prompt: Optional[str] = None,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> Union[SFTDataset, PackedDataset]:
+    """
+    Configure a custom dataset with user instruction prompts and model responses.
+
+    This builder function can be used to configure a custom instruct dataset directly from the yaml config
+    as an alternative to :class:`~torchtune.datasets.SFTDataset`, as it is made to be config friendly.
+
+    The dataset should follow this format:
+
+    .. code-block:: text
+
+        |  input          |  output          |
+        |-----------------|------------------|
+        | "user prompt"   | "model response" |
+
+    If your column names are different, you can use the ``column_map`` parameter to change
+    the expected column names. For example, if your dataset has columns ``"question"`` and
+    ``"answer"`` you can use::
+
+        column_map = {"input": "question", "output": "answer"}
+
+    Masking of the prompt during training is controlled by the ``train_on_input`` flag, which is
+    set to ``False`` by default
+    - If ``train_on_input`` is True, the prompt is used during training and
+    contributes to the loss.
+    - If ``train_on_input`` is False, the prompt is masked out (tokens replaced with -100)
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text"), pass
+            in the filepath in ``data_files``, and set ``split="train"``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details.
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected "input"
+            and "output" column names to the actual column names in the dataset. Keys should be "input" and
+            "output" and values should be the actual column names. Default is None, keeping the default "input"
+            and "output" column names.
+        train_on_input (bool): Whether the model is trained on the user prompt or not.
+            Default is False.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Default is None.
+        packed (bool): Whether or not to pack the dataset to tokenizer's ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``,
+            such as ``data_files`` or ``split``.
+
+    Examples:
+
+    ::
+
+        my_dataset.json
+        [
+            {
+                "question": "What time is it in London?",
+                "answer": "It is 10:00 AM in London.",
+            },
+            {
+                ...
+            },
+            ...,
+        ]
+
+    ::
+
+        >>> from torchtune.datasets import instruct_dataset
+        >>> dataset = instruct_dataset(
+        ...     tokenizer=tokenizer,
+        ...     source="json",
+        ...     data_files="my_dataset.json",
+        ...     column_map={
+        ...         "input": "question",
+        ...         "output": "answer",
+        ...     },
+        ...     train_on_input=False,
+        ...     packed=False,
+        ...     split="train",
+        ... )
+        >>> tokens = dataset[0]["tokens"]
+        >>> tokenizer.decode(tokens)
+        "What time is it in London?It is 10:00 AM in London."
+
+    This can also be accomplished via the yaml config:
+
+    .. code-block:: yaml
+
+        dataset:
+          _component_: torchtune.datasets.instruct_dataset
+          source: json
+          data_files: my_dataset.json
+          column_map:
+            input: question
+            output: answer
+          train_on_input: False
+          packed: False
+          split: train
+
+    Returns:
+        Union[SFTDataset, PackedDataset]: the configured :class:`~torchtune.datasets.SFTDataset`
+            or :class:`~torchtune.datasets.PackedDataset` if ``packed=True``
+
+    Raises:
+        ValueError: If ``packed=True`` and ``tokenizer.max_seq_len`` is not set.
+    """
+    message_transform = InputOutputToMessages(
+        train_on_input=train_on_input,
+        column_map=column_map,
+        new_system_prompt=new_system_prompt,
+    )
+
+    ds = SFTDataset(
+        source=source,
+        message_transform=message_transform,
+        model_transform=tokenizer,
+        filter_fn=filter_fn,
+        split=split,
+        **load_dataset_kwargs,
+    )
+
+    if packed:
+        if tokenizer.max_seq_len is None:
+            raise ValueError(
+                "PackedDataset requires a max_seq_len to be set on the tokenizer."
+            )
+        return PackedDataset(ds, max_seq_len=tokenizer.max_seq_len)
+    return ds
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/multimodal/_arc_multimodal.py marc/third_party/torchtune/torchtune/datasets/multimodal/_arc_multimodal.py
--- marc_original/third_party/torchtune/torchtune/datasets/multimodal/_arc_multimodal.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/multimodal/_arc_multimodal.py	2025-02-20 17:49:30.334025534 -0500
@@ -0,0 +1,119 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from pathlib import Path
+from typing import Any, Callable, Dict, Optional
+
+from torchtune.data._messages import ShareGPTToMessages, ARCMultiModalToMessages
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.transforms import Transform
+
+
+# TODO: point to Flamingo model transform as an example
+def arc_multimodal_dataset(
+    model_transform: Transform,
+    *,
+    source: str = "liuhaotian/LLaVA-Instruct-150K",
+    image_dir: str = "coco/train2017/",
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = False,
+    unmask_outputs: bool = False,
+    new_system_prompt: Optional[str] = None,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> SFTDataset:
+    """
+    .. code-block:: python
+
+        from torchtune.models.llama3 import llama3_tokenizer
+        from torchtune.models.clip import CLIPImageTransform
+        from torchtune.modules.transforms import Transform
+
+        class MyModelTransform(Transform):
+            def __init__(
+                self,
+                tokenizer_path: str,
+                max_seq_len: Optional[int] = None,
+            ):
+                self.tokenizer = llama3_tokenizer(tokenizer_path)
+                self.image_transform = CLIPImageTransform()
+
+            def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+                tokens, mask = self.tokenizer.tokenize_messages(sample["messages"])
+                images = self.image_transform(sample["images"])
+                return {
+                    "tokens": tokens,
+                    "mask": mask,
+                    "images": images,
+                }
+
+    See :class:`~torchtune.datasets.SFTDataset` for more details about model transforms and
+    message transforms.
+
+    Args:
+        model_transform (Transform): model-specific transform class that takes in a sample dict and applies custom
+            transforms on the keys. It should consist of at minimum two components: text tokenization (called
+            on the "messages" field) and image transform (called on the "images" field). The keys returned by
+            the model transform should be aligned with the expected inputs into the model.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+        image_dir (str): path to the directory containing the images as you are expected to download the COCO dataset
+            before using. Default is "coco/".
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns ("conversations")
+            to the new column names in the dataset. If None, assume these are identical.
+            Default is None.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+        packed (bool): Whether or not to pack the dataset to ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        data_files (str): path to the json file to load as dataset. See the `dataset repo
+            <https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/tree/main>`_ for options.
+            Default is "llava_instruct_150k.json".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``. See Hugging
+            Face's `API ref <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset>`_
+            for more details.
+
+    Returns:
+        SFTDataset: dataset configured with source data and transform
+
+    Raises:
+        ValueError: If ``packed`` is True, they are not supported for multimodal datasets yet.
+
+    Example:
+        >>> llava_instruct_ds = llava_instruct_dataset(model_transform=model_transform)
+        >>> for batch in Dataloader(llava_instruct_ds, batch_size=8):
+        >>>     print(f"Batch size: {len(batch)}")
+        >>> Batch size: 8
+    """
+
+    message_transform = ARCMultiModalToMessages(
+        train_on_input=False,
+        column_map=column_map,
+    )
+
+
+    ds = SFTDataset(
+        model_transform=model_transform,
+        source=source,
+        message_transform=message_transform,
+        filter_fn=filter_fn,
+        split=split,
+        data_files = {"train": "td_False_ttd_False_ttdwa_False_ad_True_trd_False.jsonl",
+                      "test": "td_True_ttd_False_ttdwa_False_ad_True_trd_False.jsonl",},
+        **load_dataset_kwargs,
+    )
+    if packed:
+        raise ValueError("Multimodal datasets don't support packing yet.")
+    return ds
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/multimodal/__init__.py marc/third_party/torchtune/torchtune/datasets/multimodal/__init__.py
--- marc_original/third_party/torchtune/torchtune/datasets/multimodal/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/multimodal/__init__.py	2025-02-20 17:49:30.330025528 -0500
@@ -0,0 +1,17 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._llava_instruct import llava_instruct_dataset
+from ._multimodal import multimodal_chat_dataset
+from ._the_cauldron import the_cauldron_dataset
+from ._arc_multimodal import arc_multimodal_dataset
+
+__all__ = [
+    "the_cauldron_dataset",
+    "llava_instruct_dataset",
+    "multimodal_chat_dataset",
+    "arc_multimodal_dataset",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/multimodal/_llava_instruct.py marc/third_party/torchtune/torchtune/datasets/multimodal/_llava_instruct.py
--- marc_original/third_party/torchtune/torchtune/datasets/multimodal/_llava_instruct.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/multimodal/_llava_instruct.py	2025-02-20 17:49:30.338025541 -0500
@@ -0,0 +1,142 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from pathlib import Path
+from typing import Any, Callable, Dict, Optional
+
+from torchtune.data._messages import ShareGPTToMessages
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.transforms import Transform
+
+
+# TODO: point to Flamingo model transform as an example
+def llava_instruct_dataset(
+    model_transform: Transform,
+    *,
+    source: str = "liuhaotian/LLaVA-Instruct-150K",
+    image_dir: str = "coco/train2017/",
+    column_map: Optional[Dict[str, str]] = None,
+    new_system_prompt: Optional[str] = None,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    data_files: str = "llava_instruct_150k.json",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> SFTDataset:
+    """
+    Support for family of image + text datasets similar to
+    `LLaVA-Instruct-150K <https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K>`_
+    from Hugging Face Datasets.
+
+    To use this dataset, you must first download the COCO Train 2017 image dataset. You can do so
+    by visiting https://cocodataset.org/#download or downloading it directly:
+
+    .. code-block:: bash
+
+        wget -c http://images.cocodataset.org/zips/train2017.zip
+        unzip train2017.zip -d coco/
+
+    The resulting directory should be passed into the model transform for loading
+    and processing of the images.
+
+    The model transform is expected to be a callable that applies pre-processing steps specific
+    to a model. For multimodal datasets, this is expected to be at minimum a tokenizer and
+    an image transform. The tokenizer will convert text sequences into token IDs after the dataset
+    is converted to a list of :class:`~torchtune.data.Message`. The image transform will load the
+    image and process it in accordance to the model's requirements.
+
+    Here is a minimal example for illustrative purposes:
+
+    .. code-block:: python
+
+        from torchtune.models.llama3 import llama3_tokenizer
+        from torchtune.models.clip import CLIPImageTransform
+        from torchtune.modules.transforms import Transform
+
+        class MyModelTransform(Transform):
+            def __init__(
+                self,
+                tokenizer_path: str,
+                max_seq_len: Optional[int] = None,
+            ):
+                self.tokenizer = llama3_tokenizer(tokenizer_path)
+                self.image_transform = CLIPImageTransform()
+
+            def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+                tokens, mask = self.tokenizer.tokenize_messages(sample["messages"])
+                images = self.image_transform(sample["images"])
+                return {
+                    "tokens": tokens,
+                    "mask": mask,
+                    "images": images,
+                }
+
+    See :class:`~torchtune.datasets.SFTDataset` for more details about model transforms and
+    message transforms.
+
+    Args:
+        model_transform (Transform): model-specific transform class that takes in a sample dict and applies custom
+            transforms on the keys. It should consist of at minimum two components: text tokenization (called
+            on the "messages" field) and image transform (called on the "images" field). The keys returned by
+            the model transform should be aligned with the expected inputs into the model.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+        image_dir (str): path to the directory containing the images as you are expected to download the COCO dataset
+            before using. Default is "coco/".
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns ("conversations")
+            to the new column names in the dataset. If None, assume these are identical.
+            Default is None.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+        packed (bool): Whether or not to pack the dataset to ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        data_files (str): path to the json file to load as dataset. See the `dataset repo
+            <https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/tree/main>`_ for options.
+            Default is "llava_instruct_150k.json".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``. See Hugging
+            Face's `API ref <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset>`_
+            for more details.
+
+    Returns:
+        SFTDataset: dataset configured with source data and transform
+
+    Raises:
+        ValueError: If ``packed`` is True, they are not supported for multimodal datasets yet.
+
+    Example:
+        >>> llava_instruct_ds = llava_instruct_dataset(model_transform=model_transform)
+        >>> for batch in Dataloader(llava_instruct_ds, batch_size=8):
+        >>>     print(f"Batch size: {len(batch)}")
+        >>> Batch size: 8
+    """
+
+    message_transform = ShareGPTToMessages(
+        train_on_input=False,
+        column_map=column_map,
+        new_system_prompt=new_system_prompt,
+        image_dir=Path(image_dir),
+        image_tag="<image>",
+    )
+
+    ds = SFTDataset(
+        model_transform=model_transform,
+        source=source,
+        message_transform=message_transform,
+        filter_fn=filter_fn,
+        split=split,
+        data_files=data_files,
+        **load_dataset_kwargs,
+    )
+    if packed:
+        raise ValueError("Multimodal datasets don't support packing yet.")
+    return ds
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/multimodal/_multimodal.py marc/third_party/torchtune/torchtune/datasets/multimodal/_multimodal.py
--- marc_original/third_party/torchtune/torchtune/datasets/multimodal/_multimodal.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/multimodal/_multimodal.py	2025-02-20 17:49:30.342025548 -0500
@@ -0,0 +1,182 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from pathlib import Path
+from typing import Any, Callable, Dict, Optional
+
+from torchtune.data._messages import ShareGPTToMessages
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.transforms import Transform
+
+
+def multimodal_chat_dataset(
+    model_transform: Transform,
+    *,
+    source: str,
+    column_map: Optional[Dict[str, str]] = None,
+    new_system_prompt: Optional[str] = None,
+    image_tag: Optional[str] = None,
+    image_dir: Optional[str] = None,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> SFTDataset:
+    """
+    Configure a text+image dataset with conversations between user and model assistant.
+
+    This builder function can be used to configure a custom multimodal dataset directly from the yaml config
+    as an alternative to :class:`~torchtune.datasets.SFTDataset`, as it is made to be config friendly.
+
+    The dataset is expected to follow the ShareGPT format:
+
+    .. code-block:: text
+
+        |  conversations                     | image        |
+        |------------------------------------|--------------|
+        | [{"from": "human", "value": "Q1"}, | images/1.jpg |
+        |  {"from": "gpt", "value": "A1"}]   |              |
+
+    This will be converted to:
+
+    .. code-block:: python
+
+        messages = [
+            Message(
+                role="user",
+                content=[
+                    {"type": "image", "content": [<PIL.Image.Image>]},
+                    {"type": "text", "content": "Q1"},
+                ],
+            ),
+            Message(role="assistant", content="A1"),
+        ]
+
+    This list of messages is then tokenized for model training. Currently, only a single image per conversation sample
+    is supported, and it is always added to the first user message.
+
+    If your dataset is not in the ShareGPT format, we recommend creating a custom message transform and
+    using it in a custom dataset builder function similar to :class:`~torchtune.datasets.multimodal_chat_dataset`.
+
+    If your column names are different, use the ``column_map`` parameter to point
+    towards the columns with the conversations and images.
+
+    Args:
+        model_transform (Transform): callable that applies model-specific pre-processing to the sample.
+            This includes tokenization and any modality-specific transforms. It is expected to return at
+            minimum ``"tokens"`` and ``"mask"`` keys.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text"), pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details.
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns ("conversations", "image")
+            to the new column names in the dataset. Keys should be "conversations", "image" and values should
+            be the new column names. If None, keep the default "conversations", "image".
+            Default is None.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+        image_tag (Optional[str]): placeholder tags in the text content of each message to be replaced by dictionaries
+            indicating to the tokenizer where to place image tokens. If images are present and this is None,
+            then will prepend image tokens to the first user message in the sample by default. If text-only, leave
+            this as None. Default is None.
+        image_dir (Optional[str]): path to the directory containing the images that is prepended to all image
+            paths in the dataset. If None, assume images are available in current working directory or are located
+            on a remote url. For text-only,leave as None. Default is None.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``,
+            such as ``data_files`` or ``split``.
+
+    Examples:
+
+    ::
+
+        my_dataset.json
+        [
+            {
+                "dialogue": [
+                    {
+                        "from": "human",
+                        "value": "<image>What time is it on the clock?",
+                    },
+                    {
+                        "from": "gpt",
+                        "value": "It is 10:00 AM.",
+                    },
+                ],
+                "image_path": "images/clock.jpg",
+            },
+            ...,
+        ]
+
+    ::
+
+        >>> from torchtune.datasets.multimodal import multimodal_chat_dataset
+        >>> from torchtune.models.flamingo import FlamingoTransform
+        >>> model_transform = FlamingoTransform(
+        ...     path="/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model",
+        ...     tile_size=224,
+        ...     patch_size=14,
+        ... )
+        >>> dataset = multimodal_chat_dataset(
+        ...     model_transform=model_transform,
+        ...     source="json",
+        ...     data_files="my_dataset.json",
+        ...     column_map={
+        ...         "dialogue": "conversations",
+        ...         "image_path": "image",
+        ...     },
+        ...     image_dir="/home/user/dataset/",  # /home/user/dataset/images/clock.jpg
+        ...     image_tag="<image>",
+        ...     split="train",
+        ... )
+        >>> tokens = dataset[0]["tokens"]
+        >>> model_transform.decode(tokens, skip_special_tokens=True)
+        "What time is it on the clock?It is 10:00 AM."
+        >>> print(dataset[0]["encoder_input"]["images"][0].shape)  # (num_tiles, num_channels, tile_height, tile_width)
+        torch.Size([4, 3, 224, 224])
+
+
+    This can also be accomplished via the yaml config:
+
+    .. code-block:: yaml
+
+        dataset:
+          _component_: torchtune.datasets.multimodal.multimodal_chat_dataset
+          source: json
+          data_files: my_dataset.json
+          column_map:
+            dialogue: conversations
+            image_path: image
+          image_dir: /home/user/dataset/
+          image_tag: "<image>"
+          split: train
+
+    Returns:
+        SFTDataset: the configured :class:`~torchtune.datasets.SFTDataset`
+    """
+    message_transform = ShareGPTToMessages(
+        train_on_input=False,
+        column_map=column_map,
+        new_system_prompt=new_system_prompt,
+        image_dir=Path(image_dir),
+        image_tag=image_tag,
+    )
+
+    ds = SFTDataset(
+        source=source,
+        message_transform=message_transform,
+        model_transform=model_transform,
+        filter_fn=filter_fn,
+        split=split,
+        **load_dataset_kwargs,
+    )
+
+    return ds
Binary files marc_original/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/_arc_multimodal.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/_arc_multimodal.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/_llava_instruct.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/_llava_instruct.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/_multimodal.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/_multimodal.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/_the_cauldron.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/multimodal/__pycache__/_the_cauldron.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/multimodal/_the_cauldron.py marc/third_party/torchtune/torchtune/datasets/multimodal/_the_cauldron.py
--- marc_original/third_party/torchtune/torchtune/datasets/multimodal/_the_cauldron.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/multimodal/_the_cauldron.py	2025-02-20 17:49:30.346025555 -0500
@@ -0,0 +1,237 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, Mapping, Optional
+
+from torchtune.data._messages import Message
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.transforms import Transform
+
+
+class TheCauldronToMessages(Transform):
+    """
+    Construct messages from a sample formatted similarly to
+    `The Cauldron dataset <https://huggingface.co/datasets/HuggingFaceM4/the_cauldron>`_.
+
+    Image placeholders are prepended to the text in the ``Message`` content. Images in the
+    dataset are expected to be a list of a single PIL image, so they are simply passed through
+    to the model transform with an optional column remapping if ``column_map`` is specified.
+
+    For example, a dataset row::
+
+        {
+            "texts": [
+                {
+                    "user": "What are in these images.",
+                    "assistant": "They are images of dogs.",
+                },
+                ...
+            ],
+            "images": [
+                [PIL.Image.Image, PIL.Image.Image],
+            ],
+        }
+
+    will be converted to::
+
+        [
+            Message(
+                role = "user",
+                content = [
+                    {"type": "image", "content": <PIL.Image.Image>},
+                    {"type": "image", "content": <PIL.Image.Image>},
+                    {"type": "text", "content": "What are in these images."},
+                ],
+            ),
+            Message(
+                role = "assistant",
+                content = [
+                    {"type": "text", "content": "They are images of dogs."},
+                ],
+            ),
+            ...
+        ]
+
+    Args:
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected "texts"
+            column names to the actual column names in the dataset. Default is None,
+            keeping the default column names.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Default is None.
+
+    Raises:
+        ValueError: If ``column_map`` is provided and ``texts`` not in ``column_map``.
+    """
+
+    def __init__(
+        self,
+        column_map: Optional[Dict[str, str]] = None,
+        new_system_prompt: Optional[str] = None,
+    ):
+        self.new_system_prompt = new_system_prompt
+        if column_map is not None:
+            if "images" not in column_map:
+                raise ValueError(
+                    "column_map must map 'images' to your expected column name if specified"
+                )
+            if "texts" not in column_map:
+                raise ValueError(
+                    "column_map must map 'texts' to your expected column name if specified"
+                )
+            self._column_map = column_map
+        else:
+            self._column_map = {"texts": "texts", "images": "images"}
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        # Dataset images to be prepended to the first user message
+        img_content = []
+        for img in sample[self._column_map["images"]]:
+            img_content.append({"type": "image", "content": img})
+
+        # Convert to messages
+        messages = []
+        for i, message in enumerate(sample[self._column_map["texts"]]):
+            user_content = [{"type": "text", "content": message["user"]}]
+            if i == 0:
+                user_content = img_content + user_content
+            messages.append(
+                Message(
+                    role="user",
+                    content=user_content,
+                    masked=True,
+                )
+            )
+            messages.append(
+                Message(
+                    role="assistant",
+                    content=[{"type": "text", "content": message["assistant"]}],
+                )
+            )
+
+        if self.new_system_prompt is not None:
+            messages = [
+                Message(
+                    role="system", content=self.new_system_prompt, masked=True, eot=True
+                )
+            ] + messages
+
+        return {"messages": messages}
+
+
+# TODO: point to Flamingo model transform as an example
+def the_cauldron_dataset(
+    model_transform: Transform,
+    *,
+    subset: str,
+    source: str = "HuggingFaceM4/the_cauldron",
+    column_map: Optional[Dict[str, str]] = None,
+    new_system_prompt: Optional[str] = None,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> SFTDataset:
+    """
+    Support for family of image + text datasets similar to
+    `The Cauldron <https://huggingface.co/datasets/HuggingFaceM4/the_cauldron>`_
+    from Hugging Face Datasets.
+
+    The Cauldron consists of numerous datasets. You must specify one of the datasets
+    using the ``subset`` argument.
+
+    The model transform is expected to be a callable that applies pre-processing steps specific
+    to a model. For multimodal datasets, this is expected to be at minimum a tokenizer and
+    an image transform. The tokenizer will convert text sequences into token IDs after the dataset
+    is converted to a list of :class:`~torchtune.data.Message`. The image transform will load the
+    image and process it in accordance to the model's requirements.
+
+    Here is a minimal example for illustrative purposes:
+
+    .. code-block:: python
+
+        from torchtune.models.llama3 import llama3_tokenizer
+        from torchtune.models.clip import CLIPImageTransform
+        from torchtune.modules.transforms import Transform
+
+        class MyModelTransform(Transform):
+            def __init__(
+                self,
+                tokenizer_path: str,
+                max_seq_len: Optional[int] = None,
+            ):
+                self.tokenizer = llama3_tokenizer(tokenizer_path)
+                self.image_transform = CLIPImageTransform()
+
+            def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+                tokens, mask = self.tokenizer.tokenize_messages(sample["messages"])
+                images = self.image_transform(sample["images"])
+                return {
+                    "tokens": tokens,
+                    "mask": mask,
+                    "images": images,
+                }
+
+    See :class:`~torchtune.datasets.SFTDataset` for more details about model transforms and
+    message transforms.
+
+    Args:
+        model_transform (Transform): model-specific transform class that takes in a sample dict and applies custom
+            transforms on the keys. It should consist of at minimum two components: text tokenization (called
+            on the "messages" field) and image transform (called on the "images" field). The keys returned by
+            the model transform should be aligned with the expected inputs into the model.
+        subset (str): name of the subset of the dataset to load. See the `dataset card
+            <https://huggingface.co/datasets/HuggingFaceM4/the_cauldron>`_ for options.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details. Default is ``HuggingFaceM4/the_cauldron``.
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected "images"
+            and "texts" column names to the actual column names in the dataset. Default is None,
+            keeping the default column names.
+        new_system_prompt (Optional[str]): if specified, prepend a system message. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+        packed (bool): Whether or not to pack the dataset to ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``. See Hugging
+            Face's `API ref <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset>`_
+            for more details.
+
+    Returns:
+        SFTDataset: dataset configured with source data and transform
+
+    Raises:
+        ValueError: If ``packed`` is True, they are not supported for multimodal datasets yet.
+
+    Example:
+        >>> cauldron_ds = the_cauldron_dataset(model_transform=model_transform, subset="ai2d")
+        >>> for batch in Dataloader(cauldron_ds, batch_size=8):
+        >>>     print(f"Batch size: {len(batch)}")
+        >>> Batch size: 8
+    """
+
+    message_transform = TheCauldronToMessages(
+        column_map=column_map,
+        new_system_prompt=new_system_prompt,
+    )
+
+    ds = SFTDataset(
+        model_transform=model_transform,
+        source=source,
+        message_transform=message_transform,
+        name=subset,
+        filter_fn=filter_fn,
+        split=split,
+        **load_dataset_kwargs,
+    )
+    if packed:
+        raise ValueError("Multimodal datasets don't support packing yet.")
+    return ds
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_packed.py marc/third_party/torchtune/torchtune/datasets/_packed.py
--- marc_original/third_party/torchtune/torchtune/datasets/_packed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_packed.py	2025-02-20 17:49:30.298025475 -0500
@@ -0,0 +1,274 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Dict, List, Optional
+
+import torch
+from torch.nn import functional as F
+
+from torch.utils.data import Dataset
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX, PACK_TYPE
+from tqdm import tqdm
+
+
+class PackedDataset(Dataset):
+    """
+    Performs greedy sample packing on a provided dataset. This is done as a single
+    preprocessing step before training begins. Shuffling is done outside of this
+    class on packed samples with a ``Sampler`` as part of the dataloader. Currently,
+    this only supports in-memory map-style datasets.
+
+    The class loads, tokenizes, and packs examples on initialization - no tokenization is done during training.
+
+    The general flow on initialization is: load tokenized sample -> add to buffer ->
+    when buffer is long enough, add to ``self.packs``.
+
+    During training, returns self.packs[idx] as input, label, attention mask, and
+    position ids. The attention mask is a lower triangular block mask to prevent
+    samples from cross-attending within a pack. The position ids indicate the position
+    of each token relative to its sample within a pack. These are all padded to max
+    sequence length, so a batch-wise collator is not needed.
+
+    A packed sample is made up of individual smaller sequence length samples jammed together
+    within ``max_seq_len``. For example, if max_seq_len is 6 and there are varied
+    length samples::
+
+        tokens = [
+            [S1, S1, S1, S2, S2, pad],
+            [S3, S3, S4, S4, pad, pad],
+            ...,
+        ]
+
+    To prevent cross-contamination, the following mask would be returned for the
+    first pack in the example::
+
+        mask = [
+            [1, 0, 0, 0, 0, 0],
+            [1, 1, 0, 0, 0, 0],
+            [1, 1, 1, 0, 0, 0],
+            [0, 0, 0, 1, 0, 0],
+            [0, 0, 0, 1, 1, 0],
+            [0, 0, 0, 0, 0, 1],
+        ]
+
+    The position ids would be::
+
+        input_pos = [
+            [0, 1, 2, 0, 1, 2],
+            [0, 1, 0, 1, 2, 3],
+            ...,
+        ]
+
+    The identity matrix is used in the mask for pad tokens instead of a causal mask.
+    For position ids for pad tokens, we simply continue to increment from the previous
+    sample normally.
+
+    Args:
+        ds (Dataset): dataset to sample pack. This should return a dictionary with field
+            "tokens" and "labels" containing the tokenized and label samples.
+        max_seq_len (int): Maximum number of tokens to pack
+        padding_idx (int): padding index for the tokenizer. Default is 0.
+        max_packs (Optional[int]): Maximum number of packs. Default is None, which will create as many
+            packs as possible.
+        split_across_pack (bool): if the last sample in a pack does not fit in ``max_seq_len``,
+            split the sample into the next pack, or move it entirely to the beginning of the next pack.
+            For pre-training, typically this is set to True for general text completion. For
+            fine-tuning, typically this is set to False to avoid truncating sentences in instruct
+            tuning. Default is False.
+    """
+
+    def __init__(
+        self,
+        ds: Dataset,
+        *,
+        max_seq_len: int,
+        padding_idx: int = 0,
+        max_packs: Optional[int] = None,
+        split_across_pack: bool = False,
+    ) -> None:
+        self.ds = ds
+        self.max_seq_len = max_seq_len
+        self.padding_idx = padding_idx
+        self.max_packs = max_packs
+        self.split_across_pack = split_across_pack
+        # Where final samples will be held
+        self.packs: List[PACK_TYPE] = []
+        self.previous_sample_boundary: int = 0
+        self._pack()
+
+    def _pack(self) -> None:
+        """Iterate through the dataset. Use a buffer to hold samples until max_seq_len,
+        then append the buffer to self.packs as a single "packed" sample. Continue
+        until max_packs or end of dataset."""
+        # Buffer to hold samples until they are long enough to be added to self.packs
+        current_pack = {
+            "tokens": [],
+            "labels": [],
+            "input_pos": [],
+            "seq_lens": [],
+        }
+
+        # Only show progress bar on rank 0
+        rank = (
+            torch.distributed.get_rank()
+            if torch.distributed.is_available() and torch.distributed.is_initialized()
+            else 0
+        )
+        if rank == 0:
+            pbar = tqdm(total=len(self.ds), desc="Packing dataset", dynamic_ncols=True)
+
+        for sample in self.ds:
+            tokens, labels = sample["tokens"], sample["labels"]
+
+            # If the dataset outputs samples that are larger than the specified
+            # max_seq_len and we're unable to split it, user needs to modify
+            # one of the two parameters
+            seq_len = len(tokens)
+            if seq_len > self.max_seq_len and not self.split_across_pack:
+                raise ValueError(
+                    f"Dataset sample is too long ({seq_len} > {self.max_seq_len}). "
+                    "Please set `split_across_pack=True` or increase `max_seq_len`."
+                )
+
+            # Update the current pack
+            current_pack["tokens"] += tokens
+            current_pack["labels"] += labels
+            current_pack["input_pos"] += [x % self.max_seq_len for x in range(seq_len)]
+            current_pack["seq_lens"] += [seq_len]
+
+            # If the current pack is over the max_seq_len, add it to self.packs and
+            # retain any truncated or bumped samples for next pack
+            while (
+                len(current_pack["tokens"]) > self.max_seq_len
+                and not self._should_stop_packing()
+            ):
+                current_pack = self._split_and_add_pack(current_pack)
+
+            if rank == 0:
+                pbar.update()
+
+            # Keep track of previous sample boundary
+            self.previous_sample_boundary = len(current_pack["tokens"])
+
+            if self._should_stop_packing():
+                break
+
+        # Handle the last pack if there's leftover and we haven't filled up the max packs
+        if len(current_pack["tokens"]) > 0 and (
+            self.max_packs is None or len(self.packs) < self.max_packs
+        ):
+            # No need to handle splitting at this point so we can just add the current pack
+            self._add_pack(current_pack)
+
+    def _should_stop_packing(self) -> bool:
+        """If max packs is set, stop packing when we reach that number."""
+
+        if self.max_packs is not None and len(self.packs) == self.max_packs:
+            return True
+        return False
+
+    def _split_and_add_pack(self, current_pack: PACK_TYPE) -> PACK_TYPE:
+        """Splits the current pack at the boundary, processes it, adds it to ``self.packs`` and
+        returns the start of the next pack."""
+
+        if self.split_across_pack:
+            boundary = self.max_seq_len
+            # The last elem in ``seq_lens`` ensures that ``sum(seq_lens) == self.max_seq_len``
+            leftover_seq_len = self.max_seq_len - sum(current_pack["seq_lens"][:-1])
+            seq_len_padding = [leftover_seq_len] if leftover_seq_len > 0 else []
+        else:
+            boundary = self.previous_sample_boundary
+            # If we aren't splitting across packs, we leave out the last sample b/c
+            # it will go into the next pack
+            seq_len_padding = []
+
+        pack = {
+            "tokens": current_pack["tokens"][:boundary],
+            "labels": current_pack["labels"][:boundary],
+            "input_pos": current_pack["input_pos"][:boundary],
+            "seq_lens": current_pack["seq_lens"][:-1] + seq_len_padding,
+        }
+
+        # Process and add the pack
+        self._add_pack(pack)
+
+        # Return the length of the first sample in next pack if we are splitting across packs,
+        # otherwise return the length of the last sample in the current pack
+        next_seq_len = (
+            len(current_pack["tokens"][boundary:])
+            if self.split_across_pack
+            else current_pack["seq_lens"][-1]
+        )
+
+        return {
+            "tokens": current_pack["tokens"][boundary:],
+            "labels": current_pack["labels"][boundary:],
+            "input_pos": current_pack["input_pos"][boundary:],
+            "seq_lens": [next_seq_len],
+        }
+
+    def _add_pack(self, pack: PACK_TYPE) -> None:
+        """Processes, pads and adds a pack to ``self.packs``."""
+        pack = self._convert_to_tensors(pack)
+        pack = self._pad_pack(pack, padding_idx=self.padding_idx)
+        self.packs.append(pack)
+
+    def _convert_to_tensors(self, pack: PACK_TYPE) -> PACK_TYPE:
+        """Converts a pack into tensors. Pack comes in as a dict of lists and is converted to tensors."""
+        return {
+            "tokens": torch.tensor(pack["tokens"], dtype=torch.long),
+            "labels": torch.tensor(pack["labels"], dtype=torch.long),
+            "input_pos": torch.tensor(pack["input_pos"], dtype=torch.long),
+            "seq_lens": torch.tensor(pack["seq_lens"], dtype=torch.long),
+        }
+
+    def _pad_pack(self, pack: PACK_TYPE, padding_idx: int) -> PACK_TYPE:
+        """Pads a pack to ``self.max_seq_len``."""
+        # Pad tokens
+        num_padding_tokens = self.max_seq_len - len(pack["tokens"])
+        padded_tokens = F.pad(
+            pack["tokens"],
+            (0, num_padding_tokens),
+            value=padding_idx,
+        )
+
+        # Pad labels
+        padded_labels = F.pad(
+            pack["labels"],
+            (0, self.max_seq_len - len(pack["labels"])),
+            value=CROSS_ENTROPY_IGNORE_IDX,
+        )
+
+        # Add padding tokens as a last seq len to ensure sum is max_seq_len
+        padded_seq_lens = (
+            torch.cat([pack["seq_lens"], torch.tensor([num_padding_tokens])])
+            if num_padding_tokens > 0
+            else pack["seq_lens"]
+        )
+
+        # Pad input_pos continuing the sequence from last value
+        # in input_pos
+        # e.g. [0 1 2] -> [0 1 2 3 4 5] for self.max_seq_len = 6
+        num_range = torch.arange(
+            pack["input_pos"][-1] + 1,
+            pack["input_pos"][-1] + self.max_seq_len - len(pack["input_pos"]) + 1,
+        )
+        # Clamp to max_seq_len - 1 to avoid out of bounds error
+        clamped_num_range = torch.clamp(num_range, 0, self.max_seq_len - 1)
+        padded_input_pos = torch.cat([pack["input_pos"], clamped_num_range])
+
+        return {
+            "tokens": padded_tokens,
+            "labels": padded_labels,
+            "input_pos": padded_input_pos,
+            "seq_lens": padded_seq_lens,
+        }
+
+    def __len__(self) -> int:
+        return len(self.packs)
+
+    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
+        return self.packs[idx]
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_preference.py marc/third_party/torchtune/torchtune/datasets/_preference.py
--- marc_original/third_party/torchtune/torchtune/datasets/_preference.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_preference.py	2025-02-20 17:49:30.302025482 -0500
@@ -0,0 +1,308 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, List, Mapping, Optional
+
+import numpy as np
+from datasets import load_dataset
+from torch.utils.data import Dataset
+
+from torchtune.data import ChosenRejectedToMessages, CROSS_ENTROPY_IGNORE_IDX
+
+from torchtune.modules.tokenizers import ModelTokenizer
+from torchtune.modules.transforms import Transform
+
+
+class PreferenceDataset(Dataset):
+    """
+    Primary class for fine-tuning via preference modelling techniques (e.g. training
+    a preference model for RLHF, or directly optimizing a model through DPO) on a
+    preference dataset sourced from Hugging Face Hub, local files, or remote files. This
+    class requires the dataset to have "chosen" and "rejected" model responses. These are
+    typically either full conversations between user and assistant in separate columns::
+
+        |  chosen                                |  rejected                              |
+        |----------------------------------------|----------------------------------------|
+        | [{"role": "user", "content": Q1},      | [{"role": "user", "content": Q1},      |
+        |  {"role": "assistant", "content": A1}] |  {"role": "assistant", "content": A2}] |
+
+    or a user prompt column with separate chosen and rejected assistant reponses::
+
+        |  prompt  |  chosen  |  rejected  |
+        |----------|----------|------------|
+        |  Q1      |  A1      |  A2        |
+
+
+    In the above case when the format is prompt-chosen-rejected, only single-turn interactions are supported.
+
+    At a high level, this class will load the data from source and apply the following pre-processing steps when a
+    sample is retrieved:
+
+    1. Dataset-specific transform. This is typically unique to each dataset and extracts
+       the necessary prompt and chosen/rejected columns into torchtune's :class:`~torchtune.data.Message`
+       format, a standardized API for all model tokenizers.
+    2. Tokenization with optional prompt template if configured
+
+
+    All datasets are formatted into a list of :class:`~torchtune.data.Message`
+    because preference datasets can be considered as chosen and rejected "conversations"
+    with the model, or AI assistant. Thus, we can standardize all text content as messages
+    in a conversation assigned to a role:
+
+    - ``"user"`` messages contain the input prompt into the model
+    - ``"assistant"`` messages are the response of the model and what you actually want
+      to train for and compute loss directly against
+
+    The :class:`~torchtune.data.Message` forms the core data unit that all tokenizer
+    APIs expect. The key component of this class that ensures any dataset is transformed
+    into this format is the ``message_transform``. This is a callable class that takes
+    in a sample dictionary - typically a single row from the source dataset - that
+    processes the sample in any configurable way to output a list of messages::
+
+        [
+            Message(
+                role=<system|user|assistant|ipython>,
+                content=<message>,
+            ),
+            ...
+        ]
+
+    For any custom dataset, use the ``message_transform`` to contain all pre-processing to
+    return the list of messages.
+
+    Args:
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details.
+        message_transform (Transform): callable that keys into the desired fields in the sample
+            and converts text content to a list of :class:`~torchtune.data.Message`. It is expected that the final list
+            of messages are stored in the ``"chosen"`` and ``"rejected"`` keys.
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+            Since PreferenceDataset only supports text data, it requires a
+            :class:`~torchtune.modules.tokenizers.ModelTokenizer` instead of the ``model_transform`` in
+            :class:`~torchtune.datasets.SFTDataset`.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``. See Hugging
+            Face's `API ref <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset>`_
+            for more details.
+    """
+
+    def __init__(
+        self,
+        *,
+        source: str,
+        message_transform: Transform,
+        tokenizer: ModelTokenizer,
+        filter_fn: Optional[Callable] = None,
+        **load_dataset_kwargs: Dict[str, Any],
+    ) -> None:
+        self._tokenizer = tokenizer
+        self._message_transform = message_transform
+        self._data = load_dataset(source, **load_dataset_kwargs)
+
+        if filter_fn is not None:
+            self._data = self._data.filter(filter_fn)
+
+    def __len__(self):
+        return len(self._data)
+
+    def __getitem__(self, index: int) -> Dict[str, List[int]]:
+        sample = self._data[index]
+        return self._prepare_sample(sample)
+
+    def _prepare_sample(self, sample: Mapping[str, Any]) -> Dict[str, List[int]]:
+        transformed_sample = self._message_transform(sample)
+
+        # TODO: Truncation differs from original DPO repo
+        # in DPO: first truncate prompts, then responses
+        chosen_input_ids, chosen_masks = self._tokenizer.tokenize_messages(
+            transformed_sample["chosen"],
+        )
+        chosen_labels = list(
+            np.where(chosen_masks, CROSS_ENTROPY_IGNORE_IDX, chosen_input_ids)
+        )
+
+        rejected_input_ids, rejected_masks = self._tokenizer.tokenize_messages(
+            transformed_sample["rejected"],
+        )
+        rejected_labels = list(
+            np.where(rejected_masks, CROSS_ENTROPY_IGNORE_IDX, rejected_input_ids)
+        )
+
+        assert len(chosen_input_ids) == len(chosen_labels)
+        assert len(rejected_input_ids) == len(rejected_labels)
+
+        tokenized_dict = dict(
+            chosen_input_ids=chosen_input_ids,
+            chosen_labels=chosen_labels,
+            rejected_input_ids=rejected_input_ids,
+            rejected_labels=rejected_labels,
+        )
+
+        return tokenized_dict
+
+
+def preference_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str,
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = False,
+    new_system_prompt: Optional[str] = None,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> PreferenceDataset:
+    """
+    Configures a custom preference dataset comprising interactions between user and
+    model assistant.
+
+    This builder function can be used to configure a custom preference dataset directly from the yaml config
+    as an alternative to :class:`~torchtune.datasets.PreferenceDataset`, as it is made to be config friendly.
+
+    This function requires the dataset to have "chosen" and "rejected" columns. A single sample will share an
+    identical system +/ user prompt between both "chosen" and "rejected" columns, followed by one or multiple
+    turns of user and assistant messages::
+
+        |  chosen                                |  rejected                              |
+        |----------------------------------------|----------------------------------------|
+        | [{"role": "user", "content": Q1},      | [{"role": "user", "content": Q1},      |
+        |  {"role": "assistant", "content": C1}] |  {"role": "assistant", "content": R1}] |
+
+
+    This example will be converted to:
+
+    .. code-block:: python
+
+        chosen_messages = [
+            Message(role="user", content="Q1"),
+            Message(role="assistant", content="C1"),
+        ]
+
+        rejected_messages = [
+            Message(role="user", content="Q1"),
+            Message(role="assistant", content="R1"),
+        ]
+
+
+    These lists of messages are then tokenized for model training. Currently, this function only supports
+    conversations identical to :class:`~torchtune.data.OpenAIToMessages`, and does not support custom
+    message formats.
+
+    If your dataset does not follow this format, we recommend creating a custom message transform similar to
+    :class:`~torchtune.data.ChosenRejectedToMessages` and using it in a custom dataset builder function similar
+    to :class:`~torchtune.datasets.preference_dataset`.
+
+    Masking of the prompt during training is controlled by the ``train_on_input`` flag, which is:
+    set to ``False`` by default.
+
+    - If ``train_on_input`` is True, the prompt is used during training and
+      contributes to the loss.
+    - If ``train_on_input`` is False, the prompt is masked out (tokens replaced with -100).
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text"), pass
+            in the filepath in ``data_files``, and set ``split="train"``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details.
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns "chosen" and "rejected"
+            in the message transform :class:`~torchtune.data.ChosenRejectedToMessages` to the new column names in
+            the dataset. Keys should be "chosen" and "rejected" and values should be the actual column names.
+            If None, keep the default columns "chosen" and "rejected".
+        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
+        new_system_prompt (Optional[str]): if specified, prepend a system message to every sample for both chosen
+            and rejected. This can serve as instructions to guide the model response. Setting this will OVERRIDE
+            any system messages already present in the dataset. Default is None.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
+
+    Examples:
+
+    ::
+
+        my_preference_dataset.json
+        [
+            {
+                "chosen_conversations": [
+                    {
+                        "content": "What do I do when I have a hole in my trousers?",
+                        "role": "user"
+                    },
+                    { "content": "Fix the hole.", "role": "assistant" }
+                ],
+                "rejected_conversations": [
+                    {
+                        "content": "What do I do when I have a hole in my trousers?",
+                        "role": "user"
+                    },
+                    { "content": "Take them off.", "role": "assistant" }
+                ]
+            }
+        ]
+
+    ::
+
+        >>> from torchtune.datasets import preference_dataset
+        >>> column_map = {
+        ...     "chosen": "chosen_conversations",
+        ...     "rejected": "rejected_conversations"
+        >>> }
+        >>> dataset = preference_dataset(
+        ...     tokenizer=tokenizer,
+        ...     source="json",
+        ...     column_map=column_map,
+        ...     data_files="my_preference_dataset.json",
+        ...     train_on_input=False,
+        ...     split="train",
+        >>> )
+        >>> tokenizer.decode(dataset[0]["chosen_input_ids"], skip_special_tokens=True)
+        What do I do when I have a hole in my trousers?Fix the hole.
+        >>> tokenizer.decode(dataset[0]["rejected_input_ids"], skip_special_tokens=True)
+        What do I do when I have a hole in my trousers?Take them off.
+
+    This can also be accomplished via the yaml config:
+
+    .. code-block:: yaml
+
+        dataset:
+          _component_: torchtune.datasets.preference_dataset
+          source: json
+          data_files: my_preference_dataset.json
+          column_map:
+            chosen: chosen_conversations
+            rejected: rejected_conversations
+          train_on_input: False
+          split: train
+
+
+    Returns:
+        PreferenceDataset: The preference dataset built from source paired data.
+    """
+
+    message_transform = ChosenRejectedToMessages(
+        train_on_input=train_on_input,
+        column_map=column_map,
+        new_system_prompt=new_system_prompt,
+    )
+
+    return PreferenceDataset(
+        source=source,
+        message_transform=message_transform,
+        tokenizer=tokenizer,
+        filter_fn=filter_fn,
+        split=split,
+        **load_dataset_kwargs,
+    )
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_alpaca.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_alpaca.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_arc.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_arc.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_chat.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_chat.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_cnn_dailymail.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_cnn_dailymail.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_concat.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_concat.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_grammar.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_grammar.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_hh_rlhf_helpful.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_hh_rlhf_helpful.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_instruct.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_instruct.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_packed.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_packed.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_preference.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_preference.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_samsum.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_samsum.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_sft.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_sft.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_slimorca.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_slimorca.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_stack_exchange_paired.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_stack_exchange_paired.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_text_completion.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_text_completion.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/datasets/__pycache__/_wikitext.cpython-312.pyc and marc/third_party/torchtune/torchtune/datasets/__pycache__/_wikitext.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_samsum.py marc/third_party/torchtune/torchtune/datasets/_samsum.py
--- marc_original/third_party/torchtune/torchtune/datasets/_samsum.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_samsum.py	2025-02-20 17:49:30.306025488 -0500
@@ -0,0 +1,97 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+from typing import Any, Callable, Dict, Optional, Union
+
+from torchtune.data import InputOutputToMessages
+from torchtune.datasets._packed import PackedDataset
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def samsum_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str = "Samsung/samsum",
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = False,
+    new_system_prompt: Optional[str] = None,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> Union[SFTDataset, PackedDataset]:
+    """
+    Support for summarization datasets and their variants from Hugging Face Datasets.
+    An example is the `SAMsum dataset <https://huggingface.co/datasets/samsum>`_.
+
+    It is recommended to configure the tokenizer with the :class:`~torchtune.data.SummarizeTemplate`
+    in conjunction with this dataset.
+
+    Masking of the prompt during training is controlled by the ``train_on_input`` flag, which is
+    set to ``False`` by default
+    - If ``train_on_input`` is True, the prompt is used during training and
+    contributes to the loss.
+    - If ``train_on_input`` is False, the prompt is masked out (tokens replaced with -100)
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text"), pass
+            in the filepath in ``data_files``, and set ``split="train"``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details. Default is ``Samsung/samsum``.
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns in the message transform
+            :class:`~torchtune.data.InputOutputToMessages` to the new column names in the dataset. Keys should
+            be "input" and "output" and values should be the actual column names. If None, use
+            the default column names ``{"input": "dialogue", "output": "summary"}`` in ``Samsung/samsum``.
+        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
+        new_system_prompt (Optional[str]): if specified, prepend a system message to every sample. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+        packed (bool): Whether or not to pack the dataset to tokenizer's ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
+
+    Returns:
+        Union[SFTDataset, PackedDataset]: dataset configured with source data and template
+
+    Raises:
+        ValueError: If ``packed=True`` and ``tokenizer.max_seq_len`` is not set.
+
+    Example:
+        >>> samsum_ds = samsum_dataset(model_transform=tokenizer)
+        >>> for batch in Dataloader(samsum_ds, batch_size=8):
+        >>>     print(f"Batch size: {len(batch)}")
+        >>> Batch size: 8
+    """
+    column_map = column_map or {"input": "dialogue", "output": "summary"}
+
+    message_transform = InputOutputToMessages(
+        train_on_input=train_on_input,
+        column_map=column_map,
+        new_system_prompt=new_system_prompt,
+    )
+    ds = SFTDataset(
+        source=source,
+        message_transform=message_transform,
+        model_transform=tokenizer,
+        split=split,
+        filter_fn=filter_fn,
+        **load_dataset_kwargs,
+    )
+    if packed:
+        if tokenizer.max_seq_len is None:
+            raise ValueError(
+                "PackedDataset requires a max_seq_len to be set on the tokenizer."
+            )
+        return PackedDataset(ds, max_seq_len=tokenizer.max_seq_len)
+    return ds
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_sft.py marc/third_party/torchtune/torchtune/datasets/_sft.py
--- marc_original/third_party/torchtune/torchtune/datasets/_sft.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_sft.py	2025-02-20 17:49:30.310025495 -0500
@@ -0,0 +1,147 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, Mapping, Optional
+
+import numpy as np
+
+from datasets import load_dataset
+from torch.utils.data import Dataset
+from torchtune.data._common import CROSS_ENTROPY_IGNORE_IDX
+from torchtune.data._messages import validate_messages
+from torchtune.modules.transforms import Transform
+
+
+class SFTDataset(Dataset):
+    """
+    Primary class for creating any dataset for supervised fine-tuning either from
+    Hugging Face Hub, local files, or remote files. This class supports instruct,
+    chat, tool, or multimodal data for fine-tuning. At a high level, this class
+    will load the data from source and apply the following pre-processing steps
+    when a sample is retrieved:
+
+    1. Dataset-specific transform. This is typically unique to each dataset and extracts
+       the necessary columns into torchtune's :class:`~torchtune.data.Message` format,
+       a standardized API for all model tokenizers.
+    2. Model-specific transform or tokenization with optional prompt template
+
+
+    All datasets are formatted into a list of :class:`~torchtune.data.Message`
+    because for fine-tuning, datasets can be considered as "conversations" with the model,
+    or AI assistant. Thus, we can standardize all text content as messages in a conversation assigned to
+    a role:
+
+    - ``"system"`` messages contain the system prompt
+    - ``"user"`` messages contain the input prompt into the model
+    - ``"assistant"`` messages are the response of the model and what you actually want
+      to train for and compute loss directly against
+    - ``"ipython"`` messages are the return from a tool call
+
+    Chat datasets are multiple rounds of user-assistant messages. Instruct datasets
+    are typically a single round involving a specific instruction and the model's response.
+    Tool datasets are a type of chat dataset that includes ipython messages. Multimodal
+    datasets are a type of chat dataset that incorporates media into the user messages.
+
+    The :class:`~torchtune.data.Message` forms the core data unit that all tokenizer
+    APIs expect. The key component of this class that ensures any dataset is transformed
+    into this format is the ``message_transform``. This is a callable class that takes
+    in a sample dictionary - typically a single row from the source dataset - that
+    processes the sample in any configurable way to output a list of messages::
+
+        [
+            Message(
+                role=<system|user|assistant|ipython>,
+                content=<message>,
+            ),
+            ...
+        ]
+
+    For any custom dataset, use the ``message_transform`` to contain all pre-processing to
+    return the list of messages.
+
+    Any model-specific pre-processing that needs to happen can be configured with the ``model_transform``
+    parameter. This is another callable class that contains any custom logic tied to the
+    model you are fine-tuning and will carry over to inference. For example, text + image
+    multimodal datasets requires processing the images in a way specific to the vision
+    encoder being used by the model and is agnostic to the specific dataset.
+
+    Tokenization is handled by the ``model_transform``. All :class:`~torchtune.modules.tokenizers.ModelTokenizer`
+    can be treated as a ``model_transform`` since it uses the model-specific tokenizer to
+    transform the list of messages outputted from the ``message_transform`` into tokens
+    used by the model for training. Text-only datasets will simply pass the :class:`~torchtune.modules.tokenizers.ModelTokenizer`
+    into ``model_transform``. Tokenizers handle prompt templating, if configured.
+
+    Args:
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details.
+        message_transform (Transform): callable that keys into the desired fields in the sample
+            and converts text content to a list of :class:`~torchtune.data.Message`. It is expected that the final list
+            of messages are stored in the ``"messages"`` key.
+        model_transform (Transform): callable that applies model-specific pre-processing to the sample after the list of
+            messages is created from ``message_transform``. This includes tokenization and any modality-specific
+            transforms. It is expected to return at minimum ``"tokens"`` and ``"mask"`` keys.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``. See Hugging
+            Face's `API ref <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset>`_
+            for more details.
+    """
+
+    def __init__(
+        self,
+        *,
+        source: str,
+        message_transform: Transform,
+        model_transform: Transform,
+        filter_fn: Optional[Callable] = None,
+        unmask_outputs: bool = False,
+        **load_dataset_kwargs: Dict[str, Any],
+    ) -> None:
+        self._message_transform = message_transform
+        self._model_transform = model_transform
+        self._unmask_outputs = unmask_outputs
+
+        self._data = load_dataset(source, **load_dataset_kwargs)
+        if filter_fn is not None:
+            self._data = self._data.filter(filter_fn)
+
+    def __len__(self):
+        return len(self._data)
+
+    def __getitem__(self, index: int) -> Dict[str, Any]:
+        sample = self._data[index]
+        return self._prepare_sample(sample)
+
+    def _prepare_sample(self, sample: Mapping[str, Any]) -> Dict[str, Any]:
+        transformed_sample = self._message_transform(sample)
+        if "messages" in transformed_sample:
+            validate_messages(transformed_sample["messages"])
+
+        tokenized_dict = self._model_transform(transformed_sample, unmask_outputs=self._unmask_outputs)
+
+        if not ("tokens" in tokenized_dict and "mask" in tokenized_dict):
+            keys_str = ", ".join(tokenized_dict.keys())
+            error_message = (
+                "model_transform returned the following keys: "
+                f"{keys_str}. Must return 'tokens' and 'mask' as keys."
+            )
+            raise ValueError(error_message)
+
+        # Wherever mask == True, set to CROSS_ENTROPY_IGNORE_IDX. Otherwise keep as tokens
+        tokenized_dict["labels"] = list(
+            np.where(
+                tokenized_dict["mask"],
+                CROSS_ENTROPY_IGNORE_IDX,
+                tokenized_dict["tokens"],
+            )
+        )
+        assert len(tokenized_dict["tokens"]) == len(tokenized_dict["labels"])
+
+        return tokenized_dict
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_slimorca.py marc/third_party/torchtune/torchtune/datasets/_slimorca.py
--- marc_original/third_party/torchtune/torchtune/datasets/_slimorca.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_slimorca.py	2025-02-20 17:49:30.314025502 -0500
@@ -0,0 +1,96 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, Optional, Union
+
+from torchtune.data import ShareGPTToMessages
+from torchtune.datasets._packed import PackedDataset
+
+from torchtune.datasets._sft import SFTDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def slimorca_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str = "Open-Orca/SlimOrca-Dedup",
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = False,
+    new_system_prompt: Optional[str] = None,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> Union[SFTDataset, PackedDataset]:
+    """
+    Support for `SlimOrca-style <https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup>`_
+    family of conversational datasets.
+
+    Masking of the prompt during training is controlled by the ``train_on_input`` flag, which is
+    set to ``False`` by default
+    - If ``train_on_input`` is True, the prompt is used during training and
+    contributes to the loss.
+    - If ``train_on_input`` is False, the prompt is masked out (tokens replaced with -100)
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text"), pass
+            in the filepath in ``data_files``, and set ``split="train"``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details. Default is ``Open-Orca/SlimOrca-Dedup``.
+        column_map (Optional[Dict[str, str]]): a mapping from the expected columns in the message transform
+            :class:`~torchtune.data.ShareGPTToMessages` to the new column names in the dataset. Key should
+            be "conversations" and value should be the new column name. If None, use
+            the default column name ``"conversations"`` in ``Open-Orca/SlimOrca-Dedup``.
+        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
+        new_system_prompt (Optional[str]): if specified, prepend a system message to every sample. This can
+            serve as instructions to guide the model response. Setting this will OVERRIDE any system
+            messages already present in the dataset. Default is None.
+        packed (bool): Whether or not to pack the dataset to tokenizer's ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
+
+    Returns:
+        Union[SFTDataset, PackedDataset]: dataset configured with SlimOrca source data
+
+    Raises:
+        ValueError: If ``packed=True`` and ``tokenizer.max_seq_len`` is not set.
+
+    Example:
+        >>> ds = slimorca_dataset(model_transform=tokenizer)
+        >>> for input, label in ds:
+        >>>     print(input)
+        >>>     print(label)
+        >>>
+        >>> Sample Output:
+        >>> [1, 351, 82, 391, 221, 220, 193, 12, 471, ..., 2]
+        >>> [-100, -100, -100, -100, -100, -100, -100, -100, 471, ..., 2]
+    """
+    message_transform = ShareGPTToMessages(
+        train_on_input=train_on_input,
+        column_map=column_map,
+        new_system_prompt=new_system_prompt,
+    )
+    ds = SFTDataset(
+        source=source,
+        message_transform=message_transform,
+        model_transform=tokenizer,
+        filter_fn=filter_fn,
+        split=split,
+        **load_dataset_kwargs,
+    )
+    if packed:
+        if tokenizer.max_seq_len is None:
+            raise ValueError(
+                "PackedDataset requires a max_seq_len to be set on the tokenizer."
+            )
+        return PackedDataset(ds, max_seq_len=tokenizer.max_seq_len)
+    return ds
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_stack_exchange_paired.py marc/third_party/torchtune/torchtune/datasets/_stack_exchange_paired.py
--- marc_original/third_party/torchtune/torchtune/datasets/_stack_exchange_paired.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_stack_exchange_paired.py	2025-02-20 17:49:30.318025508 -0500
@@ -0,0 +1,133 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, Mapping, Optional
+
+from torchtune.data import Message
+from torchtune.datasets._preference import PreferenceDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+from torchtune.modules.transforms import Transform
+
+
+class StackExchangePairedToMessages(Transform):
+    """
+    Transform for converting datasets similar to the format in `Stack Exchange Paired dataset
+    <https://huggingface.co/datasets/lvwerra/stack-exchange-paired>`_::
+
+        |  prompt  |  chosen  |  rejected  |
+        |----------|----------|------------|
+        |  Q1      |  A1      |  A2        |
+
+    into a list of chosen and rejected messages:
+
+    .. code-block:: python
+
+        chosen = [
+            Message(role="user", content="Q1"),
+            Message(role="assistant", content="A1"),
+        ]
+        rejected = [
+            Message(role="user", content="Q1"),
+            Message(role="assistant", content="A2"),
+        ]
+
+    Args:
+        train_on_input (bool): Whether the model is trained on the user prompt or not.
+            Default is False.
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected "prompt",
+            "chosen", and "rejected" column names to the actual column names in the dataset.
+            Keys should be "prompt", "chosen", and "rejected" and values should be the actual column names.
+            Default is None, keeping the default column names.
+    """
+
+    def __init__(
+        self, train_on_input: bool = False, column_map: Optional[Dict[str, str]] = None
+    ):
+        self.train_on_input = train_on_input
+        self._column_map = column_map
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        column_map = self._column_map or {}
+        key_prompt = column_map.get("prompt", "prompt")
+        key_chosen = column_map.get("chosen", "chosen")
+        key_rejected = column_map.get("rejected", "rejected")
+
+        chosen_messages = [
+            Message(
+                role="user", content=sample[key_prompt], masked=not self.train_on_input
+            ),
+            Message(role="assistant", content=sample[key_chosen]),
+        ]
+
+        rejected_messages = [
+            Message(
+                role="user", content=sample[key_prompt], masked=not self.train_on_input
+            ),
+            Message(role="assistant", content=sample[key_rejected]),
+        ]
+
+        return {"chosen": chosen_messages, "rejected": rejected_messages}
+
+
+def stack_exchange_paired_dataset(
+    tokenizer: ModelTokenizer,
+    *,
+    source: str = "lvwerra/stack-exchange-paired",
+    column_map: Optional[Dict[str, str]] = None,
+    train_on_input: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> PreferenceDataset:
+    """
+    Family of preference datasets similar to the `Stack Exchange Paired dataset
+    <https://huggingface.co/datasets/lvwerra/stack-exchange-paired>`_.
+
+    It is recommended to configure the tokenizer with the :class:`~torchtune.data.QuestionAnswerTemplate`
+    in conjunction with this dataset.
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See `Hugging Face's
+            <https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path>`_
+            ``load_dataset`` for more details. Default is ``lvwerra/stack-exchange-paired``.
+        column_map (Optional[Dict[str, str]]): a mapping to change the expected "prompt",
+            "chosen", and "rejected" column names to the actual column names in the dataset.
+            Keys should be "prompt", "chosen", and "rejected" and values should be the actual column names.
+            Default is None, keeping the default column names.
+        train_on_input (bool): Whether the model is trained on the prompt or not. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
+
+    Returns:
+        PreferenceDataset: The preference dataset built from source paired data.
+    """
+
+    column_map = column_map or {
+        "prompt": "question",
+        "chosen": "response_j",
+        "rejected": "response_k",
+    }
+
+    message_transform = StackExchangePairedToMessages(
+        train_on_input=train_on_input, column_map=column_map
+    )
+
+    return PreferenceDataset(
+        source=source,
+        message_transform=message_transform,
+        tokenizer=tokenizer,
+        filter_fn=filter_fn,
+        split=split,
+        data_dir="data/rl",
+        **load_dataset_kwargs,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_text_completion.py marc/third_party/torchtune/torchtune/datasets/_text_completion.py
--- marc_original/third_party/torchtune/torchtune/datasets/_text_completion.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_text_completion.py	2025-02-20 17:49:30.322025515 -0500
@@ -0,0 +1,166 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, List, Mapping, Optional, Union
+
+from datasets import load_dataset
+from torch.utils.data import Dataset
+from torchtune.data._utils import truncate
+from torchtune.datasets._packed import PackedDataset
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+class TextCompletionDataset(Dataset):
+    """
+    Freeform dataset for any unstructured text corpus. Quickly load any dataset
+    from Hugging Face or local disk and tokenize it for your model.
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See Hugging Face's ``load_dataset``
+            (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)
+            for more details.
+        column (str): name of column in the sample that contains the text data. This is typically required
+            for Hugging Face datasets or tabular data. For local datasets with a single column
+            (e.g. unstructured txt files), use the default "text" which is used by Hugging Face datasets
+            when loaded into memory. Default is "text".
+        add_eos (bool): Whether to add an EOS token to the end of the sequence. Default is True.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``,
+            such as ``data_files`` or ``split``.
+    """
+
+    def __init__(
+        self,
+        tokenizer: ModelTokenizer,
+        source: str,
+        column: str = "text",
+        add_eos: bool = True,
+        filter_fn: Optional[Callable] = None,
+        **load_dataset_kwargs: Dict[str, Any],
+    ) -> None:
+        self._tokenizer = tokenizer
+        self._data = load_dataset(source, **load_dataset_kwargs)
+        self._column = column
+        self.add_eos = add_eos
+
+        if filter_fn is not None:
+            self._data = self._data.filter(filter_fn)
+
+    def __len__(self):
+        return len(self._data)
+
+    def __getitem__(self, index: int) -> Dict[str, List[int]]:
+        sample = self._data[index]
+        return self._prepare_sample(sample)
+
+    def _prepare_sample(self, sample: Mapping[str, Any]) -> Dict[str, List[int]]:
+        prompt = sample[self._column]
+
+        tokens = self._tokenizer.encode(text=prompt, add_bos=True, add_eos=self.add_eos)
+
+        # Truncate if needed, but don't coerce EOS id
+        if self._tokenizer.max_seq_len is not None:
+            tokens = truncate(tokens, self._tokenizer.max_seq_len - 1)
+
+        # No need to offset labels by 1 - happens in the recipe
+        labels = tokens.copy()
+
+        return {"tokens": tokens, "labels": labels}
+
+
+def text_completion_dataset(
+    tokenizer: ModelTokenizer,
+    source: str,
+    column: str = "text",
+    add_eos: bool = True,
+    packed: bool = False,
+    split_across_pack: bool = True,
+    split: str = "train",
+    filter_fn: Optional[Callable] = None,
+    **load_dataset_kwargs: Dict[str, Any],
+) -> Union[TextCompletionDataset, PackedDataset]:
+    """
+    Build a configurable dataset from a freeform, unstructured text corpus similar
+    to datasets used in pre-training. This method should be
+    used to configure a custom text dataset from the yaml config instead of
+    using :class:`~torchtune.datasets.TextCompletionDataset` directly, as it is made to be config friendly.
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See Hugging Face's ``load_dataset``
+            (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)
+            for more details.
+        column (str): name of column in the sample that contains the text data. This is typically required
+            for Hugging Face datasets or tabular data. For local datasets with a single column
+            (e.g. unstructured txt files), use the default "text" which is used by Hugging Face datasets
+            when loaded into memory. Default is "text".
+        add_eos (bool): Whether to add an EOS token to the end of the sequence. Default is True.
+        packed (bool): Whether or not to pack the dataset to ``max_seq_len`` prior to training. Default is False.
+        split_across_pack (bool): if the last sample in a pack does not fit in ``max_seq_len``,
+            split the sample into the next pack, or move it entirely to the beginning of the next pack.
+            For pre-training, typically this is set to True for general text completion. For
+            fine-tuning, typically this is set to False to avoid truncating sentences in instruct
+            tuning. This argument is ignored if ``packed=False``. Default is True.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
+
+    Examples:
+        >>> from torchtune.datasets import text_completion_dataset
+        >>> dataset = text_completion_dataset(
+        ...   tokenizer=tokenizer,
+        ...   source="allenai/c4",
+        ...   column="text",
+        ...   data_dir="realnewslike",
+        ...   packed=False,
+        ...   split="train",
+        ... )
+
+    This can also be accomplished via the yaml config::
+
+        dataset:
+            _component_: torchtune.datasets.text_completion_dataset
+            source: allenai/c4
+            column: text
+            data_dir: realnewslike
+            packed: False
+            split: train
+
+    Returns:
+        Union[TextCompletionDataset, PackedDataset]: the configured :class:`~torchtune.datasets.TextCompletionDataset`
+            or :class:`~torchtune.datasets.PackedDataset` if ``packed=True``
+
+    Raises:
+        ValueError: If ``packed=True`` and ``tokenizer.max_seq_len`` is not set.
+    """
+    ds = TextCompletionDataset(
+        tokenizer=tokenizer,
+        source=source,
+        column=column,
+        add_eos=add_eos,
+        split=split,
+        filter_fn=filter_fn,
+        **load_dataset_kwargs,
+    )
+    if packed:
+        if tokenizer.max_seq_len is None:
+            raise ValueError(
+                "PackedDataset requires a max_seq_len to be set on the tokenizer."
+            )
+        return PackedDataset(
+            ds, max_seq_len=tokenizer.max_seq_len, split_across_pack=split_across_pack
+        )
+    return ds
diff -ruN marc_original/third_party/torchtune/torchtune/datasets/_wikitext.py marc/third_party/torchtune/torchtune/datasets/_wikitext.py
--- marc_original/third_party/torchtune/torchtune/datasets/_wikitext.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/datasets/_wikitext.py	2025-02-20 17:49:30.326025521 -0500
@@ -0,0 +1,70 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Callable, Dict, Optional, Union
+
+from torchtune.datasets._packed import PackedDataset
+
+from torchtune.datasets._text_completion import (
+    text_completion_dataset,
+    TextCompletionDataset,
+)
+
+from torchtune.modules.tokenizers import ModelTokenizer
+
+
+def wikitext_dataset(
+    tokenizer: ModelTokenizer,
+    source: str = "EleutherAI/wikitext_document_level",
+    subset: str = "wikitext-103-v1",
+    max_seq_len: Optional[int] = None,
+    packed: bool = False,
+    filter_fn: Optional[Callable] = None,
+    split: str = "train",
+    **load_dataset_kwargs: Dict[str, Any],
+) -> Union[TextCompletionDataset, PackedDataset]:
+    """
+    Support for family of datasets similar to `wikitext
+    <https://huggingface.co/datasets/EleutherAI/wikitext_document_level>`_,
+    an unstructured text corpus consisting of fulls articles from Wikipedia.
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer used by the model that implements the ``tokenize_messages`` method.
+        source (str): path to dataset repository on Hugging Face. For local datasets,
+            define source as the data file type (e.g. "json", "csv", "text") and pass
+            in the filepath in ``data_files``. See Hugging Face's ``load_dataset``
+            (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)
+            for more details.
+        subset (str): name of subset of data to use, see the `wikitext page
+            <https://huggingface.co/datasets/EleutherAI/wikitext_document_level#data-instances>`_
+            for available subsets. Default is ``"wikitext-103-v1"``.
+        max_seq_len (Optional[int]): Maximum number of tokens in the returned input and label token id lists.
+            Default is None, disabling truncation. We recommend setting this to the highest you can fit in memory
+            and is supported by the model. For example, llama2-7B supports up to 4096 for sequence length.
+        packed (bool): Whether or not to pack the dataset to ``max_seq_len`` prior to training. Default is False.
+        filter_fn (Optional[Callable]): callable used to filter the dataset prior to any pre-processing. See
+            the Hugging Face `docs <https://huggingface.co/docs/datasets/v2.20.0/process#select-and-filter>`_ for more
+            details.
+        split (str): ``split`` argument for ``datasets.load_dataset``. You can use this argument to load a subset
+            of a given split, e.g. ``split="train[:10%]"``. Default is "train".
+        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.
+
+    Returns:
+        Union[TextCompletionDataset, PackedDataset]: the configured :class:`~torchtune.datasets.TextCompletionDataset`
+            or :class:`~torchtune.datasets.PackedDataset` if ``packed=True``
+    """
+
+    return text_completion_dataset(
+        tokenizer=tokenizer,
+        source=source,
+        column="page",
+        max_seq_len=max_seq_len,
+        name=subset,
+        packed=packed,
+        filter_fn=filter_fn,
+        split=split,
+        **load_dataset_kwargs,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/generation/_generation.py marc/third_party/torchtune/torchtune/generation/_generation.py
--- marc_original/third_party/torchtune/torchtune/generation/_generation.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/generation/_generation.py	2025-02-20 17:49:30.354025567 -0500
@@ -0,0 +1,394 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Callable, List, Optional, Tuple
+
+import torch
+from torchtune.modules.transformer import TransformerDecoder
+
+
+def multinomial_sample_one(probs: torch.Tensor, q: torch.Tensor) -> torch.Tensor:
+    """Samples from a multinomial distribution."""
+    return torch.argmax(probs / q, dim=-1, keepdim=True).to(dtype=torch.int)
+
+
+def sample(
+    logits: torch.Tensor,
+    *,
+    temperature: float = 1.0,
+    top_k: Optional[int] = None,
+    q: Optional[torch.Tensor] = None,
+) -> torch.Tensor:
+    """Generic sample from a probability distribution. Includes support for Top-K sampling
+    and Temperature.
+
+    Args:
+        logits (torch.Tensor): logits from which to sample
+        temperature (float): value to scale the predicted logits by, default 1.0.
+        top_k (Optional[int]): If specified, we prune the sampling to only token ids within the top_k probabilities
+        q (Optional[torch.Tensor]): randomly sampled tensor for softmax sampling trick. If None,
+            we use the default softmax sampling trick. Default None.
+
+    Example:
+        >>> from torchtune.generation import sample
+        >>> logits = torch.empty(3, 3).uniform_(0, 1)
+        >>> sample(logits)
+        tensor([[1],
+                [2],
+                [0]], dtype=torch.int32)
+
+    Returns:
+        torch.Tensor: sampled token id
+    """
+    # scale the logits based on temperature
+    logits = logits / max(temperature, 1e-5)
+    if top_k is not None:
+        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
+        # select the very last value from the top_k above as the pivot
+        pivot = v.select(-1, -1).unsqueeze(-1)
+        # set everything smaller than pivot value to inf since these
+        # should be pruned
+        logits = torch.where(logits < pivot, -float("Inf"), logits)
+
+    # change logits into probabilities
+    probs = torch.nn.functional.softmax(logits, dim=-1)
+
+    # if q is None, we use the default softmax sampling trick
+    if q is None:
+        q = torch.empty_like(probs).exponential_(1)
+
+    return multinomial_sample_one(probs, q)
+
+
+def generate_next_token(
+    model: TransformerDecoder,
+    input_pos: torch.Tensor,
+    x: torch.Tensor,
+    q: torch.Tensor,
+    *,
+    mask: Optional[torch.Tensor] = None,
+    temperature: float = 1.0,
+    top_k: Optional[int] = None,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+    Generates the next tokens given a prompt, and also returns the corresponding logits.
+
+    Args:
+        model (TransformerDecoder): model used for generation
+        input_pos (torch.Tensor): tensor with the positional encodings associated with the given prompt,
+            with shape [bsz x seq_length].
+        x (torch.Tensor): tensor with the token IDs associated with the given prompt,
+            with shape [bsz x seq_length].
+        q (torch.Tensor): randomly sampled tensor for softmax sampling trick.
+            See https://github.com/pytorch-labs/gpt-fast/blob/32971d3129541c5bfb4f715abc33d1c5f408d204/generate.py#L40
+        mask (Optional[torch.Tensor]): attention mask with shape [bsz x seq_length x seq_length],
+            default None.
+        temperature (float): value to scale the predicted logits by, default 1.0.
+        top_k (Optional[int]): Top-k value to use for sampling, default None.
+
+    Returns:
+        Tuple[torch.Tensor, torch.Tensor]: tuple of two tensors:
+            - tokens (torch.Tensor): tensor with the generated tokens,
+                with shape [bsz x 1].
+            - logits (torch.Tensor): tensor with the logits associated with the generated tokens,
+                with shape [bsz x seq_length x vocab_size].
+
+    """
+    # model produces logits in [bsz, seq_length, vocab_size]
+    # we want to take the last token's logits as the input to the next model call
+    logits = model(x, input_pos=input_pos, mask=mask)
+    return (
+        sample(logits[:, -1].clone(), temperature=temperature, top_k=top_k, q=q),
+        logits,
+    )
+
+
+def update_stop_tokens_tracker(
+    tokens: torch.Tensor, stop_tokens: torch.Tensor, stop_token_reached: torch.Tensor
+) -> torch.Tensor:
+    """Updates which sequences have reached a stop token."""
+    # tokens: [bsz, 1]
+    # stop_tokens: [num_stop_tokens]
+    # stop_token_reached: [bsz]
+    stop_token_reached_curr = torch.isin(tokens, stop_tokens).flatten()
+    stop_token_reached |= stop_token_reached_curr
+    return stop_token_reached
+
+
+def get_causal_mask_from_padding_mask(
+    padding_mask: torch.Tensor, target_seq_len: Optional[int] = None
+) -> torch.Tensor:
+    """
+    Converts a padding mask of shape ``[bsz, seq_len]`` to a ``[bsz, seq_len, seq_len]`` causal attention mask suitable for
+    consumption by :func:`~torch.nn.functional.scaled_dot_product_attention`. If ``target_seq_len``
+    is provided, this will return a mask of shape ``[bsz, seq_len, target_seq_len]``. This is useful
+    when generating masks for static KV caches where the maximum length the caches have been setup with
+    are longer than the current sequence.
+
+    Args:
+        padding_mask (torch.Tensor): Boolean tensor where False indicates the corresponding token in the sequence
+            is a padding token and should be masked out in attention, with shape [bsz x seq_length]
+        target_seq_len (Optional[int]): target sequence length to create attention mask with. Default None.
+
+    Returns:
+        torch.Tensor: Boolean causal mask with shape
+            - [bsz, seq_length, seq_length] or
+            - [bsz, seq_length, target_seq_len] if ``target_seq_len`` was specified.
+
+    Raises:
+        AssertionError: if ``target_seq_len > seq_len``, the sequence length of the padding mask.
+
+    Example:
+        >>> padding_mask = torch.tensor([[False, True, True, True]])
+        >>> get_causal_mask_from_padding_mask(padding_mask, target_seq_len=5)
+        tensor([[[ True, False, False, False, False],
+                  [False,  True, False, False, False],
+                  [False,  True,  True, False, False],
+                  [False,  True,  True,  True, False]]])
+        ])
+    """
+    bsz, seq_len = padding_mask.shape
+    target_seq_len = seq_len if target_seq_len is None else target_seq_len
+
+    if target_seq_len < seq_len:
+        raise AssertionError(
+            "target_seq_len cannot be shorter than the sequence length of the padding mask."
+        )
+
+    mask = torch.tril(
+        torch.ones(seq_len, target_seq_len, device=padding_mask.device, dtype=bool),
+        diagonal=0,
+    ).repeat(bsz, 1, 1)
+    mask.narrow(2, 0, seq_len).mul_(padding_mask[:, None, :].expand(-1, seq_len, -1))
+    mask.diagonal(dim1=1, dim2=2).copy_(torch.Tensor([True]))
+    return mask
+
+
+def get_position_ids_from_padding_mask(
+    padding_mask: torch.Tensor,
+):
+    """
+    Calculates position ids given a padding mask which right-shifts position ids to start
+    from the first valid token.
+
+    Args:
+        padding_mask (torch.Tensor): Boolean tensor where False indicates the corresponding token in the sequence
+            is a padding token and should be masked out in attention. Shape [bsz, seq_len]
+
+    Returns:
+        torch.Tensor: position ids which are appropriately shifted according to any padding values.
+
+    Example:
+        >>> padding_mask = torch.tensor([False, False, False, True, True, True, True, True])
+        >>> get_position_ids_from_padding_mask(padding_mask)
+        torch.Tensor([0, 0, 0, 0, 1, 2, 3, 4])
+    """
+    return ((padding_mask.cumsum(-1) - 1) * padding_mask).to(torch.int)
+
+
+@torch.inference_mode()
+def generate(
+    model: TransformerDecoder,
+    prompt: torch.Tensor,
+    *,
+    max_generated_tokens: int,
+    pad_id: int = 0,
+    temperature: float = 1.0,
+    top_k: Optional[int] = None,
+    stop_tokens: Optional[List[int]] = None,
+    rng: Optional[torch.Generator] = None,
+    custom_generate_next_token: Optional[Callable] = None,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+    Generates tokens from a model conditioned on a prompt, and also returns logits for the generations.
+
+    Args:
+        model (TransformerDecoder): model used for generation
+        prompt (torch.Tensor): tensor with the token IDs associated with the given prompt,
+            with shape either [seq_length] or [bsz x seq_length].
+        max_generated_tokens (int): number of tokens to be generated
+        pad_id (int): token ID to use for padding, default 0.
+        temperature (float): value to scale the predicted logits by, default 1.0.
+        top_k (Optional[int]): If specified, we prune the sampling to only token ids within the top_k probabilities,
+            default None.
+        stop_tokens (Optional[List[int]]): If specified, generation is stopped when any of these tokens are generated,
+            default None.
+        rng (Optional[torch.Generator]): random number generator, default None.
+        custom_generate_next_token (Optional[Callable]): If specified, we'll use the
+            ``custom_generate_next_token function``. This is generally only useful if
+            you want to specify a ``torch.compile`` version of the generate next token for
+            performance reasons. If None, we use the default :func:`generate_next_token`.
+            Default is None.
+
+    Note:
+        This function has only been tested with decoder-only models.
+
+    Examples:
+        >>> model = torchtune.models.llama3.llama3_8b()
+        >>> tokenizer = torchtune.models.llama3.llama3_tokenizer()
+        >>> prompt = tokenizer.encode("Hi my name is")
+        >>> rng.manual_seed(42)
+        >>> output, logits = generate(model, torch.tensor(prompt), max_generated_tokens=100, pad_id=0)
+        >>> print(tokenizer.decode(output[0].tolist()))
+        Hi my name is Jeremy and I'm a friendly language model assistant!
+
+    Returns:
+        Tuple[torch.Tensor, torch.Tensor]: tuple of two tensors:
+            - tokens (torch.Tensor): tensor with the generated tokens,
+                with shape ``[bsz x seq_len + num_generated_tokens]`` where ``num_generated_tokens``
+                may be less than ``max_generated_tokens`` if ``stop_tokens`` are provided.
+            - logits (torch.Tensor): tensor with the logits associated with the generated tokens,
+                with shape ``[bsz x seq_len + num_generated_tokens x vocab_size]``.
+    """
+    prompt = prompt.view(1, -1) if prompt.ndim == 1 else prompt
+
+    if custom_generate_next_token is None:
+        custom_generate_next_token = generate_next_token
+
+    bsz, prompt_length = prompt.size()
+    total_response_length = prompt_length + max_generated_tokens
+
+    generated_tokens = prompt.clone()
+    incremental_decoding = model.caches_are_enabled()
+
+    # grab the correct max_seq_len to generate full causal masks/position ids
+    # this is the model's max cache len if incremental decoding, or the sequence
+    # length otherwise
+    max_seq_len = (
+        total_response_length
+        if not incremental_decoding
+        else model.decoder_max_cache_seq_len
+    )
+
+    padding_masks = generated_tokens != pad_id
+
+    if not padding_masks.all():
+        # we have padding in the prompt due to varying-length sequences in a batch
+        # extend padding masks out to the correct seq len
+        padding_masks = torch.nn.functional.pad(
+            padding_masks, (0, max_generated_tokens), value=True
+        )
+
+        # generate the full causal mask for the whole padding mask with padding ignored
+        masks = get_causal_mask_from_padding_mask(
+            padding_masks, target_seq_len=max_seq_len
+        )
+
+        # right-shift position IDs to account for padding
+        input_pos = get_position_ids_from_padding_mask(padding_masks)
+    else:
+        # just use a regular causal mask if there is no padding
+        masks = torch.tril(
+            torch.ones(
+                total_response_length,
+                max_seq_len,
+                dtype=torch.bool,
+                device=prompt.device,
+            )
+        ).unsqueeze(0)
+        input_pos = torch.arange(
+            0, total_response_length, device=generated_tokens.device
+        ).unsqueeze(0)
+
+    if incremental_decoding:
+        # if KV-caches are enabled, we need a causal mask of shape [bsz, prompt_length, max_cache_len]
+        # to match the key/value cache tensor shapes
+        curr_masks = masks[:, :prompt_length]
+    else:
+        # otherwise the causal mask is shape [bsz, prompt_length, prompt_length] because key/value
+        # tensors are of identical shape to the prompt
+        curr_masks = masks[:, :prompt_length, :prompt_length]
+
+    q = torch.empty(
+        (bsz, model.tok_embeddings.num_embeddings), device=prompt.device
+    ).exponential_(1, generator=rng)
+    tokens, generated_logits = generate_next_token(
+        model,
+        input_pos=input_pos[:, :prompt_length].squeeze(),
+        mask=curr_masks,
+        x=prompt,
+        temperature=temperature,
+        top_k=top_k,
+        q=q,
+    )
+
+    generated_tokens = torch.cat([generated_tokens, tokens], dim=-1)
+
+    curr_pos = prompt_length
+
+    # keeps track at a high level if we've already hit a stop token in a sequence so we can early stop
+    stop_token_reached = torch.zeros(bsz, dtype=torch.bool, device=prompt.device)
+    stop_tokens = (
+        torch.tensor(stop_tokens, device=prompt.device, dtype=tokens.dtype)
+        if stop_tokens
+        else None
+    )
+
+    # everything in stop_token_mask starts as 1s, and we'll set them to 0 for sequences
+    # that already hit a stop token
+    stop_token_mask = torch.ones(
+        (bsz, prompt_length + 1), dtype=torch.int32, device=prompt.device
+    )
+
+    # stop early if we reach a stop token in every seq
+    if stop_tokens is not None:
+        stop_token_reached = update_stop_tokens_tracker(
+            tokens, stop_tokens, stop_token_reached
+        )
+        if stop_token_reached.all().item():
+            return generated_tokens, generated_logits
+
+    for _ in range(max_generated_tokens - 1):
+        # update stop_token_mask if we reached a stop token in a previous step
+        # by appending the logical not of stop_token_reached to the end of the mask
+        # reshaped to be bsz first
+        if stop_tokens is not None:
+            stop_token_mask = torch.cat(
+                [stop_token_mask, ~stop_token_reached.reshape(bsz, 1)], dim=-1
+            )
+
+        # if incremental decoding is enabled, we can use the current position
+        # otherwise, we take the whole sequence up to the current position
+        if incremental_decoding:
+            curr_input_pos = input_pos[:, curr_pos]
+            curr_masks = masks[:, curr_pos, None, :]
+        else:
+            tokens = generated_tokens.clone()
+            curr_input_pos = input_pos[:, : curr_pos + 1]
+            curr_masks = masks[:, : curr_pos + 1, : curr_pos + 1]
+
+        q = torch.empty(
+            (bsz, model.tok_embeddings.num_embeddings), device=prompt.device
+        ).exponential_(1, generator=rng)
+        tokens, logits = custom_generate_next_token(
+            model,
+            input_pos=curr_input_pos,
+            x=tokens.clone(),
+            mask=curr_masks,
+            temperature=temperature,
+            top_k=top_k,
+            q=q,
+        )
+        generated_tokens = torch.cat([generated_tokens, tokens], dim=-1)
+        curr_pos += 1
+        if incremental_decoding:
+            generated_logits = torch.cat([generated_logits, logits], dim=1)
+        else:
+            generated_logits = logits
+
+        if stop_tokens is not None:
+            stop_token_reached = update_stop_tokens_tracker(
+                tokens, stop_tokens, stop_token_reached
+            )
+            if stop_token_reached.all():
+                break
+
+    # mask out generated tokens in seqs that already hit a stop token
+    if stop_tokens is not None:
+        generated_tokens *= stop_token_mask
+        generated_logits *= stop_token_mask[:, :-1, None]
+
+    return generated_tokens, generated_logits
diff -ruN marc_original/third_party/torchtune/torchtune/generation/__init__.py marc/third_party/torchtune/torchtune/generation/__init__.py
--- marc_original/third_party/torchtune/torchtune/generation/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/generation/__init__.py	2025-02-20 17:49:30.350025560 -0500
@@ -0,0 +1,21 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._generation import (
+    generate,
+    generate_next_token,
+    get_causal_mask_from_padding_mask,
+    get_position_ids_from_padding_mask,
+    sample,
+)
+
+__all__ = [
+    "generate",
+    "generate_next_token",
+    "get_causal_mask_from_padding_mask",
+    "get_position_ids_from_padding_mask",
+    "sample",
+]
Binary files marc_original/third_party/torchtune/torchtune/generation/__pycache__/_generation.cpython-312.pyc and marc/third_party/torchtune/torchtune/generation/__pycache__/_generation.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/generation/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/generation/__pycache__/__init__.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/__init__.py marc/third_party/torchtune/torchtune/__init__.py
--- marc_original/third_party/torchtune/torchtune/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/__init__.py	2025-02-20 17:49:30.170025265 -0500
@@ -0,0 +1,28 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+__version__ = ""
+
+
+# Check at the top-level that torchao is installed.
+# This is better than doing it at every import site.
+# We have to do this because it is not currently possible to
+# properly support both nightly and stable installs of PyTorch + torchao
+# in pyproject.toml.
+try:
+    import torchao  # noqa
+except ImportError as e:
+    raise ImportError(
+        """
+        torchao not installed.
+        Please follow the instructions at https://pytorch.org/torchtune/main/install.html#pre-requisites
+        to install torchao.
+        """
+    ) from e
+
+from torchtune import datasets, generation, models, modules, utils
+
+__all__ = [datasets, models, modules, utils, generation]
diff -ruN marc_original/third_party/torchtune/torchtune/models/clip/_component_builders.py marc/third_party/torchtune/torchtune/models/clip/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/clip/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/clip/_component_builders.py	2025-02-20 17:49:30.366025587 -0500
@@ -0,0 +1,485 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+from typing import Callable, List, Optional
+
+import torch
+from torch import nn
+
+from torchtune.modules.vision_transformer import VisionTransformer, CLSProjection
+from torchtune.models.clip._position_embeddings import TokenPositionalEmbedding, TiledTokenPositionalEmbedding, TilePositionalEmbedding
+
+from torchtune.modules import (
+    TransformerSelfAttentionLayer,
+    MultiHeadAttention,
+    TanhGate,
+    FeedForward,
+    Fp32LayerNorm
+)
+
+from torchtune.modules.common_utils import reparametrize_as_dtype_state_dict_post_hook
+
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+
+def clip_vision_encoder(
+    tile_size: int,
+    patch_size: int,
+    embed_dim: int,
+    num_layers: int,
+    num_heads: int,
+    activation: Callable = nn.SiLU,
+    cls_output_dim: int = 512,
+    attn_bias: bool = True,
+    out_indices: Optional[List[int]] = None,
+    output_cls_projection: bool = False,
+    max_num_tiles: int = 4,
+    in_channels: int = 3,
+    intermediate_act: torch.nn.Module = torch.nn.SiLU(),
+) -> VisionTransformer:
+    """
+    Builds the vision encoder associated with the clip model. This includes:
+    
+    - TransformerEncoderLayer
+    - positional embeddings
+    - CLS projection (optional)
+
+    For details, please check the documentation of
+    :class:`torchtune.modules.vision_transformer.VisionTransformer`.
+
+    Args:
+        tile_size (int): The size of your image tiles, if the image was tile-cropped in advance. Otherwise,
+            the size of the input image. In this case, the function will consider your image as a single tile.
+        patch_size (int): The size of each patch. Used to divide the tiles into patches.
+            E.g. for ``patch_size=40``, a tile of shape (400, 400) will have 10x10 grid of patches
+            with shape (40, 40) each.
+        embed_dim (int): The dimensionality of each patch embedding (token).
+        num_layers (int): The number of transformer layers.
+        num_heads (int): The number of attention heads in each transformer layer.
+        activation (Callable): The activation function to use in the MLP layer.
+        cls_output_dim (int): The dimensionality of the output tensor from the CLS projection module.
+        attn_bias (bool): Boolean for if to use bias in the attention module. Default True.
+        out_indices (Optional[List[int]]): The indices of hidden layers to return.
+            If provided, it will return the intermediate results of the transformer layers
+            before they go through a next layer. For example, ``out_indices=[0,3]`` will
+            return the tokens before they go through the first and fourth layers.
+        output_cls_projection (bool): If True, only the CLS token projection will be outputted,
+            instead of all tokens. Defaults to False.
+        max_num_tiles (int): The maximum number of tiles that can be processed. This is used to
+            determine the size of the positional embeddings.
+        in_channels (int): The number of image input channels.
+        intermediate_act (torch.nn.Module): The activation function used in the intermediate layers in the transformer encoder.
+
+    Returns:
+        A `VisionTransformer` object.
+
+    Raises:
+        AssertionError: If ``embed_dim`` is not divisible by ``num_heads``.
+    """
+    assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
+
+    cls_projection = CLSProjection(embed_dim=embed_dim, cls_output_dim=cls_output_dim) if output_cls_projection else None
+
+    # transformer layer
+    self_attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_heads,
+            head_dim=embed_dim // num_heads,
+            q_proj=nn.Linear(embed_dim, embed_dim, bias=attn_bias),
+            k_proj=nn.Linear(embed_dim, embed_dim, bias=attn_bias),
+            v_proj=nn.Linear(embed_dim, embed_dim, bias=attn_bias),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=attn_bias),
+            pos_embeddings=None,
+            attn_dropout=0.0,
+            is_causal=False,
+    )
+    mlp = clip_mlp(
+        in_dim=embed_dim,
+        hidden_dim=4 * embed_dim,
+        out_dim=embed_dim,
+        activation=activation(),
+    )
+    transformer_layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm= Fp32LayerNorm(embed_dim, eps=1e-5),
+        mlp_norm= Fp32LayerNorm(embed_dim, eps=1e-5),
+        sa_scale=None,
+        mlp_scale=None,
+    )
+
+    # position embeddings
+    if max_num_tiles == 1:
+        pre_tile_pos_embed = None
+        post_tile_pos_embed = None
+        token_pos_embedding = TokenPositionalEmbedding(
+            embed_dim=embed_dim, 
+            patch_size=patch_size, 
+            tile_size=tile_size)
+    else:
+        pre_tile_pos_embed = TilePositionalEmbedding(max_num_tiles=max_num_tiles, embed_dim=embed_dim)
+        post_tile_pos_embed = TilePositionalEmbedding(max_num_tiles=max_num_tiles, embed_dim=embed_dim)
+        token_pos_embedding = TiledTokenPositionalEmbedding(
+            max_num_tiles=max_num_tiles, 
+            embed_dim=embed_dim, 
+            patch_size=patch_size, 
+            tile_size=tile_size)
+
+    return VisionTransformer(
+        num_layers=num_layers,
+        layer=transformer_layer,
+        token_pos_embedding=token_pos_embedding,
+        pre_tile_pos_embed=pre_tile_pos_embed,
+        post_tile_pos_embed=post_tile_pos_embed,
+        cls_projection=cls_projection,
+        out_indices=out_indices,
+        tile_size=tile_size,
+        patch_size=patch_size,
+        embed_dim=embed_dim,
+        in_channels=in_channels,
+    )
+
+
+def clip_mlp(in_dim: int, out_dim: int, hidden_dim: int, activation: nn.Module, quantize_base: bool = False) -> FeedForward:
+    """
+    Build the MLP layer associated with the clip model.
+    """
+    gate_proj = nn.Linear(in_dim, hidden_dim) if not quantize_base else FrozenNF4Linear(in_dim, hidden_dim)
+    down_proj = nn.Linear(hidden_dim, out_dim) if not quantize_base else FrozenNF4Linear(hidden_dim, out_dim)
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=None, activation=activation)
+
+
+# ------------------ LoRA CLIP ------------------
+
+
+def lora_clip_vision_encoder(
+    lora_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # clip encoder parameters
+    tile_size: int,
+    patch_size: int,
+    embed_dim: int,
+    num_layers: int,
+    num_heads: int,
+    activation: Callable = nn.SiLU,
+    cls_output_dim: int = 512,
+    attn_bias: bool = True,
+    out_indices: Optional[List[int]] = None,
+    output_cls_projection: bool = False,
+    max_num_tiles: int = 4,
+    in_channels: int = 3,
+    intermediate_act: torch.nn.Module = torch.nn.SiLU(),
+    # LoRA parameters
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> VisionTransformer:
+    """
+    Build a LoRA implementation of the CLIP vision encoder.
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        tile_size (int): The size of your image tiles, if the image was tile-cropped in advance. Otherwise,
+            the size of the input image. In this case, the function will consider your image as a single tile.
+        patch_size (int): The size of each patch. Used to divide the tiles into patches.
+            E.g. for ``patch_size=40``, a tile of shape (400, 400) will have 10x10 grid of patches
+            with shape (40, 40) each.
+        embed_dim (int): The dimensionality of each patch embedding (token).
+        num_layers (int): The number of transformer layers.
+        num_heads (int): The number of attention heads in each transformer layer.
+        activation (Callable): The activation function to use in the MLP layer.
+        cls_output_dim (int): The dimensionality of the output tensor from the CLS projection module.
+        attn_bias (bool): Boolean for if to use bias in the attention module. Default True.
+        out_indices (Optional[List[int]]): The indices of hidden layers to return.
+            If provided, it will return the intermediate results of the transformer layers
+            before they go through a next layer. For example, ``out_indices=[0,3]`` will
+            return the tokens before they go through the first and fourth layers.
+        output_cls_projection (bool): If True, only the CLS token projection will be outputted,
+            instead of all tokens. Defaults to False.
+        max_num_tiles (int): The maximum number of tiles that can be processed. This is used to
+            determine the size of the positional embeddings.
+        in_channels (int): The number of image input channels.
+        intermediate_act (torch.nn.Module): The activation function used in the intermediate layers in the transformer encoder.
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Whether to use DoRA layers instead of LoRA layers. Default is ``False``.
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+        
+
+    Returns:
+        VisionTransformer: Instantiation of VisionTransformer model.
+    """
+    assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
+
+    # TODO: add support for quantizing and LoRA for the final output projection
+    cls_projection = CLSProjection(embed_dim=embed_dim, cls_output_dim=cls_output_dim) if output_cls_projection else None
+
+    # transformer layer
+    self_attn = lora_clip_attention(
+            lora_modules=lora_modules,
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_heads,
+            head_dim=embed_dim // num_heads,
+            attn_dropout=0.0,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+            quantize_base=quantize_base,
+    )
+    if apply_lora_to_mlp:
+        mlp = lora_clip_mlp(
+                in_dim=embed_dim,
+                hidden_dim=4 * embed_dim,
+                out_dim=embed_dim,
+                activation=activation(),
+                lora_rank=lora_rank,
+                lora_alpha=lora_alpha,
+                quantize_base=quantize_base,
+                lora_dropout=lora_dropout,
+                use_dora=use_dora,
+            )
+    else:
+        mlp = clip_mlp(
+            in_dim=embed_dim,
+            hidden_dim=4 * embed_dim,
+            out_dim=embed_dim,
+            activation=activation(),
+            quantize_base=quantize_base,
+        )
+    transformer_layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm= Fp32LayerNorm(embed_dim, eps=1e-5),
+        mlp_norm= Fp32LayerNorm(embed_dim, eps=1e-5),
+        sa_scale=None,
+        mlp_scale=None,
+    )
+
+    # position embeddings
+    if max_num_tiles == 1:
+        pre_tile_pos_embed = None
+        post_tile_pos_embed = None
+        token_pos_embedding = TokenPositionalEmbedding(
+            embed_dim=embed_dim, 
+            patch_size=patch_size, 
+            tile_size=tile_size)
+    else:
+        pre_tile_pos_embed = TilePositionalEmbedding(max_num_tiles=max_num_tiles, embed_dim=embed_dim)
+        post_tile_pos_embed = TilePositionalEmbedding(max_num_tiles=max_num_tiles, embed_dim=embed_dim)
+        token_pos_embedding = TiledTokenPositionalEmbedding(
+            max_num_tiles=max_num_tiles, 
+            embed_dim=embed_dim, 
+            patch_size=patch_size, 
+            tile_size=tile_size)
+
+    model = VisionTransformer(
+        num_layers=num_layers,
+        layer=transformer_layer,
+        token_pos_embedding=token_pos_embedding,
+        pre_tile_pos_embed=pre_tile_pos_embed,
+        post_tile_pos_embed=post_tile_pos_embed,
+        cls_projection=cls_projection,
+        out_indices=out_indices,
+        tile_size=tile_size,
+        patch_size=patch_size,
+        embed_dim=embed_dim,
+        in_channels=in_channels,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to bf16, and offload to CPU on the fly
+        # so as to not increase peak memory
+        model._register_state_dict_hook(
+            partial(reparametrize_as_dtype_state_dict_post_hook, offload_to_cpu=True)
+        )
+
+    return model
+
+
+def lora_clip_attention(
+    lora_modules: List[LORA_ATTN_MODULES],
+    *,
+    # MultiHeadAttention args
+    embed_dim: int,
+    head_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    attn_dropout: float = 0.0,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> MultiHeadAttention:
+    """
+    Return an instance of :func:`~torchtune.modules.MultiHeadAttention` with LoRA
+    applied to a subset of its linear layers
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to. Options are ``{"q_proj", "k_proj", "v_proj",
+            "output_proj"}``.
+        embed_dim (int): embedding dimension for self-attention
+        head_dim (int): dimension of each head in the multihead attention. Usually
+            computed as ``embed_dim // num_heads``.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Whether to use DoRA layers instead of LoRA layers. Default is ``False``.
+        quantize_base (bool): Whether to quantize base model parameters for linear layers
+            LoRA is being applied to. Default is ``False``.
+
+    Returns:
+        MultiHeadAttention: instantiation of self-attention module with LoRA
+        applied to a subset of Q, K, V, output projections.
+
+    Raises:
+        ValueError: If lora_modules arg is an empty list
+    """
+    if not lora_modules:
+        raise ValueError(
+            f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules"
+        )
+
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    q_proj = (
+        adapter_cls(
+            embed_dim,
+            num_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "q_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_heads * head_dim, bias=False)
+        )
+    )
+    k_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "k_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    v_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "v_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            embed_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "output_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, embed_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, embed_dim, bias=False)
+        )
+    )
+
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=q_proj,
+        k_proj=k_proj,
+        v_proj=v_proj,
+        output_proj=output_proj,
+        pos_embeddings=None,
+        attn_dropout=attn_dropout,
+    )
+    return self_attn
+
+
+def lora_clip_mlp(
+    *,
+    in_dim: int,
+    out_dim: int,
+    hidden_dim: int,
+    activation: nn.Module,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> FeedForward:
+    """
+    Build the MLP layer with LoRA applied to the gate and down projections.
+    """
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    gate_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    down_proj = adapter_cls(
+        in_dim=hidden_dim,
+        out_dim=dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=None, activation=activation)
diff -ruN marc_original/third_party/torchtune/torchtune/models/clip/inference/_transform.py marc/third_party/torchtune/torchtune/models/clip/inference/_transform.py
--- marc_original/third_party/torchtune/torchtune/models/clip/inference/_transform.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/clip/inference/_transform.py	2025-02-20 17:49:30.382025613 -0500
@@ -0,0 +1,299 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+from typing import Any, List, Mapping, Optional, Tuple
+
+import torch
+import torchvision
+from PIL import Image
+
+from torchtune.modules.transforms.vision_utils.get_canvas_best_fit import (
+    find_supported_resolutions,
+    get_canvas_best_fit,
+)
+from torchtune.modules.transforms.vision_utils.get_inscribed_size import (
+    get_inscribed_size,
+)
+from torchtune.modules.transforms.vision_utils.pad_dim_to_size import pad_dim_to_size
+from torchtune.modules.transforms.vision_utils.tile_crop import tile_crop
+
+from torchvision.transforms.v2 import functional as F
+
+logger = logging.getLogger(__name__)
+
+
+class _CLIPImageTransform(torch.nn.Module):
+    def __init__(
+        self,
+        resample: str,
+        image_mean: Optional[List[float]],
+        image_std: Optional[List[float]],
+        tile_size: int,
+        max_num_tiles: int,
+        antialias: bool,
+        pad_max_tiles: bool = False,
+    ):
+        super().__init__()
+        self.resample = resample
+        self.image_mean = image_mean
+        self.image_std = image_std
+        self.tile_size = tile_size
+        self.max_num_tiles = max_num_tiles
+        self.pad_tile_size = max_num_tiles if pad_max_tiles else None
+        self.antialias = antialias
+        self.tile_crop = tile_crop
+        self.pad = torch.nn.functional.pad
+
+    def check_variable_bounds_for_export(
+        self,
+        target_size: List[int],
+        canvas_size: List[int],
+        lower: int,
+        upper: int,
+    ) -> None:
+        """
+        Performs torch._checks used to export the model. For eager mode usage, please disregard.
+        The check mitigates data dependent errors that may occur during torch.export. It installs a
+        deferred runtime assert, instead of a compile-time guard. Data dependent errors usually occur
+        in models with data-dependent control flow, eg. via .item(), tolist(), nonzero(). For more
+        context: https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit
+        """
+        # Check lower <= canvas_size <= upper.
+        for var in canvas_size:
+            torch._check(var >= lower)
+            torch._check(var <= upper)
+
+        # Check lower <= target_size <= canvas_size.
+        for i in range(len(target_size)):
+            torch._check(target_size[i] >= lower)
+            torch._check(target_size[i] <= canvas_size[i])
+
+    def forward(
+        self, image: torch.Tensor, target_size: torch.Tensor, canvas_size: torch.Tensor
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        """
+        Performs the core transformations involved in CLIPImageTransform;
+        1. Resize the image to target_size.
+        2. Pad the image to canvas_size.
+        3. Normalize the image using image_mean and image_std.
+        4. Reshape the image tensor into [n, channels, tile_size, tile_size].
+        Args:
+            image (torch.Tensor): image as a 3D tensor in form [C, H, W].
+            target_size (torch.Tensor): tensor of shape (2,) containing the target_height and target_width for resize.
+            canvas_size (torch.Tensor): tensor of shape (2,) containing the canvas_height and canvas_width for padding.
+        Returns:
+            Tuple[torch.Tensor, torch.Tensor]: Image tensor of shape [n, channels, tile_size, tile_size]
+                and aspect ratio tensor of shape [1, 2].
+        """
+
+        target_h, target_w = target_size.tolist()
+        canvas_h, canvas_w = canvas_size.tolist()
+
+        # Checks to allow the model to export via torch.export.
+        self.check_variable_bounds_for_export(
+            target_size=[target_h, target_w],
+            canvas_size=[canvas_h, canvas_w],
+            lower=2,
+            upper=self.tile_size * self.max_num_tiles,
+        )
+
+        # Resize.
+        image = torchvision.transforms._functional_tensor.resize(
+            image,
+            size=[target_h, target_w],
+            interpolation=self.resample,
+            antialias=self.antialias,
+        )
+
+        # Pad, such that the image is on the top-left and padded on the right-bottom.
+        # padding = [left, right, top, bottom]
+        padding = [0, canvas_w - target_w, 0, canvas_h - target_h]
+        output = self.pad(image, padding)
+
+        # Normalize.
+        if self.image_mean is not None and self.image_std is not None:
+            output = F.normalize(output, self.image_mean, self.image_std)
+
+        # Reshape.
+        tiles = self.tile_crop(output, self.tile_size)
+
+        if self.pad_tile_size:
+            tiles = pad_dim_to_size(tiles, size=self.pad_tile_size, dim=0)
+
+        # Calculate aspect ratio.
+        aspect_ratio = canvas_size // self.tile_size
+
+        return tiles, aspect_ratio
+
+
+class CLIPImageTransform:
+    """
+    Note: this class is functionally the same as CLIPImageTransform from torchtune/models/clip/_transforms
+    and should produce identical output results. This version is structured to be (more easily)
+    exported via torch.export for inference use cases in, eg. ExecuTorch or AOTI.
+
+    This class accepts images of any size and dynamically resizes, pads, normalizes and tiles it
+    based on the image aspect ratio and the number of image tiles we allow.
+
+    The algorithm will NOT distort the image to fit a certain aspect ratio, because
+    that leads to a significant degradation in image quality.
+
+    The user can choose if they want to allow upscaling by using the flag ``resize_to_max_canvas``.
+
+    For example, if an input image is of size 300x800, and we want to allow
+    a maximum of 16 image tiles, with side 224px, then:
+
+    If ``resize_to_max_canvas=False``, then:
+    best_resolution = (448, 896) -> smallest canvas, up to 16 tiles, that doesn't require downscaling
+    image is NOT resized
+    image is padded (300, 800) -> 448,896
+    Image is tiled 2x4, for a final output shape of (8, 3, 224, 224)
+
+    If ``resize_to_max_canvas=True``, then:
+    best_resolution = (448, 1344) # canvas that allows maximum upscaling, with minimum padding, up to 16 tiles
+    image is resized without distortion (300,800) -> (448, 1194) #448 is the limiting side for the resize
+    image is padded (448, 1194) -> (448, 1344)
+    Image is tiled 2x5, for a final output shape of (10, 3, 224, 224)
+
+    Args:
+        image_mean (Optional[List[float]]): Mean values of each channel, used for normalization.
+            Should be the same used for the pre-trained model. If None, no normalization is performed. Default None.
+        image_std (Optional[List[float]]): Standard deviation values of each channel, used for normalization.
+            Should be the same used for the pre-trained model. If None, no normalization is performed. Default None.
+        possible_resolutions (Optional[List[Tuple[int, int]]]): List of possible resolutions as tuples (height, width).
+            where each tuple represents a possible canvas to fit the image into when calling ``get_canvas_best_fit``.
+            If None, this will be calculated using max_num_tiles and tile_size. Default None.
+        tile_size (int): Size of the tiles to divide the image into. Default 224.
+        max_num_tiles (Optional[int]): Only used if possible_resolutions is NOT given.
+            Maximum number of tiles to break an image into.
+            This will be used to generate possible_resolutions,
+            e.g. [(224, 224), (224, 448), (448, 224)] if max_num_tiles = 2 and tile_size = 224.
+            Default 4.
+        resample (str): Resampling method used when resizing images. Supports any enum of
+            ``torchvision.transforms.InterpolationMode``, e.g. "nearest", "nearest_exact", "bilinear", "bicubic".
+            Default 'bilinear'.
+        resize_to_max_canvas (bool): "If True, the image will be upscaled without distortion to fit the largest possible
+            resolution from possible_resolutions.
+            If False, it will pick the resolution that minimizes downscaling, including no downscaling at all.
+            In this case, the image will only be upscaled if it's size < tile_size. Default False.
+        antialias (bool): Whether to apply antialiasing when resizing the image. Default True.
+        pad_max_tiles (bool): If True, the image will be padded to have tiles == max_num_tiles.
+    Examples:
+        >>> image_transform = CLIPImageTransform(
+        ...    image_mean=None,
+        ...    image_std=None,
+        ...    tile_size=224,
+        ...    possible_resolutions=None,
+        ...    max_num_tiles=4,
+        ...    resample="bilinear",
+        ...    resize_to_max_canvas=True,
+        ...)
+        >>> # create random image
+        >>> image = (np.random.rand(100,200,3) * 255).astype(np.uint8)
+        >>> image = PIL.Image.fromarray(image)
+        >>> output = image_transform(image)
+        >>> output['image'].shape # [num_tiles, num_channels, tile_size, tile_size]
+        torch.Size([2, 3, 224, 224])
+        >>> output['ar'] # image best fits the canvas 224x448
+        torch.tensor([1,2])
+    """
+
+    def __init__(
+        self,
+        image_mean: Optional[List[float]] = None,
+        image_std: Optional[List[float]] = None,
+        possible_resolutions: Optional[List[Tuple[int, int]]] = None,
+        tile_size: int = 224,
+        max_num_tiles: Optional[int] = 4,
+        resample: str = "bilinear",
+        resize_to_max_canvas: bool = False,
+        antialias: bool = True,
+        pad_max_tiles: bool = False,
+    ) -> None:
+
+        # get_canvas_best_fit
+        assert (
+            possible_resolutions is not None or max_num_tiles is not None
+        ), f"Either possible_resolutions or max_num_tiles must be given. Got {possible_resolutions=} and {max_num_tiles=}"
+
+        # If possible_resolutions are not given, then calculate possible ones based on max_num_tiles
+        if not possible_resolutions and max_num_tiles:
+            possible_resolutions = find_supported_resolutions(
+                max_num_tiles=max_num_tiles, tile_size=tile_size
+            )
+        else:
+            possible_resolutions = possible_resolutions
+
+        self.possible_resolutions = torch.tensor(possible_resolutions).reshape(-1, 2)
+        logger.info(
+            f"Found possible_resolutions: {self.possible_resolutions}. Will fit the images into the canvas with best fit."
+        )
+
+        self.resize_to_max_canvas = resize_to_max_canvas
+        self.max_num_tiles = max_num_tiles
+
+        # normalize
+        assert (image_mean is None) == (
+            image_std is None
+        ), f"Need to provide both or none of image_mean and image_std. Got {image_mean=} and {image_std=}"
+        self.image_mean = image_mean
+        self.image_std = image_std
+
+        # resize
+        self.max_size = None if resize_to_max_canvas else tile_size
+        self.resample = resample
+        self.antialias = antialias
+
+        # tile_crop
+        self.tile_size = tile_size
+
+        self.core_transform = _CLIPImageTransform(
+            resample=self.resample,
+            image_mean=self.image_mean,
+            image_std=self.image_std,
+            tile_size=self.tile_size,
+            max_num_tiles=self.max_num_tiles,
+            antialias=self.antialias,
+            pad_max_tiles=pad_max_tiles,
+        )
+
+    def __call__(self, *, image: Image.Image, **kwargs) -> Mapping[str, Any]:
+
+        assert isinstance(image, Image.Image), "Input image must be a PIL image."
+
+        # Make image torch.tensor((3, H, W), dtype='float32'), 0<=values<=1.
+        image_tensor = F.to_dtype(
+            F.grayscale_to_rgb_image(F.to_image(image)), scale=True
+        )
+
+        # Find the best canvas to fit the image without distortion.
+        best_resolution = get_canvas_best_fit(
+            image=image_tensor,
+            possible_resolutions=self.possible_resolutions,
+            resize_to_max_canvas=self.resize_to_max_canvas,
+        )
+
+        # Find the dimensions of the image, such that it is inscribed within best_resolution.
+        inscribed_size = get_inscribed_size(
+            image_tensor.shape[-2:], best_resolution, self.max_size
+        )
+
+        # Call _CLIPImageTransform to perform resize, pad, normalize and reshape transforms.
+        tiles, aspect_ratio = self.core_transform(
+            image=image_tensor,
+            target_size=torch.tensor(inscribed_size),
+            canvas_size=torch.tensor(best_resolution),
+        )
+
+        kwargs.update(
+            {
+                "image": tiles,
+                "aspect_ratio": aspect_ratio,
+            }
+        )
+
+        return kwargs
diff -ruN marc_original/third_party/torchtune/torchtune/models/clip/__init__.py marc/third_party/torchtune/torchtune/models/clip/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/clip/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/clip/__init__.py	2025-02-20 17:49:30.362025580 -0500
@@ -0,0 +1,23 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import clip_mlp, clip_vision_encoder
+
+from ._position_embeddings import (
+    TiledTokenPositionalEmbedding,
+    TilePositionalEmbedding,
+    TokenPositionalEmbedding,
+)
+from ._transform import CLIPImageTransform
+
+__all__ = [
+    "clip_mlp",
+    "clip_vision_encoder",
+    "CLIPImageTransform",
+    "TokenPositionalEmbedding",
+    "TiledTokenPositionalEmbedding",
+    "TilePositionalEmbedding",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/clip/_model_builders.py marc/third_party/torchtune/torchtune/models/clip/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/clip/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/clip/_model_builders.py	2025-02-20 17:49:30.370025594 -0500
@@ -0,0 +1,14 @@
+from torchtune.models.clip._transforms import CLIPImageTransform
+
+def clip_vit_224_transform():
+    image_transform = CLIPImageTransform(
+        image_mean=[0.48145466, 0.4578275, 0.40821073],
+        image_std=[0.26862954, 0.26130258, 0.27577711],
+        tile_size=224,
+        possible_resolutions=None,
+        max_num_tiles=1,
+        resample="bilinear",
+        resize_to_max_canvas=True,
+    )
+
+    return image_transform
diff -ruN marc_original/third_party/torchtune/torchtune/models/clip/_position_embeddings.py marc/third_party/torchtune/torchtune/models/clip/_position_embeddings.py
--- marc_original/third_party/torchtune/torchtune/models/clip/_position_embeddings.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/clip/_position_embeddings.py	2025-02-20 17:49:30.374025601 -0500
@@ -0,0 +1,662 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+from typing import Any, Dict, Tuple
+
+import torch
+import torch.nn.functional as F
+from torch import nn
+from torch.distributed._tensor import distribute_tensor, DTensor
+
+
+class TokenPositionalEmbedding(nn.Module):
+    """
+    Token positional embedding for images, different for every token in an image.
+
+    Notice that tile is different from patch (token). For details, please check the documentation of
+    :class:`torchtune.modules.vision_transformer.VisionTransformer`.
+
+    Args:
+        embed_dim (int): The dimensionality of each token embedding.
+        tile_size (int): The size of your image tiles, if the image was tile-cropped in advance. Otherwise,
+            the size of the input image. In this case, the function will consider your image as a single tile.
+        patch_size (int): The size of each patch. Used to divide the tiles into patches.
+            E.g. for ``patch_size=40``, a tile of shape (400, 400) will have 10x10 grid of patches
+            with shape (40, 40) each.
+    """
+
+    def __init__(self, embed_dim: int, tile_size: int, patch_size: int) -> None:
+        super().__init__()
+        patch_grid_size = tile_size // patch_size
+        n_tokens_per_tile = patch_grid_size**2 + 1  # +1 for cls token
+        scale = embed_dim**-0.5
+        self.positional_embedding = nn.Parameter(
+            scale * torch.randn((n_tokens_per_tile, embed_dim))
+        )
+
+    def forward(self, x: torch.Tensor, *args: Tuple[Any]) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): torch.Tensor with shape (..., n_tokens_per_tile, embed_dim)
+            *args (Tuple[Any]): Optional args.
+
+        Returns:
+            torch.Tensor: The input tensor with added positional embeddings.
+        """
+        return x + self.positional_embedding
+
+
+class TiledTokenPositionalEmbedding(nn.Module):
+    """
+
+    Token positional embedding for tiled images, different for every tile, different for every token.
+
+    There are two positional embeddings in this module:
+
+    * local_token_positional_embedding: same for every tile, different for every token. Equivalent \
+        to :class:`torchtune.models.clip._position_embeddings.TokenPositionalEmbedding`, but gated.
+    * global_token_positional_embedding: different for every tile, different for every token.
+
+    Notice that tile is different from patch (token). For details, please check the documentation of
+    :class:`torchtune.modules.vision_transformer.VisionTransformer`.
+
+    Args:
+        max_num_tiles (int): The maximum number of tiles an image can be divided into.
+        embed_dim (int): The dimensionality of each token embedding.
+        tile_size (int): The size of your image tiles, if the image was tile-cropped in advance. Otherwise,
+            the size of the input image. In this case, the function will consider your image as a single tile.
+        patch_size (int): The size of each patch. Used to divide the tiles into patches.
+            E.g. for ``patch_size=40``, a tile of shape (400, 400) will have 10x10 grid of patches
+            with shape (40, 40) each.
+    """
+
+    def __init__(
+        self, max_num_tiles: int, embed_dim: int, tile_size: int, patch_size: int
+    ) -> None:
+        super().__init__()
+
+        patch_grid_size = tile_size // patch_size
+        self.n_tokens_per_tile = patch_grid_size**2 + 1  # +1 for cls token
+        scale = embed_dim**-0.5
+
+        # different for every token, same for every tile
+        self.local_token_positional_embedding = nn.Parameter(
+            scale * torch.randn((self.n_tokens_per_tile, embed_dim))
+        )
+
+        # different for every token, different for every tile
+        self.global_token_positional_embedding = nn.Parameter(
+            scale
+            * torch.randn(
+                max_num_tiles,
+                max_num_tiles,
+                self.n_tokens_per_tile,
+                embed_dim,
+            )
+        )
+
+        self.gate = nn.Parameter(torch.zeros(1))
+
+        self._register_load_state_dict_pre_hook(self._load_state_dict_hook)
+
+    @torch.no_grad()
+    def _load_state_dict_hook(
+        self,
+        state_dict: Dict[str, Any],
+        prefix: str,
+        *args: Tuple[Any],
+        **kwargs: Dict[str, Any],
+    ) -> None:
+        """
+        Interpolates positional embeddings to accomodate different number of tiles
+        and tokens per tile, in case the model was instantiated with different
+        settings than the one you are loading the state dict from.
+
+        For more info, please check self._resize_local_position_embedding and
+        self._resize_global_position_embedding functions.
+
+        Args:
+            state_dict (Dict[str, Any]): The state dict to load.
+            prefix (str): The prefix of the state dict.
+            *args (Tuple[Any]): Additional positional arguments.
+            **kwargs (Dict[str, Any]): Additional keyword arguments.
+
+        Raises:
+            ValueError: if loaded local or global embedding n_tokens_per_tile is not derived
+                from a squared grid.
+            ValueError: if after interpolation, the shape of the loaded local embedding
+                is not compatible with the current embedding.
+            ValueError: if after interpolation, the shape of the loaded global embedding
+                is not compatible with the current embedding.
+        """
+
+        # process local_token_positional_embedding
+        inpt_local_pos_embed = state_dict.get(
+            prefix + "local_token_positional_embedding"
+        )
+
+        if inpt_local_pos_embed is not None:
+
+            # We can only apply F.interpolate to vanilla tensors, not DTensors
+            # If pos embeds are a DTensor, we gather the full tensor, apply
+            # interpolate, and then reshard after
+            if isinstance(inpt_local_pos_embed, DTensor):
+                local_embed_is_sharded = True
+                local_embed_device_mesh = inpt_local_pos_embed.device_mesh
+                local_embed_placements = inpt_local_pos_embed.placements
+                inpt_local_pos_embed = inpt_local_pos_embed.full_tensor()
+            else:
+                local_embed_is_sharded = False
+
+            # sanity check
+            inpt_n_tokens_per_tile, inpt_embed_dim = inpt_local_pos_embed.shape
+            if math.sqrt(inpt_n_tokens_per_tile - 1) % 1 != 0:
+                raise ValueError(
+                    f"Loaded local positional embedding has shape {inpt_n_tokens_per_tile=}, "
+                    f"which indicates a grid_size that is not squared. This is currently not supported."
+                )
+
+            # instantiated pos emb
+            (
+                tgt_n_tokens_per_tile,
+                tgt_embed_dim,
+            ) = self.local_token_positional_embedding.shape
+
+            # resize ckpt to match instantiated shape
+            inpt_local_pos_embed = self._resize_local_position_embedding(
+                local_pos_embed=inpt_local_pos_embed,
+                tgt_patch_grid_size=int(math.sqrt(tgt_n_tokens_per_tile - 1)),
+            )
+
+            if local_embed_is_sharded:
+                inpt_local_pos_embed = distribute_tensor(
+                    inpt_local_pos_embed,
+                    device_mesh=local_embed_device_mesh,
+                    placements=local_embed_placements,
+                )
+
+            # update state dict
+            state_dict[
+                prefix + "local_token_positional_embedding"
+            ] = inpt_local_pos_embed
+            if (
+                inpt_local_pos_embed.shape
+                != self.local_token_positional_embedding.shape
+            ):
+                raise ValueError(
+                    f"Loaded local positional embedding has shape {inpt_local_pos_embed.shape}, "
+                    f"after interpolation. Expected shape {self.local_token_positional_embedding.shape}."
+                )
+
+        # process global_token_positional_embedding
+        inpt_global_pos_embed = state_dict.get(
+            prefix + "global_token_positional_embedding"
+        )
+
+        if inpt_global_pos_embed is not None:
+
+            # We can only apply F.interpolate to vanilla tensors, not DTensors
+            # If pos embeds are a DTensor, we gather the full tensor, apply
+            # interpolate, and then reshard after
+            if isinstance(inpt_global_pos_embed, DTensor):
+                global_embed_is_sharded = True
+                global_embed_device_mesh = inpt_global_pos_embed.device_mesh
+                global_embed_placements = inpt_global_pos_embed.placements
+                inpt_global_pos_embed = inpt_global_pos_embed.full_tensor()
+            else:
+                global_embed_is_sharded = False
+
+            _, _, inpt_n_tokens_per_tile, _ = inpt_global_pos_embed.shape
+
+            # sanity check
+            if math.sqrt(inpt_n_tokens_per_tile - 1) % 1 != 0:
+                raise ValueError(
+                    f"Loaded local positional embedding has shape {inpt_n_tokens_per_tile=}, "
+                    f"which indicates a grid_size that is not squared. This is currently not supported."
+                )
+
+            # instantiated pos emb
+            (
+                tgt_max_num_tiles_x,
+                tgt_max_num_tiles_y,  # not used, same as tgt_max_num_tiles_x
+                tgt_n_tokens_per_tile,
+                tgt_embed_dim,
+            ) = self.global_token_positional_embedding.shape
+
+            # resize ckpt to match instantiated shape
+            inpt_global_pos_embed = self._resize_global_position_embedding(
+                global_pos_embed=inpt_global_pos_embed,
+                tgt_max_num_tiles=tgt_max_num_tiles_x,
+                tgt_patch_grid_size=int(math.sqrt(tgt_n_tokens_per_tile - 1)),
+            )
+
+            if global_embed_is_sharded:
+                inpt_global_pos_embed = distribute_tensor(
+                    inpt_global_pos_embed,
+                    device_mesh=global_embed_device_mesh,
+                    placements=global_embed_placements,
+                )
+
+            # update state dict
+            state_dict[
+                prefix + "global_token_positional_embedding"
+            ] = inpt_global_pos_embed
+            if (
+                inpt_global_pos_embed.shape
+                != self.global_token_positional_embedding.shape
+            ):
+                raise ValueError(
+                    f"Loaded global positional embedding has shape {inpt_global_pos_embed.shape}, "
+                    f"after interpolation. Expected shape {self.global_token_positional_embedding.shape}."
+                )
+
+    @staticmethod
+    def _resize_local_position_embedding(
+        local_pos_embed: torch.Tensor, tgt_patch_grid_size: int
+    ) -> torch.Tensor:
+        """
+        Interpolates the local position embedding for a vision encoder to accommodate
+        a different number of tokens per tile. This is the only dimension that
+        changes during interpolation.
+
+        Args:
+            local_pos_embed (torch.Tensor): The position embeddings tensor to be resized. It
+                has shape [n_tokens_per_tile, emb_dim], where the first token is the CLS token
+                and n_tokens_per_tile = patch_grid_size**2 + 1.
+            tgt_patch_grid_size (int): The target size of each patch grid, i.e.,
+                the square root of the number of tokens per tile, excluding the class token.
+
+        Returns:
+            torch.Tensor: The resized position embeddings tensor of shape
+                [tgt_n_tokens_per_tile, dim], where tgt_n_tokens_per_tile = tgt_patch_grid_size**2 + 1.
+
+        Example:
+            >>> import torch
+            >>> import math
+            >>> local_pos_embed = torch.randn((10*10+1, 64))  # Example input tensor
+            >>> tgt_patch_grid_size = 20  # Target number of tokens per tile
+            >>> resized_pos_embed = _resize_local_position_embedding(local_pos_embed, tgt_patch_grid_size)
+            >>> print(resized_pos_embed.shape)
+            torch.Size([20*20+1, 64])
+        """
+        # inverse n_tokens_per_tile = patch_grid_size**2 + 1, where +1 is the cls token
+        inpt_n_tokens_per_tile, inpt_embed_dim = local_pos_embed.shape
+        inpt_patch_grid_size = int(math.sqrt(inpt_n_tokens_per_tile - 1))
+
+        # split tokens between cls and img tokens.
+        # we don't want to interpolate cls token.
+        cls_token, local_pos_embed = (
+            local_pos_embed[[0]],  # cls token
+            local_pos_embed[1:],  # image tokens
+        )
+
+        # we reshape n_tokens_per_tile - 1 --> (inpt_patch_grid_size, inpt_patch_grid_size)
+        # and permute to have inpt_patch_grid_size as the last two dimensions
+        # we also add a batch dim to the tensor, since F.interpolate expects it
+        local_pos_embed = local_pos_embed.reshape(
+            1, inpt_patch_grid_size, inpt_patch_grid_size, -1
+        ).permute(0, 3, 1, 2)
+
+        local_pos_embed = F.interpolate(
+            local_pos_embed,
+            size=[tgt_patch_grid_size, tgt_patch_grid_size],
+            mode="bilinear",
+            align_corners=True,  # defaults from internal-llama-models
+        )
+
+        # reshape back to [1, tokens_per_tile, embed_dim]
+        local_pos_embed = local_pos_embed.permute(0, 2, 3, 1).reshape(
+            1, -1, inpt_embed_dim
+        )
+
+        # remove batch dim added previously
+        local_pos_embed = local_pos_embed.squeeze(0)
+
+        # add cls token back in
+        local_pos_embed = torch.cat([cls_token, local_pos_embed], dim=0)
+
+        return local_pos_embed
+
+    # TODO: Switch to public method after 2.5 is stable
+    @staticmethod
+    def _resize_global_position_embedding(
+        global_pos_embed: torch.Tensor,
+        tgt_max_num_tiles: int,
+        tgt_patch_grid_size: int,
+    ) -> torch.Tensor:
+        """
+        Interpolates the global position embedding for a vision encoder to accommodate new grid dimensions.
+        The embedding dimension is not changed during interpolation, only max_num_tiles and num_tokens_per_tile.
+
+        Args:
+            global_pos_embed (torch.Tensor): The input global position embeddings tensor of shape
+                [max_num_tiles_x, max_num_tiles_y, num_tokens_per_tile, embed_dim],
+                where num_tokens_per_tile = inpt_patch_grid_size * inpt_patch_grid_size + 1 (CLS token), and
+                max_num_tiles_x == max_num_tiles_y.
+            tgt_max_num_tiles (int): The target maximum number of tiles along one dimension (assumed square grid).
+            tgt_patch_grid_size (int): The target size of each patch grid, i.e., the square root of the number of tokens
+                per tile, excluding the class token.
+
+
+        Returns:
+            torch.Tensor: The resized global position embeddings tensor of shape
+                [tgt_max_num_tiles, tgt_max_num_tiles, tgt_patch_grid_size * tgt_patch_grid_size + 1, embed_dim].
+
+        Example:
+            >>> import torch
+            >>> global_pos_embed = torch.arange(3*3*(2*2+1)*4).reshape((3, 3, 2*2+1, 4))  # Example input tensor
+            >>> tgt_max_num_tiles = 2  # Target maximum number of tiles
+            >>> tgt_patch_grid_size = 3  # Target patch grid size
+            >>> resized_global_pos_embed = (
+            >>> _resize_global_position_embedding(global_pos_embed, tgt_max_num_tiles, tgt_patch_grid_size))
+            >>> print(resized_global_pos_embed.shape)
+            torch.Size([2, 2, 3*3+1, 4])
+        """
+
+        # remove cls token to interpolate it separately
+        pos_embed = global_pos_embed[:, :, 1:, :]
+        cls_embed = global_pos_embed[:, :, [0], :]
+
+        (
+            max_num_tiles_x,
+            max_num_tiles_y,
+            n_tokens_per_tile,
+            embed_dim,
+        ) = pos_embed.shape
+
+        # tokens_per_tile == inpt_patch_grid_size**2
+        # we reshape n_tokens_per_tile --> (inpt_patch_grid_size, inpt_patch_grid_size)
+        inpt_patch_grid_size = int(math.sqrt(n_tokens_per_tile))
+        pos_embed = pos_embed.reshape(
+            max_num_tiles_x,
+            max_num_tiles_y,
+            inpt_patch_grid_size,
+            inpt_patch_grid_size,
+            embed_dim,
+        )
+
+        # combine max_num_tiles and patch_grid_size into one dimension
+        pos_embed = pos_embed.permute(0, 2, 1, 3, 4).contiguous()
+        pos_embed = pos_embed.reshape(
+            max_num_tiles_x * inpt_patch_grid_size,
+            max_num_tiles_y * inpt_patch_grid_size,
+            embed_dim,
+        )
+
+        # add batch dim for interpolation
+        pos_embed = pos_embed.unsqueeze(0)
+
+        tgt_size = (
+            int(tgt_max_num_tiles * tgt_patch_grid_size),
+            int(tgt_max_num_tiles * tgt_patch_grid_size),
+        )
+
+        # move to the last two dim for interpolation
+        pos_embed = pos_embed.permute(0, 3, 1, 2)
+        pos_embed = F.interpolate(
+            pos_embed,
+            size=tgt_size,
+            mode="bilinear",
+            align_corners=True,  # defaults from internal-llama-models
+        )
+
+        # return to original shape and remove batch dim
+        pos_embed = pos_embed.permute(0, 2, 3, 1).squeeze(0)
+
+        # move it back in place
+        pos_embed = pos_embed.view(
+            tgt_max_num_tiles,
+            tgt_patch_grid_size,
+            tgt_max_num_tiles,
+            tgt_patch_grid_size,
+            embed_dim,
+        )
+        pos_embed = pos_embed.permute(0, 2, 1, 3, 4).contiguous()
+        pos_embed = pos_embed.view(
+            tgt_max_num_tiles,
+            tgt_max_num_tiles,
+            int(tgt_patch_grid_size**2),
+            embed_dim,
+        )
+
+        # interpolate cls token
+        cls_embed = cls_embed.permute(2, 3, 0, 1)
+        cls_embed_resized = F.interpolate(
+            cls_embed,
+            size=(tgt_max_num_tiles, tgt_max_num_tiles),
+            mode="bilinear",
+            align_corners=True,  # defaults from internal-llama-models
+        )
+        cls_embed = cls_embed_resized.permute(2, 3, 0, 1)
+
+        # add cls token back in
+        global_pos_embed = torch.cat([cls_embed, pos_embed], dim=2)
+
+        return global_pos_embed
+
+    def forward(self, x: torch.Tensor, aspect_ratio: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): torch.Tensor with shape
+                (bsz * n_imgs, n_tiles, n_tokens_per_tile, embed_dim).
+            aspect_ratio (torch.Tensor): torch.Tensor with shape (bsz * n_imgs, 2),
+                where aspect_ratio[k] represents the aspect ratio of the k^th image
+                of the batch before tile-cropping,  e.g. aspect_ratio[k] = (2,1).
+        Returns:
+            torch.Tensor: The input tensor with added positional embeddings.
+        """
+        bsz_and_n_imgs, n_tiles, n_tokens_per_tile, embed_dim = x.shape
+
+        # apply local position embedding (same for every tile)
+        x = x + (self.local_token_positional_embedding * (1 - self.gate.tanh()))
+
+        # apply global positional embedding (different for every tile)
+        x = x.view(bsz_and_n_imgs, n_tiles, n_tokens_per_tile, embed_dim)
+        for batch_idx, (n_tiles_h, n_tiles_w) in enumerate(aspect_ratio):
+            # When we batch images, all are padded to the same amount of tiles.
+            # The aspect_ratio lets us know the non padded tiles for each image.
+            # We only add positional encoding to those.
+            n_non_padded_tiles = int(n_tiles_h * n_tiles_w)
+
+            # We get only the positional encoding for non padded tiles,
+            # i.e. n_tiles_h, n_tiles_w.
+            pos_embed = self.global_token_positional_embedding[
+                :n_tiles_h, :n_tiles_w, :, :
+            ]
+
+            # Add pos encoding to the non padded tiles.
+            pos_embed = pos_embed.reshape(
+                n_non_padded_tiles, self.n_tokens_per_tile, embed_dim
+            )
+            pos_embed = pos_embed * self.gate.tanh()
+            x[batch_idx, :n_non_padded_tiles, :, :] += pos_embed
+
+        return x
+
+
+class TilePositionalEmbedding(nn.Module):
+    """
+    Positional embedding for tiles, different for every tile, same for every token within a tile.
+
+    Notice that tile is different from patch (token). For details, please check the documentation of
+    :class:`torchtune.modules.vision_transformer.VisionTransformer`.
+
+    Args:
+        max_num_tiles (int): The maximum number of tiles an image can be divided into.
+        embed_dim (int): The dimensionality of each tile embedding.
+    """
+
+    def __init__(
+        self,
+        max_num_tiles: int,
+        embed_dim: int,
+    ):
+        super().__init__()
+        self.embed_dim = embed_dim
+
+        scale = embed_dim**-0.5
+        self.embedding = nn.Parameter(
+            scale * torch.randn(max_num_tiles, max_num_tiles, 1, embed_dim)
+        )
+        self.gate = nn.Parameter(torch.zeros(1))
+
+        # Register load hook to interpolate positional embeddings
+        self._register_load_state_dict_pre_hook(self._load_state_dict_hook)
+
+    # TODO: Switch to public method after 2.5 is stable
+    @torch.no_grad()
+    def _load_state_dict_hook(
+        self,
+        state_dict: Dict[str, Any],
+        prefix: str,
+        *args: Tuple[Any],
+        **kwargs: Dict[str, Any],
+    ):
+        """
+        Interpolates positional embeddings to accomodate different number of tiles,
+        in case the model was instantiated with different
+        settings than the one you are loading the state dict from.
+
+        For more info, check self._dynamic_resize function.
+
+        Args:
+            state_dict (Dict[str, Any]): The state dict to load.
+            prefix (str): The prefix of the state dict.
+            *args (Tuple[Any]): Additional positional arguments.
+            **kwargs (Dict[str, Any]): Additional keyword arguments.
+
+        Raises:
+            ValueError: if the shape of the loaded embedding is not compatible with the current embedding.
+            ValueError: if max_num_tiles_x, max_num_tiles_y are not equal.
+            ValueError: if after interpolation, the shape of the loaded embedding is not compatible with the current embedding.
+        """
+
+        embedding = state_dict.get(prefix + "embedding")
+
+        if embedding is not None:
+
+            # We can only apply F.interpolate to vanilla tensors, not DTensors
+            # If pos embeds are a DTensor, we gather the full tensor, apply
+            # interpolate, and then reshard after
+            if isinstance(embedding, DTensor):
+                embedding_is_sharded = True
+                device_mesh = embedding.device_mesh
+                placements = embedding.placements
+                embedding = embedding.full_tensor()
+            else:
+                embedding_is_sharded = False
+
+            # ckpt pos emb
+            (
+                tgt_max_num_tiles_x,
+                tgt_max_num_tiles_y,
+                tgt_num_tokens,
+                tgt_emb,
+            ) = self.embedding.shape
+
+            # instantiated pos emb
+            (
+                inpt_max_num_tiles_x,
+                inpt_max_num_tiles_y,
+                inpt_num_tokens,
+                inpt_emb,
+            ) = state_dict[prefix + "embedding"].shape
+
+            # sanity check
+            if inpt_num_tokens != tgt_num_tokens or inpt_emb != tgt_emb:
+                raise ValueError(
+                    "Expected embedding shape to be (..., num_tokens, tgt_emb) to match"
+                    f" but found shapes {self.embedding.shape} and {state_dict[prefix+'embedding'].shape}"
+                )
+
+            if inpt_max_num_tiles_x != inpt_max_num_tiles_y:
+                raise ValueError(
+                    "Expected max_num_tiles_x, max_num_tiles_y to be equal but found, but found"
+                    f"(max_num_tiles_x, max_num_tiles_y, 1, embed_dim) = {self.embedding.shape}"
+                )
+
+            # resize ckpt to match instantiated shape
+            embedding_new = self._resize_position_embedding(
+                embedding, tgt_max_num_tiles=tgt_max_num_tiles_x
+            )
+
+            if embedding_is_sharded:
+                embedding_new = distribute_tensor(
+                    embedding_new,
+                    device_mesh=device_mesh,
+                    placements=placements,
+                )
+
+            # update state dict
+            state_dict[prefix + "embedding"] = embedding_new
+            if embedding_new.shape != self.embedding.shape:
+                raise ValueError(
+                    "Expected embedding shape and embedding_new.shape to match"
+                    f" but found shapes {self.embedding.shape} and {embedding_new.shape}"
+                )
+
+    @staticmethod
+    def _resize_position_embedding(
+        embedding: torch.Tensor, tgt_max_num_tiles: int
+    ) -> torch.Tensor:
+        """
+        Interpolates positional embeddings to accomodate a different max_num_tiles. These
+        are the only dimensions that changes during interpolation.
+
+        Args:
+            embedding (torch.Tensor): torch.Tensor with shape (max_num_tiles, max_num_tiles, 1, embed_dim
+            tgt_max_num_tiles (int): The number of tiles to resize to.
+
+        Returns:
+            torch.Tensor: The resized embedding.
+
+        Example:
+            >>> import torch
+            >>> # create dummy embedding
+            >>> embedding = torch.arange(2*2*2*2).reshape(2, 2, 2, 2).float()
+            >>> resized_embed = _dynamic_resize(embedding, tgt_max_num_tiles=1)
+            >>> print(resized_embed.shape)
+            >>> torch.Size([1, 1, 2, 2])
+        """
+        # set max_num_tiles to the last dimension
+        embedding = embedding.permute(2, 3, 0, 1)
+
+        embedding = F.interpolate(
+            embedding,
+            size=(tgt_max_num_tiles, tgt_max_num_tiles),
+            mode="bilinear",
+            align_corners=True,
+        )
+        # permute to the original shape
+        embedding = embedding.permute(2, 3, 0, 1)
+        return embedding
+
+    def forward(self, x: torch.Tensor, aspect_ratio: torch.Tensor) -> torch.Tensor:
+        """
+        args:
+            x (torch.Tensor): torch.Tensor with shape (bsz * n_imgs, n_tiles, n_tokens_per_tile, embed_dim).
+            aspect_ratio (torch.Tensor): torch.Tensor with shape (bsz * n_imgs, 2),
+                representing the aspect ratio of the image before tile-cropping, e.g. (2,1).
+        returns:
+            torch.Tensor: The input tensor with added positional embeddings.
+        """
+
+        for batch_idx, (n_tiles_h, n_tiles_w) in enumerate(aspect_ratio):
+            # When we batch images, all are padded to the same amount of tiles.
+            # The aspect_ratio lets us know the non padded tiles for each image.
+            # We only add positional encoding to those.
+            n_non_padded_tiles = int(n_tiles_h * n_tiles_w)
+
+            # We get only the positional encoding for non padded tiles,
+            # i.e. n_tiles_h, n_tiles_w.
+            pos_embed = self.embedding[:n_tiles_h, :n_tiles_w, :, :]
+
+            # Add pos encoding to the non padded tiles.
+            pos_embed = pos_embed.reshape(n_non_padded_tiles, 1, self.embed_dim)
+            x[batch_idx, :n_non_padded_tiles, :, :] += pos_embed * self.gate.tanh()
+
+        return x
diff -ruN marc_original/third_party/torchtune/torchtune/models/clip/_transform.py marc/third_party/torchtune/torchtune/models/clip/_transform.py
--- marc_original/third_party/torchtune/torchtune/models/clip/_transform.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/clip/_transform.py	2025-02-20 17:49:30.378025607 -0500
@@ -0,0 +1,205 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+from typing import Any, List, Mapping, Optional, Tuple
+
+import torch
+import torchvision
+from PIL import Image
+
+from torchtune.modules.transforms.vision_utils.get_canvas_best_fit import (
+    find_supported_resolutions,
+    get_canvas_best_fit,
+)
+from torchtune.modules.transforms.vision_utils.pad_dim_to_size import pad_dim_to_size
+from torchtune.modules.transforms.vision_utils.resize_with_pad import resize_with_pad
+from torchtune.modules.transforms.vision_utils.tile_crop import tile_crop
+
+from torchvision.transforms.v2 import functional as F
+
+logger = logging.getLogger(__name__)
+
+
+class CLIPImageTransform:
+    """
+    This class accepts images of any size and dynamically resizes, pads, normalizes and tiles it
+    based on the image aspect ratio and the number of image tiles we allow.
+
+    The algorithm will NOT distort the image to fit a certain aspect ratio, because
+    that leads to a significant degradation in image quality.
+
+    The user can choose if they want to allow upscaling by using the flag ``resize_to_max_canvas``.
+
+    For example, if an input image is of size 300x800, and we want to allow
+    a maximum of 16 image tiles, with side 224px, then:
+
+    If ``resize_to_max_canvas=False``, then:
+    best_resolution = (448, 896) -> smallest canvas, up to 16 tiles, that doesn't require downscaling
+    image is NOT resized
+    image is padded (300, 800) -> 448,896
+    Image is tiled 2x4, for a final output shape of (8, 3, 224, 224)
+
+    If ``resize_to_max_canvas=True``, then:
+    best_resolution = (448, 1344) # canvas that allows maximum upscaling, with minimum padding, up to 16 tiles
+    image is resized without distortion (300,800) -> (448, 1194) #448 is the limiting side for the resize
+    image is padded (448, 1194) -> (448, 1344)
+    Image is tiled 2x5, for a final output shape of (10, 3, 224, 224)
+
+    Args:
+        image_mean (Optional[List[float]]): Mean values of each channel, used for normalization.
+            Should be the same used for the pre-trained model. If None, no normalization is performed. Default None.
+        image_std (Optional[List[float]]): Standard deviation values of each channel, used for normalization.
+            Should be the same used for the pre-trained model. If None, no normalization is performed. Default None.
+        possible_resolutions (Optional[List[Tuple[int, int]]]): List of possible resolutions as tuples (height, width).
+            where each tuple represents a possible canvas to fit the image into when calling ``get_canvas_best_fit``.
+            If None, this will be calculated using max_num_tiles and tile_size. Default None.
+        tile_size (int): Size of the tiles to divide the image into. Default 224.
+        max_num_tiles (Optional[int]): Only used if possible_resolutions is NOT given.
+            Maximum number of tiles to break an image into.
+            This will be used to generate possible_resolutions,
+            e.g. [(224, 224), (224, 448), (448, 224)] if max_num_tiles = 2 and tile_size = 224.
+            Default 4.
+        pad_max_tiles (bool): If True, the image will be padded to have tiles == max_num_tiles. Default False.
+        dtype (torch.dtype): Data type of the output image. Default torch.bfloat16.
+        resample (str): Resampling method used when resizing images. Supports any enum of
+            ``torchvision.transforms.InterpolationMode``, e.g. "nearest", "nearest_exact", "bilinear", "bicubic".
+            Default 'bilinear'.
+        resize_to_max_canvas (bool): "If True, the image will be upscaled without distortion to fit the largest possible
+            resolution from possible_resolutions.
+            If False, it will pick the resolution that minimizes downscaling, including no downscaling at all.
+            In this case, the image will only be upscaled if it's size < tile_size. Default False.
+
+    Examples:
+        >>> image_transform = CLIPImageTransform(
+        ...    image_mean=None,
+        ...    image_std=None,
+        ...    tile_size=224,
+        ...    possible_resolutions=None,
+        ...    max_num_tiles=4,
+        ...    resample="bilinear",
+        ...    resize_to_max_canvas=True,
+        ...)
+        >>> # create random image
+        >>> image = (np.random.rand(100,200,3) * 255).astype(np.uint8)
+        >>> image = PIL.Image.fromarray(image)
+        >>> output = image_transform(image)
+        >>> output['image'].shape # [num_tiles, num_channels, tile_size, tile_size]
+        torch.Size([2, 3, 224, 224])
+        >>> output['ar'] # image best fits the canvas 224x448
+        torch.tensor([1,2])
+    """
+
+    def __init__(
+        self,
+        *,
+        image_mean: Optional[List[float]] = None,
+        image_std: Optional[List[float]] = None,
+        possible_resolutions: Optional[List[Tuple[int, int]]] = None,
+        tile_size: int = 224,
+        max_num_tiles: Optional[int] = 4,
+        pad_max_tiles: bool = False,
+        dtype: torch.dtype = torch.bfloat16,
+        resample: str = "bilinear",
+        resize_to_max_canvas: bool = False,
+    ) -> None:
+
+        # get_canvas_best_fit
+        assert (
+            possible_resolutions is not None or max_num_tiles is not None
+        ), f"Either possible_resolutions or max_num_tiles must be given. Got {possible_resolutions=} and {max_num_tiles=}"
+
+        # If possible_resolutions are not given, then calculate possible ones based on max_num_tiles
+        if not possible_resolutions and max_num_tiles:
+            possible_resolutions = find_supported_resolutions(
+                max_num_tiles=max_num_tiles, tile_size=tile_size
+            )
+        else:
+            possible_resolutions = possible_resolutions
+
+        self.possible_resolutions = torch.tensor(possible_resolutions).reshape(-1, 2)
+        logger.debug(
+            f"Found possible_resolutions: {self.possible_resolutions}. Will fit the images into the canvas with best fit."
+        )
+
+        self.resize_to_max_canvas = resize_to_max_canvas
+
+        # normalize
+        assert (image_mean is None) == (
+            image_std is None
+        ), f"Need to provide both or none of image_mean and image_std. Got {image_mean=} and {image_std=}"
+        self.mean = image_mean
+        self.std = image_std
+
+        # resize_with_pad
+        self.max_size = None if resize_to_max_canvas else tile_size
+        self.dtype = dtype
+        self.resample = torchvision.transforms.InterpolationMode[resample.upper()]
+
+        # tile_crop
+        self.tile_size = tile_size
+        self.tile_crop = tile_crop
+        self.pad_tile_size = max_num_tiles if pad_max_tiles else None
+
+    def __call__(
+        self, sample: Mapping[str, Any], inference: bool = False
+    ) -> Mapping[str, Any]:
+        """
+        Apply image decoding and transformations to the "image" field in the sample.
+
+        Args:
+            sample (Mapping[str, Any]): A sample with an "image" field containing
+                a List[Message] to tokenize
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            Mapping[str, Any]: The sample with an updated "image" filed and added
+                "aspect_ratio" field.
+        """
+        image = sample["image"]
+        assert isinstance(image, Image.Image), "Input image must be a PIL image."
+
+        # Make image torch.tensor((3, H, W), dtype=dtype), 0<=values<=1
+        if hasattr(image, "mode") and image.mode == "RGBA":
+            image = image.convert("RGB")
+        image = F.to_image(image)
+        image = F.grayscale_to_rgb_image(image)
+        image = F.to_dtype(image, dtype=self.dtype, scale=True)
+
+        # Find the best canvas to fit the image without distortion
+        best_resolution = get_canvas_best_fit(
+            image=image,
+            possible_resolutions=self.possible_resolutions,
+            resize_to_max_canvas=self.resize_to_max_canvas,
+        )
+
+        # resize without distortion + pad to fit best_resolution
+        image = resize_with_pad(
+            image=image,
+            target_size=best_resolution,
+            resample=self.resample,
+            max_size=self.max_size,
+        )
+
+        # Normalize
+        if self.mean:
+            image = F.normalize(image, mean=self.mean, std=self.std)
+
+        # Divide the image into equally sized tiles
+        image = self.tile_crop(image=image, tile_size=self.tile_size)
+        if self.pad_tile_size:
+            image = pad_dim_to_size(image, size=self.pad_tile_size, dim=0)
+
+        aspect_ratio = torch.tensor(best_resolution).reshape(-1) // self.tile_size
+
+        sample.update(
+            {
+                "image": image,
+                "aspect_ratio": aspect_ratio,
+            }
+        )
+
+        return sample
diff -ruN marc_original/third_party/torchtune/torchtune/models/code_llama2/__init__.py marc/third_party/torchtune/torchtune/models/code_llama2/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/code_llama2/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/code_llama2/__init__.py	2025-02-20 17:49:30.386025620 -0500
@@ -0,0 +1,29 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._model_builders import (  # noqa
+    code_llama2_13b,
+    code_llama2_70b,
+    code_llama2_7b,
+    lora_code_llama2_13b,
+    lora_code_llama2_70b,
+    lora_code_llama2_7b,
+    qlora_code_llama2_13b,
+    qlora_code_llama2_70b,
+    qlora_code_llama2_7b,
+)
+
+__all__ = [
+    "code_llama2_13b",
+    "code_llama2_70b",
+    "code_llama2_7b",
+    "lora_code_llama2_13b",
+    "lora_code_llama2_70b",
+    "lora_code_llama2_7b",
+    "qlora_code_llama2_13b",
+    "qlora_code_llama2_70b",
+    "qlora_code_llama2_7b",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/code_llama2/_model_builders.py marc/third_party/torchtune/torchtune/models/code_llama2/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/code_llama2/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/code_llama2/_model_builders.py	2025-02-20 17:49:30.390025627 -0500
@@ -0,0 +1,271 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List
+from functools import partial
+
+from torchtune.models.llama2._component_builders import llama2, lora_llama2
+
+from torchtune.modules import TransformerDecoder
+from torchtune.modules.peft import LORA_ATTN_MODULES
+
+
+def code_llama2_7b() -> TransformerDecoder:
+    """
+    Builder for creating a Code-Llama2 model initialized w/ the default 7B parameter values
+    from https://arxiv.org/pdf/2308.12950.pdf
+
+    Returns:
+        TransformerDecoder: Instantiation of Code-Llama2 7B model
+    """
+    return llama2(
+        vocab_size=32_016,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=32,
+        embed_dim=4096,
+        max_seq_len=16384,
+        attn_dropout=0.0,
+        intermediate_dim=11008,
+        norm_eps=1e-5,
+        rope_base=1_000_000,
+    )
+
+
+def lora_code_llama2_7b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Code-Llama2 7B model with LoRA enabled.
+
+    The Llama2 defaults are the same as in :func:`~torchtune.models.llama2.code_llama2_7b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for LoRA linear layers. Default: 0.0
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Code-Llama2 7B model with LoRA applied
+    """
+    return lora_llama2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=32_016,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=32,
+        embed_dim=4096,
+        max_seq_len=16384,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_code_llama2_7b = partial(lora_code_llama2_7b, quantize_base=True)
+
+qlora_code_llama2_7b.__doc__ = """
+Builder for creating a Code-Llama2 7B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_code_llama2_7b` for full API arguments.
+"""
+
+
+def code_llama2_13b() -> TransformerDecoder:
+    """
+    Builder for creating a Code-Llama2 model initialized w/ the default 13B parameter values
+    from https://arxiv.org/pdf/2308.12950.pdf
+
+    Returns:
+        TransformerDecoder: Instantiation of Code-Llama2 13B model
+    """
+    return llama2(
+        vocab_size=32_016,
+        num_layers=40,
+        num_heads=40,
+        num_kv_heads=40,
+        embed_dim=5120,
+        intermediate_dim=13824,
+        max_seq_len=16384,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=1_000_000,
+    )
+
+
+def lora_code_llama2_13b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Code-Llama2 13B model with LoRA enabled.
+
+    The Llama2 defaults are the same as in :func:`~torchtune.models.llama2.code_llama2_13b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Code-Llama2 13B model with LoRA applied
+    """
+    return lora_llama2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=32_016,
+        num_layers=40,
+        num_heads=40,
+        num_kv_heads=40,
+        embed_dim=5120,
+        intermediate_dim=13824,
+        max_seq_len=16384,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_code_llama2_13b = partial(lora_code_llama2_13b, quantize_base=True)
+
+qlora_code_llama2_13b.__doc__ = """
+Builder for creating a Code-Llama2 13B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_code_llama2_13b` for full API arguments.
+"""
+
+
+def code_llama2_70b() -> TransformerDecoder:
+    """
+    Builder for creating a Code-Llama2 model initialized w/ the default 70B parameter values
+    from https://arxiv.org/pdf/2308.12950.pdf
+
+    Returns:
+        TransformerDecoder: Instantiation of Code-Llama2 70B model
+    """
+    return llama2(
+        vocab_size=32_016,
+        num_layers=80,
+        num_heads=64,
+        num_kv_heads=8,
+        embed_dim=8192,
+        intermediate_dim=28672,
+        max_seq_len=16384,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=1_000_000,
+    )
+
+
+def lora_code_llama2_70b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Code-Llama2 70B model with LoRA enabled.
+
+    The Llama2 defaults are the same as in :func:`~torchtune.models.llama2.code_llama2_70b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Code-Llama2 70B model with LoRA applied
+    """
+    return lora_llama2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=32_016,
+        num_layers=80,
+        num_heads=64,
+        num_kv_heads=8,
+        embed_dim=8192,
+        intermediate_dim=28672,
+        max_seq_len=16384,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_code_llama2_70b = partial(lora_code_llama2_70b, quantize_base=True)
+
+qlora_code_llama2_70b.__doc__ = """
+Builder for creating a Code-Llama2 70B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_code_llama2_70b` for full API arguments.
+"""
diff -ruN marc_original/third_party/torchtune/torchtune/models/convert_weights.py marc/third_party/torchtune/torchtune/models/convert_weights.py
--- marc_original/third_party/torchtune/torchtune/models/convert_weights.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/convert_weights.py	2025-02-20 17:49:30.394025633 -0500
@@ -0,0 +1,291 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import re
+
+from typing import Any, Dict
+
+import torch
+
+
+# state dict key mappings from Meta's format to torchtune's format
+_FROM_META = {
+    "tok_embeddings.weight": "tok_embeddings.weight",
+    "norm.weight": "norm.scale",
+    "output.weight": "output.weight",
+    "layers.{}.attention.wk.weight": "layers.{}.attn.k_proj.weight",
+    "layers.{}.attention.wq.weight": "layers.{}.attn.q_proj.weight",
+    "layers.{}.attention.wv.weight": "layers.{}.attn.v_proj.weight",
+    "layers.{}.attention.wo.weight": "layers.{}.attn.output_proj.weight",
+    "layers.{}.attention_norm.weight": "layers.{}.sa_norm.scale",
+    "layers.{}.ffn_norm.weight": "layers.{}.mlp_norm.scale",
+    "layers.{}.feed_forward.w1.weight": "layers.{}.mlp.w1.weight",
+    "layers.{}.feed_forward.w2.weight": "layers.{}.mlp.w2.weight",
+    "layers.{}.feed_forward.w3.weight": "layers.{}.mlp.w3.weight",
+}
+
+# state dict key mappings from HF's format to torchtune's format
+_FROM_HF = {
+    "model.embed_tokens.weight": "tok_embeddings.weight",
+    "model.layers.{}.self_attn.q_proj.weight": "layers.{}.attn.q_proj.weight",
+    "model.layers.{}.self_attn.k_proj.weight": "layers.{}.attn.k_proj.weight",
+    "model.layers.{}.self_attn.v_proj.weight": "layers.{}.attn.v_proj.weight",
+    "model.layers.{}.self_attn.o_proj.weight": "layers.{}.attn.output_proj.weight",
+    "model.layers.{}.self_attn.rotary_emb.inv_freq": None,
+    "model.layers.{}.mlp.gate_proj.weight": "layers.{}.mlp.w1.weight",
+    "model.layers.{}.mlp.up_proj.weight": "layers.{}.mlp.w3.weight",
+    "model.layers.{}.mlp.down_proj.weight": "layers.{}.mlp.w2.weight",
+    "model.layers.{}.input_layernorm.weight": "layers.{}.sa_norm.scale",
+    "model.layers.{}.post_attention_layernorm.weight": "layers.{}.mlp_norm.scale",
+    "model.norm.weight": "norm.scale",
+    "lm_head.weight": "output.weight",
+}
+
+
+def get_mapped_key(key: str, mapping_dict: Dict[str, str]) -> str:
+    try:
+        # Checks if there is a layer # in the key
+        if any(k.isdigit() for k in key.split(".")):
+            # Replace layer number with "{}" to create key for lookup
+            abstract_key = re.sub(r"(\.\d+)", ".{}", key)
+            layer_num = re.search(r"\d+", key).group(0)
+            new_key = mapping_dict[abstract_key]
+            new_key = new_key.format(layer_num)
+        else:
+            new_key = mapping_dict[key]
+    except KeyError as e:
+        raise Exception(
+            f'Error converting the state dict. Found unexpected key: "{key}". '
+            "Please make sure you're loading a checkpoint with the right format. "
+        ) from e
+
+    return new_key
+
+
+def meta_to_tune(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
+    """
+    Convert a state dict from Meta's format to torchtune's format. State dicts
+    from multiple checkpoint files should be consolidated into a single state dict
+    before calling this function.
+
+    Eg of Meta-format state dict can be found in the ``meta-llama/Llama-2-7b``
+    repo in HF (https://huggingface.co/meta-llama/Llama-2-7b).
+
+    Args:
+        state_dict (Dict[str, torch.Tensor]): State dict in Meta's format.
+
+    Returns:
+        Dict[str, torch.Tensor]: State dict in torchtune's format.
+    """
+    converted_state_dict = {}
+    for key, value in state_dict.items():
+        if key not in ["rope.freqs"]:  # Skip loading the position embeddings
+            new_key = get_mapped_key(key, _FROM_META)
+            converted_state_dict[new_key] = value
+
+    return converted_state_dict
+
+
+def tune_to_meta(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
+    """
+    Convert a state dict from torchtune's format to Meta's format. This function
+    doesn't handle any sharding or splitting of state dicts. It follows the
+    state_dict IN -> state_dict OUT pattern.
+
+    Args:
+        state_dict (Dict[str, torch.Tensor]): State dict in torchtune's format.
+
+    Returns:
+        Dict[str, torch.Tensor]: State dict in Meta's format.
+    """
+    converted_state_dict = {}
+    inverted_mapping_dict = {v: k for k, v in _FROM_META.items()}
+
+    for key, value in state_dict.items():
+        new_key = get_mapped_key(key, inverted_mapping_dict)
+        converted_state_dict[new_key] = value
+
+    return converted_state_dict
+
+
+def hf_to_tune(
+    state_dict: Dict[str, torch.Tensor],
+    num_heads: int = 32,
+    num_kv_heads: int = 32,
+    dim: int = 4096,
+    head_dim: int = None,
+) -> Dict[str, torch.Tensor]:
+    """
+    Convert a state dict from HF's format to torchtune's format. State dicts
+    from multiple checkpoint files should be consolidated into a single state dict
+    before calling this function.
+
+    Eg of HF-format state dict can be found in the ``meta-llama/Llama-2-7b-hf``
+    repo in HF (https://huggingface.co/meta-llama/Llama-2-7b-hf).
+
+    Args:
+        state_dict (Dict[str, torch.Tensor]): State dict in HF's format.
+        num_heads (int): Number of heads in the model.
+        num_kv_heads (int): Number of heads in the key/value projection layers.
+        dim (int): Dimension of the model.
+        head_dim (int): Dimension of the head. If not provided, it will be calculated
+            as dim // num_heads.
+
+    Returns:
+        Dict[str, torch.Tensor]: State dict in torchtune's format.
+    """
+    converted_state_dict = {}
+    if head_dim is None:
+        head_dim = dim // num_heads
+
+    def _permute(t, n_heads):
+        return (
+            t.view(n_heads, 2, head_dim // 2, dim)
+            .transpose(1, 2)
+            .reshape((head_dim * n_heads), dim)
+        )
+
+    for key, value in state_dict.items():
+        if "rotary_emb.inv_freq" not in key:  # Skip loading the position embeddings
+            new_key = get_mapped_key(key, _FROM_HF)
+            if "q_proj" in key:
+                value = _permute(value, num_heads)
+            elif "k_proj" in key:
+                value = _permute(value, num_kv_heads)
+
+            converted_state_dict[new_key] = value
+    return converted_state_dict
+
+
+def tune_to_hf(
+    state_dict: Dict[str, torch.Tensor],
+    num_heads: int = 32,
+    num_kv_heads: int = 32,
+    dim: int = 4096,
+    head_dim: int = None,
+):
+    """
+    Convert a state dict from torchtune's format to HF's format. This function
+    doesn't handle any sharding or splitting of state dicts. It follows the
+    state_dict IN -> state_dict OUT pattern.
+
+    Args:
+        state_dict (Dict[str, torch.Tensor]): State dict in torchtune's format.
+        num_heads (int): Number of heads in the model.
+        num_kv_heads (int): Number of heads in the key/value projection layers.
+        dim (int): Dimension of the model.
+        head_dim (int): Dimension of model attention heads. Default None.
+
+    Returns:
+        Dict[str, torch.Tensor]: State dict in HF's format.
+    """
+    converted_state_dict = {}
+    inverted_mapping_dict = {v: k for k, v in _FROM_HF.items()}
+
+    if head_dim is None:
+        head_dim = dim // num_heads
+
+    def _permute(t, n_heads):
+        return (
+            t.view(n_heads, head_dim // 2, 2, dim)
+            .transpose(1, 2)
+            .reshape((head_dim * n_heads), dim)
+        )
+
+    for key, value in state_dict.items():
+        new_key = get_mapped_key(key, inverted_mapping_dict)
+        if "q_proj" in key:
+            value = _permute(value, num_heads)
+        elif "k_proj" in key:
+            value = _permute(value, num_kv_heads)
+        converted_state_dict[new_key] = value
+
+    return converted_state_dict
+
+
+# Mapping from torchtune LoRA module names to PEFT LoRA module names
+_TO_PEFT_KEYS = {
+    "lora_a": "lora_A",
+    "lora_b": "lora_B",
+    "magnitude": "lora_magnitude_vector",
+}
+
+# Mapping from torchtune module names to target modules for PEFT adapter config
+_TO_PEFT_TARGET_MODULES = {
+    "q_proj": "q_proj",
+    "k_proj": "k_proj",
+    "v_proj": "v_proj",
+    "output_proj": "o_proj",
+    "w1": "gate_proj",
+    "w2": "down_proj",
+    "w3": "up_proj",
+    "output": "lm_head",
+}
+
+# Keys expected in PEFT's adapter_config.json
+_PEFT_CONFIG_EXPECTED_KEYS = ["target_modules", "r", "lora_alpha"]
+
+
+def tune_to_peft_adapter_config(
+    adapter_config: Dict[str, Any],
+):
+    if not all([x in adapter_config.keys() for x in _PEFT_CONFIG_EXPECTED_KEYS]):
+        raise ValueError(
+            f"PEFT adapter config requires {_PEFT_CONFIG_EXPECTED_KEYS}, found {adapter_config.keys()}"
+        )
+
+    for k in adapter_config["target_modules"]:
+        if k not in _TO_PEFT_TARGET_MODULES:
+            raise ValueError(f"Unknown target module {k}")
+    adapter_config["target_modules"] = list(
+        map(_TO_PEFT_TARGET_MODULES.get, adapter_config["target_modules"])
+    )
+
+    return adapter_config
+
+
+def tune_to_peft_adapter_weights(
+    state_dict: Dict[str, torch.Tensor],
+    num_heads: int = 32,
+    num_kv_heads: int = 32,
+    dim: int = 4096,
+    head_dim: int = None,
+):
+    converted_state_dict = {}
+    full_mapping = {}
+    # Rather than recreate a separate mapping for LoRA adapter weights, we just
+    # re-use the _FROM_HF mapping for base model weights. We iterate over it twice:
+    # once to add mappings for LoRA A matrices and once to add mappings for LoRA B matrices.
+    for k, v in _TO_PEFT_KEYS.items():
+        full_mapping.update(
+            {
+                vv.replace(".weight", f".{k}.weight"): kk.replace(
+                    ".weight", f".{v}.weight"
+                )
+                for kk, vv in _FROM_HF.items()
+                if vv is not None
+            }
+        )
+
+    if head_dim is None:
+        head_dim = dim // num_heads
+
+    def _permute_lora_matrix(t, n_heads):
+        rank = t.shape[-1]
+        return (
+            t.view(n_heads, head_dim // 2, 2, rank)
+            .transpose(1, 2)
+            .reshape((head_dim * n_heads), rank)
+        )
+
+    for key, value in state_dict.items():
+        new_key = get_mapped_key(key, full_mapping)
+        if "q_proj" in new_key and "lora_B" in new_key:
+            value = _permute_lora_matrix(value, num_heads)
+        elif "k_proj" in new_key and "lora_B" in new_key:
+            value = _permute_lora_matrix(value, num_kv_heads)
+        converted_state_dict["base_model.model." + new_key] = value
+    return converted_state_dict
diff -ruN marc_original/third_party/torchtune/torchtune/models/gemma/_component_builders.py marc/third_party/torchtune/torchtune/models/gemma/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/gemma/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/gemma/_component_builders.py	2025-02-20 17:49:30.402025646 -0500
@@ -0,0 +1,398 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from torch import nn
+from typing import List
+from torchtune.modules.common_utils import _register_reparametrize_state_dict_hooks
+
+from torchtune.modules import (
+    MultiHeadAttention,
+    FeedForward,
+    FrozenNF4Linear,
+    RotaryPositionalEmbeddings,
+    TransformerSelfAttentionLayer,
+)
+
+from torchtune.models.gemma.rms_norm import GemmaRMSNorm
+from torchtune.modules import TransformerDecoder, TiedLinear
+from torchtune.models.gemma.gemma_norm_embedding import GemmaNormEmbeddings
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+"""
+Component builders for the Gemma 2B models and popular variants such as LoRA.
+
+torchtune provides composable building blocks. Builder functions help
+stitch these building blocks into higher-level components. This design has
+two benefits:
+- The building blocks themselves are very flexible. For example, ``MultiHeadAttention``
+can take either nn.Linear or nn.LoRALinear for ``q_proj``.
+- Builder functions expose a set of configurable params which keep the constructors of
+the building blocks simple.
+"""
+
+
+def gemma(
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    head_dim: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-6,
+    rope_base: int = 10_000,
+    norm_embeddings: bool = True,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the gemma model. This includes:
+    - Token embeddings
+    - num_layers number of TransformerSelfAttentionLayer blocks
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    This does NOT currently include inference-time optimizations such as
+    sliding-window attention
+
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        head_dim (int): dimension of head
+        num_kv_heads (int): number of key and value heads.
+        embed_dim (int): embedding dimension for self-attention
+        intermediate_dim (int): intermediate dimension for MLP
+        max_seq_len (int): maximum sequence length the model will be run with,
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        norm_eps (float): epsilon in RMS norms Default: 1e-6
+        rope_base (int): base for the rotary positional embeddings. Default: 10_000
+        norm_embeddings (bool): whether to apply layer norm before the self-attention
+            and mlp layers. Default: True
+
+    Returns:
+        TransformerDecoder: Instantiation of gemma model.
+    """
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    self_att = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(num_heads * head_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    mlp = gemma_mlp(dim=embed_dim, hidden_dim=intermediate_dim)
+    layer = TransformerSelfAttentionLayer(
+        attn=self_att,
+        mlp=mlp,
+        sa_norm=GemmaRMSNorm(embed_dim, eps=norm_eps),
+        mlp_norm=GemmaRMSNorm(embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = GemmaNormEmbeddings(vocab_size, embed_dim)
+    output_proj = TiedLinear(tok_embeddings)
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        output=output_proj,
+        head_dim=head_dim,
+        norm=GemmaRMSNorm(embed_dim, eps=norm_eps)
+    )
+    return model
+
+
+def gemma_mlp(dim: int, hidden_dim: int, quantize_base: bool = False) -> FeedForward:
+    """
+    Build the MLP layer associated with the Gemma model.
+
+    Args:
+        dim (int): input dimension to the MLP
+        hidden_dim (int): hidden dimension of the MLP
+    """
+    gate_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    down_proj = nn.Linear(hidden_dim, dim, bias=False) if not quantize_base else FrozenNF4Linear(hidden_dim, dim, bias=False)
+    up_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    activation = nn.GELU(approximate="tanh")
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj, activation=activation)
+
+
+def lora_gemma(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    *,
+    # gemma args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    head_dim: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-6,
+    rope_base: int = 10_000,
+    norm_embeddings: bool = True,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Gemma with LoRA applied based on the passed in configuration.
+    Note: output projection lora is not supported because it is tied to token embeddings
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        head_dim (int): dimension of head
+        num_kv_heads (int): number of key and value heads.
+        embed_dim (int): embedding dimension for self-attention
+        intermediate_dim (int): intermediate dimension for MLP
+        max_seq_len (int): maximum sequence length the model will be run with,
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        norm_eps (float): epsilon in RMS norms Default: 1e-6
+        rope_base (int): base for the rotary positional embeddings. Default: 10_000
+        norm_embeddings (bool): whether to apply layer norm before the self-attention
+            and mlp layers. Default: True
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Gemma model with LoRA applied to
+        a subset of the attention projections in each layer.
+    """
+    self_attn = lora_gemma_self_attention(
+        lora_modules=lora_attn_modules,
+        embed_dim=embed_dim,
+        head_dim=head_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+        rope_base=rope_base,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+    if apply_lora_to_mlp:
+        mlp = lora_gemma_mlp(
+            dim=embed_dim,
+            hidden_dim=intermediate_dim,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+            quantize_base=quantize_base,
+        )
+    else:
+        mlp = gemma_mlp(dim=embed_dim, hidden_dim=intermediate_dim, quantize_base=quantize_base)
+
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=GemmaRMSNorm(embed_dim, eps=norm_eps),
+        mlp_norm=GemmaRMSNorm(embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = GemmaNormEmbeddings(vocab_size, embed_dim)
+    output_proj = TiedLinear(tok_embeddings)
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        output=output_proj,
+        head_dim=head_dim,
+        norm=GemmaRMSNorm(embed_dim, eps=norm_eps)
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to higher precision, and offload to CPU on the fly
+        # so as to not increase peak memory
+        # TODO this is clowny, figure out a better way to get what precision the rest
+        # of the model is in
+        _register_reparametrize_state_dict_hooks(model, dtype=tok_embeddings.weight.dtype)
+
+    return model
+
+
+def lora_gemma_self_attention(
+    lora_modules: List[LORA_ATTN_MODULES],
+    *,
+    # MultiHeadAttention args
+    embed_dim: int,
+    num_heads: int,
+    head_dim: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    rope_base: int = 10_000,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> MultiHeadAttention:
+    if not lora_modules:
+        raise ValueError(
+            f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules"
+        )
+
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+
+    q_proj = (
+        adapter_cls(
+            embed_dim,
+            num_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "q_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_heads * head_dim, bias=False)
+        )
+    )
+    k_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "k_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    v_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "v_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    output_proj = (
+        adapter_cls(
+            num_heads * head_dim,
+            embed_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "output_proj" in lora_modules
+        else (
+            nn.Linear(num_heads * head_dim, embed_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(num_heads * head_dim, embed_dim, bias=False)
+        )
+    )
+
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=q_proj,
+        k_proj=k_proj,
+        v_proj=v_proj,
+        output_proj=output_proj,
+        pos_embeddings=rope,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    return self_attn
+
+
+def lora_gemma_mlp(
+    *,
+    dim: int,
+    hidden_dim: int,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> FeedForward:
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    gate_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    down_proj = adapter_cls(
+        in_dim=hidden_dim,
+        out_dim=dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    up_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    activation = nn.GELU(approximate="tanh")
+
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj, activation=activation)
diff -ruN marc_original/third_party/torchtune/torchtune/models/gemma/gemma_norm_embedding.py marc/third_party/torchtune/torchtune/models/gemma/gemma_norm_embedding.py
--- marc_original/third_party/torchtune/torchtune/models/gemma/gemma_norm_embedding.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/gemma/gemma_norm_embedding.py	2025-02-20 17:49:30.414025666 -0500
@@ -0,0 +1,47 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+import torch.nn as nn
+
+
+class GemmaNormEmbeddings(nn.Embedding):
+    """Module with Embedding and normalization specific to Gemma.
+    Gemma requires normalization right after the embeddings. By merging both
+    steps in a single module, we can utilize directly
+    :class:`~torch.modules.TransformerDecoder`.
+
+    For more details about the embedding module, please see
+    https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
+
+    Args:
+        num_embeddings (int): size of the dictionary of embeddings.
+        embedding_dim (int): the size of each embedding vector.
+        *args: Variable length argument list to be passed to the Embedding module.
+        **kwargs: Arbitrary keyword arguments to be passed to the Embedding module.
+
+    Example:
+        >>> import torch
+        >>> from torchtune.models.gemma import GemmaNormEmbeddings
+        >>> embeddings = GemmaNormEmbeddings(2, 4)
+        >>> x = torch.randint(0, 2, (1, 3)) # ids can be 0 or 1
+        >>> print(x)
+        >>> print(embeddings(x))
+        >>> print(embeddings(x).shape)
+        tensor([[1, 0, 0]])
+        tensor([[[-0.2152, -2.1914,  2.8491, -0.4824],
+                 [-3.6621, -1.0267,  1.5947, -1.7349],
+                 [-3.6621, -1.0267,  1.5947, -1.7349]]], grad_fn=<MulBackward0>)
+        torch.Size([1, 3, 4])
+    """
+
+    def __init__(self, num_embeddings: int, embedding_dim: int, *args, **kwargs):
+        super().__init__(num_embeddings, embedding_dim, *args, **kwargs)
+        self.embedding_dim = embedding_dim
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        x = super().forward(x)
+        return x * torch.tensor(self.embedding_dim**0.5, dtype=x.dtype)
diff -ruN marc_original/third_party/torchtune/torchtune/models/gemma/__init__.py marc/third_party/torchtune/torchtune/models/gemma/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/gemma/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/gemma/__init__.py	2025-02-20 17:49:30.398025640 -0500
@@ -0,0 +1,32 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import gemma, lora_gemma  # noqa
+from ._model_builders import (  # noqa
+    gemma_2b,
+    gemma_7b,
+    gemma_tokenizer,
+    lora_gemma_2b,
+    lora_gemma_7b,
+    qlora_gemma_2b,
+    qlora_gemma_7b,
+)
+from ._tokenizer import GemmaTokenizer  # noqa
+
+__all__ = [
+    "GemmaTokenizer",
+    "gemma",
+    "gemma_2b",
+    "gemma_7b",
+    "gemma_tokenizer",
+    "lora_gemma",
+    "lora_gemma_2b",
+    "lora_gemma_7b",
+    "qlora_gemma_2b",
+    "qlora_gemma_7b",
+    "gemma_hf_to_tune",
+    "gemma_tune_to_hf",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/gemma/_model_builders.py marc/third_party/torchtune/torchtune/models/gemma/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/gemma/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/gemma/_model_builders.py	2025-02-20 17:49:30.406025653 -0500
@@ -0,0 +1,208 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import List, Optional
+
+from torchtune.models.gemma._component_builders import gemma, lora_gemma
+from torchtune.modules import TransformerDecoder
+
+from torchtune.models.gemma._tokenizer import GemmaTokenizer
+from torchtune.modules.peft import LORA_ATTN_MODULES
+from torchtune.data._prompt_templates import _TemplateType
+from torchtune.data._prompt_templates import _get_prompt_template
+
+from functools import partial
+
+"""
+Model builders build specific instantiations using component builders. For example
+the ``gemma_2b`` model builder uses the ``gemma`` component builder.
+"""
+
+
+def gemma_2b() -> TransformerDecoder:
+    """
+    Builder for creating a Gemma 2B model initialized w/ the default 2b parameter values
+    from: https://blog.google/technology/developers/gemma-open-models/
+
+    Returns:
+        TransformerDecoder: Instantiation of Gemma 2B model
+    """
+    return gemma(
+        vocab_size=256_000,
+        num_layers=18,
+        num_heads=8,
+        head_dim=256,
+        num_kv_heads=1,
+        embed_dim=2048,
+        intermediate_dim=16384,
+        max_seq_len=8192,
+        attn_dropout=0.0,
+        norm_eps=1e-6,
+    )
+
+
+def gemma_tokenizer(path: str, max_seq_len: Optional[int] = None, prompt_template: Optional[_TemplateType] = None) -> GemmaTokenizer:
+    """
+    Tokenizer for Gemma.
+
+    Args:
+        path (str): path to the tokenizer
+        max_seq_len (Optional[int]): maximum sequence length for tokenizing a single list of messages,
+            after which the input will be truncated. Default is None.
+        prompt_template (Optional[_TemplateType]): optional specified prompt template.
+            If a string, it is assumed to be the dotpath of a :class:`~torchtune.data.PromptTemplateInterface`
+            class. If a dictionary, it is assumed to be a custom prompt template mapping role to the
+            prepend/append tags.
+        
+
+    Returns:
+        GemmaTokenizer: Instantiation of the Gemma tokenizer
+    """
+    return GemmaTokenizer(path=path, max_seq_len=max_seq_len, prompt_template=_get_prompt_template(prompt_template) if prompt_template is not None else None)
+
+
+def lora_gemma_2b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Gemma 2B model with LoRA enabled.
+
+    The Gemma defaults are the same as in :func:`~torchtune.models.gemma.gemma_2b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Gemma 2B model with LoRA applied
+    """
+    return lora_gemma(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        vocab_size=256_000,
+        num_layers=18,
+        num_heads=8,
+        head_dim=256,
+        num_kv_heads=1,
+        embed_dim=2048,
+        intermediate_dim=16384,
+        max_seq_len=8192,
+        attn_dropout=0.0,
+        norm_eps=1e-6,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+qlora_gemma_2b = partial(lora_gemma_2b, quantize_base=True)
+
+qlora_gemma_2b.__doc__ = """
+Builder for creating a Gemma model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_gemma_2b` for full API arguments.
+"""
+
+
+
+def gemma_7b() -> TransformerDecoder:
+    """
+    Builder for creating a Gemma 7B model initialized w/ the default 7b parameter values
+    from: https://blog.google/technology/developers/gemma-open-models/
+
+    Returns:
+        TransformerDecoder: Instantiation of Gemma 7B model
+    """
+    return gemma(
+        vocab_size=256_000,
+        num_layers=28,
+        num_heads=16,
+        head_dim=256,
+        num_kv_heads=16,
+        embed_dim=3072,
+        intermediate_dim=24576,
+        max_seq_len=8192,
+        attn_dropout=0.0,
+        norm_eps=1e-6,
+    )
+    
+    
+def lora_gemma_7b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Gemma 7B model with LoRA enabled.
+
+    The Gemma defaults are the same as in :func:`~torchtune.models.gemma.gemma_7b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Gemma 7B model with LoRA applied
+    """
+    return lora_gemma(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        vocab_size=256_000,
+        num_layers=28,
+        num_heads=16,
+        head_dim=256,
+        num_kv_heads=16,
+        embed_dim=3072,
+        intermediate_dim=24576,
+        max_seq_len=8192,
+        attn_dropout=0.0,
+        norm_eps=1e-6,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+qlora_gemma_7b = partial(lora_gemma_7b, quantize_base=True)
+
+qlora_gemma_7b.__doc__ = """
+Builder for creating a Gemma model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_gemma_7b` for full API arguments.
+"""
diff -ruN marc_original/third_party/torchtune/torchtune/models/gemma/rms_norm.py marc/third_party/torchtune/torchtune/models/gemma/rms_norm.py
--- marc_original/third_party/torchtune/torchtune/models/gemma/rms_norm.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/gemma/rms_norm.py	2025-02-20 17:49:30.418025673 -0500
@@ -0,0 +1,26 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+from torch import nn
+
+
+class GemmaRMSNorm(nn.Module):
+    # Copied from https://github.com/google/gemma_pytorch/blob/main/gemma/model.py
+    def __init__(self, dim: int, eps: float = 1e-6):
+        super().__init__()
+        self.eps = eps
+        self.scale = nn.Parameter(torch.zeros(dim))
+
+    def _norm(self, x):
+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
+
+    def forward(self, x):
+        output = self._norm(x.float())
+        # Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)
+        # See https://github.com/huggingface/transformers/pull/29402
+        output = output * (1.0 + self.scale.float())
+        return output.type_as(x)
diff -ruN marc_original/third_party/torchtune/torchtune/models/gemma/_tokenizer.py marc/third_party/torchtune/torchtune/models/gemma/_tokenizer.py
--- marc_original/third_party/torchtune/torchtune/models/gemma/_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/gemma/_tokenizer.py	2025-02-20 17:49:30.410025659 -0500
@@ -0,0 +1,166 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, List, Mapping, Optional, Tuple
+
+from torchtune.data import Message, PromptTemplate
+from torchtune.modules.tokenizers import (
+    ModelTokenizer,
+    SentencePieceBaseTokenizer,
+    tokenize_messages_no_special_tokens,
+)
+from torchtune.modules.transforms import Transform
+
+WHITESPACE_CHARS = [" ", "\n", "\t", "\r", "\v"]
+
+
+class GemmaTokenizer(ModelTokenizer, Transform):
+    """
+    Gemma's implementation of the SentencePiece tokenizer
+
+    Args:
+        path (str): Path to pretrained tokenizer file.
+        max_seq_len (Optional[int]): A max sequence length to truncate tokens to.
+            Default: None
+        prompt_template (Optional[PromptTemplate]): template used to format the messages based on their role. This is used
+            to add structured text around the actual messages. The structured text is used in three scenarios:
+
+            - Task-specific templates to gear models for a particular task that it will expect after training
+            - Model-specific templates that are required whenever the model is prompted, such as the [INST]
+              tags in Llama2 and in Mistral
+            - Community standardized templates, such as :class:`~torchtune.data.ChatMLTemplate`
+
+            The extra text will still get tokenized as normal text, not as special tokens. Default is None.
+
+    Examples:
+        >>> tokenizer = GemmaTokenizer("/path/to/spm_model")
+        >>> tokenized_text = tokenizer.encode("Hello world!", add_bos=True, add_eos=True)
+        >>> print(tokenized_text)
+        [1, 31587, 29644, 102, 2]
+    """
+
+    def __init__(
+        self,
+        path: str,
+        max_seq_len: Optional[int] = None,
+        prompt_template: Optional[PromptTemplate] = None,
+    ):
+        self._spm_model = SentencePieceBaseTokenizer(path)
+
+        # Original tokenizer has no pad_id, which causes indexing errors when batch training
+        self._spm_model.pad_id = 0
+
+        # During generation, stop when eos_id is encountered
+        self.stop_tokens = [self.eos_id]
+
+        self.max_seq_len = max_seq_len
+
+        self.prompt_template = prompt_template
+
+    @property
+    def eos_id(self):
+        return self._spm_model.eos_id
+
+    @property
+    def bos_id(self):
+        return self._spm_model.bos_id
+
+    @property
+    def pad_id(self):
+        return self._spm_model.pad_id
+
+    @property
+    def vocab_size(self):
+        return self._spm_model.vocab_size
+
+    def encode(
+        self,
+        text: str,
+        add_bos: bool = True,
+        add_eos: bool = True,
+        trim_leading_whitespace: bool = False,
+    ) -> List[int]:
+        return self._spm_model.encode(
+            text,
+            add_bos=add_bos,
+            add_eos=add_eos,
+            trim_leading_whitespace=trim_leading_whitespace,
+        )
+
+    def decode(
+        self,
+        token_ids: List[int],
+    ) -> str:
+        return self._spm_model.decode(token_ids)
+
+    def tokenize_messages(
+        self,
+        messages: List[Message],
+        *,
+        add_eos: bool = True,
+    ) -> Tuple[List[int], List[bool]]:
+        r"""Tokenize a list of messages one at a time then concatenate them,
+        returning a list of tokens and a list of masks.
+
+
+        Example:
+            >>> tokenizer = GemmaTokenizer(tokenizer_path, max_seq_len)
+            >>> messages = [
+                Message(role="system", content="system message\n", masked=True),
+                Message(role="user", content="user prompt\n", masked=True),
+                Message(role="assistant", content="assistant response\n"),
+            ]
+
+            >>> # tokenize_messages encodes messages separately and concats
+            >>> tokenizer.tokenize_messages(messages)[0]
+            [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+
+
+            >>> # Same result as encoding the full string in one go
+            >>> tokenizer.encode(''.join([message.content for message in messages]))
+            [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+
+
+        Args:
+            messages (List[Message]): A list of messages, each containing role, content,
+                and masked attributes.
+            add_eos (bool): Whether to append EOS after assistant message, default to True
+
+        Returns:
+            Tuple[List[int], List[bool]]: The tokenized messages
+        """
+        templated_messages = (
+            self.prompt_template(messages)
+            if self.prompt_template is not None
+            else messages
+        )
+        return tokenize_messages_no_special_tokens(
+            tokenizer=self,
+            messages=templated_messages,
+            bos_id=self.bos_id,
+            eos_id=self.eos_id if add_eos else None,
+        )
+
+    def __call__(
+        self, sample: Mapping[str, Any], inference: bool = False
+    ) -> Mapping[str, Any]:
+        """
+        Apply ``tokenize_messages`` to the "messages" field in the sample.
+
+        Args:
+            sample (Mapping[str, Any]): A sample with a "messages" field containing
+                a List[Message] to tokenize
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            Mapping[str, Any]: The sample with added "tokens" and "mask" fields
+                and the "messages" field removed.
+        """
+        messages = sample.pop("messages")
+        tokens, mask = self.tokenize_messages(messages)
+        sample["tokens"] = tokens
+        sample["mask"] = mask
+        return sample
diff -ruN marc_original/third_party/torchtune/torchtune/models/gemma/transformer.py marc/third_party/torchtune/torchtune/models/gemma/transformer.py
--- marc_original/third_party/torchtune/torchtune/models/gemma/transformer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/gemma/transformer.py	2025-02-20 17:49:30.422025679 -0500
@@ -0,0 +1,216 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torchtune.modules.transformer import _get_clones, TransformerSelfAttentionLayer
+from torchtune.utils._logging import deprecated
+
+
+@deprecated(
+    msg="Please use torchtune.modules.TransformerDecoder instead. \
+If you need an example, see torchtune.models.gemma._component_builders.py"
+)
+class GemmaTransformerDecoder(nn.Module):
+    """
+    GemmaTransformer Decoder derived from Gemma architecture. A key difference between
+    the Gemma transformer decoder and :class:`~torchtune.modules.TransformerDecoder`
+    is that the output projection is replaced instead with a reverse projection
+    using the transposed token embedding weights from output dim to input dim
+    (see https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/layers/modeling/reversible_embedding.py#L21).
+
+    Args:
+        tok_embeddings (nn.Embedding): PyTorch embedding layer, to be used to move
+            tokens to an embedding space and as the output projection.
+        layer (TransformerSelfAttentionLayer): Transformer Decoder layer.
+        num_layers (int): Number of Transformer Decoder layers.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value. This is used to setup the
+            :func:`~torchtune.modules.KVCache`
+        head_dim (int): embedding dimension for each head in self-attention. This is used
+            to setup the :func:`~torchtune.modules.KVCache`
+        norm (nn.Module): Callable that applies normalization to the output of the decoder,
+            before final MLP.
+        norm_embeddings (bool): Whether to normalize the embeddings before passing them
+            through the decoder layers. Defaults to False.
+
+    Note:
+        Arg values are checked for correctness (eg: ``attn_dropout`` belongs to [0,1])
+        in the module where they are used. This helps reduces the number of raise
+        statements in code and improves readability.
+    """
+
+    def __init__(
+        self,
+        tok_embeddings: nn.Embedding,
+        layer: TransformerSelfAttentionLayer,
+        num_layers: int,
+        max_seq_len: int,
+        num_heads: int,
+        head_dim: int,
+        norm: nn.Module,
+        norm_embeddings: bool = False,
+    ) -> None:
+        super().__init__()
+        self.tok_embeddings = tok_embeddings
+        self.layers = _get_clones(layer, num_layers)
+        self.norm = norm
+        self.max_seq_len = max_seq_len
+        self.num_heads = num_heads
+        self.head_dim = head_dim
+        self.causal_mask = None
+        self.norm_embeddings = norm_embeddings
+        self.num_output_chunks = 0
+
+    def caches_are_enabled(self) -> bool:
+        """Check if the key value caches are setup."""
+        return self.layers[0].cache_enabled
+
+    def set_num_output_chunks(self, num_output_chunks: int) -> None:
+        """Used to save memory in combination with :class:`~torchtune.modules.loss.CEWithChunkedOutputLoss`.
+        This should be called before the first forward pass, in the recipe."""
+        self.num_output_chunks = num_output_chunks
+
+    def setup_caches(
+        self,
+        batch_size: int,
+        dtype: torch.dtype,
+        *,
+        encoder_max_seq_len: Optional[int] = None,
+        decoder_max_seq_len: Optional[int] = None,
+    ):
+        """
+        Sets up key-value attention caches for inference. For each layer in ``self.layers``:
+        - :class:`torchtune.modules.TransformerSelfAttentionLayer` will use ``decoder_max_seq_len``.
+        - :class:`torchtune.modules.TransformerCrossAttentionLayer` will use ``encoder_max_seq_len``.
+        - :class:`torchtune.modules.fusion.FusionLayer` will use both ``decoder_max_seq_len`` and ``encoder_max_seq_len``.
+
+        Args:
+            batch_size (int): batch size for the caches.
+            dtype (torch.dtype): dtype for the caches.
+            encoder_max_seq_len (Optional[int]): maximum encoder cache sequence length.
+            decoder_max_seq_len (Optional[int]): maximum decoder cache sequence length.
+        """
+        if encoder_max_seq_len is not None:
+            self.encoder_max_seq_len = encoder_max_seq_len
+        if decoder_max_seq_len is not None:
+            self.decoder_max_seq_len = decoder_max_seq_len
+        for layer in self.layers:
+            layer.setup_cache(
+                batch_size,
+                dtype,
+                encoder_max_seq_len=encoder_max_seq_len,
+                decoder_max_seq_len=decoder_max_seq_len,
+            )
+
+        # causal_mask is used during inference to ensure we're attending
+        # to the right tokens
+        self.causal_mask = torch.tril(
+            torch.ones(self.max_seq_len, self.max_seq_len, dtype=torch.bool)
+        )
+
+    @torch.compiler.disable
+    def chunked_output(self, last_hidden_state: torch.Tensor) -> List[torch.Tensor]:
+        """
+        Apply output projection in chunks. This should be applied in conjunction with
+        :class:`~torchtune.modules.loss.CEWithChunkedOutputLoss` as upcasting to fp32 is done there.
+
+        To use this method, you should first call
+        :func:`~torchtune.models.gemma.GemmaTransformerDecoder.set_num_output_chunks`.
+
+        Args:
+            last_hidden_state (torch.Tensor): last hidden state of the decoder, having shape
+                [b, seq_len, embed_dim].
+
+        Returns:
+            List[torch.Tensor]: List of num_chunks output tensors, each with shape
+                [b, seq_len/num_chunks, out_dim], where out_dim is usually the vocab size.
+        """
+        return [
+            F.linear(chunk, self.tok_embeddings.weight)
+            for chunk in last_hidden_state.chunk(self.num_output_chunks, dim=1)
+        ]
+
+    def forward(
+        self,
+        tokens: torch.Tensor,
+        *,
+        mask: Optional[torch.Tensor] = None,
+        input_pos: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        """
+        Args:
+            tokens (torch.Tensor): input tensor with shape [b x s]
+            mask (Optional[torch.Tensor]): Optional boolean tensor which contains the attention mask
+                with shape [b x s x s]. This is applied after the query-key multiplication and
+                before the softmax. A value of True in row i and column j means token i attends
+                to token j. A value of False means token i does not attend to token j. If no
+                mask is specified, a causal mask is used by default. Default is None.
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape [b x s].
+                During inference, this indicates the position of the current token and
+                is required.
+
+        Note: At the very first step of inference, when the model is provided with a prompt,
+        ``input_pos`` should contain the positions of all of the tokens in the prompt
+        (eg: ``torch.arange(prompt_length)``). This is because we will need to compute the
+        KV values for each position.
+
+        Returns:
+            torch.Tensor: output tensor with shape [b x s x v]
+
+        Raises:
+            ValueError: if causal_mask is set but input_pos is None
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - s: sequence length
+            - v: vocab size
+            - d: embed dim
+            - m_s: max seq len
+        """
+        # input tensor of shape [b, s]
+        bsz, seq_len = tokens.shape
+
+        # shape: [b, s, d]
+        h = self.tok_embeddings(tokens)
+
+        if self.causal_mask is not None:
+            if input_pos is None:
+                raise ValueError(
+                    "Caches are setup, but the position of input token is missing"
+                )
+            if mask is not None:
+                raise ValueError(
+                    "An attention mask was set. Cannot use a non-causal mask for inference"
+                )
+            # shape: [1, input_pos_len, m_s]
+            # in most cases input_pos_len should be 1
+            mask = self.causal_mask[None, input_pos]
+
+        if self.norm_embeddings:
+            hidden_dim = h.size(-1)
+            h = h * torch.tensor(hidden_dim**0.5, dtype=h.dtype)
+
+        for layer in self.layers:
+            # shape: [b, s, d]
+            h = layer(h, mask=mask, input_pos=input_pos)
+
+        # shape: [b, s, d]
+        h = self.norm(h)
+
+        if self.num_output_chunks > 0:
+            output = self.chunked_output(h)
+        else:
+            # shape: [b, seq_len, out_dim]
+            output = F.linear(h, self.tok_embeddings.weight).float()
+        return output
diff -ruN marc_original/third_party/torchtune/torchtune/models/__init__.py marc/third_party/torchtune/torchtune/models/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/__init__.py	2025-02-20 17:49:30.358025574 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama2/_component_builders.py marc/third_party/torchtune/torchtune/models/llama2/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama2/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama2/_component_builders.py	2025-02-20 17:49:30.430025692 -0500
@@ -0,0 +1,690 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional
+
+from torch import nn
+
+from torchtune.models.llama2._model_utils import scale_hidden_dim_for_mlp
+
+from torchtune.modules import (
+    FeedForward,
+    FrozenNF4Linear,
+    MultiHeadAttention,
+    RMSNorm,
+    RotaryPositionalEmbeddings,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+from torchtune.modules.common_utils import _register_reparametrize_state_dict_hooks
+
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+"""
+Component builders for the Llama2 model and popular variants such as LoRA.
+
+torchtune provides composable building blocks. Builder functions help
+stitch these building blocks into higher-level components. This design has
+two benefits:
+- The building blocks themselves are very flexible. For example, ``MultiHeadAttention``
+can take either nn.Linear or nn.LoRALinear for ``q_proj``.
+- Builder functions expose a set of configurable params which keep the constructors of
+the building blocks simple.
+"""
+
+
+# ------------------ Vanilla Llama2 ------------------
+
+
+def llama2(
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    intermediate_dim: Optional[int] = None,
+    norm_eps: float = 1e-5,
+    rope_base: float = 10000.0,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the Llama2 model. This includes:
+    - Token embeddings
+    - num_layers number of TransformerSelfAttentionLayer blocks
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        rope_base (float): base for rotary embeddings. Default: 10000.0
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+
+    rope = RotaryPositionalEmbeddings(
+        dim=head_dim, max_seq_len=max_seq_len, base=rope_base
+    )
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    hidden_dim = (
+        intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+    )
+    mlp = llama2_mlp(dim=embed_dim, hidden_dim=hidden_dim)
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    output_proj = nn.Linear(embed_dim, vocab_size, bias=False)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+
+def llama2_mlp(dim: int, hidden_dim: int, quantize_base: bool = False) -> FeedForward:
+    """
+    Build the MLP layer associated with the Llama model.
+    """
+    gate_proj = (
+        nn.Linear(dim, hidden_dim, bias=False)
+        if not quantize_base
+        else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    )
+    down_proj = (
+        nn.Linear(hidden_dim, dim, bias=False)
+        if not quantize_base
+        else FrozenNF4Linear(hidden_dim, dim, bias=False)
+    )
+    up_proj = (
+        nn.Linear(dim, hidden_dim, bias=False)
+        if not quantize_base
+        else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    )
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj)
+
+
+# ------------------ LoRA Llama2 ------------------
+
+
+def lora_llama2(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # llama2 args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    intermediate_dim: Optional[int] = None,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    # Quantization args
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Llama2 (an instance of :func:`~torchtune.modules.TransformerDecoder`)
+    with LoRA applied based on the passed in configuration.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    """
+
+    self_attn = lora_llama2_self_attention(
+        lora_modules=lora_attn_modules,
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+    hidden_dim = (
+        intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+    )
+    if apply_lora_to_mlp:
+        mlp = lora_llama2_mlp(
+            dim=embed_dim,
+            hidden_dim=hidden_dim,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            quantize_base=quantize_base,
+            use_dora=use_dora,
+            lora_dropout=lora_dropout,
+        )
+    else:
+        mlp = llama2_mlp(
+            dim=embed_dim, hidden_dim=hidden_dim, quantize_base=quantize_base
+        )
+
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+
+    # TODO: quantize_base is not applied to final output_proj currently.
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            vocab_size,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+        )
+        if apply_lora_to_output
+        else nn.Linear(embed_dim, vocab_size, bias=False)
+    )
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=(embed_dim // num_heads),
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to higher precision, and offload to CPU on the fly
+        # so as to not increase peak memory
+        # TODO this is clowny, figure out a better way to get what precision the rest
+        # of the model is in
+        _register_reparametrize_state_dict_hooks(model, dtype=tok_embeddings.weight.dtype)
+
+    return model
+
+
+def lora_llama2_self_attention(
+    lora_modules: List[LORA_ATTN_MODULES],
+    *,
+    # MultiHeadAttention args
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> MultiHeadAttention:
+    """
+    Return an instance of :func:`~torchtune.modules.MultiHeadAttention` with LoRA
+    applied to a subset of its linear layers
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to. Options are ``{"q_proj", "k_proj", "v_proj",
+            "output_proj"}``.
+        embed_dim (int): embedding dimension for self-attention
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model parameters for linear layers
+            LoRA is being applied to. Default is ``False``.
+
+    Returns:
+        MultiHeadAttention: instantiation of self-attention module with LoRA
+        applied to a subset of Q, K, V, output projections.
+
+    Raises:
+        ValueError: If lora_modules arg is an empty list
+    """
+    if not lora_modules:
+        raise ValueError(
+            f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules"
+        )
+
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    q_proj = (
+        adapter_cls(
+            embed_dim,
+            num_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "q_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_heads * head_dim, bias=False)
+        )
+    )
+    k_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "k_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    v_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "v_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            embed_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "output_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, embed_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, embed_dim, bias=False)
+        )
+    )
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=q_proj,
+        k_proj=k_proj,
+        v_proj=v_proj,
+        output_proj=output_proj,
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    return self_attn
+
+
+def lora_llama2_mlp(
+    *,
+    dim: int,
+    hidden_dim: int,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> FeedForward:
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    gate_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    down_proj = adapter_cls(
+        in_dim=hidden_dim,
+        out_dim=dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    up_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    return FeedForward(
+        gate_proj=gate_proj,
+        down_proj=down_proj,
+        up_proj=up_proj,
+    )
+
+
+# ------------------ Llama2 Classifier ------------------
+
+
+def llama2_classifier(
+    num_classes: int,
+    *,
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    intermediate_dim: Optional[int] = None,
+    norm_eps: float = 1e-5,
+) -> TransformerDecoder:
+    """
+    Build a base Llama2 model with the final projection replaced with a classification layer.
+
+    Args:
+        num_classes (int): number of classes for classification.
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. If specified,
+            user should ensure `num_heads` % `num_kv_heads` == 0. Default value is
+            `None`, in which case this is the same as MHA
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    hidden_dim = (
+        intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+    )
+    mlp = llama2_mlp(dim=embed_dim, hidden_dim=hidden_dim)
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    output_proj = nn.Linear(embed_dim, num_classes, bias=False)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+
+def lora_llama2_classifier(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # llama2 classifier args,
+    num_classes: int,
+    # llama2 args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    intermediate_dim: Optional[int] = None,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    # Quantization args
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Llama2 (an instance of :func:`~torchtune.modules.TransformerDecoder`)
+    with LoRA applied based on the passed in configuration.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        num_classes (int): number of classes for classification.
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    """
+
+    self_attn = lora_llama2_self_attention(
+        lora_modules=lora_attn_modules,
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+    hidden_dim = (
+        intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+    )
+    if apply_lora_to_mlp:
+        mlp = lora_llama2_mlp(
+            dim=embed_dim,
+            hidden_dim=hidden_dim,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            quantize_base=quantize_base,
+            use_dora=use_dora,
+            lora_dropout=lora_dropout,
+        )
+    else:
+        mlp = llama2_mlp(dim=embed_dim, hidden_dim=hidden_dim)
+
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+
+    # TODO: quantize_base is not applied to final output_proj currently.
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            num_classes,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+        )
+        if apply_lora_to_output
+        else nn.Linear(embed_dim, num_classes, bias=False)
+    )
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=(embed_dim // num_heads),
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to higher precision, and offload to CPU on the fly
+        # so as to not increase peak memory
+        # TODO this is clowny, figure out a better way to get what precision the rest
+        # of the model is in
+        _register_reparametrize_state_dict_hooks(model, dtype=tok_embeddings.weight.dtype)
+
+    return model
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama2/__init__.py marc/third_party/torchtune/torchtune/models/llama2/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/llama2/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama2/__init__.py	2025-02-20 17:49:30.426025686 -0500
@@ -0,0 +1,54 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import (
+    llama2,
+    llama2_classifier,
+    lora_llama2,
+    lora_llama2_classifier,
+)
+
+from ._model_builders import (  # noqa
+    llama2_13b,
+    llama2_70b,
+    llama2_7b,
+    llama2_reward_7b,
+    llama2_tokenizer,
+    lora_llama2_13b,
+    lora_llama2_70b,
+    lora_llama2_7b,
+    lora_llama2_reward_7b,
+    qlora_llama2_13b,
+    qlora_llama2_70b,
+    qlora_llama2_7b,
+    qlora_llama2_reward_7b,
+)
+from ._prompt_template import Llama2ChatTemplate
+from ._tokenizer import Llama2Tokenizer
+
+__all__ = [
+    "Llama2Tokenizer",
+    "Llama2ChatTemplate",
+    "llama2",
+    "llama2_classifier",
+    "lora_llama2_classifier",
+    "llama2_reward_7b",
+    "lora_llama2_reward_7b",
+    "qlora_llama2_reward_7b",
+    "lora_llama2",
+    "llama2_13b",
+    "llama2_70b",
+    "llama2_7b",
+    "llama2_tokenizer",
+    "lora_llama2",
+    "llama2_classifier",
+    "lora_llama2_13b",
+    "lora_llama2_70b",
+    "lora_llama2_7b",
+    "qlora_llama2_13b",
+    "qlora_llama2_70b",
+    "qlora_llama2_7b",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama2/_model_builders.py marc/third_party/torchtune/torchtune/models/llama2/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama2/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama2/_model_builders.py	2025-02-20 17:49:30.434025699 -0500
@@ -0,0 +1,379 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import List, Optional
+from functools import partial
+
+from torchtune.models.llama2._component_builders import llama2, lora_llama2, llama2_classifier, lora_llama2_classifier
+
+from torchtune.modules import TransformerDecoder
+from torchtune.models.llama2._tokenizer import Llama2Tokenizer
+from torchtune.modules.peft import LORA_ATTN_MODULES
+from torchtune.data._prompt_templates import _TemplateType
+from torchtune.data._prompt_templates import _get_prompt_template
+
+
+"""
+Model builders build specific instantiations using component builders. For example
+the llama2_7b model builder uses the llama2 component builder to create the
+llama2 7B model.
+"""
+
+
+def llama2_7b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama2 model initialized w/ the default 7B parameter values
+    from https://arxiv.org/abs/2307.09288
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 7B model
+    """
+    return llama2(
+        vocab_size=32_000,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=32,
+        embed_dim=4096,
+        max_seq_len=4096,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+    )
+
+
+def llama2_tokenizer(path: str, max_seq_len: Optional[int] = None, prompt_template: Optional[_TemplateType] = "torchtune.models.llama2.Llama2ChatTemplate") -> Llama2Tokenizer:
+    """
+    Tokenizer for Llama2.
+
+    Args:
+        path (str): path to the tokenizer
+        max_seq_len (Optional[int]): maximum sequence length for tokenizing a single list of messages,
+            after which the input will be truncated. Default is None.
+        prompt_template (Optional[_TemplateType]): optional specified prompt template.
+            If a string, it is assumed to be the dotpath of a :class:`~torchtune.data.PromptTemplateInterface`
+            class. If a dictionary, it is assumed to be a custom prompt template mapping role to the
+            prepend/append tags. Default is :class:`~torchtune.models.llama2.Llama2ChatTemplate`.
+
+    Returns:
+        Llama2Tokenizer: Instantiation of the Llama2 tokenizer
+    """
+    return Llama2Tokenizer(path=path, max_seq_len=max_seq_len, prompt_template=_get_prompt_template(prompt_template) if prompt_template is not None else None)
+
+
+def lora_llama2_7b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama2 7B model with LoRA enabled.
+
+    The Llama2 defaults are the same as in :func:`~torchtune.models.llama2.llama2_7b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+        
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 7B model with LoRA applied
+    """
+    return lora_llama2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=32_000,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=32,
+        embed_dim=4096,
+        max_seq_len=4096,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_llama2_7b = partial(lora_llama2_7b, quantize_base=True)
+
+qlora_llama2_7b.__doc__ = """
+Builder for creating a Llama2 7B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama2_7b` for full API arguments.
+"""
+
+
+def llama2_13b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama2 model initialized w/ the default 13B parameter values
+    from https://arxiv.org/abs/2307.09288
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 13B model
+    """
+    return llama2(
+        vocab_size=32_000,
+        num_layers=40,
+        num_heads=40,
+        num_kv_heads=40,
+        embed_dim=5120,
+        intermediate_dim=13824,
+        max_seq_len=4096,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+    )
+
+
+def lora_llama2_13b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama2 13B model with LoRA enabled.
+
+    The Llama2 defaults are the same as in :func:`~torchtune.models.llama2.llama2_13b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 13B model with LoRA applied
+    """
+
+    return lora_llama2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=32_000,
+        num_layers=40,
+        num_heads=40,
+        num_kv_heads=40,
+        embed_dim=5120,
+        intermediate_dim=13824,
+        max_seq_len=4096,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_llama2_13b = partial(lora_llama2_13b, quantize_base=True)
+qlora_llama2_13b.__doc__ = """
+Builder for creating a Llama2 13B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama2_13b` for full API arguments.
+"""
+
+
+def llama2_70b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama2 model initialized w/ the default 70B parameter values
+    from https://arxiv.org/abs/2307.09288
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 70B model
+    """
+    return llama2(
+        vocab_size=32_000,
+        num_layers=80,
+        num_heads=64,
+        num_kv_heads=8,
+        embed_dim=8192,
+        intermediate_dim=28672,
+        max_seq_len=4096,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+    )
+
+
+def lora_llama2_70b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama2 70B model with LoRA enabled.
+
+    The Llama2 defaults are the same as in :func:`~torchtune.models.llama2.llama2_70b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 70B model with LoRA applied
+    """
+    return lora_llama2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=32_000,
+        num_layers=80,
+        num_heads=64,
+        num_kv_heads=8,
+        embed_dim=8192,
+        max_seq_len=4096,
+        intermediate_dim=28672,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_llama2_70b = partial(lora_llama2_70b, quantize_base=True)
+qlora_llama2_70b.__doc__ = """
+Builder for creating a Llama2 70B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama2_70b` for full API arguments.
+"""
+
+
+def llama2_reward_7b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama2 model initialized w/ the default 7B parameter values
+    from https://arxiv.org/abs/2307.09288, where the output layer is a classification layer
+    projecting to a single class for reward modelling.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 7B model
+    """
+    return llama2_classifier(
+        num_classes=1,
+        vocab_size=32_000,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=32,
+        embed_dim=4096,
+        max_seq_len=4096,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+    )
+
+
+def lora_llama2_reward_7b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama2 7B reward model with LoRA enabled.
+
+    The Llama2 classifier defaults are the same as in :func:`~torchtune.models.llama2.llama2_reward_7b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama2 7B model with LoRA applied
+    """
+    return lora_llama2_classifier(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        num_classes=1,
+        vocab_size=32_000,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=32,
+        embed_dim=4096,
+        max_seq_len=4096,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_llama2_reward_7b = partial(lora_llama2_7b, quantize_base=True)
+qlora_llama2_reward_7b.__doc__ = """
+Builder for creating a Llama2 reward 7b model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama2_reward_7b` for full API arguments.
+"""
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama2/_model_utils.py marc/third_party/torchtune/torchtune/models/llama2/_model_utils.py
--- marc_original/third_party/torchtune/torchtune/models/llama2/_model_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama2/_model_utils.py	2025-02-20 17:49:30.438025705 -0500
@@ -0,0 +1,23 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+def scale_hidden_dim_for_mlp(dim: int, multiple_of: int = 256) -> int:
+    """Scale hidden dimension for MLP to keep number of parameters and computation constant.
+
+    Args:
+        dim (int): Input dimension.
+        multiple_of (int): Round scaled dimension to nearest multiple of `multiple_of` for clean computation.
+
+    Returns:
+        Scaled hidden dimension.
+    """
+    # Scale hidden dimension by (2/3)4d for SwiGLU to keep number of
+    # parameters and computation constant
+    hidden_dim = 4 * int(2 * dim / 3)
+    # Round hidden dimension to nearest multiple of `multiple_of`
+    hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)
+    return hidden_dim
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama2/_prompt_template.py marc/third_party/torchtune/torchtune/models/llama2/_prompt_template.py
--- marc_original/third_party/torchtune/torchtune/models/llama2/_prompt_template.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama2/_prompt_template.py	2025-02-20 17:49:30.442025712 -0500
@@ -0,0 +1,82 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import List
+
+from torchtune.data import Message, PromptTemplateInterface
+
+
+class Llama2ChatTemplate(PromptTemplateInterface):
+    """
+    Prompt template that formats chat data of human and system prompts with appropriate tags
+    used in Llama2 pre-training. Taken from Meta's official `Llama inference
+    repository <https://github.com/meta-llama/llama/blob/main/llama/generation.py>`_.
+
+    .. code-block:: text
+
+        "[INST] <<SYS>>
+        You are a helpful, respectful and honest assistant.
+        <</SYS>>"
+
+        I am going to Paris, what should I see? [/INST] Paris, the capital of France, is known for its stunning architecture..."
+
+
+    """
+
+    template = {
+        "system": ("<<SYS>>\n", "\n<</SYS>>\n\n"),
+        "user": ("[INST] ", " [/INST] "),
+        "assistant": ("", ""),
+        "ipython": ("", ""),
+    }
+
+    def __call__(
+        self,
+        messages: List[Message],
+    ) -> List[Message]:
+        """
+        Format user and system messages with appropriate tags.
+
+        Args:
+            messages (List[Message]): a single conversation, structured as a list
+                of `Message` objects
+
+        Returns:
+            The formatted list of messages
+        """
+        system_message = []
+        formatted_dialogue = []
+        for message in messages:
+            if message.role == "system":
+                system_message = (
+                    [{"type": "text", "content": self.template["system"][0]}]
+                    + message.content
+                    + [{"type": "text", "content": self.template["system"][1]}]
+                )
+                # Incorporate the system message in the user message - Llama2 only
+                # looks for the <<SYS>> tags and not the explicit role so this will
+                # be treated the same as an actual system message. We do this because
+                # of the nesting of the system prompt in the user message.
+                continue
+            elif message.role == "user":
+                content = (
+                    [{"type": "text", "content": self.template["user"][0]}]
+                    + system_message
+                    + message.content
+                    + [{"type": "text", "content": self.template["user"][1]}]
+                )
+            elif message.role == "assistant":
+                # No special formatting needed for assistant message
+                content = message.content
+            formatted_dialogue.append(
+                Message(
+                    role=message.role,
+                    content=content,
+                    masked=message.masked,
+                    ipython=message.ipython,
+                    eot=message.eot,
+                ),
+            )
+        return formatted_dialogue
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama2/_tokenizer.py marc/third_party/torchtune/torchtune/models/llama2/_tokenizer.py
--- marc_original/third_party/torchtune/torchtune/models/llama2/_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama2/_tokenizer.py	2025-02-20 17:49:30.446025719 -0500
@@ -0,0 +1,183 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, List, Mapping, Optional, Tuple
+
+from torchtune.data import Message, PromptTemplate
+from torchtune.models.llama2._prompt_template import Llama2ChatTemplate
+from torchtune.modules.tokenizers import (
+    ModelTokenizer,
+    SentencePieceBaseTokenizer,
+    tokenize_messages_no_special_tokens,
+)
+from torchtune.modules.transforms import Transform
+
+WHITESPACE_CHARS = [" ", "\n", "\t", "\r", "\v"]
+
+
+class Llama2Tokenizer(ModelTokenizer, Transform):
+    """
+    Llama2's implementation of the SentencePiece tokenizer. Llama2Tokenizer does
+    not include any additional special tokens. The prompt template described in
+    https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-2/ describes
+    [INST][/INST] and <<SYS>><</SYS>> as special tokens but these are not registered
+    as unique ids and are tokenized as normal text. When using this tokenizer on the
+    pre-trained model for inference, the prompt template
+    :class:`~torchtune.models.llama2.Llama2ChatTemplate` is by default applied to your data
+    before tokenization to add the [INST] and <<SYS>> tags for optimal performance.
+    For more details, see https://pytorch.org/torchtune/main/tutorials/chat.html#tokenizing-prompt-templates-special-tokens.
+
+    Args:
+        path (str): Path to pretrained SentencePiece tokenizer file.
+        max_seq_len (Optional[int]): A max sequence length to truncate tokens to.
+            Default: None
+        prompt_template (Optional[PromptTemplate]): template used to format the messages based on their role. This is used
+            to add structured text around the actual messages. The structured text is used in three scenarios:
+
+            - Task-specific templates to gear models for a particular task that it will expect after training
+            - Model-specific templates that are required whenever the model is prompted, such as the [INST]
+              tags in Llama2 and in Mistral
+            - Community standardized templates, such as :class:`~torchtune.data.ChatMLTemplate`
+
+            The extra text will still get tokenized as normal text, not as special tokens.
+            Default is :class:`~torchtune.models.llama2.Llama2ChatTemplate`.
+
+    Examples:
+        >>> tokenizer = Llama2Tokenizer("/path/to/spm_model")
+        >>> tokenized_text = tokenizer.encode("Hello world!", add_bos=True, add_eos=True)
+        >>> print(tokenized_text)
+        [1, 31587, 29644, 102, 2]
+    """
+
+    def __init__(
+        self,
+        path: str,
+        max_seq_len: Optional[int] = None,
+        prompt_template: Optional[PromptTemplate] = Llama2ChatTemplate(),
+    ):
+        self._spm_model = SentencePieceBaseTokenizer(path)
+
+        # Original tokenizer has no pad_id, which causes indexing errors when batch training
+        self._spm_model.pad_id = 0
+
+        # During generation, stop when eos_id is encountered
+        self.stop_tokens = [self.eos_id]
+
+        self.max_seq_len = max_seq_len
+
+        self.prompt_template = prompt_template
+
+    @property
+    def eos_id(self):
+        return self._spm_model.eos_id
+
+    @property
+    def bos_id(self):
+        return self._spm_model.bos_id
+
+    @property
+    def pad_id(self):
+        return self._spm_model.pad_id
+
+    @property
+    def vocab_size(self):
+        return self._spm_model.vocab_size
+
+    def encode(
+        self,
+        text: str,
+        add_bos: bool = True,
+        add_eos: bool = True,
+        trim_leading_whitespace: bool = False,
+    ) -> List[int]:
+        return self._spm_model.encode(
+            text,
+            add_bos=add_bos,
+            add_eos=add_eos,
+            trim_leading_whitespace=trim_leading_whitespace,
+        )
+
+    def decode(
+        self,
+        token_ids: List[int],
+    ) -> str:
+        return self._spm_model.decode(token_ids)
+
+    def tokenize_messages(
+        self,
+        messages: List[Message],
+        *,
+        add_start_tokens: bool = True,
+        add_end_tokens: bool = True,
+    ) -> Tuple[List[int], List[bool]]:
+        r"""Tokenize a list of messages one at a time then concatenate them,
+        returning a list of tokens and a list of masks.
+
+        Note:
+            sentencepiece has problems where in general
+            encode(s1 + s2) != encode(s1) + encode(s2) due to whitespace handling.
+            We can get around this by prepending s2 with a known token and slicing the
+            beginning off the tokenized s2.
+
+        Example:
+            >>> tokenizer = Llama2Tokenizer(tokenizer_path, max_seq_len)
+            >>> messages = [
+                Message(role="system", content="system message\n", masked=True),
+                Message(role="user", content="user prompt\n", masked=True),
+                Message(role="assistant", content="assistant response\n"),
+            ]
+
+            >>> # tokenize_messages encodes messages separately and concats
+            >>> tokenizer.tokenize_messages(messages)[0]
+            [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+
+            >>> # Same result as encoding the full string in one go
+            >>> tokenizer.encode(''.join([message.content for message in messages]))
+            [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+
+
+        Args:
+            messages (List[Message]): A list of messages, each containing role, content,
+                and masked attributes.
+            add_start_tokens (bool): Whether to add BOS token to the beginning of the first message.
+                Default True.
+            add_end_tokens (bool): Whether to add EOS token to the end of the last message. Default True.
+
+        Returns:
+            Tuple[List[int], List[bool]]: The tokenized messages
+        """
+        templated_messages = (
+            self.prompt_template(messages)
+            if self.prompt_template is not None
+            else messages
+        )
+        return tokenize_messages_no_special_tokens(
+            tokenizer=self,
+            messages=templated_messages,
+            bos_id=self.bos_id if add_start_tokens else None,
+            eos_id=self.eos_id if add_end_tokens else None,
+        )
+
+    def __call__(
+        self, sample: Mapping[str, Any], inference: bool = False
+    ) -> Mapping[str, Any]:
+        """
+        Apply ``tokenize_messages`` to the "messages" field in the sample.
+
+        Args:
+            sample (Mapping[str, Any]): A sample with a "messages" field containing
+                a List[Message] to tokenize
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            Mapping[str, Any]: The sample with added "tokens" and "mask" fields
+                and the "messages" field removed.
+        """
+        messages = sample.pop("messages")
+        tokens, mask = self.tokenize_messages(messages, add_end_tokens=not inference)
+        sample["tokens"] = tokens
+        sample["mask"] = mask
+        return sample
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3/_component_builders.py marc/third_party/torchtune/torchtune/models/llama3/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama3/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3/_component_builders.py	2025-02-20 17:49:30.454025731 -0500
@@ -0,0 +1,441 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+from typing import List, Optional
+
+from torch import nn
+
+from torchtune.models.llama3._model_utils import scale_hidden_dim_for_mlp
+
+from torchtune.modules import (
+    MultiHeadAttention,
+    FeedForward,
+    FrozenNF4Linear,
+    RMSNorm,
+    RotaryPositionalEmbeddings,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+
+from torchtune.modules.common_utils import _register_reparametrize_state_dict_hooks
+
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+"""
+Component builders for the Llama3 model and popular variants such as LoRA.
+
+torchtune provides composable building blocks. Builder functions help
+stitch these building blocks into higher-level components. This design has
+two benefits:
+- The building blocks themselves are very flexible. For example, ``MultiHeadAttention``
+can take either nn.Linear or nn.LoRALinear for ``q_proj``.
+- Builder functions expose a set of configurable params which keep the constructors of
+the building blocks simple.
+"""
+
+
+# ------------------ Vanilla Llama3 ------------------
+
+def llama3(
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    rope_base: int = 500000.0,
+    intermediate_dim: Optional[int] = None,
+    norm_eps: float = 1e-5,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the Llama3 model. This includes:
+    - Token embeddings
+    - num_layers number of TransformerSelfAttentionLayer blocks
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3 model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    hidden_dim = intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+    mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim)
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    output_proj = nn.Linear(embed_dim, vocab_size, bias=False)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+def llama3_mlp(dim: int, hidden_dim: int, quantize_base: bool = False) -> FeedForward:
+    """
+    Build the MLP layer associated with the Llama model.
+    """
+    gate_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    down_proj = nn.Linear(hidden_dim, dim, bias=False) if not quantize_base else FrozenNF4Linear(hidden_dim, dim, bias=False)
+    up_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj)
+
+
+
+# ------------------ LoRA Llama3 ------------------
+
+
+def lora_llama3(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # llama3 args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    intermediate_dim: Optional[int] = None,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: float = 500000.0,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    # Quantization args
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Llama3 (an instance of :func:`~torchtune.modules.TransformerDecoder`)
+    with LoRA applied based on the passed in configuration.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3 model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    """
+
+    self_attn = lora_llama3_self_attention(
+        lora_modules=lora_attn_modules,
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+        rope_base=rope_base,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        quantize_base=quantize_base,
+        use_dora=use_dora,
+    )
+
+    hidden_dim = intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+    if apply_lora_to_mlp:
+        mlp = lora_llama3_mlp(
+            dim=embed_dim,
+            hidden_dim=hidden_dim,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            quantize_base=quantize_base,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+        )
+    else:
+        mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim, quantize_base=quantize_base)
+
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+
+    # TODO: quantize_base is not applied to final output_proj currently.
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    output_proj = (
+        adapter_cls(embed_dim, vocab_size, rank=lora_rank, alpha=lora_alpha, dropout=lora_dropout)
+        if apply_lora_to_output
+        else nn.Linear(embed_dim, vocab_size, bias=False)
+    )
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=(embed_dim // num_heads),
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to bf16, and offload to CPU on the fly
+        # so as to not increase peak memory
+        _register_reparametrize_state_dict_hooks(model)
+
+    return model
+
+
+def lora_llama3_self_attention(
+    lora_modules: List[LORA_ATTN_MODULES],
+    *,
+    # MultiHeadAttention args
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    rope_base: float = 500000.0,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    quantize_base: bool = False,
+    use_dora: bool = False,
+) -> MultiHeadAttention:
+    """
+    Return an instance of :func:`~torchtune.modules.MultiHeadAttention` with LoRA
+    applied to a subset of its linear layers
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to. Options are ``{"q_proj", "k_proj", "v_proj",
+            "output_proj"}``.
+        embed_dim (int): embedding dimension for self-attention
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        quantize_base (bool): Whether to quantize base model parameters for linear layers
+            LoRA is being applied to. Default is ``False``.
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+
+    Returns:
+        MultiHeadAttention: instantiation of self-attention module with LoRA
+        applied to a subset of Q, K, V, output projections.
+
+    Raises:
+        ValueError: If lora_modules arg is an empty list
+    """
+    if not lora_modules:
+        raise ValueError(
+            f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules"
+        )
+
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    q_proj = (
+        adapter_cls(
+            embed_dim,
+            num_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "q_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_heads * head_dim, bias=False)
+        )
+    )
+    k_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "k_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    v_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "v_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            embed_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "output_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, embed_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, embed_dim, bias=False)
+        )
+    )
+    rope = RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=q_proj,
+        k_proj=k_proj,
+        v_proj=v_proj,
+        output_proj=output_proj,
+        pos_embeddings=rope,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    return self_attn
+
+
+def lora_llama3_mlp(
+    *,
+    dim: int,
+    hidden_dim: int,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    quantize_base: bool = False,
+    use_dora: bool = False,
+) -> FeedForward:
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    gate_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    down_proj = adapter_cls(
+        in_dim=hidden_dim,
+        out_dim=dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    up_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    return FeedForward(
+        gate_proj=gate_proj,
+        down_proj=down_proj,
+        up_proj=up_proj,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3/__init__.py marc/third_party/torchtune/torchtune/models/llama3/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/llama3/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3/__init__.py	2025-02-20 17:49:30.450025726 -0500
@@ -0,0 +1,31 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import llama3, lora_llama3
+
+from ._model_builders import (  # noqa
+    llama3_70b,
+    llama3_8b,
+    llama3_tokenizer,
+    lora_llama3_70b,
+    lora_llama3_8b,
+    qlora_llama3_70b,
+    qlora_llama3_8b,
+)
+from ._tokenizer import Llama3Tokenizer
+
+__all__ = [
+    "Llama3Tokenizer",
+    "llama3",
+    "llama3_8b",
+    "llama3_70b",
+    "llama3_tokenizer",
+    "lora_llama3",
+    "lora_llama3_8b",
+    "lora_llama3_70b",
+    "qlora_llama3_8b",
+    "qlora_llama3_70b",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3/_model_builders.py marc/third_party/torchtune/torchtune/models/llama3/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama3/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3/_model_builders.py	2025-02-20 17:49:30.458025738 -0500
@@ -0,0 +1,220 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import List, Optional
+from functools import partial
+
+from torchtune.models.llama3._component_builders import llama3, lora_llama3
+
+from torchtune.modules import TransformerDecoder
+from torchtune.models.llama3._tokenizer import Llama3Tokenizer
+from torchtune.modules.peft import LORA_ATTN_MODULES
+from torchtune.modules.tokenizers import parse_hf_tokenizer_json
+from torchtune.data._prompt_templates import _TemplateType
+from torchtune.data._prompt_templates import _get_prompt_template
+
+
+"""
+Model builders build specific instantiations using component builders. For example
+the llama3_8b model builder uses the llama3 component builder to create the
+Llama3 8B model.
+"""
+
+
+def llama3_8b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama3 model initialized w/ the default 8b parameter values.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3 8B model
+    """
+    return llama3(
+        vocab_size=128_256,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        max_seq_len=8192,
+        intermediate_dim=14336,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500000.0,
+    )
+
+
+def llama3_70b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama3 model initialized w/ the default 70B parameter values.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3 70 model
+    """
+    return llama3(
+        vocab_size=128_256,
+        num_layers=80,
+        num_heads=64,
+        num_kv_heads=8,
+        embed_dim=8192,
+        max_seq_len=8192,
+        intermediate_dim=28672,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500000.0,
+    )
+
+ 
+def llama3_tokenizer(path: str, special_tokens_path: Optional[str] = None, max_seq_len: Optional[int] = None, prompt_template: Optional[_TemplateType] = None) -> Llama3Tokenizer:
+    """
+    Tokenizer for Llama3.
+
+    Args:
+        path (str): path to the tokenizer
+        special_tokens_path (Optional[str]): Path to ``tokenizer.json`` from Hugging Face
+            model files that contains all registered special tokens, or a local json file 
+            structured similarly. Default is None to use the canonical Llama3 special tokens.
+        max_seq_len (Optional[int]): maximum sequence length for tokenizing a single list of messages,
+            after which the input will be truncated. Default is None.
+        prompt_template (Optional[_TemplateType]): optional specified prompt template.
+            If a string, it is assumed to be the dotpath of a :class:`~torchtune.data.PromptTemplateInterface`
+            class. If a dictionary, it is assumed to be a custom prompt template mapping role to the
+            prepend/append tags.
+    
+    Returns:
+        Llama3Tokenizer: Instantiation of the Llama3 tokenizer
+    """
+    special_tokens = parse_hf_tokenizer_json(special_tokens_path) if special_tokens_path is not None else None
+    template = _get_prompt_template(prompt_template) if prompt_template is not None else None
+    return Llama3Tokenizer(path=path, special_tokens=special_tokens, max_seq_len=max_seq_len, prompt_template=template)
+
+
+def lora_llama3_8b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    quantize_base: bool = False,
+    use_dora: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama3 8B model with LoRA enabled.
+
+    The Llama3 defaults are the same as in :func:`~torchtune.models.llama3.llama3_8b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        quantize_base (bool): Whether to quantize base model weights
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3 8B model with LoRA applied
+    """
+    return lora_llama3(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=128_256,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        max_seq_len=8192,
+        intermediate_dim=14336,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500000.0,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        quantize_base=quantize_base,
+        use_dora=use_dora,
+    )
+
+
+def lora_llama3_70b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    quantize_base: bool = False,
+    use_dora: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama3 70B model with LoRA enabled.
+
+    The Llama3 defaults are the same as in :func:`~torchtune.models.llama3.llama3_70b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        quantize_base (bool): Whether to quantize base model weights
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3 70B model with LoRA applied
+    """
+    return lora_llama3(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=128_256,
+        num_layers=80,
+        num_heads=64,
+        num_kv_heads=8,
+        embed_dim=8192,
+        max_seq_len=8192,
+        intermediate_dim=28672,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500000.0,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        quantize_base=quantize_base,
+        use_dora=use_dora,
+    )
+
+
+qlora_llama3_8b = partial(lora_llama3_8b, quantize_base=True)
+
+qlora_llama3_8b.__doc__ = """
+Builder for creating a Llama3 8B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama3_8b` for full API arguments.
+"""
+
+qlora_llama3_70b = partial(lora_llama3_70b, quantize_base=True)
+
+qlora_llama3_70b.__doc__ = """
+Builder for creating a Llama3 70B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama3_70b` for full API arguments.
+"""
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3/_model_utils.py marc/third_party/torchtune/torchtune/models/llama3/_model_utils.py
--- marc_original/third_party/torchtune/torchtune/models/llama3/_model_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3/_model_utils.py	2025-02-20 17:49:30.462025745 -0500
@@ -0,0 +1,23 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+def scale_hidden_dim_for_mlp(dim: int, multiple_of: int = 256) -> int:
+    """Scale hidden dimension for MLP to keep number of parameters and computation constant.
+
+    Args:
+        dim (int): Input dimension.
+        multiple_of (int): Round scaled dimension to nearest multiple of `multiple_of` for clean computation.
+
+    Returns:
+        Scaled hidden dimension.
+    """
+    # Scale hidden dimension by (2/3)4d for SwiGLU to keep number of
+    # parameters and computation constant
+    hidden_dim = 4 * int(2 * dim / 3)
+    # Round hidden dimension to nearest multiple of `multiple_of`
+    hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)
+    return hidden_dim
Binary files marc_original/third_party/torchtune/torchtune/models/llama3/__pycache__/_component_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3/__pycache__/_component_builders.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/llama3/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/llama3/__pycache__/_model_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3/__pycache__/_model_builders.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/llama3/__pycache__/_model_utils.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3/__pycache__/_model_utils.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/llama3/__pycache__/_tokenizer.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3/__pycache__/_tokenizer.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3/_tokenizer.py marc/third_party/torchtune/torchtune/models/llama3/_tokenizer.py
--- marc_original/third_party/torchtune/torchtune/models/llama3/_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3/_tokenizer.py	2025-02-20 17:49:30.466025751 -0500
@@ -0,0 +1,387 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import re
+from typing import Any, Dict, List, Mapping, Optional, Tuple
+
+from torchtune.data import Message, PromptTemplate, truncate
+from torchtune.modules.tokenizers import ModelTokenizer, TikTokenBaseTokenizer
+from torchtune.modules.transforms import Transform
+
+
+CL100K_PATTERN = r"""(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"""  # noqa
+
+SPECIAL_TOKENS = {
+    "<|begin_of_text|>": 128000,
+    "<|end_of_text|>": 128001,
+    "<|reserved_special_token_0|>": 128002,
+    "<|reserved_special_token_1|>": 128003,
+    "<|finetune_right_pad_id|>": 128004,
+    "<|step_id|>": 128005,
+    "<|start_header_id|>": 128006,
+    "<|end_header_id|>": 128007,
+    "<|eom_id|>": 128008,
+    "<|eot_id|>": 128009,
+    "<|python_tag|>": 128010,
+    "<|image|>": 128256,
+    "<|video|>": 128012,
+}
+
+NUM_RESERVED_SPECIAL_TOKENS = 256
+
+RESERVED_TOKENS = {
+    f"<|reserved_special_token_{2 + i}|>": 128013 + i
+    for i in range(NUM_RESERVED_SPECIAL_TOKENS - len(SPECIAL_TOKENS))
+}
+
+LLAMA3_SPECIAL_TOKENS = {**SPECIAL_TOKENS, **RESERVED_TOKENS}
+
+# For previous version
+# ARC_END_TOKENS = (5163, 14623)
+
+# For current version
+ARC_END_TOKENS = (2, 5062)
+ARC_SEP_TOKENS = (1492,)
+ARC_FEWSHOT_ROLE = "system"
+print("UNMASK TOKENS IN CURRENT MODE")
+
+# For barc
+# ARC_SEP_TOKENS = (5207,)
+# ARC_END_TOKENS = (13617, 1432)
+# ARC_FEWSHOT_ROLE = "user"
+# print("UNMASK TOKENS IN BARC MODE")
+
+class Llama3Tokenizer(ModelTokenizer, Transform):
+    """
+    tiktoken tokenizer configured with Llama3 Instruct's special tokens, as described in
+    https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3
+
+    Args:
+        path (str): Path to pretrained tiktoken tokenizer file.
+        special_tokens (Optional[Dict[str, int]]): mapping containing special text tokens and
+            their registered token IDs. If left as None, this will be set to the canonical
+            Llama3 special tokens.
+        max_seq_len (Optional[int]): maximum sequence length for tokenizing a single list of messages,
+            after which the input will be truncated. Default is None.
+        prompt_template (Optional[PromptTemplate]): template used to format the messages based on their role. This is used
+            to add structured text around the actual messages. The structured text is used in three scenarios:
+
+            - Task-specific templates to gear models for a particular task that it will expect after training
+            - Model-specific templates that are required whenever the model is prompted, such as the [INST]
+              tags in Llama2 and in Mistral
+            - Community standardized templates, such as :class:`~torchtune.data.ChatMLTemplate`
+
+            The extra text will still get tokenized as normal text, not as special tokens. Default is None.
+
+    Examples:
+        >>> tokenizer = Llama3Tokenizer("/path/to/tt_model")
+        >>> tokenized_text = tokenizer.encode("Hello world!", add_bos=True, add_eos=True)
+        >>> print(tokenized_text)
+        [1, 31587, 29644, 102, 2]
+    """
+
+    def __init__(
+        self,
+        path: str,
+        special_tokens: Optional[Dict[str, int]] = None,
+        max_seq_len: Optional[int] = None,
+        prompt_template: Optional[PromptTemplate] = None,
+    ):
+        self.special_tokens = (
+            special_tokens if special_tokens is not None else LLAMA3_SPECIAL_TOKENS
+        )
+
+        self._validate_special_tokens()
+
+        # Encode BOS and EOS, define pad ID
+        self.bos_id = self.special_tokens["<|begin_of_text|>"]
+        self.eos_id = self.special_tokens["<|end_of_text|>"]
+        self.pad_id = self.special_tokens["<|finetune_right_pad_id|>"]
+        self.step_id = self.special_tokens["<|step_id|>"]
+
+        # Encode extra special tokens
+        self.start_header_id = self.special_tokens["<|start_header_id|>"]
+        self.end_header_id = self.special_tokens["<|end_header_id|>"]
+        self.eot_id = self.special_tokens["<|eot_id|>"]
+
+        self.eom_id = self.special_tokens["<|eom_id|>"]
+        self.python_tag = self.special_tokens["<|python_tag|>"]
+
+        # Media tokens
+        self.image_id = self.special_tokens["<|image|>"]
+
+        # During generation, stop when either eos_id, eot_id, or eom_id is encountered
+        self.stop_tokens = [self.eos_id, self.eot_id, self.eom_id]
+
+        self.tt_model = TikTokenBaseTokenizer(
+            path=path,
+            name="llama3_tiktoken",
+            pattern=CL100K_PATTERN,
+            bos_id=self.bos_id,
+            eos_id=self.eos_id,
+            special_tokens=self.special_tokens,
+        )
+        self.max_seq_len = max_seq_len
+
+        self.prompt_template = prompt_template
+
+        # Regex for removing special tokens from the decoded string
+        self._special_token_regex = re.compile(r"<\|.*?\|>")
+        self._special_token_header_regex = re.compile(
+            r"<\|start_header_id\|>.*?<\|end_header_id\|>\n\n"
+        )
+
+    def _validate_special_tokens(
+        self,
+    ):
+        """
+        Validate that required special tokens are passed into the tokenizer.
+        """
+        for token in [
+            "<|begin_of_text|>",
+            "<|end_of_text|>",
+            "<|start_header_id|>",
+            "<|end_header_id|>",
+            "<|eom_id|>",
+            "<|eot_id|>",
+            "<|python_tag|>",
+        ]:
+            if token not in self.special_tokens:
+                raise ValueError(f"{token} missing from special_tokens")
+
+    def _remove_special_tokens(self, text: str) -> str:
+        """
+        Remove special tokens from the decoded string.
+        """
+        # First remove the headers, then the remaining special tokens
+        return self._special_token_regex.sub(
+            "", self._special_token_header_regex.sub("", text)
+        )
+
+    @property
+    def base_vocab_size(self) -> int:
+        return self.tt_model.base_vocab_size
+
+    @property
+    def vocab_size(self) -> int:
+        return self.tt_model.vocab_size
+
+    def encode(
+        self,
+        text: str,
+        add_bos: bool = True,
+        add_eos: bool = True,
+    ) -> List[int]:
+        return self.tt_model.encode(text=text, add_bos=add_bos, add_eos=add_eos)
+
+    def decode(
+        self,
+        token_ids: List[int],
+        truncate_at_eos: bool = True,
+        skip_special_tokens: bool = True,
+    ) -> str:
+        """
+        Decode a list of token ids into a string.
+
+        Args:
+            token_ids (List[int]): The list of token ids.
+            truncate_at_eos (bool): Whether to truncate the string at the end of
+                sequence token. Default is True.
+            skip_special_tokens (bool): Whether to show or skip special tokens in the decoded string.
+                Default is True.
+
+        Returns:
+            str: The decoded string.
+        """
+        # We will remove special tokens manually via regex on the decoded string.
+        # This is because removing all special tokens does not remove the role and
+        # whitespace added from the special tokens, i.e., the "user" and "\n\n" in
+        # "<|start_header_id|>user<|end_header_id|>\n\n"
+        decoded_string = self.tt_model.decode(
+            token_ids=token_ids,
+            truncate_at_eos=truncate_at_eos,
+        )
+        return (
+            self._remove_special_tokens(decoded_string)
+            if skip_special_tokens
+            else decoded_string
+        )
+
+    def _tokenize_header(self, message: Message) -> List[int]:
+        """
+        Tokenize header start, message role, and header end as list of ids
+        """
+        return (
+            [self.start_header_id]
+            + self.encode(message.role.strip(), add_bos=False, add_eos=False)
+            + [self.end_header_id]
+            + self.encode("\n\n", add_bos=False, add_eos=False)
+        )
+
+    def _tokenize_end(self, message: Message) -> List[int]:
+        """
+        Add eot or eom id at the end of the message.
+        """
+        return [self.eot_id] if message.eot else [self.eom_id]
+
+    def _tokenize_body(self, message: Message) -> List[int]:
+        """
+        Tokenize message content as list of ids
+        """
+        tokenized_body = []
+        for item in message.content:
+            if item["type"] == "text":
+                tokenized_body += self.encode(
+                    item["content"].strip(), add_bos=False, add_eos=False
+                )
+            elif item["type"] == "image":
+                tokenized_body += [self.image_id]
+            else:
+                raise RuntimeError(f"Unsupported message content type: {item['type']}")
+
+        if message.ipython:
+            tokenized_body = [self.python_tag] + tokenized_body
+
+        return tokenized_body
+
+    def tokenize_message(
+        self,
+        message: Message,
+        *,
+        add_start_tokens: bool = True,
+        add_end_tokens: bool = True,
+    ) -> List[int]:
+        """
+        Tokenize a message into a list of token ids.
+
+        Args:
+            message (Message): The message to tokenize.
+            add_start_tokens (bool): Whether to prepend a tokenized header to the message. Default is True.
+            add_end_tokens (bool): Whether to append eot or eom id at the end of the message. Default is True.
+
+        Returns:
+            List[int]: The list of token ids.
+        """
+        tokenized_header = self._tokenize_header(message) if add_start_tokens else []
+        tokenized_body = self._tokenize_body(message)
+        tokenized_end = self._tokenize_end(message) if add_end_tokens else []
+
+        tokenized_message = tokenized_header + tokenized_body + tokenized_end
+        return tokenized_message
+
+    def tokenize_messages(
+        self,
+        messages: List[Message],
+        *,
+        add_end_tokens: bool = True,
+        unmask_outputs: bool = False,
+    ) -> Tuple[List[int], List[bool]]:
+        """
+        Tokenize a list of messages into a list of token ids and masks.
+
+        Args:
+            messages (List[Message]): The list of messages to tokenize.
+            add_end_tokens (bool): Whether to append end tokens ids (end-of-seq, end-of-turn, end-of-message) at the end of the
+                last assistant message. This value should be set to False for generation. Default is True.
+
+        Examples:
+            >>> # Tokenize a list of messages with default settings
+            >>> messages = [
+            ...     Message(role="user", content="Hello world!", masked=True),
+            ...     Message(role="assistant", content="How are you?", masked=False),
+            ... ]
+            >>> tokenizer = Llama3Tokenizer("/path/to/tt_model")
+            >>> tokenizer.tokenize_messages(messages)
+            ([1, 31587, 29644, 102, 1, 31587, 29644, 102, 2], [True, True, True, True, True, False, False, False, True])
+
+            >>> # Tokenize a list of messages with add_end_tokens set to False
+            >>> tokenizer.tokenize_messages(messages, add_end_tokens=False)
+            ([1, 31587, 29644, 102, 1, 31587, 29644], [True, True, True, True, True, False, False])
+
+        Returns:
+            Tuple[List[int], List[bool]]: The list of token ids and the list of masks.
+        """
+        templated_messages = (
+            self.prompt_template(messages)
+            if self.prompt_template is not None
+            else messages
+        )
+        tokens = [self.bos_id]
+        # bos and eos are always masked
+        mask = [True]
+
+        num_messages = len(templated_messages)
+        for i, message in enumerate(templated_messages):
+            # Add end tokens to the last assistant message if add_end_tokens is True
+            # Otherwise, end tokens should always be added
+            add_end_tokens_to_message = (
+                add_end_tokens if i == num_messages - 1 else True
+            )
+            tokenized_message = self.tokenize_message(
+                message, add_end_tokens=add_end_tokens_to_message
+            )
+
+            tokens = tokens + tokenized_message
+
+            if unmask_outputs and message.role == ARC_FEWSHOT_ROLE: # and "code" not in message.text_content:
+                # we want to mask outputs after first example in the sequence
+                # find second all positions of -> token 1492
+                # fast find all 1492 in tokenized_message
+                all_sep_positions = [i for i, x in enumerate(tokenized_message) if x in ARC_SEP_TOKENS]
+                # find all ]]
+
+                # all_close_positions = [i for i, x in enumerate(tokenized_message) if x == 5163 or x == 14623]
+                all_close_positions = [i for i, x in enumerate(tokenized_message) if x in ARC_END_TOKENS]
+
+                mask_for_system = [True] * len(tokenized_message)
+                if len(all_sep_positions) > 1:
+                    count_step = 1 if (len(all_close_positions) / len(all_sep_positions)) >= 4 else 0
+                    for sep_position in all_sep_positions[1:]:
+                        # find the next close bracket
+                        close_position = [x for x in all_close_positions if x > sep_position][count_step]
+                        mask_for_system[sep_position+1:close_position+1] = [False] * (close_position - sep_position)
+                # mask positions 2 - 3
+                mask = mask + mask_for_system
+            else:
+                mask = mask + ([message.masked] * len(tokenized_message))
+
+
+
+            if self.max_seq_len and len(tokens) >= self.max_seq_len:
+                break
+
+        if add_end_tokens:
+            tokens = tokens + [self.eos_id]
+            mask = mask + [True]
+
+        if self.max_seq_len:
+            tokens = truncate(
+                tokens, self.max_seq_len, self.eos_id if add_end_tokens else None
+            )
+            mask = truncate(mask, self.max_seq_len, True if add_end_tokens else None)
+
+        return tokens, mask
+
+    def __call__(
+        self, sample: Mapping[str, Any], inference: bool = False, unmask_outputs: bool = False
+    ) -> Mapping[str, Any]:
+        """
+        Apply ``tokenize_messages`` to the "messages" field in the sample.
+
+        Args:
+            sample (Mapping[str, Any]): A sample with a "messages" field containing
+                a List[Message] to tokenize
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            Mapping[str, Any]: The sample with added "tokens" and "mask" fields
+                and the "messages" field removed.
+        """
+        messages = sample.pop("messages")
+        tokens, mask = self.tokenize_messages(messages, add_end_tokens=not inference, unmask_outputs=unmask_outputs)
+        sample["tokens"] = tokens
+        sample["mask"] = mask
+        return sample
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_1/_component_builders.py marc/third_party/torchtune/torchtune/models/llama3_1/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_1/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_1/_component_builders.py	2025-02-20 17:49:30.474025765 -0500
@@ -0,0 +1,465 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional
+
+from torch import nn
+
+from torchtune.models.llama3._model_utils import scale_hidden_dim_for_mlp
+from torchtune.models.llama3_1._position_embeddings import Llama3ScaledRoPE
+from torchtune.modules import (
+    MultiHeadAttention,
+    FeedForward,
+    FrozenNF4Linear,
+    RMSNorm,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+
+from torchtune.modules.common_utils import _register_reparametrize_state_dict_hooks
+
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+"""
+Component builders for the Llama3.1 model and popular variants such as LoRA.
+
+torchtune provides composable building blocks. Builder functions help
+stitch these building blocks into higher-level components. This design has
+two benefits:
+- The building blocks themselves are very flexible. For example, ``MultiHeadAttention``
+can take either nn.Linear or nn.LoRALinear for ``q_proj``.
+- Builder functions expose a set of configurable params which keep the constructors of
+the building blocks simple.
+"""
+
+
+# ------------------ Vanilla Llama3.1 ------------------
+
+def llama3_1(
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    rope_base: int = 500_000,
+    intermediate_dim: Optional[int] = None,
+    norm_eps: float = 1e-5,
+    scale_factor: int = 8,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the Llama3.1 model. This includes:
+    - Token embeddings
+    - num_layers number of TransformerSelfAttentionLayer blocks
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        rope_base (int): base for the rotary positional embeddings. Default: 500_000
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        scale_factor (int): scaling factor for RoPE. Default: 8
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.1 model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    rope = Llama3ScaledRoPE(dim=head_dim, max_seq_len=max_seq_len, base=rope_base, scale_factor=scale_factor)
+    layers = []
+    for _ in range(num_layers):
+        self_attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            max_seq_len=max_seq_len,
+            attn_dropout=attn_dropout,
+        )
+        hidden_dim = intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+        mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim)
+        layer = TransformerSelfAttentionLayer(
+            attn=self_attn,
+            mlp=mlp,
+            sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+            mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        )
+        layers.append(layer)
+    layers = nn.ModuleList(layers)
+
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    output_proj = nn.Linear(embed_dim, vocab_size, bias=False)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+def llama3_mlp(dim: int, hidden_dim: int, quantize_base: bool = False) -> FeedForward:
+    """
+    Build the MLP layer associated with the Llama model.
+    """
+    gate_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    down_proj = nn.Linear(hidden_dim, dim, bias=False) if not quantize_base else FrozenNF4Linear(hidden_dim, dim, bias=False)
+    up_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj)
+
+
+
+# ------------------ LoRA Llama3.1 ------------------
+
+
+def lora_llama3_1(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # llama3.1 args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    intermediate_dim: Optional[int] = None,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: int = 500_000,
+    scale_factor: int = 8,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    # Quantization args
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Llama3.1 (an instance of :func:`~torchtune.modules.TransformerDecoder`)
+    with LoRA applied based on the passed in configuration.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        rope_base (int): base for the rotary positional embeddings. Default: 500_000
+        scale_factor (int): scaling factor for RoPE. Default: 8
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Whether to use DoRA layers instead of LoRA layers. Default is ``False``.
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.1 model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    """
+
+    hidden_dim = intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+    head_dim = embed_dim // num_heads
+    rope = Llama3ScaledRoPE(dim=head_dim, max_seq_len=max_seq_len, base=rope_base, scale_factor=scale_factor)
+    layers = []
+    for _ in range(num_layers):
+        self_attn = lora_llama3_attention(
+            lora_modules=lora_attn_modules,
+            pos_embeddings=rope,
+            head_dim=head_dim,
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            max_seq_len=max_seq_len,
+            attn_dropout=attn_dropout,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+            quantize_base=quantize_base,
+        )
+
+        if apply_lora_to_mlp:
+            mlp = lora_llama3_mlp(
+                dim=embed_dim,
+                hidden_dim=hidden_dim,
+                lora_rank=lora_rank,
+                lora_alpha=lora_alpha,
+                quantize_base=quantize_base,
+                lora_dropout=lora_dropout,
+                use_dora=use_dora,
+            )
+        else:
+            mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim, quantize_base=quantize_base)
+
+        layer = TransformerSelfAttentionLayer(
+            attn=self_attn,
+            mlp=mlp,
+            sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+            mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        )
+        layers.append(layer)
+    layers = nn.ModuleList(layers)
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+
+    # TODO: quantize_base is not applied to final output_proj currently.
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    output_proj = (
+        adapter_cls(embed_dim, vocab_size, rank=lora_rank, alpha=lora_alpha, dropout=lora_dropout)
+        if apply_lora_to_output
+        else nn.Linear(embed_dim, vocab_size, bias=False)
+    )
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=(embed_dim // num_heads),
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to bf16, and offload to CPU on the fly
+        # so as to not increase peak memory
+        _register_reparametrize_state_dict_hooks(model)
+
+    return model
+
+
+def lora_llama3_attention(
+    lora_modules: List[LORA_ATTN_MODULES],
+    pos_embeddings: nn.Module,
+    *,
+    # MultiHeadAttention args
+    head_dim: int,
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    q_norm: Optional[nn.Module] = None,
+    k_norm: Optional[nn.Module] = None,
+    max_seq_len: int,
+    is_causal: bool = True,
+    attn_dropout: float = 0.0,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> MultiHeadAttention:
+    """
+    Return an instance of :func:`~torchtune.modules.MultiHeadAttention` with LoRA
+    applied to a subset of its linear layers
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to. Options are ``{"q_proj", "k_proj", "v_proj",
+            "output_proj"}``.
+        pos_embeddings (nn.Module): positional embeddings module to be passed to
+            MultiHeadAttention.
+        head_dim (int): dimension of each head in the multihead attention. Usually
+            computed as ``embed_dim // num_heads``.
+        embed_dim (int): embedding dimension for self-attention
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        q_norm (Optional[nn.Module]): normalization applied to query. Default: None
+        k_norm (Optional[nn.Module]): normalization applied to key. Default: None
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        is_causal (bool): whether to apply causal attention mask. Default: True
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Whether to use DoRA layers instead of LoRA layers. Default is ``False``.
+        quantize_base (bool): Whether to quantize base model parameters for linear layers
+            LoRA is being applied to. Default is ``False``.
+
+    Returns:
+        MultiHeadAttention: instantiation of self-attention module with LoRA
+        applied to a subset of Q, K, V, output projections.
+
+    Raises:
+        ValueError: If lora_modules arg is an empty list
+    """
+    if not lora_modules:
+        raise ValueError(
+            f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules"
+        )
+
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    q_proj = (
+        adapter_cls(
+            embed_dim,
+            num_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "q_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_heads * head_dim, bias=False)
+        )
+    )
+    k_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "k_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    v_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "v_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            embed_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "output_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, embed_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, embed_dim, bias=False)
+        )
+    )
+
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=q_proj,
+        k_proj=k_proj,
+        v_proj=v_proj,
+        output_proj=output_proj,
+        q_norm=q_norm,
+        k_norm=k_norm,
+        pos_embeddings=pos_embeddings,
+        max_seq_len=max_seq_len,
+        is_causal=is_causal,
+        attn_dropout=attn_dropout,
+    )
+    return self_attn
+
+
+def lora_llama3_mlp(
+    *,
+    dim: int,
+    hidden_dim: int,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> FeedForward:
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    gate_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    down_proj = adapter_cls(
+        in_dim=hidden_dim,
+        out_dim=dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    up_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    return FeedForward(
+        gate_proj=gate_proj,
+        down_proj=down_proj,
+        up_proj=up_proj,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_1/__init__.py marc/third_party/torchtune/torchtune/models/llama3_1/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_1/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_1/__init__.py	2025-02-20 17:49:30.470025758 -0500
@@ -0,0 +1,35 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import llama3_1, lora_llama3_1
+
+from ._model_builders import (  # noqa
+    llama3_1_405b,
+    llama3_1_70b,
+    llama3_1_8b,
+    lora_llama3_1_405b,
+    lora_llama3_1_70b,
+    lora_llama3_1_8b,
+    qlora_llama3_1_405b,
+    qlora_llama3_1_70b,
+    qlora_llama3_1_8b,
+)
+from ._position_embeddings import Llama3ScaledRoPE
+
+__all__ = [
+    "llama3_1",
+    "llama3_1_8b",
+    "llama3_1_70b",
+    "llama3_1_405b",
+    "lora_llama3_1",
+    "lora_llama3_1_8b",
+    "lora_llama3_1_70b",
+    "lora_llama3_1_405b",
+    "qlora_llama3_1_8b",
+    "qlora_llama3_1_70b",
+    "qlora_llama3_1_405b",
+    "Llama3ScaledRoPE",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_1/_model_builders.py marc/third_party/torchtune/torchtune/models/llama3_1/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_1/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_1/_model_builders.py	2025-02-20 17:49:30.478025772 -0500
@@ -0,0 +1,273 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import List
+from functools import partial
+
+from torchtune.models.llama3_1._component_builders import llama3_1, lora_llama3_1
+
+from torchtune.modules import TransformerDecoder
+from torchtune.modules.peft import LORA_ATTN_MODULES
+
+"""
+Model builders build specific instantiations using component builders. For example
+the llama3_1_8b model builder uses the llama3 component builder to create the
+Llama3.1 8B model.
+"""
+
+
+def llama3_1_8b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.1 model initialized w/ the default 8b parameter values.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.1 8B model
+    """
+    return llama3_1(
+        vocab_size=128_256,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        max_seq_len=131072,
+        intermediate_dim=14336,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+    )
+
+
+def llama3_1_70b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.1 model initialized w/ the default 70B parameter values.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.1 70B model
+    """
+    return llama3_1(
+        vocab_size=128_256,
+        num_layers=80,
+        num_heads=64,
+        num_kv_heads=8,
+        embed_dim=8192,
+        max_seq_len=131072,
+        intermediate_dim=28672,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+    )
+
+
+def llama3_1_405b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.1 model initialized w/ the default 405B parameter values.
+    
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.1 405B model
+    """
+    return llama3_1(
+        vocab_size=128_256,
+        num_layers=126,
+        num_heads=128,
+        num_kv_heads=8,
+        embed_dim=16384,
+        max_seq_len=8192,
+        intermediate_dim=53248,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+    )
+
+
+def lora_llama3_1_8b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.1 8B model with LoRA enabled.
+
+    The Llama3.1 defaults are the same as in :func:`~torchtune.models.llama3_1.llama3_1_8b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.1 8B model with LoRA applied
+    """
+    return lora_llama3_1(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=128_256,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        max_seq_len=131072,
+        intermediate_dim=14336,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+def lora_llama3_1_70b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.1 70B model with LoRA enabled.
+
+    The Llama3.1 defaults are the same as in :func:`~torchtune.models.llama3_1.llama3_1_70b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.1 70B model with LoRA applied
+    """
+    return lora_llama3_1(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=128_256,
+        num_layers=80,
+        num_heads=64,
+        num_kv_heads=8,
+        embed_dim=8192,
+        max_seq_len=131072,
+        intermediate_dim=28672,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+def lora_llama3_1_405b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.1 405B model with LoRA enabled.
+
+    The Llama3.1 defaults are the same as in :func:`~torchtune.models.llama3.llama3_8b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.1 8B model with LoRA applied
+    """
+    return lora_llama3_1(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=128_256,
+        num_layers=126,
+        num_heads=128,
+        num_kv_heads=8,
+        embed_dim=16384,
+        max_seq_len=8192,
+        intermediate_dim=53248,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_llama3_1_8b = partial(lora_llama3_1_8b, quantize_base=True)
+
+qlora_llama3_1_8b.__doc__ = """
+Builder for creating a Llama3.1 8B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama3_1_8b` for full API arguments.
+"""
+
+qlora_llama3_1_70b = partial(lora_llama3_1_70b, quantize_base=True)
+
+qlora_llama3_1_70b.__doc__ = """
+Builder for creating a Llama3.1 70B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama3_1_70b` for full API arguments.
+"""
+
+qlora_llama3_1_405b = partial(lora_llama3_1_405b, quantize_base=True)
+
+qlora_llama3_1_405b.__doc__ = """
+Builder for creating a Llama3.1 405B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama3_1_405b` for full API arguments.
+"""
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_1/_position_embeddings.py marc/third_party/torchtune/torchtune/models/llama3_1/_position_embeddings.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_1/_position_embeddings.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_1/_position_embeddings.py	2025-02-20 17:49:30.482025777 -0500
@@ -0,0 +1,191 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+from typing import Optional
+
+import torch
+
+from torch import nn
+
+
+class Llama3ScaledRoPE(nn.Module):
+    """
+    This class implements Rotary Positional Embeddings (RoPE)
+    proposed in https://arxiv.org/abs/2104.09864 with additional
+    scaling from https://github.com/meta-llama/llama-models/blob/dc42f22a3b05502e7296402b019a51f57fa045c9/models/llama3_1.
+
+    In this implementation we cache the embeddings for each position upto
+    ``max_seq_len`` by computing this during init.
+
+    Default scaling factors are from the following Meta-Llama code:
+    https://github.com/meta-llama/llama-models/blob/dc42f22a3b05502e7296402b019a51f57fa045c9/models/llama3_1/api/model.py#L41
+
+    Args:
+        dim (int): Embedding dimension. This is usually set to the dim of each
+            head in the attention module computed as ````embed_dim`` // ``num_heads````
+        max_seq_len (int): Maximum expected sequence length for the
+            model, if exceeded the cached freqs will be recomputed
+        base (int): The base for the geometric progression used to compute
+            the rotation angles
+        scale_factor (int): scaling factor for theta. Default: 8
+        low_freq_factor (int): low frequency factor for scaling theta. Default: 1
+        high_freq_factor (int): high frequency factor for scaling theta. Default: 4
+        old_context_len (int): old context length for scaling theta. Default: 8192
+    """
+
+    def __init__(
+        self,
+        dim: int,
+        max_seq_len: int = 4096,
+        base: int = 10_000,
+        scale_factor: int = 8,
+        low_freq_factor: int = 1,
+        high_freq_factor: int = 4,
+        old_context_len: int = 8192,
+    ) -> None:
+        super().__init__()
+        self.dim = dim
+        self.base = base
+        self.max_seq_len = max_seq_len
+
+        self.scale_factor = scale_factor
+        self.low_freq_factor = low_freq_factor
+        self.high_freq_factor = high_freq_factor
+        self.old_context_len = old_context_len
+        self.is_cache_built = False
+        self.rope_init()
+
+    def rope_init(self):
+        """
+        Warning: this is called in recipes before torch.compile,
+        so that the cache is built in advance.
+        """
+        freqs = 1.0 / (
+            self.base
+            ** (torch.arange(0, self.dim, 2)[: (self.dim // 2)].float() / self.dim)
+        )
+
+        # If we're on meta device return early.
+        # We can't apply scaling until freqs is filled with real data
+        if freqs.is_meta:
+            return
+
+        theta = self.apply_scaling(
+            freqs,
+            self.scale_factor,
+            self.low_freq_factor,
+            self.high_freq_factor,
+            self.old_context_len,
+        )
+        self.register_buffer("theta", theta, persistent=False)
+        self.build_rope_cache(self.max_seq_len)
+        self.is_cache_built = True
+
+    def build_rope_cache(self, max_seq_len: int = 4096) -> None:
+        # Create position indexes `[0, 1, ..., max_seq_len - 1]`
+        seq_idx = torch.arange(
+            max_seq_len, dtype=self.theta.dtype, device=self.theta.device
+        )
+
+        # Outer product of theta and position index; output tensor has
+        # a shape of [max_seq_len, dim // 2]
+        idx_theta = torch.einsum("i, j -> ij", seq_idx, self.theta).float()
+
+        # cache includes both the cos and sin components and so the output shape is
+        # [max_seq_len, dim // 2, 2]
+        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)
+        self.register_buffer("cache", cache, persistent=False)
+
+    def apply_scaling(
+        self,
+        freqs: torch.Tensor,
+        scale_factor: int,
+        low_freq_factor: int,
+        high_freq_factor: int,
+        old_context_len: int,
+    ):
+
+        low_freq_wavelen = old_context_len / low_freq_factor
+        high_freq_wavelen = old_context_len / high_freq_factor
+        new_freqs = []
+        for freq in freqs:
+            wavelen = 2 * math.pi / freq
+            if wavelen < high_freq_wavelen:
+                new_freqs.append(freq)
+            elif wavelen > low_freq_wavelen:
+                new_freqs.append(freq / scale_factor)
+            else:
+                assert low_freq_wavelen != high_freq_wavelen
+                smooth = (old_context_len / wavelen - low_freq_factor) / (
+                    high_freq_factor - low_freq_factor
+                )
+                new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)
+        return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)
+
+    def forward(
+        self, x: torch.Tensor, *, input_pos: Optional[torch.Tensor] = None
+    ) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape
+                [b, s, n_h, h_d]
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape [b, s].
+                During inference, this indicates the position of the current token.
+                If none, assume the index of the token is its position id. Default is None.
+
+        Returns:
+            Tensor: output tensor with RoPE applied
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - s: sequence length
+            - n_h: num heads
+            - h_d: head dim
+
+        Raises:
+            RuntimeError: if RoPE cache is not initialized prior to forward call
+        """
+
+        if not self.is_cache_built:
+            raise RuntimeError(
+                "RoPE cache is not built. Please call rope_init() first."
+            )
+
+        # input tensor has shape [b, s, n_h, h_d]
+        seq_len = x.size(1)
+
+        # extract the values based on whether input_pos is set or not
+        rope_cache = (
+            self.cache[:seq_len] if input_pos is None else self.cache[input_pos]
+        )
+
+        # reshape input; the last dimension is used for computing the output.
+        # Cast to float to match the reference implementation
+        # tensor has shape [b, s, n_h, h_d // 2, 2]
+        xshaped = x.float().reshape(*x.shape[:-1], -1, 2)
+
+        # reshape the cache for broadcasting
+        # tensor has shape [b, s, 1, h_d // 2, 2] if packed samples,
+        # otherwise has shape [1, s, 1, h_d // 2, 2]
+        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2)
+
+        # tensor has shape [b, s, n_h, h_d // 2, 2]
+        x_out = torch.stack(
+            [
+                xshaped[..., 0] * rope_cache[..., 0]
+                - xshaped[..., 1] * rope_cache[..., 1],
+                xshaped[..., 1] * rope_cache[..., 0]
+                + xshaped[..., 0] * rope_cache[..., 1],
+            ],
+            -1,
+        )
+
+        # tensor has shape [b, s, n_h, h_d]
+        x_out = x_out.flatten(3)
+        return x_out.type_as(x)
Binary files marc_original/third_party/torchtune/torchtune/models/llama3_1/__pycache__/_component_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3_1/__pycache__/_component_builders.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/llama3_1/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3_1/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/llama3_1/__pycache__/_model_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3_1/__pycache__/_model_builders.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/llama3_1/__pycache__/_position_embeddings.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3_1/__pycache__/_position_embeddings.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_2/_component_builders.py marc/third_party/torchtune/torchtune/models/llama3_2/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_2/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_2/_component_builders.py	2025-02-20 17:49:30.490025791 -0500
@@ -0,0 +1,457 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+from typing import List, Optional
+
+from torch import nn
+
+from torchtune.models.llama3._model_utils import scale_hidden_dim_for_mlp
+from torchtune.models.llama3_1._position_embeddings import Llama3ScaledRoPE
+from torchtune.modules import TiedLinear
+from torchtune.modules import (
+    MultiHeadAttention,
+    FeedForward,
+    FrozenNF4Linear,
+    RMSNorm,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+
+from torchtune.modules.common_utils import reparametrize_as_dtype_state_dict_post_hook
+
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+"""
+Component builders for the Llama3.2 model and popular variants such as LoRA.
+
+torchtune provides composable building blocks. Builder functions help
+stitch these building blocks into higher-level components. This design has
+two benefits:
+- The building blocks themselves are very flexible. For example, ``MultiHeadAttention``
+can take either nn.Linear or nn.LoRALinear for ``q_proj``.
+- Builder functions expose a set of configurable params which keep the constructors of
+the building blocks simple.
+"""
+
+
+# ------------------ Vanilla Llama3.2 ------------------
+
+def llama3_2(
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    rope_base: int = 500_000,
+    intermediate_dim: Optional[int] = None,
+    norm_eps: float = 1e-5,
+    scale_factor: int = 32,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the Llama3.2 model. This includes:
+    - Token embeddings
+    - num_layers number of TransformerSelfAttentionLayer blocks
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        rope_base (int): base for the rotary positional embeddings. Default: 500_000
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        scale_factor (int): scaling factor for RoPE. Default: 32
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.2 model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    rope = Llama3ScaledRoPE(dim=head_dim, max_seq_len=max_seq_len, base=rope_base, scale_factor=scale_factor)
+    layers = []
+    for _ in range(num_layers):
+        self_attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            max_seq_len=max_seq_len,
+            attn_dropout=attn_dropout,
+        )
+        hidden_dim = intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+        mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim)
+        layer = TransformerSelfAttentionLayer(
+            attn=self_attn,
+            mlp=mlp,
+            sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+            mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        )
+        layers.append(layer)
+    layers = nn.ModuleList(layers)
+
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    output_proj = TiedLinear(tok_embeddings)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+def llama3_mlp(dim: int, hidden_dim: int, quantize_base: bool = False) -> FeedForward:
+    """
+    Build the MLP layer associated with the Llama model.
+    """
+    gate_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    down_proj = nn.Linear(hidden_dim, dim, bias=False) if not quantize_base else FrozenNF4Linear(hidden_dim, dim, bias=False)
+    up_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj)
+
+
+
+# ------------------ LoRA Llama3.2 ------------------
+
+
+def lora_llama3_2(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # llama3.2 args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    intermediate_dim: Optional[int] = None,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: int = 500_000,
+    scale_factor: int = 32,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    # Quantization args
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Llama3.2 (an instance of :func:`~torchtune.modules.TransformerDecoder`)
+    with LoRA applied based on the passed in configuration.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        rope_base (int): base for the rotary positional embeddings. Default: 500_000
+        scale_factor (int): scaling factor for RoPE. Default: 32
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.2 model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    """
+
+    hidden_dim = intermediate_dim if intermediate_dim else scale_hidden_dim_for_mlp(embed_dim)
+    head_dim = embed_dim // num_heads
+    rope = Llama3ScaledRoPE(dim=head_dim, max_seq_len=max_seq_len, base=rope_base, scale_factor=scale_factor)
+    layers = []
+    for _ in range(num_layers):
+        self_attn = lora_llama3_2_self_attention(
+            lora_modules=lora_attn_modules,
+            pos_embeddings=rope,
+            head_dim=head_dim,
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            max_seq_len=max_seq_len,
+            attn_dropout=attn_dropout,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+            quantize_base=quantize_base,
+        )
+
+        if apply_lora_to_mlp:
+            mlp = lora_llama3_mlp(
+                dim=embed_dim,
+                hidden_dim=hidden_dim,
+                lora_rank=lora_rank,
+                lora_alpha=lora_alpha,
+                quantize_base=quantize_base,
+                lora_dropout=lora_dropout,
+                use_dora=use_dora,
+            )
+        else:
+            mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim, quantize_base=quantize_base)
+
+        layer = TransformerSelfAttentionLayer(
+            attn=self_attn,
+            mlp=mlp,
+            sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+            mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        )
+        layers.append(layer)
+    layers = nn.ModuleList(layers)
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+
+    if apply_lora_to_output:
+        raise ValueError(
+            "apply_lora_to_output is currently not supporting in llama3.2 1b and 3b,"
+            "as the projection layer weights are tied to the embeddings"
+        )
+    output_proj = TiedLinear(tok_embeddings)
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=(embed_dim // num_heads),
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to bf16, and offload to CPU on the fly
+        # so as to not increase peak memory
+        model._register_state_dict_hook(
+            partial(reparametrize_as_dtype_state_dict_post_hook, offload_to_cpu=True)
+        )
+
+    return model
+
+
+def lora_llama3_2_self_attention(
+    lora_modules: List[LORA_ATTN_MODULES],
+    pos_embeddings: nn.Module,
+    *,
+    # MultiHeadAttention args
+    head_dim: int,
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> MultiHeadAttention:
+    """
+    Return an instance of :func:`~torchtune.modules.MultiHeadAttention` with LoRA
+    applied to a subset of its linear layers
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to. Options are ``{"q_proj", "k_proj", "v_proj",
+            "output_proj"}``.
+        pos_embeddings (nn.Module): positional embeddings module to be passed to
+            MultiHeadAttention.
+        head_dim (int): dimension of each head in the multihead attention. Usually
+            computed as ``embed_dim // num_heads``.
+        embed_dim (int): embedding dimension for self-attention
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        quantize_base (bool): Whether to quantize base model parameters for linear layers
+            LoRA is being applied to. Default is ``False``.
+
+    Returns:
+        MultiHeadAttention: instantiation of self-attention module with LoRA
+        applied to a subset of Q, K, V, output projections.
+
+    Raises:
+        ValueError: If lora_modules arg is an empty list
+    """
+    if not lora_modules:
+        raise ValueError(
+            f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules"
+        )
+
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    q_proj = (
+        adapter_cls(
+            embed_dim,
+            num_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "q_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_heads * head_dim, bias=False)
+        )
+    )
+    k_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "k_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    v_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "v_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            embed_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "output_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, embed_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, embed_dim, bias=False)
+        )
+    )
+
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=q_proj,
+        k_proj=k_proj,
+        v_proj=v_proj,
+        output_proj=output_proj,
+        pos_embeddings=pos_embeddings,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    return self_attn
+
+
+def lora_llama3_mlp(
+    *,
+    dim: int,
+    hidden_dim: int,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> FeedForward:
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    gate_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    down_proj = adapter_cls(
+        in_dim=hidden_dim,
+        out_dim=dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    up_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    return FeedForward(
+        gate_proj=gate_proj,
+        down_proj=down_proj,
+        up_proj=up_proj,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_2/__init__.py marc/third_party/torchtune/torchtune/models/llama3_2/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_2/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_2/__init__.py	2025-02-20 17:49:30.486025784 -0500
@@ -0,0 +1,27 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import llama3_2, lora_llama3_2
+
+from ._model_builders import (  # noqa
+    llama3_2_1b,
+    llama3_2_3b,
+    lora_llama3_2_1b,
+    lora_llama3_2_3b,
+    qlora_llama3_2_1b,
+    qlora_llama3_2_3b,
+)
+
+__all__ = [
+    "llama3_2",
+    "llama3_2_1b",
+    "llama3_2_3b",
+    "lora_llama3_2",
+    "lora_llama3_2_1b",
+    "lora_llama3_2_3b",
+    "qlora_llama3_2_1b",
+    "qlora_llama3_2_3b",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_2/_model_builders.py marc/third_party/torchtune/torchtune/models/llama3_2/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_2/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_2/_model_builders.py	2025-02-20 17:49:30.494025798 -0500
@@ -0,0 +1,181 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import List
+from functools import partial
+
+from torchtune.models.llama3_2._component_builders import llama3_2, lora_llama3_2
+
+from torchtune.modules import TransformerDecoder
+from torchtune.modules.peft import LORA_ATTN_MODULES
+
+"""
+Model builders build specific instantiations using component builders. For example
+the llama3_2_1b model builder uses the llama3_2 component builder to create the
+Llama3.2 1B model.
+"""
+def llama3_2_1b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.2 model initialized w/ the default 1b parameter values.
+    
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.2 1B model
+    """
+    return llama3_2(
+        vocab_size=128_256,
+        num_layers=16,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=2048,
+        max_seq_len=131072,
+        intermediate_dim=8192,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+        scale_factor=32,
+    )
+def llama3_2_3b() -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.2 model initialized w/ the default 3b parameter values.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.2 3B model
+    """
+    return llama3_2(
+        vocab_size=128_256,
+        num_layers=28,
+        num_heads=24,
+        num_kv_heads=8,
+        embed_dim=3072,
+        max_seq_len=131072,
+        intermediate_dim=8192,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+        scale_factor=32,
+    )
+def lora_llama3_2_1b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.2 1B model with LoRA enabled.
+    The Llama3.2 defaults are the same as in :func:`~torchtune.models.llama3_2.llama3_2_1b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+    
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.2 1B model with LoRA applied
+    """
+    return lora_llama3_2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=128_256,
+        num_layers=16,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=2048,
+        max_seq_len=131072,
+        intermediate_dim=8192,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+        scale_factor=32,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+def lora_llama3_2_3b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Llama3.2 3B model with LoRA enabled.
+    The Llama3.2 defaults are the same as in :func:`~torchtune.models.llama3_2.llama3_2_3b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3.2 3B model with LoRA applied
+    """
+           
+    return lora_llama3_2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=128_256,
+        num_layers=28,
+        num_heads=24,
+        num_kv_heads=8,
+        embed_dim=3072,
+        max_seq_len=131072,
+        intermediate_dim=8192,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=500_000,
+        scale_factor=32,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+qlora_llama3_2_1b = partial(lora_llama3_2_1b, quantize_base=True)
+qlora_llama3_2_1b.__doc__ = """
+Builder for creating a Llama3.2 1B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama3_2_1b` for full API arguments.
+"""
+qlora_llama3_2_3b = partial(lora_llama3_2_3b, quantize_base=True)
+qlora_llama3_2_3b.__doc__ = """
+Builder for creating a Llama3.2 3B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama3_2_3b` for full API arguments.
+"""
Binary files marc_original/third_party/torchtune/torchtune/models/llama3_2/__pycache__/_component_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3_2/__pycache__/_component_builders.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/llama3_2/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3_2/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/llama3_2/__pycache__/_model_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/llama3_2/__pycache__/_model_builders.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_component_builders.py marc/third_party/torchtune/torchtune/models/llama3_2_vision/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_2_vision/_component_builders.py	2025-02-20 17:49:30.502025811 -0500
@@ -0,0 +1,744 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+from enum import Enum
+from typing import Optional, List
+
+from torch import nn
+
+from torchtune.models.llama3._model_utils import scale_hidden_dim_for_mlp
+from torchtune.models.llama3_1._component_builders import llama3_mlp, lora_llama3_mlp, lora_llama3_attention
+from torchtune.models.llama3_1._position_embeddings import Llama3ScaledRoPE
+from torchtune.models.clip._component_builders import clip_vision_encoder, clip_mlp, lora_clip_attention, lora_clip_mlp, lora_clip_vision_encoder
+from torchtune.models.llama3_2_vision._encoder import Llama3VisionProjectionHead, Llama3VisionEncoder
+
+from torchtune.modules.model_fusion import FusionEmbedding, FusionLayer
+from torchtune.modules import (
+    RMSNorm,
+    TanhGate,
+    TransformerCrossAttentionLayer,
+    MultiHeadAttention,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+    Fp32LayerNorm
+)
+
+from torchtune.modules.common_utils import reparametrize_as_dtype_state_dict_post_hook
+
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+
+"""
+Component builders for the Llama 3.2 Vision model and its constituent models.
+torchtune provides composable building blocks. Builder functions help
+stitch these building blocks into higher-level components. This design has
+two benefits:
+- The building blocks themselves are very flexible. For example, ``GroupedQueryAttention``
+can take either nn.Linear or nn.LoRALinear for ``q_proj``.
+- Builder functions expose a set of configurable params which keep the constructors of
+the building blocks simple.
+"""
+
+
+def llama3_2_vision_encoder(
+    # clip encoder parameters
+    *,
+    patch_size: int,
+    num_heads: int,
+    clip_embed_dim: int,
+    clip_num_layers: int,
+    clip_hidden_states: Optional[List[int]],
+    # projection parameters
+    num_layers_projection: int,
+    decoder_embed_dim: int,
+    # image parameters
+    tile_size: int,
+    max_num_tiles: int = 4,
+    in_channels: int = 3,
+    ) -> Llama3VisionEncoder:
+    """
+    Build the Llama 3.2 vision encoder by combining the CLIP image model with an additional
+    projection head fusion module. This includes:
+    - Spatial positional encodings
+    - CLIP model backbone
+    - Projection head on top of CLIP
+    - Final projection into token embedding dimension
+
+    Args:
+        patch_size (int): The size of each patch. Used to divide the tiles into patches.
+            E.g. for ``patch_size=40``, a tile of shape (400, 400) will have 10x10 grid of patches
+            with shape (40, 40) each.
+        num_heads (int): The number of attention heads in each transformer layer.
+        clip_embed_dim (int): The dimensionality of each patch embedding in CLIP.
+        clip_num_layers (int): The number of transformer layers.
+        clip_hidden_states (Optional[List[int]]): The indices of CLIP hidden layers to return
+            to return to the encoder projection head. It will return the intermediate results 
+            of the vision transformer layers which will be concatenated with the CLIP output
+            and input into the projection head. For example, ``clip_hidden_states=[0,3]`` will
+            return the embeddings before they go through the first and fourth layers.
+        num_layers_projection (int): The number of transformer layers in the projection head.
+        decoder_embed_dim (int): The dimensionality of the final output embeddings for the decoder.
+        tile_size (int): The size of your image tiles, if the image was tile-cropped in advance. Otherwise,
+            the size of the input image. In this case, the function will consider your image as a single tile.
+        max_num_tiles (int): The maximum number of tiles that can be processed. This is used to
+            determine the size of the positional embeddings.
+        in_channels (int): The number of image input channels.
+
+    Returns:
+        Llama3VisionEncoder: Instantiation of Llama 3.2 vision encoder.
+    """
+
+    # clip encoder
+    clip = clip_vision_encoder(
+        tile_size=tile_size,
+        patch_size=patch_size,
+        embed_dim=clip_embed_dim,
+        num_layers=clip_num_layers,
+        num_heads=num_heads,
+        activation=nn.GELU,
+        out_indices=clip_hidden_states,
+        max_num_tiles=max_num_tiles,
+        in_channels=in_channels,
+        attn_bias=False,
+        output_cls_projection=False,
+    )
+
+    # Projection head
+    projection_head = llama3_2_vision_projection_head(
+        num_layers=num_layers_projection,
+        num_heads=num_heads,
+        decoder_embed_dim=decoder_embed_dim,
+        clip_embed_dim=clip_embed_dim,
+        num_hidden_inputs=len(clip_hidden_states or [])
+    )
+
+    return Llama3VisionEncoder(clip=clip, projection_head=projection_head)
+
+
+def llama3_2_vision_decoder(
+    *,
+    vocab_size: int,
+    num_layers: int,
+    fusion_interval: int,
+    num_special_tokens: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    encoder_max_seq_len: int,
+    rope_base: int = 500000.0,
+    intermediate_dim: Optional[int] = None,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the Llama3 model with additional fused
+    cross attention layers. This includes:
+    - Token embeddings
+    - num_layers number of CausalSelfAttention blocks
+    - Fused cross attention layers every fusion_interval number of layers
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        fusion_interval (int): interval number of layers between fusion layers.
+        num_special_tokens (int): number of special tokens added for the fusion model.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value.
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`.
+        encoder_max_seq_len (int): maximum sequence length the encoder will be run with, as used
+            by :func:`~torchtune.modules.KVCache`.
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama 3.2 vision decoder.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    hidden_dim = intermediate_dim or scale_hidden_dim_for_mlp(embed_dim)
+    layers = []
+
+    rope = Llama3ScaledRoPE(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    for idx in range(1, num_layers + 1):
+
+        # Self attention layers for text decoder
+        self_attn = MultiHeadAttention(
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+            pos_embeddings=rope,
+            max_seq_len=max_seq_len,
+            attn_dropout=0.0,
+        )
+        mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim)
+        decoder_layer = TransformerSelfAttentionLayer(
+            attn=self_attn,
+            mlp=mlp,
+            sa_norm=RMSNorm(dim=embed_dim, eps=1e-5),
+            mlp_norm=RMSNorm(dim=embed_dim, eps=1e-5),
+        )
+
+        # cross attention layers, mixing text and vision,
+        # placed every `fusion_interval` layers
+        if idx % fusion_interval == 0:
+            attn = MultiHeadAttention(
+                embed_dim=embed_dim,
+                num_heads=num_heads,
+                num_kv_heads=num_kv_heads,
+                head_dim=head_dim,
+                q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+                k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+                v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+                output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+                q_norm=RMSNorm(dim=head_dim, eps=1e-05),
+                k_norm=RMSNorm(dim=head_dim, eps=1e-05),
+                pos_embeddings=None,
+                max_seq_len=encoder_max_seq_len,
+                is_causal=False,
+                attn_dropout=0.0,
+            )
+            mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim)
+            xattn_layer = TransformerCrossAttentionLayer(
+                attn=attn,
+                mlp=mlp,
+                ca_norm=RMSNorm(dim=embed_dim),
+                mlp_norm=RMSNorm(dim=embed_dim),
+                ca_scale=TanhGate(),
+                mlp_scale=TanhGate(),
+            )
+            fusion_layer = FusionLayer(layer=decoder_layer, fusion_layer=xattn_layer)
+            layers.append(fusion_layer)
+        else:
+            layers.append(decoder_layer)
+
+    tok_embeddings = FusionEmbedding(vocab_size, num_special_tokens, embed_dim)
+    output_proj = nn.Linear(embed_dim, vocab_size, bias=False)
+
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=1e-05),
+        output=output_proj,
+    )
+
+def llama3_2_vision_projection_head(
+    *,
+    num_layers: int,
+    num_heads: int,
+    decoder_embed_dim: int,
+    clip_embed_dim: int,
+    num_hidden_inputs: int,
+) -> Llama3VisionProjectionHead:
+    """
+    Build the Llama 3.2 Vision Projection Head that maps the output of the CLIP encoder
+    to the decoder cross attention input.
+
+    Args:
+        num_layers (int): number of layers in the projection head.
+        num_heads (int): number of heads in the projection head.
+        decoder_embed_dim (int): embedding dimension for the decoder.
+        clip_embed_dim (int): embedding dimension for the CLIP encoder.
+        num_hidden_inputs (int): number of hidden inputs to the projection head.
+
+    Returns:
+        Llama3VisionProjectionHead: Instantiation of Llama 3.2 vision projection head.
+    """
+    mlp_ratio = 4
+    hidden_dim = int(mlp_ratio * clip_embed_dim)
+    head_dim = clip_embed_dim // num_heads
+    num_kv_heads = num_heads
+
+    layers = []
+    for _ in range(num_layers):
+        self_attn = MultiHeadAttention(
+            embed_dim=clip_embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_heads,
+            head_dim=head_dim,
+            q_proj=nn.Linear(clip_embed_dim, num_heads * head_dim, bias=False),
+            k_proj=nn.Linear(clip_embed_dim, num_kv_heads * head_dim, bias=False),
+            v_proj=nn.Linear(clip_embed_dim, num_kv_heads * head_dim, bias=False),
+            output_proj=nn.Linear(clip_embed_dim, clip_embed_dim, bias=False),
+            pos_embeddings=None,
+            attn_dropout=0.0,
+            is_causal=False,
+        )
+
+        mlp = clip_mlp(
+            in_dim=clip_embed_dim,
+            hidden_dim=hidden_dim,
+            out_dim=clip_embed_dim,
+            activation=nn.GELU(),
+        )
+
+        layer = TransformerSelfAttentionLayer(
+            attn=self_attn,
+            mlp=mlp,
+            sa_norm=Fp32LayerNorm(clip_embed_dim, eps=1e-5),
+            mlp_norm=Fp32LayerNorm(clip_embed_dim, eps=1e-5),
+            sa_scale=TanhGate(),
+            mlp_scale=TanhGate(),
+        )
+        layers.append(layer)
+
+    # we concatenate clip embeddings and hidden layers output
+    # and project it to embed_dim_out, which will be used for the
+    # cross encoding
+    proj_in = clip_embed_dim * (num_hidden_inputs + 1)
+    return Llama3VisionProjectionHead(
+        layers=layers,
+        output=nn.Linear(proj_in, decoder_embed_dim),
+        num_hidden_inputs=num_hidden_inputs
+    )
+
+# ------------------ LoRA Llama 3.2 Vision ------------------
+
+
+class LoRATrainable(Enum):
+    FULL = "full"
+    LORA = "lora"
+    FROZEN = "frozen"
+
+
+def lora_llama3_2_vision_encoder(
+    encoder_lora: bool,
+    fusion_lora: bool,
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # clip encoder parameters
+    patch_size: int,
+    num_heads: int,
+    clip_embed_dim: int,
+    clip_num_layers: int,
+    clip_hidden_states: Optional[List[int]],
+    # projection parameters
+    num_layers_projection: int,
+    decoder_embed_dim: int,
+    # image parameters
+    tile_size: int,
+    max_num_tiles: int = 4,
+    in_channels: int = 3,
+    # LoRA parameters
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+    ) -> Llama3VisionEncoder:
+    """
+    Build the Llama 3.2 vision encoder by combining the CLIP image model with an additional
+    projection head fusion module. This includes:
+    - Spatial positional encodings
+    - CLIP model backbone
+    - Projection head on top of CLIP
+    - Final projection into token embedding dimension
+
+    Args:
+        encoder_lora (bool): whether to apply LoRA to the CLIP encoder
+        fusion_lora (bool): whether to apply LoRA to the projection head
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        patch_size (int): The size of each patch. Used to divide the tiles into patches.
+            E.g. for ``patch_size=40``, a tile of shape (400, 400) will have 10x10 grid of patches
+            with shape (40, 40) each.
+        num_heads (int): The number of attention heads in each transformer layer.
+        clip_embed_dim (int): The dimensionality of each patch embedding in CLIP.
+        clip_num_layers (int): The number of transformer layers.
+        clip_hidden_states (Optional[List[int]]): The indices of CLIP hidden layers to return
+            to return to the encoder projection head. It will return the intermediate results 
+            of the vision transformer layers which will be concatenated with the CLIP output
+            and input into the projection head. For example, ``clip_hidden_states=[0,3]`` will
+            return the embeddings before they go through the first and fourth layers.
+        num_layers_projection (int): The number of transformer layers in the projection head.
+        decoder_embed_dim (int): The dimensionality of the final output embeddings for the decoder.
+        tile_size (int): The size of your image tiles, if the image was tile-cropped in advance. Otherwise,
+            the size of the input image. In this case, the function will consider your image as a single tile.
+        max_num_tiles (int): The maximum number of tiles that can be processed. This is used to
+            determine the size of the positional embeddings.
+        in_channels (int): The number of image input channels.
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Whether to use DoRA layers instead of LoRA layers. Default is ``False``.
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+        
+
+    Returns:
+        Llama3VisionEncoder: Instantiation of Llama 3.2 vision encoder.
+    """
+    lora_options = {
+        "lora_modules": lora_attn_modules,
+        "apply_lora_to_mlp": apply_lora_to_mlp,
+        "apply_lora_to_output": apply_lora_to_output,
+        "lora_rank": lora_rank,
+        "lora_alpha": lora_alpha,
+        "lora_dropout": lora_dropout,
+        "use_dora": use_dora,
+        "quantize_base": quantize_base,
+    }
+
+    # clip encoder
+    clip_options = {
+        "tile_size": tile_size,
+        "patch_size": patch_size,
+        "embed_dim": clip_embed_dim,
+        "num_layers": clip_num_layers,
+        "num_heads": num_heads,
+        "activation": nn.GELU,
+        "out_indices": clip_hidden_states,
+        "max_num_tiles": max_num_tiles,
+        "in_channels": in_channels,
+        "attn_bias": False,
+        "output_cls_projection": False,
+    }
+    if encoder_lora:
+        clip = lora_clip_vision_encoder(**clip_options, **lora_options)
+    else:
+        clip = clip_vision_encoder(**clip_options)
+
+    # Projection 
+    projection_options = {
+        "num_layers": num_layers_projection,
+        "num_heads": num_heads,
+        "decoder_embed_dim": decoder_embed_dim,
+        "clip_embed_dim": clip_embed_dim,
+        "num_hidden_inputs": len(clip_hidden_states or []),
+    }
+    if fusion_lora:
+        projection_head = lora_llama3_2_vision_projection_head(**projection_options, **lora_options)
+    else:
+        projection_head = lora_llama3_2_vision_projection_head(**projection_options)
+
+    return Llama3VisionEncoder(clip=clip, projection_head=projection_head)
+
+
+def lora_llama3_2_vision_decoder(
+    decoder_lora: bool,
+    fusion_lora: bool,
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # decoder params
+    vocab_size: int,
+    num_layers: int,
+    fusion_interval: int,
+    num_special_tokens: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    encoder_max_seq_len: int,
+    rope_base: int = 500000.0,
+    intermediate_dim: Optional[int] = None,
+     # LoRA parameters
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the Llama3 model with additional fused
+    cross attention layers. This includes:
+    - Token embeddings
+    - num_layers number of CausalSelfAttention blocks
+    - Fused cross attention layers every fusion_interval number of layers
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    Args:
+        decoder_lora (bool): whether to apply LoRA to the language decoder
+        fusion_lora (bool): whether to apply LoRA to the projection head
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        fusion_interval (int): interval number of layers between fusion layers.
+        num_special_tokens (int): number of special tokens added for the fusion model.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value.
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`.
+        encoder_max_seq_len (int): maximum sequence length the encoder will be run with, as used
+            by :func:`~torchtune.modules.KVCache`.
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`.
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Whether to use DoRA layers instead of LoRA layers. Default is ``False``.
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama 3.2 vision decoder.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    hidden_dim = intermediate_dim or scale_hidden_dim_for_mlp(embed_dim)
+    layers = []
+
+    rope = Llama3ScaledRoPE(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    for idx in range(1, num_layers + 1):
+
+        # Self attention layers for text decoder
+        self_attn = lora_llama3_attention(
+            lora_modules=lora_attn_modules,
+            pos_embeddings=rope,
+            head_dim=head_dim,
+            embed_dim=embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_kv_heads,
+            max_seq_len=max_seq_len,
+            attn_dropout=0.0,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+            quantize_base=quantize_base,
+        )
+        if apply_lora_to_mlp:
+            mlp = lora_llama3_mlp(
+                dim=embed_dim,
+                hidden_dim=hidden_dim,
+                lora_rank=lora_rank,
+                lora_alpha=lora_alpha,
+                quantize_base=quantize_base,
+                lora_dropout=lora_dropout,
+                use_dora=use_dora,
+            )
+        else:
+            mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim, quantize_base=quantize_base)
+        decoder_layer = TransformerSelfAttentionLayer(
+            attn=self_attn,
+            mlp=mlp,
+            sa_norm=RMSNorm(dim=embed_dim, eps=1e-5),
+            mlp_norm=RMSNorm(dim=embed_dim, eps=1e-5),
+        )
+
+        # cross attention layers, mixing text and vision,
+        # placed every `fusion_interval` layers
+        if idx % fusion_interval == 0:
+            attn = lora_llama3_attention(
+                lora_modules=lora_attn_modules,
+                pos_embeddings=None,
+                head_dim=head_dim,
+                embed_dim=embed_dim,
+                num_heads=num_heads,
+                num_kv_heads=num_kv_heads,
+                q_norm=RMSNorm(dim=head_dim, eps=1e-05),
+                k_norm=RMSNorm(dim=head_dim, eps=1e-05),
+                max_seq_len=encoder_max_seq_len,
+                is_causal=False,
+                attn_dropout=0.0,
+                lora_rank=lora_rank,
+                lora_alpha=lora_alpha,
+                lora_dropout=lora_dropout,
+                use_dora=use_dora,
+                quantize_base=quantize_base,
+            )
+            if apply_lora_to_mlp:
+                mlp = lora_llama3_mlp(
+                    dim=embed_dim,
+                    hidden_dim=hidden_dim,
+                    lora_rank=lora_rank,
+                    lora_alpha=lora_alpha,
+                    quantize_base=quantize_base,
+                    lora_dropout=lora_dropout,
+                    use_dora=use_dora,
+                )
+            else:
+                mlp = llama3_mlp(dim=embed_dim, hidden_dim=hidden_dim, quantize_base=quantize_base)
+            xattn_layer = TransformerCrossAttentionLayer(
+                attn=attn,
+                mlp=mlp,
+                ca_norm=RMSNorm(dim=embed_dim),
+                mlp_norm=RMSNorm(dim=embed_dim),
+                ca_scale=TanhGate(),
+                mlp_scale=TanhGate(),
+            )
+            fusion_layer = FusionLayer(layer=decoder_layer, fusion_layer=xattn_layer)
+            layers.append(fusion_layer)
+        else:
+            layers.append(decoder_layer)
+
+    tok_embeddings = FusionEmbedding(vocab_size, num_special_tokens, embed_dim)
+    
+     # TODO: quantize_base is not applied to final output_proj currently.
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    output_proj = (
+        adapter_cls(embed_dim, vocab_size, rank=lora_rank, alpha=lora_alpha, dropout=lora_dropout)
+        if apply_lora_to_output
+        else nn.Linear(embed_dim, vocab_size, bias=False)
+    )
+
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=1e-05),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to bf16, and offload to CPU on the fly
+        # so as to not increase peak memory
+        model._register_state_dict_hook(
+            partial(reparametrize_as_dtype_state_dict_post_hook, offload_to_cpu=True)
+        )
+
+    return model
+
+
+def lora_llama3_2_vision_projection_head(
+    lora_modules: List[LORA_ATTN_MODULES],
+    *,
+    # projection head parameters
+    num_layers: int,
+    num_heads: int,
+    decoder_embed_dim: int,
+    clip_embed_dim: int,
+    num_hidden_inputs: int,
+    # LoRA args
+    apply_lora_to_mlp: bool,
+    apply_lora_to_output: bool,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> Llama3VisionProjectionHead:
+    """
+    Build the Llama 3.2 Vision Projection Head with LoRA applied to a subset of the layers.
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to. Options are ``{"q_proj", "k_proj", "v_proj",
+            "output_proj"}``.
+        num_layers (int): number of layers in the projection head.
+        num_heads (int): number of heads in the projection head.
+        decoder_embed_dim (int): embedding dimension for the decoder.
+        clip_embed_dim (int): embedding dimension for the CLIP encoder.
+        num_hidden_inputs (int): number of hidden inputs to the projection head.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Whether to use DoRA layers instead of LoRA layers. Default is ``False``.
+        quantize_base (bool): Whether to quantize base model parameters for linear layers
+            LoRA is being applied to. Default is ``False``.
+
+    Returns:
+        Llama3VisionProjectionHead: Instantiation of Llama 3.2 vision projection head.
+    """
+    mlp_ratio = 4
+    hidden_dim = int(mlp_ratio * clip_embed_dim)
+    head_dim = clip_embed_dim // num_heads
+    num_kv_heads = num_heads
+
+    layers = []
+    for _ in range(num_layers):
+        self_attn = lora_clip_attention(
+            lora_modules=lora_modules,
+            embed_dim=clip_embed_dim,
+            num_heads=num_heads,
+            num_kv_heads=num_heads,
+            head_dim=head_dim,
+            attn_dropout=0.0,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+            quantize_base=quantize_base,
+        )
+
+        if apply_lora_to_mlp:
+            mlp = lora_clip_mlp(
+                in_dim=clip_embed_dim,
+                hidden_dim=hidden_dim,
+                out_dim=clip_embed_dim,
+                activation=nn.GELU(),
+                lora_rank=lora_rank,
+                lora_alpha=lora_alpha,
+                quantize_base=quantize_base,
+                lora_dropout=lora_dropout,
+                use_dora=use_dora,
+            )
+        else:
+            mlp = clip_mlp(
+                in_dim=clip_embed_dim,
+                hidden_dim=hidden_dim,
+                out_dim=clip_embed_dim,
+                activation=nn.GELU(),
+                quantize_base=quantize_base
+            )
+
+        layer = TransformerSelfAttentionLayer(
+            attn=self_attn,
+            mlp=mlp,
+            sa_norm=Fp32LayerNorm(clip_embed_dim, eps=1e-5),
+            mlp_norm=Fp32LayerNorm(clip_embed_dim, eps=1e-5),
+            sa_scale=TanhGate(),
+            mlp_scale=TanhGate(),
+        )
+        layers.append(layer)
+
+    # we concatenate clip embeddings and hidden layers output
+    # and project it to embed_dim_out, which will be used for the
+    # cross encoding
+    # TODO: quantize_base is not applied to final output_proj currently.
+    proj_in = clip_embed_dim * (num_hidden_inputs + 1)
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    output_proj = (
+        adapter_cls(proj_in, decoder_embed_dim, rank=lora_rank, alpha=lora_alpha, dropout=lora_dropout, use_bias=True)
+        if apply_lora_to_output
+        else nn.Linear(proj_in, decoder_embed_dim)
+    )
+    return Llama3VisionProjectionHead(
+        layers=layers,
+        output=output_proj,
+        num_hidden_inputs=num_hidden_inputs,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_convert_weights.py marc/third_party/torchtune/torchtune/models/llama3_2_vision/_convert_weights.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_convert_weights.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_2_vision/_convert_weights.py	2025-02-20 17:49:30.506025817 -0500
@@ -0,0 +1,429 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+from typing import Dict, List, Optional, Tuple
+
+import torch
+
+from torchtune.models.convert_weights import get_mapped_key
+
+_FROM_META = {
+    "text_model.tok_embeddings.weight": "decoder.tok_embeddings.weight",
+    "text_model.learnable_embedding.weight": "decoder.tok_embeddings.fusion_embedding.weight",
+    "text_model.norm.weight": "decoder.norm.scale",
+    "text_model.output.weight": "decoder.output.weight",
+    "text_model.layers.{}.attention_norm.weight": "decoder.layers.{}.sa_norm.scale",
+    "text_model.layers.{}.attention.wq.weight": "decoder.layers.{}.attn.q_proj.weight",
+    "text_model.layers.{}.attention.wk.weight": "decoder.layers.{}.attn.k_proj.weight",
+    "text_model.layers.{}.attention.wv.weight": "decoder.layers.{}.attn.v_proj.weight",
+    "text_model.layers.{}.attention.wo.weight": "decoder.layers.{}.attn.output_proj.weight",
+    "text_model.layers.{}.ffn_norm.weight": "decoder.layers.{}.mlp_norm.scale",
+    "text_model.layers.{}.feed_forward.w1.weight": "decoder.layers.{}.mlp.w1.weight",
+    "text_model.layers.{}.feed_forward.w3.weight": "decoder.layers.{}.mlp.w3.weight",
+    "text_model.layers.{}.feed_forward.w2.weight": "decoder.layers.{}.mlp.w2.weight",
+    "text_model.cross_attention_layers.{}.gate_attn": "decoder.layers.{}.fusion_layer.ca_scale.scale",
+    "text_model.cross_attention_layers.{}.gate_ffwd": "decoder.layers.{}.fusion_layer.mlp_scale.scale",
+    "text_model.cross_attention_layers.{}.attention_norm.weight": "decoder.layers.{}.fusion_layer.ca_norm.scale",
+    "text_model.cross_attention_layers.{}.ffn_norm.weight": "decoder.layers.{}.fusion_layer.mlp_norm.scale",
+    "text_model.cross_attention_layers.{}.attention.wq.weight": "decoder.layers.{}.fusion_layer.attn.q_proj.weight",
+    "text_model.cross_attention_layers.{}.attention.wk.weight": "decoder.layers.{}.fusion_layer.attn.k_proj.weight",
+    "text_model.cross_attention_layers.{}.attention.wv.weight": "decoder.layers.{}.fusion_layer.attn.v_proj.weight",
+    "text_model.cross_attention_layers.{}.attention.wo.weight": "decoder.layers.{}.fusion_layer.attn.output_proj.weight",
+    "text_model.cross_attention_layers.{}.attention.q_norm.weight": "decoder.layers.{}.fusion_layer.attn.q_norm.scale",
+    "text_model.cross_attention_layers.{}.attention.k_norm.weight": "decoder.layers.{}.fusion_layer.attn.k_norm.scale",
+    "text_model.cross_attention_layers.{}.feed_forward.w1.weight": "decoder.layers.{}.fusion_layer.mlp.w1.weight",
+    "text_model.cross_attention_layers.{}.feed_forward.w3.weight": "decoder.layers.{}.fusion_layer.mlp.w3.weight",
+    "text_model.cross_attention_layers.{}.feed_forward.w2.weight": "decoder.layers.{}.fusion_layer.mlp.w2.weight",
+    "vision_model.vision_encoder.positional_embedding": "encoder.clip.token_pos_embedding.local_token_positional_embedding",
+    "vision_model.vision_encoder.gated_positional_embedding": "encoder.clip.token_pos_embedding.global_token_positional_embedding",
+    "vision_model.vision_encoder.gated_positional_embedding_gate": "encoder.clip.token_pos_embedding.gate",
+    "vision_model.vision_encoder.ln_pre.weight": "encoder.clip.ln_pre.weight",
+    "vision_model.vision_encoder.ln_pre.bias": "encoder.clip.ln_pre.bias",
+    "vision_model.vision_encoder.ln_post.weight": "encoder.clip.ln_post.weight",
+    "vision_model.vision_encoder.ln_post.bias": "encoder.clip.ln_post.bias",
+    "vision_model.vision_encoder.pre_tile_pos_embed.embedding": "encoder.clip.pre_tile_pos_embed.embedding",
+    "vision_model.vision_encoder.pre_tile_pos_embed.gate": "encoder.clip.pre_tile_pos_embed.gate",
+    "vision_model.vision_encoder.post_tile_pos_embed.embedding": "encoder.clip.post_tile_pos_embed.embedding",
+    "vision_model.vision_encoder.post_tile_pos_embed.gate": "encoder.clip.post_tile_pos_embed.gate",
+    "vision_model.vision_encoder.class_embedding": "encoder.clip.cls_token_embedding.weight",
+    "vision_model.vision_encoder.conv1._linear.weight": "encoder.clip.conv.weight",
+    "vision_model.vision_encoder.transformer.resblocks.{}.attn.wq.weight": "encoder.clip.layers.{}.attn.q_proj.weight",
+    "vision_model.vision_encoder.transformer.resblocks.{}.attn.wk.weight": "encoder.clip.layers.{}.attn.k_proj.weight",
+    "vision_model.vision_encoder.transformer.resblocks.{}.attn.wv.weight": "encoder.clip.layers.{}.attn.v_proj.weight",
+    "vision_model.vision_encoder.transformer.resblocks.{}.attn.wo.weight": "encoder.clip.layers.{}.attn.output_proj.weight",
+    "vision_model.vision_encoder.transformer.resblocks.{}.mlp.c_fc.weight": "encoder.clip.layers.{}.mlp.w1.weight",
+    "vision_model.vision_encoder.transformer.resblocks.{}.mlp.c_fc.bias": "encoder.clip.layers.{}.mlp.w1.bias",
+    "vision_model.vision_encoder.transformer.resblocks.{}.mlp.c_proj.weight": "encoder.clip.layers.{}.mlp.w2.weight",
+    "vision_model.vision_encoder.transformer.resblocks.{}.mlp.c_proj.bias": "encoder.clip.layers.{}.mlp.w2.bias",
+    "vision_model.vision_encoder.transformer.resblocks.{}.ln_1.weight": "encoder.clip.layers.{}.sa_norm.weight",
+    "vision_model.vision_encoder.transformer.resblocks.{}.ln_1.bias": "encoder.clip.layers.{}.sa_norm.bias",
+    "vision_model.vision_encoder.transformer.resblocks.{}.ln_2.weight": "encoder.clip.layers.{}.mlp_norm.weight",
+    "vision_model.vision_encoder.transformer.resblocks.{}.ln_2.bias": "encoder.clip.layers.{}.mlp_norm.bias",
+    "vision_model.vision_projection.weight": "encoder.projection.output.weight",
+    "vision_model.vision_projection.bias": "encoder.projection.output.bias",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.attn.wq.weight": "encoder.projection.layers.{}.attn.q_proj.weight",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.attn.wk.weight": "encoder.projection.layers.{}.attn.k_proj.weight",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.attn.wv.weight": "encoder.projection.layers.{}.attn.v_proj.weight",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.attn.wo.weight": "encoder.projection.layers.{}.attn.output_proj.weight",  # noqa
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.mlp.c_fc.weight": "encoder.projection.layers.{}.mlp.w1.weight",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.mlp.c_fc.bias": "encoder.projection.layers.{}.mlp.w1.bias",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.mlp.c_proj.weight": "encoder.projection.layers.{}.mlp.w2.weight",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.mlp.c_proj.bias": "encoder.projection.layers.{}.mlp.w2.bias",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.ln_1.weight": "encoder.projection.layers.{}.sa_norm.weight",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.ln_1.bias": "encoder.projection.layers.{}.sa_norm.bias",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.ln_2.weight": "encoder.projection.layers.{}.mlp_norm.weight",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.ln_2.bias": "encoder.projection.layers.{}.mlp_norm.bias",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.gate_attn": "encoder.projection.layers.{}.sa_scale.scale",
+    "vision_model.vision_encoder.global_transformer.resblocks.{}.gate_ffn": "encoder.projection.layers.{}.mlp_scale.scale",
+}
+
+
+_FROM_HF = {
+    "language_model.model.embed_tokens.weight": "decoder.tok_embeddings.weight",
+    "language_model.model.layers.{}.self_attn.q_proj.weight": "decoder.layers.{}.attn.q_proj.weight",
+    "language_model.model.layers.{}.self_attn.k_proj.weight": "decoder.layers.{}.attn.k_proj.weight",
+    "language_model.model.layers.{}.self_attn.v_proj.weight": "decoder.layers.{}.attn.v_proj.weight",
+    "language_model.model.layers.{}.self_attn.o_proj.weight": "decoder.layers.{}.attn.output_proj.weight",
+    "language_model.model.layers.{}.self_attn.rotary_emb.inv_freq": None,
+    "language_model.model.layers.{}.mlp.gate_proj.weight": "decoder.layers.{}.mlp.w1.weight",
+    "language_model.model.layers.{}.mlp.up_proj.weight": "decoder.layers.{}.mlp.w3.weight",
+    "language_model.model.layers.{}.mlp.down_proj.weight": "decoder.layers.{}.mlp.w2.weight",
+    "language_model.model.layers.{}.input_layernorm.weight": "decoder.layers.{}.sa_norm.scale",
+    "language_model.model.layers.{}.post_attention_layernorm.weight": "decoder.layers.{}.mlp_norm.scale",
+    "language_model.model.norm.weight": "decoder.norm.scale",
+    "language_model.lm_head.weight": "decoder.output.weight",
+    "language_model.model.layers.{}.cross_attn_attn_gate": "decoder.layers.{}.fusion_layer.ca_scale.scale",
+    "language_model.model.layers.{}.cross_attn_mlp_gate": "decoder.layers.{}.fusion_layer.mlp_scale.scale",
+    "language_model.model.layers.{}.cross_attn.q_proj.weight": "decoder.layers.{}.fusion_layer.attn.q_proj.weight",
+    "language_model.model.layers.{}.cross_attn.k_proj.weight": "decoder.layers.{}.fusion_layer.attn.k_proj.weight",
+    "language_model.model.layers.{}.cross_attn.v_proj.weight": "decoder.layers.{}.fusion_layer.attn.v_proj.weight",
+    "language_model.model.layers.{}.cross_attn.o_proj.weight": "decoder.layers.{}.fusion_layer.attn.output_proj.weight",
+    "language_model.model.layers.{}.cross_attn.q_norm.weight": "decoder.layers.{}.fusion_layer.attn.q_norm.scale",
+    "language_model.model.layers.{}.cross_attn.k_norm.weight": "decoder.layers.{}.fusion_layer.attn.k_norm.scale",
+    "vision_model.gated_positional_embedding.embedding": "encoder.clip.token_pos_embedding.local_token_positional_embedding",
+    "vision_model.gated_positional_embedding.tile_embedding.weight": "encoder.clip.token_pos_embedding.global_token_positional_embedding",  # noqa
+    "vision_model.gated_positional_embedding.gate": "encoder.clip.token_pos_embedding.gate",
+    "vision_model.layernorm_pre.weight": "encoder.clip.ln_pre.weight",
+    "vision_model.layernorm_pre.bias": "encoder.clip.ln_pre.bias",
+    "vision_model.layernorm_post.weight": "encoder.clip.ln_post.weight",
+    "vision_model.layernorm_post.bias": "encoder.clip.ln_post.bias",
+    "vision_model.pre_tile_positional_embedding.embedding.weight": "encoder.clip.pre_tile_pos_embed.embedding",
+    "vision_model.pre_tile_positional_embedding.gate": "encoder.clip.pre_tile_pos_embed.gate",
+    "vision_model.post_tile_positional_embedding.embedding.weight": "encoder.clip.post_tile_pos_embed.embedding",
+    "vision_model.post_tile_positional_embedding.gate": "encoder.clip.post_tile_pos_embed.gate",
+    "vision_model.class_embedding": "encoder.clip.cls_token_embedding.weight",
+    "vision_model.patch_embedding.weight": "encoder.clip.conv.weight",
+    "vision_model.transformer.layers.{}.self_attn.q_proj.weight": "encoder.clip.layers.{}.attn.q_proj.weight",
+    "vision_model.transformer.layers.{}.self_attn.k_proj.weight": "encoder.clip.layers.{}.attn.k_proj.weight",
+    "vision_model.transformer.layers.{}.self_attn.v_proj.weight": "encoder.clip.layers.{}.attn.v_proj.weight",
+    "vision_model.transformer.layers.{}.self_attn.o_proj.weight": "encoder.clip.layers.{}.attn.output_proj.weight",
+    "vision_model.transformer.layers.{}.mlp.fc1.weight": "encoder.clip.layers.{}.mlp.w1.weight",
+    "vision_model.transformer.layers.{}.mlp.fc1.bias": "encoder.clip.layers.{}.mlp.w1.bias",
+    "vision_model.transformer.layers.{}.mlp.fc2.weight": "encoder.clip.layers.{}.mlp.w2.weight",
+    "vision_model.transformer.layers.{}.mlp.fc2.bias": "encoder.clip.layers.{}.mlp.w2.bias",
+    "vision_model.transformer.layers.{}.input_layernorm.weight": "encoder.clip.layers.{}.sa_norm.weight",
+    "vision_model.transformer.layers.{}.input_layernorm.bias": "encoder.clip.layers.{}.sa_norm.bias",
+    "vision_model.transformer.layers.{}.post_attention_layernorm.weight": "encoder.clip.layers.{}.mlp_norm.weight",
+    "vision_model.transformer.layers.{}.post_attention_layernorm.bias": "encoder.clip.layers.{}.mlp_norm.bias",
+    "vision_model.global_transformer.layers.{}.self_attn.q_proj.weight": "encoder.projection.layers.{}.attn.q_proj.weight",
+    "vision_model.global_transformer.layers.{}.self_attn.k_proj.weight": "encoder.projection.layers.{}.attn.k_proj.weight",
+    "vision_model.global_transformer.layers.{}.self_attn.v_proj.weight": "encoder.projection.layers.{}.attn.v_proj.weight",
+    "vision_model.global_transformer.layers.{}.self_attn.o_proj.weight": "encoder.projection.layers.{}.attn.output_proj.weight",
+    "vision_model.global_transformer.layers.{}.mlp.fc1.weight": "encoder.projection.layers.{}.mlp.w1.weight",
+    "vision_model.global_transformer.layers.{}.mlp.fc1.bias": "encoder.projection.layers.{}.mlp.w1.bias",
+    "vision_model.global_transformer.layers.{}.mlp.fc2.weight": "encoder.projection.layers.{}.mlp.w2.weight",
+    "vision_model.global_transformer.layers.{}.mlp.fc2.bias": "encoder.projection.layers.{}.mlp.w2.bias",
+    "vision_model.global_transformer.layers.{}.input_layernorm.weight": "encoder.projection.layers.{}.sa_norm.weight",
+    "vision_model.global_transformer.layers.{}.input_layernorm.bias": "encoder.projection.layers.{}.sa_norm.bias",
+    "vision_model.global_transformer.layers.{}.post_attention_layernorm.weight": "encoder.projection.layers.{}.mlp_norm.weight",
+    "vision_model.global_transformer.layers.{}.post_attention_layernorm.bias": "encoder.projection.layers.{}.mlp_norm.bias",
+    "vision_model.global_transformer.layers.{}.gate_attn": "encoder.projection.layers.{}.sa_scale.scale",
+    "vision_model.global_transformer.layers.{}.gate_ffn": "encoder.projection.layers.{}.mlp_scale.scale",
+    "multi_modal_projector.weight": "encoder.projection.output.weight",
+    "multi_modal_projector.bias": "encoder.projection.output.bias",
+}
+
+
+def _layer_num(key: str):
+    """Get layer number from key or return None"""
+    layer_num = [int(k) for k in key.split(".") if k.isdigit()]
+    if len(layer_num) > 1:
+        raise ValueError("More than one number in key, ambiguous input")
+    elif len(layer_num) == 1:
+        return int(layer_num[0])
+    else:
+        return None
+
+
+def llama3_vision_meta_to_tune(
+    state_dict: Dict[str, torch.Tensor]
+) -> Dict[str, torch.Tensor]:
+    """
+    Convertor from Meta state dict to torchtune state dict. This handles:
+    - Updateing the cross attention layer numbers
+    - reshaping the convolution weights
+    - skip loading the rope embeddings
+    """
+    converted_state_dict = {}
+
+    # Calculate fusion_interval: layer interval where cross attention layers are fused
+    num_layers = max(_layer_num(k) for k in state_dict if "layers" in k) + 1
+    num_fusion_layers = (
+        max(_layer_num(k) for k in state_dict if "cross_attention_layers" in k) + 1
+    )
+    assert (
+        num_layers % num_fusion_layers == 0
+    ), "Conversion assumes cross attention is added at regular intervals"
+    fusion_interval = num_layers // num_fusion_layers
+
+    for key, value in state_dict.items():
+        if key == "text_model.rope.freqs":
+            continue
+        new_key = get_mapped_key(key, _FROM_META)
+        if "cross_attention_layers" in key:
+            layer = int(key.split(".")[2])
+            new_layer = (layer + 1) * fusion_interval - 1
+            key_lst = new_key.split(".")
+            key_lst[2] = str(new_layer)
+            new_key = ".".join(key_lst)
+        elif "conv1" in key:
+            dim, flat_patch = value.shape
+            patch_size = int(math.sqrt(flat_patch / 3))
+            assert (
+                3 * patch_size**2 == flat_patch
+            ), "Conversion assumes 3 channel inputs and square patch size"
+            value = value.reshape(dim, 3, patch_size, patch_size)
+        converted_state_dict[new_key] = value
+    return converted_state_dict
+
+
+def llama3_vision_tune_to_meta(
+    state_dict: Dict[str, torch.Tensor]
+) -> Dict[str, torch.Tensor]:
+    """
+    Convertor from torchtune state dict to Meta state dict. This handles:
+    - Updateing the cross attention layer numbers
+    - reshaping the convolution weights
+    """
+    converted_state_dict = {}
+    inverted_mapping_dict = {v: k for k, v in _FROM_META.items()}
+
+    # Calculate fusion_interval: layer interval where cross attention layers are fused
+    num_layers = max(_layer_num(k) for k in state_dict if "layers" in k) + 1
+    # Get the number of unique fusion layers.
+    # Keys have the form decoder.fusion_layer.i. ... where i is the layer number
+    num_fusion_layers = len(
+        set([k.split(".")[2] for k in state_dict if "fusion_layer" in k])
+    )
+    assert (
+        num_layers % num_fusion_layers == 0
+    ), "Conversion assumes cross attention is added at regular intervals"
+    fusion_interval = num_layers // num_fusion_layers
+
+    for key, value in state_dict.items():
+        new_key = get_mapped_key(key, inverted_mapping_dict)
+        if "fusion_layer" in key:
+            layer = int(key.split(".")[2])
+            new_layer = (layer + 1) // fusion_interval - 1
+            key_lst = new_key.split(".")
+            key_lst[2] = str(new_layer)
+            new_key = ".".join(key_lst)
+        elif "conv" in key:
+            dim = value.shape[0]
+            value = value.reshape(dim, -1)
+        converted_state_dict[new_key] = value
+    return converted_state_dict
+
+
+def llama3_vision_hf_to_tune(
+    state_dict: Dict[str, torch.Tensor],
+    num_heads: int = 32,
+    num_kv_heads: int = 32,
+    dim: int = 4096,
+    head_dim: int = None,
+    vocab_size: int = 128256,
+    cross_attention_layers: Optional[List[int]] = None,
+    # Vision Encoder Paramters
+    encoder_dim: int = 1280,
+    tile_size: int = 448,
+    num_tiles: int = 4,
+    supported_aspect_ratios: List[Tuple[int, int]] = None,
+) -> Dict[str, torch.Tensor]:
+    """
+    Convertor from HF state dict to torchtune state dict. This handles:
+    - Updating the cross attention layer numbers
+    - skip loading the rope embeddings
+    - reshaping q, k projections
+    - reversing the precomputed vision positional embeddings
+    """
+    converted_state_dict = {}
+    if head_dim is None:
+        head_dim = dim // num_heads
+    if cross_attention_layers is None:
+        cross_attention_layers = []
+
+    def _permute(t, n_heads):
+        return (
+            t.view(n_heads, 2, head_dim // 2, dim)
+            .transpose(1, 2)
+            .reshape((head_dim * n_heads), dim)
+        )
+
+    for key, value in state_dict.items():
+        if "rotary_emb.inv_freq" in key:  # Skip loading the position embeddings
+            continue
+        new_key = get_mapped_key(key, _FROM_HF)
+        if "language_model" in key:
+            if "layers" in key:  # Update layer numbers
+                layer = int(key.split(".")[3])
+                num_shifts = sum(layer > l for l in cross_attention_layers)
+                new_layer = layer - num_shifts
+                key_lst = new_key.split(".")
+                if layer in cross_attention_layers and "fusion_layer" not in new_key:
+                    # some keys are the same for sa and ca, so we need to edit them here
+                    key_lst[2] = f"{new_layer}.fusion_layer"
+                    if "sa_norm" in new_key:
+                        key_lst[3] = "ca_norm"
+                else:
+                    key_lst[2] = str(new_layer)
+                new_key = ".".join(key_lst)
+            if "q_proj" in key and "cross_attn" not in key:
+                value = _permute(value, num_heads)
+            elif "k_proj" in key and "cross_attn" not in key:
+                value = _permute(value, num_kv_heads)
+            elif new_key == "decoder.tok_embeddings.weight":
+                # Split embedding between learnable embeddings and original text embedding
+                learned_embedding = "decoder.tok_embeddings.fusion_embedding.weight"
+                converted_state_dict[learned_embedding] = value[vocab_size:]
+                value = value[:vocab_size]
+        elif "vision_model" in key:
+            if (
+                "tile_pos_embed.embedding" in new_key
+                or "global_token_positional_embedding" in new_key
+            ):
+                # WARNING: META format postional embeddings contain embeddings that
+                # the model can never use (4 tiles -> 4 x 4 embeddings -> a 4 x 4 image would be 16 tiles).
+                # HF removes these extra embeddings, for us to convert to the META format we set those
+                # unused embeddings as 0 instead of the original random (untrained) values in the original
+                # META checkpoing
+                num_embeds = value.shape[-1] // encoder_dim // num_tiles
+                pos_embedding = torch.zeros(
+                    num_tiles,
+                    num_tiles,
+                    num_embeds,
+                    encoder_dim,
+                    device=value.device,
+                    dtype=value.dtype,
+                )
+                # Loop through aspect ratios and assign precomputed embeds back to Meta Llama embeddings
+                for i, (h, w) in enumerate(supported_aspect_ratios or []):
+                    if h * w == num_tiles:  # h*w < num_tiles is redundant
+                        # i == 0 is used for padding in HF
+                        pos_embedding[:h, :w] = value[i + 1].reshape(
+                            h, w, num_embeds, encoder_dim
+                        )
+                value = pos_embedding
+
+        converted_state_dict[new_key] = value
+    return converted_state_dict
+
+
+def llama3_vision_tune_to_hf(
+    state_dict: Dict[str, torch.Tensor],
+    num_heads: int = 32,
+    num_kv_heads: int = 32,
+    dim: int = 4096,
+    head_dim: int = None,
+    vocab_size: int = 128256,
+    cross_attention_layers: Optional[List[int]] = None,
+    # Vision Encoder Paramters
+    encoder_dim: int = 1280,
+    tile_size: int = 448,
+    num_tiles: int = 4,
+    supported_aspect_ratios: List[Tuple[int, int]] = None,
+) -> Dict[str, torch.Tensor]:
+    """
+    Convertor from Tune state dict to HF state dict. This handles:
+    - Updateing the cross attention layer numbers
+    - skip loading the rope embeddings
+    - reshaping q, k projections
+    """
+    converted_state_dict = {}
+    inverted_mapping_dict = {v: k for k, v in _FROM_HF.items()}
+    # missing keys in _FROM_HF due to naming collisions
+    missing_keys = {
+        "decoder.layers.{}.fusion_layer.ca_norm.scale": "language_model.model.layers.{}.input_layernorm.weight",
+        "decoder.layers.{}.fusion_layer.mlp_norm.scale": "language_model.model.layers.{}.post_attention_layernorm.weight",
+        "decoder.layers.{}.fusion_layer.mlp.w1.weight": "language_model.model.layers.{}.mlp.gate_proj.weight",
+        "decoder.layers.{}.fusion_layer.mlp.w3.weight": "language_model.model.layers.{}.mlp.up_proj.weight",
+        "decoder.layers.{}.fusion_layer.mlp.w2.weight": "language_model.model.layers.{}.mlp.down_proj.weight",
+        "decoder.tok_embeddings.fusion_embedding.weight": None,
+    }
+    inverted_mapping_dict.update(missing_keys)
+
+    if head_dim is None:
+        head_dim = dim // num_heads
+    if cross_attention_layers is None:
+        cross_attention_layers = []
+    # convert hf layer numbers to tune numbers
+    cross_attention_layers = [
+        l - i for i, l in enumerate(sorted(cross_attention_layers))
+    ]
+
+    def _permute(t, n_heads):
+        return (
+            t.view(n_heads, head_dim // 2, 2, dim)
+            .transpose(1, 2)
+            .reshape((head_dim * n_heads), dim)
+        )
+
+    for key, value in state_dict.items():
+        new_key = get_mapped_key(key, inverted_mapping_dict)
+        if "decoder" in key:
+            if "layers" in key:  # Update layer numbers
+                layer = int(key.split(".")[2])
+                num_shifts = sum(layer > l for l in cross_attention_layers)
+                new_layer = layer + num_shifts
+                key_lst = new_key.split(".")
+                if layer in cross_attention_layers and "fusion_layer" not in key:
+                    new_layer += 1  # hf treats the fusion_layer as an additional layer
+                key_lst[3] = str(new_layer)
+                new_key = ".".join(key_lst)
+            if "q_proj" in key and "cross_attn" not in new_key:
+                value = _permute(value, num_heads)
+            elif "k_proj" in key and "cross_attn" not in new_key:
+                value = _permute(value, num_kv_heads)
+            elif key == "decoder.tok_embeddings.weight":
+                learned_embedding = state_dict[
+                    "decoder.tok_embeddings.fusion_embedding.weight"
+                ]
+                value = torch.cat([value, learned_embedding])
+            elif key == "decoder.tok_embeddings.fusion_embedding.weight":
+                continue
+        elif "encoder" in key:
+            if (
+                "tile_pos_embed.embedding" in key
+                or "global_token_positional_embedding" in key
+            ):
+                num_embeds = value.shape[-2]
+                pos_embedding = torch.zeros(
+                    len(supported_aspect_ratios) + 1,
+                    num_tiles,
+                    num_embeds,
+                    encoder_dim,
+                    device=value.device,
+                    dtype=value.dtype,
+                )
+                # Loop through aspect ratios and precompute embeds per aspect ratio
+                for i, (h, w) in enumerate(supported_aspect_ratios or []):
+                    pos_embedding[i + 1, : h * w] = value[:h, :w].reshape(
+                        h * w, num_embeds, encoder_dim
+                    )
+                value = pos_embedding.flatten(1)
+
+        converted_state_dict[new_key] = value
+    return converted_state_dict
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_encoder.py marc/third_party/torchtune/torchtune/models/llama3_2_vision/_encoder.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_encoder.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_2_vision/_encoder.py	2025-02-20 17:49:30.510025824 -0500
@@ -0,0 +1,125 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional
+
+import torch
+
+from torch import nn
+from torchtune.modules.model_fusion import register_fusion_module
+
+
+class Llama3VisionProjectionHead(nn.Module):
+    """Projection transformer to adapt the output of a
+    pretrained frozen encoder (CLIP) to a pretrained decoder model.
+    For example, nn.Sequential(CLIP(), Llama3VisionProjectionHead()).
+
+    Args:
+        layers (nn.Module): Transformer Decoder layers
+        output (nn.Module): Output linear layer. Input dim is
+            (num_hidden + 1) * encoder_dim and output is decoder_dim.
+        num_hidden_inputs (int): Number of expected hidden state inputs
+    """
+
+    def __init__(
+        self,
+        layers: nn.Module,
+        output: nn.Module,
+        num_hidden_inputs: int = 0,
+    ) -> None:
+        super().__init__()
+        self.layers = nn.ModuleList(layers)
+        self.output = output
+        self.num_hidden = num_hidden_inputs
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        hidden_states: Optional[List[torch.Tensor]] = None,
+    ) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape [b x i x t x e x d]
+            hidden_states (Optional[List[torch.Tensor]]): list of hidden states
+                from the encoder. Each hidden state has the same shape as x.
+
+        Returns:
+            Tensor: output tensor of a sequence of embedings [b x s x d]
+                where sequence length is num_imgs*num_tiles+num_embeds
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - i: number of images
+            - t: number of tiles (where a single image is broken into multiple tiles)
+            - e: number of embeds per tile (e.g. CLS embed + patch embeds, etc.)
+            - s: sequence length computed by i*t*e
+            - d: embed dim
+        """
+        bsz, imgs, tiles, embeds, dim = x.shape
+
+        # apply transformer layers
+        x = x.view(bsz * imgs, tiles * embeds, dim)
+        for layer in self.layers:
+            x = layer(x)
+        x = x.view(bsz, imgs, tiles, embeds, dim)
+
+        # interleave hidden states and cat with x
+        if self.num_hidden > 0:
+            hidden_states = torch.stack(hidden_states, dim=-1)
+            hidden_states = hidden_states.view(bsz, imgs, tiles, embeds, -1)
+            x = torch.cat([x, hidden_states], dim=-1)
+
+        # shape [b x s x d]
+        x = self.output(x).reshape(bsz, imgs * tiles * embeds, -1)
+
+        return x
+
+
+class Llama3VisionEncoder(nn.Module):
+    """Vision encoder model for Llama 3.2 Vision. This combines a pretrained
+    vision encoder with a learnable projection head. The projection head
+    is converted to a fusion module and supports fusion utils.
+
+    Args:
+        clip (nn.Module): CLIP encoder vision model
+        projection_head (nn.Module): projection_head that takes embeddings
+            with dimension encoder_dim as input and outputs embeddings of
+            size decoder_dim.
+    """
+
+    def __init__(self, clip: nn.Module, projection_head: nn.Module) -> None:
+        super().__init__()
+        self.clip = clip
+        self.projection = projection_head
+        register_fusion_module(self.projection)
+
+    def forward(
+        self, images: torch.Tensor, aspect_ratio: Optional[torch.Tensor] = None
+    ) -> torch.Tensor:
+        """
+        Args:
+            images (torch.Tensor): Image tensor with shape [b x i x t x c x w x h]
+            aspect_ratio (Optional[torch.Tensor]): Tensor with shape [b x i x 2]. If all
+                images have a single tile, i.e. they were not tile-cropped, it should be None.
+                Used to calculate the positional embeddings for the tiles.
+
+        Returns:
+            Tensor: output tensor of a sequence of embedings [b x s x d]
+                where sequence length is num_imgs*num_tiles+num_embeds
+
+         Notation used for tensor shapes:
+            - b: batch size
+            - i: number of images
+            - t: number of tiles (where a single image is broken into multiple tiles)
+            - c: number of image channels (e.g. rgb = 3)
+            - w: image width
+            - h: image height
+            - s: sequence length computed by i*t*clip_embeds_per_tile
+            - d: embed dim
+        """
+        x, hidden_states = self.clip(images, aspect_ratio)
+        x = self.projection(x, hidden_states)
+        return x
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/__init__.py marc/third_party/torchtune/torchtune/models/llama3_2_vision/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_2_vision/__init__.py	2025-02-20 17:49:30.498025804 -0500
@@ -0,0 +1,35 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import (  # noqa
+    llama3_2_vision_decoder,
+    llama3_2_vision_encoder,
+    lora_llama3_2_vision_decoder,
+    lora_llama3_2_vision_encoder,
+)
+from ._encoder import Llama3VisionEncoder, Llama3VisionProjectionHead
+
+from ._model_builders import (  # noqa
+    llama3_2_vision_11b,
+    llama3_2_vision_transform,
+    lora_llama3_2_vision_11b,
+    qlora_llama3_2_vision_11b,
+)
+from ._transform import Llama3VisionTransform
+
+__all__ = [
+    "llama3_2_vision_11b",
+    "llama3_2_vision_transform",
+    "lora_llama3_2_vision_11b",
+    "qlora_llama3_2_vision_11b",
+    "llama3_2_vision_decoder",
+    "llama3_2_vision_encoder",
+    "lora_llama3_2_vision_decoder",
+    "lora_llama3_2_vision_encoder",
+    "Llama3VisionEncoder",
+    "Llama3VisionProjectionHead",
+    "Llama3VisionTransform",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_model_builders.py marc/third_party/torchtune/torchtune/models/llama3_2_vision/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_2_vision/_model_builders.py	2025-02-20 17:49:30.514025830 -0500
@@ -0,0 +1,225 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+from typing import List, Optional
+
+import torch
+from torchtune.models.llama3_2_vision._component_builders import (  # noqa
+    lora_llama3_2_vision_decoder,
+    lora_llama3_2_vision_encoder,
+    LoRATrainable,
+    llama3_2_vision_decoder,
+    llama3_2_vision_encoder,
+)
+from torchtune.models.llama3_2_vision._encoder import Llama3VisionEncoder
+from torchtune.models.llama3_2_vision._transform import Llama3VisionTransform
+from torchtune.modules.model_fusion import DeepFusionModel
+from torchtune.modules.tokenizers import parse_hf_tokenizer_json
+from torchtune.data._prompt_templates import _TemplateType
+from torchtune.data._prompt_templates import _get_prompt_template
+from torchtune.modules.peft import LORA_ATTN_MODULES
+
+def llama3_2_vision_11b(
+    decoder_trainable: bool = False,
+    encoder_trainable: bool = True,
+    fusion_trainable: bool = True,
+    image_size: int = 560
+    ) -> DeepFusionModel:
+    """ Llama 3.2 Vision 11B model
+
+    Args:
+        decoder_trainable (bool): Whether to make decoder params trainable. Default is False.
+        encoder_trainable (bool): Whether to make encoder params trainable. Default is True.
+        fusion_trainable (bool): Whether to make fusion params trainable. Default is True.
+        image_size (int): Base image size that images will be tiled and resized to. 
+            Default is 560 for Instruct weights, use 448 for pre-trained.
+
+    Returns:
+        DeepFusionModel: Instantiation of the Llama 3.2 Vision 11B model
+    """
+    encoder = llama3_2_vision_encoder(
+        patch_size=14,
+        num_heads=16,
+        clip_embed_dim=1280,
+        clip_num_layers=32,
+        clip_hidden_states=[3, 7, 15, 23, 30],
+        decoder_embed_dim=4096,
+        num_layers_projection=8,
+        tile_size=image_size,
+        max_num_tiles=4,
+        in_channels=3,
+    )
+    decoder = llama3_2_vision_decoder(
+        vocab_size=128_256,
+        num_layers=32,
+        fusion_interval=4,
+        num_special_tokens=8,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        max_seq_len=131_072,
+        encoder_max_seq_len=128_080,
+        rope_base=500000.0,
+        intermediate_dim=14336,
+    )
+    return DeepFusionModel(
+        encoder=encoder,
+        decoder=decoder,
+        encoder_trainable=encoder_trainable,
+        decoder_trainable=decoder_trainable,
+        fusion_trainable=fusion_trainable,
+    )
+
+
+def llama3_2_vision_transform(
+        path: str, max_seq_len: int = 8192, image_size: int = 560, special_tokens_path: Optional[str] = None, prompt_template: Optional[_TemplateType] = None
+    ) -> Llama3VisionTransform:
+    """
+    Data Transforms (including Tokenizer) for Llama3 Vision.
+
+    Args:
+        path (str): path to the tokenizer
+        max_seq_len (int): maximum sequence length for tokenizing a single list of messages,
+            after which the input will be truncated.
+        image_size (int): Base image size that images will be tiled and resized to. 
+            Default is 560 for Instruct weights, use 448 for pre-trained.
+        special_tokens_path (Optional[str]): Path to ``tokenizer.json`` from Hugging Face
+            model files that contains all registered special tokens, or a local json file 
+            structured similarly. Default is None to use the canonical Llama3 special tokens.
+        prompt_template (Optional[_TemplateType]): optional specified prompt template.
+            If a string, it is assumed to be the dotpath of a :class:`~torchtune.data.PromptTemplateInterface`
+            class. If a dictionary, it is assumed to be a custom prompt template mapping role to the
+            prepend/append tags.
+    
+    Returns:
+        Llama3VisionTransform: Instantiation of the Llama 3.2 vision transform
+    """
+    special_tokens = parse_hf_tokenizer_json(special_tokens_path) if special_tokens_path is not None else None
+    template = _get_prompt_template(prompt_template) if prompt_template is not None else None
+    return Llama3VisionTransform(
+        path=path,
+        special_tokens=special_tokens,
+        tile_size=image_size,
+        patch_size=14,
+        max_num_tiles=4,
+        max_seq_len=max_seq_len,
+        image_mean=(0.48145466, 0.4578275, 0.40821073),
+        image_std=(0.26862954, 0.26130258, 0.27577711),
+        prompt_template=template,
+    )
+
+
+def lora_llama3_2_vision_11b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    decoder_trainable: str = "frozen", 
+    encoder_trainable: str = "lora",
+    fusion_trainable: str = "lora",
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+    image_size: int = 560
+) -> DeepFusionModel:
+    """
+    Return a version of Llama3.2 vision (an instance of :func:`~torchtune.modules.model_fusion.DeepFusionModel`)
+    with LoRA applied based on the passed in configuration.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        decoder_trainable (str): Option to set decoder params as fully trainble (full), lora trainable (lora), 
+            or frozen (frozen). The default is "frozen".
+        encoder_trainable (str): Option to set encoder params as fully trainble (full), lora trainable (lora), 
+            or frozen (frozen). The default is "lora".
+        fusion_trainable (str): Option to set fusion params as fully trainble (full), lora trainable (lora), 
+            or frozen (frozen). The default is "lora".
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+        image_size (int): Base image size that images will be tiled and resized to. 
+            Default is 560 for Instruct weights, use 448 for pre-trained.
+
+    Returns:
+        DeepFusionModel: Instantiation of Llama3.2 vision model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    """
+    decoder_type = LoRATrainable(decoder_trainable.lower())
+    encoder_type = LoRATrainable(encoder_trainable.lower())
+    fusion_type = LoRATrainable(fusion_trainable.lower())
+    encoder = lora_llama3_2_vision_encoder(
+        encoder_lora=encoder_type == LoRATrainable.LORA,
+        fusion_lora=fusion_type == LoRATrainable.LORA,
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        patch_size=14,
+        num_heads=16,
+        clip_embed_dim=1280,
+        clip_num_layers=32,
+        clip_hidden_states=[3, 7, 15, 23, 30],
+        decoder_embed_dim=4096,
+        num_layers_projection=8,
+        tile_size=image_size,
+        max_num_tiles=4,
+        in_channels=3,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+    decoder = lora_llama3_2_vision_decoder(
+        decoder_lora=decoder_type == LoRATrainable.LORA,
+        fusion_lora=fusion_type == LoRATrainable.LORA,
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=128_256,
+        num_layers=32,
+        fusion_interval=4,
+        num_special_tokens=8,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        max_seq_len=8192,
+        encoder_max_seq_len=64040,
+        rope_base=500000.0,
+        intermediate_dim=14336,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+    return DeepFusionModel(
+        encoder=encoder,
+        decoder=decoder,
+        encoder_trainable=encoder_type != LoRATrainable.FROZEN,
+        decoder_trainable=decoder_type != LoRATrainable.FROZEN,
+        fusion_trainable=fusion_type != LoRATrainable.FROZEN,
+    )
+
+
+qlora_llama3_2_vision_11b = partial(lora_llama3_2_vision_11b, quantize_base=True)
+
+qlora_llama3_2_vision_11b.__doc__ = """
+Builder for creating a Llama3.2 vision 11B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_llama3_2_vision_11b` for full API arguments.
+"""
diff -ruN marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_transform.py marc/third_party/torchtune/torchtune/models/llama3_2_vision/_transform.py
--- marc_original/third_party/torchtune/torchtune/models/llama3_2_vision/_transform.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/llama3_2_vision/_transform.py	2025-02-20 17:49:30.518025837 -0500
@@ -0,0 +1,218 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Dict, List, Mapping, Optional, Tuple
+
+from torchtune.data import Message, PromptTemplate
+
+from torchtune.models.clip import CLIPImageTransform
+from torchtune.models.llama3 import llama3_tokenizer
+from torchtune.modules.tokenizers import ModelTokenizer
+from torchtune.modules.transforms import Transform, VisionCrossAttentionMask
+
+
+class Llama3VisionTransform(ModelTokenizer, Transform):
+    """
+    This transform combines the transforms for the different modalities of Llama 3.2 Vision. It
+    is made up of the following transforms:
+    - :class:`torchtune.models.llama3.Llama3Tokenizer`
+    - :class:`torchtune.models.clip.CLIPImageTransform`
+    - :class:`torchtune.modules.transforms.VisionCrossAttentionMask`
+
+    This transform can be used as a drop-in replacement for tokenizers in recipes and generation
+    but handles additional transformations from the `__call__` method.
+
+    Args:
+        path (str): Path to pretrained tiktoken tokenizer file.
+        tile_size (int): Size of the tiles to divide the image into.
+        patch_size (int): Size of the patches used in the CLIP vision tranformer model. This is
+            used to calculate the number of image embeddings per image.
+        max_num_tiles (int): Only used if possible_resolutions is NOT given.
+            Maximum number of tiles to break an image into.
+            This will be used to generate possible_resolutions,
+            e.g. [(224, 224), (224, 448), (448, 224)] if max_num_tiles = 2 and tile_size = 224.
+            Default 4.
+        special_tokens (Optional[Dict[str, int]]): mapping containing special text tokens and
+            their registered token IDs. If left as None, this will be set to the canonical
+            Llama3 special tokens.
+        max_seq_len (Optional[int]): maximum sequence length for tokenizing a single list of messages,
+            after which the input will be truncated. Default is None.
+        image_mean (Optional[Tuple[float, float, float]]): Mean values of each channel, used for normalization.
+        image_std (Optional[Tuple[float, float, float]]): Standard deviations for each channel, used for normalization.
+        prompt_template (Optional[PromptTemplate]): template used to format the messages based on their role. This is used
+            to add structured text around the actual messages. The structured text is used in three scenarios:
+
+            - Task-specific templates to gear models for a particular task that it will expect after training
+            - Model-specific templates that are required whenever the model is prompted, such as the [INST]
+              tags in Llama2 and in Mistral
+            - Community standardized templates, such as :class:`~torchtune.data.ChatMLTemplate`
+
+            The extra text will still get tokenized as normal text, not as special tokens. Default is None.
+
+    Examples:
+        >>> model_transform = Llama3VisionTransform("/path/to/tokenizer.model", tile_size=224, patch_size=14)
+        >>> transformed_data = model_transform({"messages": user_message, "images": [img1, img2]})
+        >>> print(transformed_data["tokens"])
+        [1, 31587, 29644, 102, 2]
+        >>> print(transformed_data["images"][0].shape)
+        torch.Size([4, 3, 224, 224])
+    """
+
+    def __init__(
+        self,
+        path: str,
+        *,
+        tile_size: int,
+        patch_size: int,
+        max_num_tiles: int = 4,
+        special_tokens: Optional[Dict[str, int]] = None,
+        max_seq_len: Optional[int] = None,
+        image_mean: Optional[Tuple[float, float, float]] = None,
+        image_std: Optional[Tuple[float, float, float]] = None,
+        prompt_template: Optional[PromptTemplate] = None,
+    ):
+        self.tokenizer = llama3_tokenizer(
+            path,
+            special_tokens_path=special_tokens,
+            max_seq_len=max_seq_len,
+            prompt_template=prompt_template,
+        )
+        self.transform_image = CLIPImageTransform(
+            image_mean=image_mean,
+            image_std=image_std,
+            tile_size=tile_size,
+            possible_resolutions=None,
+            max_num_tiles=max_num_tiles,
+            pad_max_tiles=True,
+            resample="bilinear",
+            resize_to_max_canvas=False,
+        )
+        self.xattn_mask = VisionCrossAttentionMask(
+            tile_size=tile_size,
+            patch_size=patch_size,
+            image_token_id=self.tokenizer.image_id,
+            max_num_tiles=max_num_tiles,
+        )
+
+        self.stop_tokens = self.tokenizer.stop_tokens
+        self.max_seq_len = max_seq_len
+        self.image_seq_len = max_num_tiles * (self.xattn_mask.patches_per_tile + 1)
+        self.prompt_template = prompt_template
+        self.pad_id = self.tokenizer.pad_id
+
+    @property
+    def base_vocab_size(self) -> int:
+        return self.tokenizer.base_vocab_size
+
+    @property
+    def vocab_size(self) -> int:
+        return self.tokenizer.vocab_size
+
+    def encode(
+        self,
+        text: str,
+        add_bos: bool = True,
+        add_eos: bool = True,
+    ) -> List[int]:
+        return self.tokenizer.encode(text=text, add_bos=add_bos, add_eos=add_eos)
+
+    def decode(
+        self,
+        token_ids: List[int],
+        truncate_at_eos: bool = True,
+        skip_special_tokens: bool = True,
+    ) -> str:
+        """
+        Decode a list of token ids into a string.
+
+        Args:
+            token_ids (List[int]): The list of token ids.
+            truncate_at_eos (bool): Whether to truncate the string at the end of
+                sequence token. Default is True.
+            skip_special_tokens (bool): Whether to show or skip special tokens in the decoded string.
+                Default is True.
+
+        Returns:
+            str: The decoded string.
+        """
+        return self.tokenizer.decode(
+            token_ids,
+            truncate_at_eos=truncate_at_eos,
+            skip_special_tokens=skip_special_tokens,
+        )
+
+    def tokenize_message(
+        self,
+        message: Message,
+        tokenize_header: bool = True,
+        tokenize_end: bool = True,
+    ) -> List[int]:
+        """
+        Tokenize a message into a list of token ids.
+
+        Args:
+            message (Message): The message to tokenize.
+            tokenize_header (bool): Whether to prepend a tokenized header to the message.
+            tokenize_end (bool): Whether to append eot or eom id at the end of the message.
+
+        Returns:
+            List[int]: The list of token ids.
+        """
+        return self.tokenizer.tokenize_message(
+            message=message,
+            tokenize_header=tokenize_header,
+            tokenize_end=tokenize_end,
+        )
+
+    def tokenize_messages(
+        self,
+        messages: List[Message],
+        add_eos: bool = True,
+    ) -> Tuple[List[int], List[bool]]:
+        """
+        Tokenize a list of messages into a list of token ids and masks.
+
+        Args:
+            messages (List[Message]): The list of messages to tokenize.
+            add_eos (bool): Wether to add the tokenizer's eos_id. Default True.
+
+        Returns:
+            Tuple[List[int], List[bool]]: The list of token ids and the list of masks.
+        """
+        return self.tokenizer.tokenize_messages(
+            messages=messages,
+            add_eos=add_eos,
+        )
+
+    def __call__(
+        self, sample: Mapping[str, Any], inference: bool = False
+    ) -> Mapping[str, Any]:
+        """
+        Apply image decoding, transformations and tokenization to messages in the sample.
+
+        Args:
+            sample (Mapping[str, Any]): A sample with a "messages" field.
+            inference (bool): Whether to run in inference mode. Default is True.
+
+        Returns:
+            Mapping[str, Any]: The transformed sample with the following fields:
+                - tokens: List[int] of tokenized messages
+                - mask: List[bool] of masks for the tokenized messages
+                - encoder_input: Dict[str, Any] of transformed images
+                - encoder_mask: List[bool] of masks for the transformed images
+        """
+        encoder_input = {"images": [], "aspect_ratio": []}
+        messages = sample["messages"]
+        for message in messages:
+            for image in message.get_media():
+                out = self.transform_image({"image": image}, inference=inference)
+                encoder_input["images"].append(out["image"])
+                encoder_input["aspect_ratio"].append(out["aspect_ratio"])
+
+        sample["encoder_input"] = encoder_input
+        sample = self.tokenizer(sample, inference=inference)
+        sample = self.xattn_mask(sample, inference=inference)
+        return sample
diff -ruN marc_original/third_party/torchtune/torchtune/models/mistral/_component_builders.py marc/third_party/torchtune/torchtune/models/mistral/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/mistral/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/mistral/_component_builders.py	2025-02-20 17:49:30.526025850 -0500
@@ -0,0 +1,672 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List
+
+from torch import nn
+
+from torchtune.modules import (
+    FeedForward,
+    FrozenNF4Linear,
+    MultiHeadAttention,
+    RMSNorm,
+    RotaryPositionalEmbeddings,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+from torchtune.modules.common_utils import _register_reparametrize_state_dict_hooks
+
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+"""
+Component builders for the Mistral 7B models and popular variants such as LoRA.
+
+torchtune provides composable building blocks. Builder functions help
+stitch these building blocks into higher-level components. This design has
+two benefits:
+- The building blocks themselves are very flexible. For example, ``MultiHeadAttention``
+can take either nn.Linear or nn.LoRALinear for ``q_proj``.
+- Builder functions expose a set of configurable params which keep the constructors of
+the building blocks simple.
+"""
+
+
+def mistral(
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: int = 10_000,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the mistral model. This includes:
+    - Token embeddings
+    - num_layers number of TransformerSelfAttentionLayer blocks
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    This does NOT currently include inference-time optimizations such as
+    sliding-window attention
+
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        intermediate_dim (int): intermediate dimension for MLP
+        max_seq_len (int): maximum sequence length the model will be run with,
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        norm_eps (float): epsilon in RMS norms
+        rope_base (int): base for the rotary positional embeddings. Default: 10_000
+
+    Returns:
+        TransformerDecoder: Instantiation of mistral model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+
+    rope = RotaryPositionalEmbeddings(
+        dim=head_dim, max_seq_len=max_seq_len, base=rope_base
+    )
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    mlp = mistral_mlp(dim=embed_dim, hidden_dim=intermediate_dim)
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    output_proj = nn.Linear(embed_dim, vocab_size, bias=False)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+
+def mistral_mlp(dim: int, hidden_dim: int, quantize_base: bool = False) -> FeedForward:
+    """
+    Build the MLP layer associated with the Mistral model.
+    """
+    gate_proj = (
+        nn.Linear(dim, hidden_dim, bias=False)
+        if not quantize_base
+        else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    )
+    down_proj = (
+        nn.Linear(hidden_dim, dim, bias=False)
+        if not quantize_base
+        else FrozenNF4Linear(hidden_dim, dim, bias=False)
+    )
+    up_proj = (
+        nn.Linear(dim, hidden_dim, bias=False)
+        if not quantize_base
+        else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    )
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj)
+
+
+def lora_mistral(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # mistral args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    intermediate_dim: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: int = 10_000,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Mistral (an instance of :func:`~torchtune.modules.TransformerDecoder`)
+    with LoRA applied based on the passed in configuration.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with
+        intermediate_dim (int): intermediate dimension for MLP.
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        norm_eps (float): epsilon in RMS norms.
+        rope_base (int): base for the rotary positional embeddings. Default: 10_000
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Mistral model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    """
+
+    self_attn = lora_mistral_self_attention(
+        lora_modules=lora_attn_modules,
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+        rope_base=rope_base,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+    if apply_lora_to_mlp:
+        mlp = lora_mistral_mlp(
+            dim=embed_dim,
+            hidden_dim=intermediate_dim,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+            quantize_base=quantize_base,
+        )
+    else:
+        mlp = mistral_mlp(
+            dim=embed_dim, hidden_dim=intermediate_dim, quantize_base=quantize_base
+        )
+
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+
+    # TODO: quantize_base is not applied to final output_proj currently.
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    output_proj = (
+        adapter_cls(embed_dim, vocab_size, rank=lora_rank, alpha=lora_alpha)
+        if apply_lora_to_output
+        else nn.Linear(embed_dim, vocab_size, bias=False)
+    )
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=(embed_dim // num_heads),
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to higher precision, and offload to CPU on the fly
+        # so as to not increase peak memory
+        # TODO this is clowny, figure out a better way to get what precision the rest
+        # of the model is in
+        _register_reparametrize_state_dict_hooks(model, dtype=tok_embeddings.weight.dtype)
+
+    return model
+
+
+def lora_mistral_self_attention(
+    lora_modules: List[LORA_ATTN_MODULES],
+    *,
+    # MultiHeadAttention args
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    rope_base: int = 10_000,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> MultiHeadAttention:
+    """
+    Return an instance of :func:`~torchtune.modules.MultiHeadAttention` with LoRA
+    applied to a subset of its linear layers
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to. Options are ``{"q_proj", "k_proj", "v_proj",
+            "output_proj"}``.
+        embed_dim (int): embedding dimension for self-attention
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        max_seq_len (int): maximum sequence length the model will be run with
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        rope_base (int): base for the rotary positional embeddings. Default: 10_000
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model parameters for linear layers
+            LoRA is being applied to. Default is ``False``.
+
+    Returns:
+        MultiHeadAttention: instantiation of self-attention module with LoRA
+        applied to a subset of Q, K, V, output projections.
+
+    Raises:
+        ValueError: If lora_modules arg is an empty list
+    """
+    if not lora_modules:
+        raise ValueError(
+            f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules"
+        )
+
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+
+    q_proj = (
+        adapter_cls(
+            embed_dim,
+            num_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "q_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_heads * head_dim, bias=False)
+        )
+    )
+    k_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "k_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    v_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "v_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            embed_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "output_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, embed_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, embed_dim, bias=False)
+        )
+    )
+    rope = RotaryPositionalEmbeddings(
+        dim=head_dim, max_seq_len=max_seq_len, base=rope_base
+    )
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=q_proj,
+        k_proj=k_proj,
+        v_proj=v_proj,
+        output_proj=output_proj,
+        pos_embeddings=rope,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    return self_attn
+
+
+def lora_mistral_mlp(
+    *,
+    dim: int,
+    hidden_dim: int,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> FeedForward:
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    gate_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    down_proj = adapter_cls(
+        in_dim=hidden_dim,
+        out_dim=dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    up_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    return FeedForward(
+        gate_proj=gate_proj,
+        down_proj=down_proj,
+        up_proj=up_proj,
+    )
+
+
+def mistral_classifier(
+    num_classes: int,
+    *,
+    # base mistral args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: int = 10_000,
+) -> TransformerDecoder:
+    """
+    Build a base mistral model with an added classification layer.
+    See :func:`~torchtune.models.mistral.mistral_classifier`
+    for details on the base mistral classifier model.
+
+    Args:
+        num_classes (int): number of classes for the classification layer.
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        intermediate_dim (int): intermediate dimension for MLP
+        max_seq_len (int): maximum sequence length the model will be run with,
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        norm_eps (float): epsilon in RMS norms
+        rope_base (int): base for the rotary positional embeddings. Default: 10_000
+
+    Returns:
+        TransformerDecoder: Instantiation of mistral classification model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+
+    rope = RotaryPositionalEmbeddings(
+        dim=head_dim, max_seq_len=max_seq_len, base=rope_base
+    )
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    mlp = mistral_mlp(dim=embed_dim, hidden_dim=intermediate_dim)
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    output_proj = nn.Linear(embed_dim, num_classes, bias=False)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+
+def lora_mistral_classifier(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # mistral classifier args
+    num_classes: int,
+    # mistral args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    max_seq_len: int,
+    intermediate_dim: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: int = 10_000,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Mistral classifier (an instance of :func:`~torchtune.modules.TransformerDecoder`)
+    with LoRA applied to some of the linear layers in its self-attention modules.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        num_classes (int): number of classes for the classification layer.
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with
+        intermediate_dim (int): intermediate dimension for MLP.
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        norm_eps (float): epsilon in RMS norms.
+        rope_base (int): base for the rotary positional embeddings. Default: 10_000
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Mistral classifier model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    """
+
+    self_attn = lora_mistral_self_attention(
+        lora_modules=lora_attn_modules,
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+        rope_base=rope_base,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+    if apply_lora_to_mlp:
+        mlp = lora_mistral_mlp(
+            dim=embed_dim,
+            hidden_dim=intermediate_dim,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+            quantize_base=quantize_base,
+        )
+    else:
+        mlp = mistral_mlp(dim=embed_dim, hidden_dim=intermediate_dim)
+
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+
+    # TODO: quantize_base is not applied to final output_proj currently.
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            num_classes,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+        )
+        if apply_lora_to_output
+        else nn.Linear(embed_dim, num_classes, bias=False)
+    )
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=(embed_dim // num_heads),
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to higher precision, and offload to CPU on the fly
+        # so as to not increase peak memory
+        # TODO this is clowny, figure out a better way to get what precision the rest
+        # of the model is in
+        _register_reparametrize_state_dict_hooks(model, dtype=tok_embeddings.weight.dtype)
+
+    return model
diff -ruN marc_original/third_party/torchtune/torchtune/models/mistral/__init__.py marc/third_party/torchtune/torchtune/models/mistral/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/mistral/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/mistral/__init__.py	2025-02-20 17:49:30.522025844 -0500
@@ -0,0 +1,39 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import (
+    lora_mistral,
+    lora_mistral_classifier,
+    mistral,
+    mistral_classifier,
+)
+from ._model_builders import (
+    lora_mistral_7b,
+    lora_mistral_reward_7b,
+    mistral_7b,
+    mistral_reward_7b,
+    mistral_tokenizer,
+    qlora_mistral_7b,
+    qlora_mistral_reward_7b,
+)
+from ._prompt_template import MistralChatTemplate
+from ._tokenizer import MistralTokenizer
+
+__all__ = [
+    "MistralTokenizer",
+    "MistralChatTemplate",
+    "lora_mistral",
+    "lora_mistral_classifier",
+    "mistral",
+    "mistral_classifier",
+    "lora_mistral_7b",
+    "lora_mistral_reward_7b",
+    "mistral_7b",
+    "mistral_reward_7b",
+    "mistral_tokenizer",
+    "qlora_mistral_7b",
+    "qlora_mistral_reward_7b",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/mistral/_model_builders.py marc/third_party/torchtune/torchtune/models/mistral/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/mistral/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/mistral/_model_builders.py	2025-02-20 17:49:30.526025850 -0500
@@ -0,0 +1,216 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import List, Optional
+
+from torchtune.models.mistral._component_builders import (
+    mistral,
+    lora_mistral,
+    mistral_classifier,
+    lora_mistral_classifier,
+)
+from torchtune.data._prompt_templates import _TemplateType
+from torchtune.data._prompt_templates import _get_prompt_template
+
+from torchtune.modules import TransformerDecoder
+from torchtune.models.mistral._tokenizer import MistralTokenizer
+from torchtune.modules.peft import LORA_ATTN_MODULES
+from functools import partial
+
+
+"""
+Model builders build specific instantiations using component builders. For example
+the ``mistral_7b`` model builder uses the ``mistral`` component builder.
+"""
+
+
+def mistral_7b() -> TransformerDecoder:
+    """
+    Builder for creating a Mistral 7B model initialized w/ the default 7b parameter values
+    from https://mistral.ai/news/announcing-mistral-7b/
+
+
+    Returns:
+        TransformerDecoder: Instantiation of Mistral 7B model
+    """
+    return mistral(
+        vocab_size=32_000,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        intermediate_dim=14336,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+    )
+
+
+def mistral_tokenizer(path: str, max_seq_len: Optional[int] = None, prompt_template: Optional[_TemplateType] = "torchtune.models.mistral.MistralChatTemplate") -> MistralTokenizer:
+    """
+    Tokenizer for Mistral models.
+
+    Args:
+        path (str): path to the tokenizer
+        max_seq_len (Optional[int]): maximum sequence length for tokenizing a single list of messages,
+            after which the input will be truncated. Default is None.
+        prompt_template (Optional[_TemplateType]): optional specified prompt template.
+            If a string, it is assumed to be the dotpath of a :class:`~torchtune.data.PromptTemplateInterface`
+            class. If a dictionary, it is assumed to be a custom prompt template mapping role to the
+            prepend/append tags. Default is :class:`~torchtune.models.mistral.MistralChatTemplate`.
+
+    Returns:
+        MistralTokenizer: Instantiation of the Mistral tokenizer
+    """
+    return MistralTokenizer(path=path, max_seq_len=max_seq_len, prompt_template=_get_prompt_template(prompt_template) if prompt_template is not None else None)
+
+
+def lora_mistral_7b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Mistral 7B model with LoRA enabled.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Mistral 7B model with LoRA applied
+    """
+    return lora_mistral(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=32_000,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        intermediate_dim=14336,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=10_000,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_mistral_7b = partial(lora_mistral_7b, quantize_base=True)
+
+qlora_mistral_7b.__doc__ = """
+Builder for creating a Mistral model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_mistral_7b` for full API arguments.
+"""
+
+
+def mistral_reward_7b() -> TransformerDecoder:
+    """
+    Builder for creating a Mistral 7B model initialized w/ the default 7b
+    parameter values from:
+    https://huggingface.co/Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback
+    where the output layer is a classification layer projecting to a single class for reward modelling.
+
+    Returns:
+        TransformerDecoder: Instantiation of Mistral 7B classifier model
+    """
+    return mistral_classifier(
+        num_classes=1,
+        vocab_size=32_000,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        intermediate_dim=14336,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+    )
+
+
+def lora_mistral_reward_7b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Mistral reward 7B model with LoRA enabled.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Mistral 7B model with LoRA applied
+    """
+    return lora_mistral_classifier(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        num_classes=1,
+        vocab_size=32_000,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=8,
+        embed_dim=4096,
+        intermediate_dim=14336,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        rope_base=10_000,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_mistral_reward_7b = partial(lora_mistral_reward_7b, quantize_base=True)
+
+qlora_mistral_reward_7b.__doc__ = """
+Builder for creating a Mistral reward 7B model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_mistral_reward_7b` for full API arguments.
+"""
diff -ruN marc_original/third_party/torchtune/torchtune/models/mistral/_prompt_template.py marc/third_party/torchtune/torchtune/models/mistral/_prompt_template.py
--- marc_original/third_party/torchtune/torchtune/models/mistral/_prompt_template.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/mistral/_prompt_template.py	2025-02-20 17:49:30.530025857 -0500
@@ -0,0 +1,76 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import List
+
+from torchtune.data import Message, PromptTemplateInterface
+
+
+class MistralChatTemplate(PromptTemplateInterface):
+    """
+    Formats according to Mistral's `instruct model
+    <https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format>`_.
+
+    It is identical to :class:`~torchtune.data.Llama2ChatTemplate`, except it does not support system
+    prompts.
+
+    Note:
+        This template is only recommended for Mistral's Instruct-v0.1 and Instruct-v0.2 models.
+        Instruct-v0.3 adds additional tags for tool calls, which is not yet supported by this
+        template.
+
+    .. code-block:: text
+
+        "[INST] I am going to Paris, what should I see? [/INST] Paris, the capital
+        of France, is known for its stunning architecture..."
+
+    """
+
+    template = {
+        "system": None,
+        "user": ("[INST] ", " [/INST] "),
+        "assistant": ("", ""),
+        "ipython": ("", ""),
+    }
+
+    def __call__(
+        self,
+        messages: List[Message],
+    ) -> List[Message]:
+        """
+        Format user and system messages with appropriate tags.
+
+        Args:
+            messages (List[Message]): a single conversation, structured as a list
+                of `Message` objects
+
+        Returns:
+            The formatted list of messages
+
+        Raises:
+            ValueError: If system prompts are provided
+        """
+        formatted_dialogue = []
+        for message in messages:
+            if message.role == "system":
+                raise ValueError(
+                    "System prompts are not supported in MistralChatTemplate"
+                )
+            else:
+                content = (
+                    [{"type": "text", "content": self.template[message.role][0]}]
+                    + message.content
+                    + [{"type": "text", "content": self.template[message.role][1]}]
+                )
+            formatted_dialogue.append(
+                Message(
+                    role=message.role,
+                    content=content,
+                    masked=message.masked,
+                    ipython=message.ipython,
+                    eot=message.eot,
+                ),
+            )
+        return formatted_dialogue
diff -ruN marc_original/third_party/torchtune/torchtune/models/mistral/_tokenizer.py marc/third_party/torchtune/torchtune/models/mistral/_tokenizer.py
--- marc_original/third_party/torchtune/torchtune/models/mistral/_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/mistral/_tokenizer.py	2025-02-20 17:49:30.534025863 -0500
@@ -0,0 +1,197 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, List, Mapping, Optional, Tuple
+
+from torchtune.data import Message, PromptTemplate
+from torchtune.models.mistral._prompt_template import MistralChatTemplate
+from torchtune.modules.tokenizers import (
+    ModelTokenizer,
+    SentencePieceBaseTokenizer,
+    tokenize_messages_no_special_tokens,
+)
+from torchtune.modules.transforms import Transform
+
+WHITESPACE_CHARS = [" ", "\n", "\t", "\r", "\v"]
+
+
+class MistralTokenizer(ModelTokenizer, Transform):
+    """
+    Mistral's implementation of the SentencePiece tokenizer
+
+    Args:
+        path (str): Path to pretrained tokenizer file.
+        max_seq_len (Optional[int]): A max sequence length to truncate tokens to.
+            Default: None
+        prompt_template (Optional[PromptTemplate]): template used to format the messages based on their role. This is used
+            to add structured text around the actual messages. The structured text is used in three scenarios:
+
+            - Task-specific templates to gear models for a particular task that it will expect after training
+            - Model-specific templates that are required whenever the model is prompted, such as the [INST]
+              tags in Llama2 and in Mistral
+            - Community standardized templates, such as :class:`~torchtune.data.ChatMLTemplate`
+
+            The extra text will still get tokenized as normal text, not as special tokens.
+            Default is :class:`~torchtune.models.mistral.MistralChatTemplate`.
+
+    Examples:
+        >>> tokenizer = MistralTokenizer("/path/to/spm_model")
+        >>> tokenized_text = tokenizer.encode("Hello world!", add_bos=True, add_eos=True)
+        >>> print(tokenized_text)
+        [1, 31587, 29644, 102, 2]
+    """
+
+    def __init__(
+        self,
+        path: str,
+        max_seq_len: Optional[int] = None,
+        prompt_template: Optional[PromptTemplate] = MistralChatTemplate(),
+    ):
+        self._spm_model = SentencePieceBaseTokenizer(path)
+
+        # Original tokenizer has no pad_id, which causes indexing errors when batch training
+        self._spm_model.pad_id = 0
+
+        # During generation, stop when eos_id is encountered
+        self.stop_tokens = [self.eos_id]
+
+        self.max_seq_len = max_seq_len
+
+        self.prompt_template = prompt_template
+
+    @property
+    def eos_id(self):
+        return self._spm_model.eos_id
+
+    @property
+    def bos_id(self):
+        return self._spm_model.bos_id
+
+    @property
+    def pad_id(self):
+        return self._spm_model.pad_id
+
+    @property
+    def vocab_size(self):
+        return self._spm_model.vocab_size
+
+    def encode(
+        self,
+        text: str,
+        add_bos: bool = True,
+        add_eos: bool = True,
+        trim_leading_whitespace: bool = False,
+    ) -> List[int]:
+        """
+        Encode a string into a list of token IDs
+
+        Args:
+            text (str): The input text to be encoded, unbatched.
+            add_bos (bool): Whether to prepend BOS special token (Beginning of Sentence) to the input, defaults to True.
+            add_eos (bool): Whether to append EOS special token (End of Sentence) to the input, defaults to True.
+            trim_leading_whitespace (bool): Whether to trim leading whitespace from
+                underlying sentencepiece tokenization. Sentencepiece normally prepends
+                whitespace to any tokenized text, which can cause differences where
+                encode(s1) + encode(s2) != encode(s1 + s2) due to leading whitespace
+                added to s2. Default: False
+        Returns:
+            List[int]: The encoded token IDs.
+        """
+        return self._spm_model.encode(
+            text,
+            add_bos=add_bos,
+            add_eos=add_eos,
+            trim_leading_whitespace=trim_leading_whitespace,
+        )
+
+    def decode(
+        self,
+        token_ids: List[int],
+    ) -> str:
+        """Decode token IDs to strings.
+
+        Args:
+            token_ids (List[int]): The input token IDs to be decoded.
+
+        Returns:
+            str: The decoded text.
+        """
+        return self._spm_model.decode(token_ids)
+
+    def tokenize_messages(
+        self,
+        messages: List[Message],
+        *,
+        add_eos: bool = True,
+    ) -> Tuple[List[int], List[bool]]:
+        r"""Tokenize a list of messages one at a time then concatenate them,
+        returning a list of tokens and a list of masks.
+
+        Note:
+            sentencepiece has problems where in general
+            encode(s1 + s2) != encode(s1) + encode(s2) due to whitespace handling.
+            We can get around this by prepending s2 with a known token and slicing the
+            beginning off the tokenized s2.
+
+        Example:
+            >>> tokenizer = MistralTokenizer(tokenizer_path, max_seq_len)
+            >>> messages = [
+                Message(role="system", content="system message\n", masked=True),
+                Message(role="user", content="user prompt\n", masked=True),
+                Message(role="assistant", content="assistant response\n"),
+            ]
+
+            >>> # tokenize_messages encodes messages separately and concats
+            >>> tokenizer.tokenize_messages(messages)[0]
+            [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+
+
+            >>> # Same result as encoding the full string in one go
+            >>> tokenizer.encode(''.join([message.content for message in messages]))
+            [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+
+
+        Args:
+            messages (List[Message]): A list of messages, each containing role, content,
+                and masked attributes.
+            add_eos (bool): Whether to append EOS after assistant message, default to True
+
+        Returns:
+            Tuple[List[int], List[bool]]: The tokenized messages
+        """
+        templated_messages = (
+            self.prompt_template(messages)
+            if self.prompt_template is not None
+            else messages
+        )
+        return tokenize_messages_no_special_tokens(
+            tokenizer=self,
+            messages=templated_messages,
+            bos_id=self.bos_id,
+            eos_id=self.eos_id if add_eos else None,
+        )
+
+    def __call__(
+        self, sample: Mapping[str, Any], inference: bool = False
+    ) -> Mapping[str, Any]:
+        """
+        Apply ``tokenize_messages`` to the "messages" field in the sample.
+
+        Args:
+            sample (Mapping[str, Any]): A sample with a "messages" field containing
+                a List[Message] to tokenize
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            Mapping[str, Any]: The sample with added "tokens" and "mask" fields
+                and the "messages" field removed.
+            inference (bool): Whether the template is being used for inference or not.
+        """
+        messages = sample.pop("messages")
+        tokens, mask = self.tokenize_messages(messages)
+        sample["tokens"] = tokens
+        sample["mask"] = mask
+        return sample
diff -ruN marc_original/third_party/torchtune/torchtune/models/phi3/_component_builders.py marc/third_party/torchtune/torchtune/models/phi3/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/phi3/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/phi3/_component_builders.py	2025-02-20 17:49:30.546025883 -0500
@@ -0,0 +1,430 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List
+
+from torch import nn
+
+from torchtune.models.phi3._position_embeddings import Phi3RotaryPositionalEmbeddings
+from torchtune.modules import (
+    MultiHeadAttention,
+    FeedForward,
+    FrozenNF4Linear,
+    RMSNorm,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+
+from torchtune.modules.common_utils import _register_reparametrize_state_dict_hooks
+
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+"""
+Component builders for the Phi3 4K Mini Instruct model.
+
+torchtune provides composable building blocks. Builder functions help
+stitch these building blocks into higher-level components. This design has
+two benefits:
+- The building blocks themselves are very flexible. For example, ``MultiHeadAttention``
+can take either nn.Linear or nn.LoRALinear for ``q_proj``.
+- Builder functions expose a set of configurable params which keep the constructors of
+the building blocks simple.
+"""
+
+def phi3(
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: int = 10_000,
+) -> TransformerDecoder:
+    """
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        intermediate_dim (int): intermediate dimension for MLP
+        max_seq_len (int): maximum sequence length the model will be run with,
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        norm_eps (float): epsilon in RMS norms
+        rope_base (int): base for the rotary positional embeddings. Default: 10_000
+
+    Returns:
+        TransformerDecoder: Instantiation of Phi3 Mini 4K Instruct model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+
+    rope = Phi3RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=False),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    mlp = phi3_mlp(dim=embed_dim, hidden_dim=intermediate_dim)
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    output_proj = nn.Linear(embed_dim, vocab_size, bias=False)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+def phi3_mlp(dim: int, hidden_dim: int, quantize_base: bool = False) -> FeedForward:
+    """
+    Build the MLP layer associated with the Phi3 Mini 4K Instruct model.
+    """
+    gate_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    down_proj = nn.Linear(hidden_dim, dim, bias=False) if not quantize_base else FrozenNF4Linear(hidden_dim, dim, bias=False)
+    up_proj = nn.Linear(dim, hidden_dim, bias=False) if not quantize_base else FrozenNF4Linear(dim, hidden_dim, bias=False)
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj)
+
+
+# ------------------ LoRA Phi3 ------------------
+
+
+def lora_phi3(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # phi3 args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: int = 10_000,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    # Quantization args
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Phi3 (an instance of :func:`~torchtune.modules.TransformerDecoder`)
+    with LoRA applied based on the passed in configuration.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        intermediate_dim (int): intermediate dimension for MLP
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        norm_eps (float): epsilon in RMS norms.
+        rope_base (int): base value for Rotary Position Embeddings.
+            Default: 10000
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Llama3 model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    """
+
+    self_attn = lora_phi3_self_attention(
+        lora_modules=lora_attn_modules,
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+        rope_base=rope_base,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+    if apply_lora_to_mlp:
+        mlp = lora_phi3_mlp(
+            dim=embed_dim,
+            hidden_dim=intermediate_dim,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            lora_dropout=lora_dropout,
+            use_dora=use_dora,
+            quantize_base=quantize_base,
+        )
+    else:
+        mlp = phi3_mlp(dim=embed_dim, hidden_dim=intermediate_dim, quantize_base=quantize_base)
+
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+
+    # TODO: quantize_base is not applied to final output_proj currently.
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    output_proj = (
+        adapter_cls(embed_dim, vocab_size, rank=lora_rank, alpha=lora_alpha, dropout=lora_dropout)
+        if apply_lora_to_output
+        else nn.Linear(embed_dim, vocab_size, bias=False)
+    )
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=(embed_dim // num_heads),
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to bf16, and offload to CPU on the fly
+        # so as to not increase peak memory
+        _register_reparametrize_state_dict_hooks(model, dtype=tok_embeddings.weight.dtype)
+
+    return model
+
+
+def lora_phi3_self_attention(
+    lora_modules: List[LORA_ATTN_MODULES],
+    *,
+    # MultiHeadAttention args
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    rope_base: int = 10_000,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> MultiHeadAttention:
+    """
+    Return an instance of :func:`~torchtune.modules.MultiHeadAttention` with LoRA
+    applied to a subset of its linear layers
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to. Options are ``{"q_proj", "k_proj", "v_proj",
+            "output_proj"}``.
+        embed_dim (int): embedding dimension for self-attention
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        rope_base (int): base value for Rotary Position Embeddings.
+            Default: 10000
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model parameters for linear layers
+            LoRA is being applied to. Default is ``False``.
+
+    Returns:
+        MultiHeadAttention: instantiation of self-attention module with LoRA
+        applied to a subset of Q, K, V, output projections.
+
+    Raises:
+        ValueError: If lora_modules arg is an empty list
+    """
+    if not lora_modules:
+        raise ValueError(
+            f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules"
+        )
+
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    q_proj = (
+        adapter_cls(
+            embed_dim,
+            num_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "q_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_heads * head_dim, bias=False)
+        )
+    )
+    k_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "k_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    v_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "v_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, num_kv_heads * head_dim, bias=False)
+        )
+    )
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            embed_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "output_proj" in lora_modules
+        else (
+            nn.Linear(embed_dim, embed_dim, bias=False)
+            if not quantize_base
+            else FrozenNF4Linear(embed_dim, embed_dim, bias=False)
+        )
+    )
+    rope = Phi3RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=q_proj,
+        k_proj=k_proj,
+        v_proj=v_proj,
+        output_proj=output_proj,
+        pos_embeddings=rope,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    return self_attn
+
+
+def lora_phi3_mlp(
+    *,
+    dim: int,
+    hidden_dim: int,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> FeedForward:
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    gate_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    down_proj = adapter_cls(
+        in_dim=hidden_dim,
+        out_dim=dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    up_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    return FeedForward(
+        gate_proj=gate_proj,
+        down_proj=down_proj,
+        up_proj=up_proj,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/models/phi3/_convert_weights.py marc/third_party/torchtune/torchtune/models/phi3/_convert_weights.py
--- marc_original/third_party/torchtune/torchtune/models/phi3/_convert_weights.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/phi3/_convert_weights.py	2025-02-20 17:49:30.550025890 -0500
@@ -0,0 +1,87 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Dict
+
+import torch
+
+from torchtune.models.convert_weights import get_mapped_key
+
+
+_PHI3_MINI = {
+    "model.embed_tokens.weight": "tok_embeddings.weight",
+    "model.layers.{}.self_attn.qkv_proj.weight": "layers.{}.attn.q_proj.weight",
+    "model.layers.{}.self_attn.o_proj.weight": "layers.{}.attn.output_proj.weight",
+    "model.layers.{}.mlp.gate_up_proj.weight": "layers.{}.mlp.w1.weight",
+    "model.layers.{}.mlp.down_proj.weight": "layers.{}.mlp.w2.weight",
+    "model.layers.{}.input_layernorm.weight": "layers.{}.sa_norm.scale",
+    "model.layers.{}.post_attention_layernorm.weight": "layers.{}.mlp_norm.scale",
+    "model.norm.weight": "norm.scale",
+    "lm_head.weight": "output.weight",
+}
+
+
+def phi3_hf_to_tune(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
+    """
+    Convertor from HF state dict to torchtune state dict. This handles:
+    - Splitting the fused q,k and v matrix
+    - Splitting the fused gate and up projection matrix
+    """
+    converted_state_dict = {}
+
+    for key, value in state_dict.items():
+        new_key = get_mapped_key(key, _PHI3_MINI)
+        if "qkv" in key:
+            (
+                q,
+                k,
+                v,
+            ) = value.chunk(3, dim=0)
+            converted_state_dict[new_key] = q
+            converted_state_dict[new_key.replace("q_proj", "k_proj")] = k
+            converted_state_dict[new_key.replace("q_proj", "v_proj")] = v
+        elif "gate" in key:
+            w1, w3 = value.chunk(2, dim=0)
+            converted_state_dict[new_key] = w1
+            converted_state_dict[new_key.replace("w1", "w3")] = w3
+        else:
+            converted_state_dict[new_key] = value
+    return converted_state_dict
+
+
+def phi3_tune_to_hf(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
+    """
+    Convertor from torchtune state dict to HF state dict. This handles:
+    - Fusing q,k and v matrix
+    - Fusing gate and up projection matrix
+    """
+    converted_state_dict = {}
+    inverted_mapping_dict = {v: k for k, v in _PHI3_MINI.items()}
+
+    for key, value in state_dict.items():
+        if "k_proj" in key or "v_proj" in key or "w3" in key:
+            # these keys are accounted for separately and should be skipped
+            continue
+        new_key = get_mapped_key(key, inverted_mapping_dict)
+
+        if "q_proj" in key:
+            q = value
+            k = state_dict[key.replace("q_proj", "k_proj")]
+            v = state_dict[key.replace("q_proj", "v_proj")]
+            qkv = torch.cat([q, k, v], dim=0)
+            # q_proj maps to qkv_proj; no need to string replace
+            converted_state_dict[new_key] = qkv
+
+        elif "w1" in key:
+            gate_proj = value
+            up_proj = state_dict[key.replace("w1", "w3")]
+            gate_up_proj = torch.cat([gate_proj, up_proj], dim=0)
+            # w1 maps to gate_up_proj; no need to string replace
+            converted_state_dict[new_key] = gate_up_proj
+
+        else:
+            converted_state_dict[new_key] = value
+    return converted_state_dict
diff -ruN marc_original/third_party/torchtune/torchtune/models/phi3/__init__.py marc/third_party/torchtune/torchtune/models/phi3/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/phi3/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/phi3/__init__.py	2025-02-20 17:49:30.542025876 -0500
@@ -0,0 +1,29 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import lora_phi3, phi3  # noqa
+from ._convert_weights import phi3_hf_to_tune, phi3_tune_to_hf  # noqa
+from ._model_builders import (  # noqa
+    lora_phi3_mini,
+    phi3_mini,
+    phi3_mini_tokenizer,
+    qlora_phi3_mini,
+)
+from ._position_embeddings import Phi3RotaryPositionalEmbeddings  # noqa
+from ._tokenizer import Phi3MiniTokenizer  # noqa
+
+__all__ = [
+    "phi3_mini",
+    "phi3_mini_tokenizer",
+    "lora_phi3_mini",
+    "qlora_phi3_mini",
+    "Phi3RotaryPositionalEmbeddings",
+    "Phi3MiniTokenizer",
+    "phi3_hf_to_tune",
+    "phi3_tune_to_hf",
+    "phi3",
+    "lora_phi3",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/phi3/_model_builders.py marc/third_party/torchtune/torchtune/models/phi3/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/phi3/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/phi3/_model_builders.py	2025-02-20 17:49:30.550025890 -0500
@@ -0,0 +1,135 @@
+from typing import List, Optional
+
+from torchtune.models.phi3._component_builders import phi3, lora_phi3
+from torchtune.models.phi3._tokenizer import Phi3MiniTokenizer
+
+from torchtune.modules import TransformerDecoder
+from torchtune.modules.peft import LORA_ATTN_MODULES
+from functools import partial
+from torchtune.modules.tokenizers import parse_hf_tokenizer_json
+from torchtune.data._prompt_templates import _TemplateType
+from torchtune.data._prompt_templates import _get_prompt_template
+
+
+"""
+Model builders build specific instantiations using component builders. For example
+the ``phi3_mini`` model builder uses the ``phi3`` component builder.
+"""
+
+
+def phi3_mini() -> TransformerDecoder:
+    """
+    Builder for creating the Phi3 Mini 4K Instruct Model.
+    Ref: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
+
+    Note:
+        This model does not currently support 128K context length nor optimizations
+        such as sliding window attention.
+
+    Returns:
+        TransformerDecoder: Instantiation of Phi3 Mini 4K Instruct Model
+    """
+    return phi3(
+        vocab_size=32_064,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=32,
+        embed_dim=3072,
+        intermediate_dim=8192,
+        max_seq_len=4096,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+    )
+
+def phi3_mini_tokenizer(path: str, special_tokens_path: Optional[str] = None, max_seq_len: Optional[int] = None, prompt_template: Optional[_TemplateType] = None) -> Phi3MiniTokenizer:
+    """Phi-3 Mini tokenizer.
+    Ref: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/tokenizer_config.json
+
+    Args:
+        path (str): Path to the SPM tokenizer model.
+        special_tokens_path (Optional[str]): Path to ``tokenizer.json`` from Hugging Face
+            model files that contains all registered special tokens, or a local json file 
+            structured similarly. Default is None to use the canonical Phi3 special tokens.
+        max_seq_len (Optional[int]): maximum sequence length for tokenizing a single list of messages,
+            after which the input will be truncated. Default is None.
+        prompt_template (Optional[_TemplateType]): optional specified prompt template.
+            If a string, it is assumed to be the dotpath of a :class:`~torchtune.data.PromptTemplateInterface`
+            class. If a dictionary, it is assumed to be a custom prompt template mapping role to the
+            prepend/append tags.
+
+    Note:
+        This tokenizer includes typical LM EOS and BOS tokens like
+        <s>, </s>, and <unk>. However, to support chat completion,
+        it is also augmented with special tokens like <endoftext>
+        and <assistant>.
+
+    Returns:
+        Phi3MiniSentencePieceBaseTokenizer: Instantiation of the SPM tokenizer.
+    """
+    special_tokens = parse_hf_tokenizer_json(special_tokens_path) if special_tokens_path is not None else None
+    template = _get_prompt_template(prompt_template) if prompt_template is not None else None
+    return Phi3MiniTokenizer(path=path, special_tokens=special_tokens, max_seq_len=max_seq_len, prompt_template=template)
+
+
+def lora_phi3_mini(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Phi3 Mini (3.8b) model with LoRA enabled.
+
+    The Phi3 defaults are the same as in :func:`~torchtune.models.phi3.phi3_mini`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        use_dora (bool): Decompose the LoRA weight into magnitude and direction, as
+            introduced in "DoRA: Weight-Decomposed Low-Rank Adaptation" (https://arxiv.org/abs/2402.09353).
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Phi3 Mini model with LoRA applied
+    """
+    return lora_phi3(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=32_064,
+        num_layers=32,
+        num_heads=32,
+        num_kv_heads=32,
+        embed_dim=3072,
+        intermediate_dim=8192,
+        max_seq_len=4096,
+        attn_dropout=0.0,
+        norm_eps=1e-5,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+qlora_phi3_mini = partial(lora_phi3_mini, quantize_base=True)
+qlora_phi3_mini.__doc__ = """
+Builder for creating a Phi3 mini model with QLoRA enabled. Base model weights in linear layers
+that LoRA is applied to are quantized per the QLoRA paper: https://arxiv.org/abs/2305.14314.
+Please see `lora_phi3_mini` for full API arguments.
+"""
diff -ruN marc_original/third_party/torchtune/torchtune/models/phi3/_position_embeddings.py marc/third_party/torchtune/torchtune/models/phi3/_position_embeddings.py
--- marc_original/third_party/torchtune/torchtune/models/phi3/_position_embeddings.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/phi3/_position_embeddings.py	2025-02-20 17:49:30.554025896 -0500
@@ -0,0 +1,119 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional
+
+import torch
+
+from torch import nn
+
+
+class Phi3RotaryPositionalEmbeddings(nn.Module):
+    """
+    RoPE Embeddings used in the Phi3 model.
+    Ref: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
+
+    This class is not numerically equivalent to the RoPE Embedding module
+    used by Llama2 and Llama3.
+
+    Args:
+        dim (int): Embedding dimension. This is usually set to the dim of each
+            head in the attention module computed as ``embed_dim`` // ``num_heads``
+        max_seq_len (int): Maximum expected sequence length for the
+            model, if exceeded the cached freqs will be recomputed
+        base (int): The base for the geometric progression used to compute
+            the rotation angles
+    """
+
+    def __init__(
+        self,
+        dim: int,
+        max_seq_len: int = 4096,
+        base: int = 10_000,
+    ) -> None:
+        super().__init__()
+        self.dim = dim
+        self.base = base
+        self.max_seq_len = max_seq_len
+        self.rope_init()
+
+    def rope_init(self):
+        theta = 1.0 / (
+            self.base
+            ** (torch.arange(0, self.dim, 2)[: (self.dim // 2)].float() / self.dim)
+        )
+        self.register_buffer("theta", theta, persistent=False)
+        self.build_rope_cache(self.max_seq_len)
+
+    def build_rope_cache(self, max_seq_len: int = 4096) -> None:
+        # Create position indexes `[0, 1, ..., max_seq_len - 1]`
+        seq_idx = torch.arange(
+            max_seq_len, dtype=self.theta.dtype, device=self.theta.device
+        )
+
+        # Outer product of theta and position index; output tensor has
+        # a shape of [max_seq_len, dim // 2]
+        idx_theta = torch.einsum("i, j -> ij", seq_idx, self.theta).float()
+
+        # We cache the cos and sin embeddings instead of the IDs. This helps
+        # ensure we have correct behavior when training with bf16
+        # Size: [max_seq_len, (dim * 2)]
+        freqs = torch.cat([idx_theta, idx_theta], dim=-1)
+        cache = torch.cat([freqs.cos(), freqs.sin()], dim=-1)
+        self.register_buffer("cache", cache, persistent=False)
+
+    def forward(
+        self, x: torch.Tensor, input_pos: Optional[torch.Tensor] = None
+    ) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape
+                [b, s, n_h, h_d]
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape [b, s].
+                During inference, this indicates the position of the current token.
+                If none, assume the index of the token is its position id. Default is None.
+
+        Returns:
+            Tensor: output tensor with RoPE applied
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - s: sequence length
+            - n_h: num heads
+            - h_d: head dim
+
+        TODO: The implementation below can be made more efficient
+        for inference.
+        """
+        # input tensor has shape [b, s, n_h, h_d]
+        seq_len = x.size(1)
+        head_dim = x.size(-1)
+
+        # extract the values based on whether input_pos is set or not. When
+        # input_pos is provided, we're in inference mode
+        rope_cache = (
+            self.cache[:seq_len] if input_pos is None else self.cache[input_pos]
+        )
+
+        # reshape the cache for broadcasting
+        # tensor has shape [b, s, 1, h_d * 2] if packed samples,
+        # otherwise has shape [1, s, 1, h_d * 2]
+        rope_cache = rope_cache.view(-1, seq_len, 1, head_dim * 2)
+
+        # [b, s, 1, h_d]
+        cos = rope_cache[..., :head_dim]
+        sin = rope_cache[..., head_dim:]
+
+        x1 = x[..., : x.shape[-1] // 2]
+        x2 = x[..., x.shape[-1] // 2 :]
+        rotated = torch.cat((-x2, x1), dim=-1)
+
+        # cos: [b, s, 1, h_d]
+        # x: [b, s, n_h, h_d]
+        x_out = (x * cos) + (rotated * sin)
+        return x_out.type_as(x)
Binary files marc_original/third_party/torchtune/torchtune/models/phi3/__pycache__/_component_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/phi3/__pycache__/_component_builders.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/phi3/__pycache__/_convert_weights.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/phi3/__pycache__/_convert_weights.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/phi3/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/phi3/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/phi3/__pycache__/_model_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/phi3/__pycache__/_model_builders.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/phi3/__pycache__/_position_embeddings.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/phi3/__pycache__/_position_embeddings.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/phi3/__pycache__/_tokenizer.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/phi3/__pycache__/_tokenizer.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/models/phi3/_tokenizer.py marc/third_party/torchtune/torchtune/models/phi3/_tokenizer.py
--- marc_original/third_party/torchtune/torchtune/models/phi3/_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/phi3/_tokenizer.py	2025-02-20 17:49:30.558025903 -0500
@@ -0,0 +1,270 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, Dict, List, Mapping, Optional, Tuple
+
+from torchtune.data._messages import Message
+from torchtune.data._prompt_templates import PromptTemplate
+from torchtune.data._utils import truncate
+from torchtune.modules.tokenizers import ModelTokenizer, SentencePieceBaseTokenizer
+from torchtune.modules.transforms import Transform
+
+PHI3_SPECIAL_TOKENS = {
+    "<|endoftext|>": 32000,
+    "<|assistant|>": 32001,
+    "<|placeholder1|>": 32002,
+    "<|placeholder2|>": 32003,
+    "<|placeholder3|>": 32004,
+    "<|placeholder4|>": 32005,
+    "<|system|>": 32006,
+    "<|end|>": 32007,
+    "<|placeholder5|>": 32008,
+    "<|placeholder6|>": 32009,
+    "<|user|>": 32010,
+}
+
+
+class Phi3MiniTokenizer(ModelTokenizer, Transform):
+    """
+    SentencePiece tokenizer configured with Phi3 Mini's special tokens.
+
+    Args:
+        path (str): Path to pretrained tokenizer file.
+        special_tokens (Optional[Dict[str, int]]): mapping containing special text tokens and
+            their registered token IDs. If left as None, this will be set to the canonical
+            Phi3 special tokens.
+        max_seq_len (Optional[int]): A max sequence length to truncate tokens to.
+            Default: None
+        prompt_template (Optional[PromptTemplate]): template used to format the messages based on their role. This is used
+            to add structured text around the actual messages. The structured text is used in three scenarios:
+
+            - Task-specific templates to gear models for a particular task that it will expect after training
+            - Model-specific templates that are required whenever the model is prompted, such as the [INST]
+              tags in Llama2 and in Mistral
+            - Community standardized templates, such as :class:`~torchtune.data.ChatMLTemplate`
+
+            The extra text will still get tokenized as normal text, not as special tokens. Default is None.
+
+    Examples:
+        >>> tokenizer = Phi3MiniTokenizer("/path/to/spm_model")
+        >>> tokenized_text = tokenizer.encode("Hello world!", add_bos=True, add_eos=True)
+        >>> print(tokenized_text)
+        [1, 31587, 29644, 102, 2]
+    """
+
+    def __init__(
+        self,
+        path: str,
+        special_tokens: Optional[Dict[str, int]] = None,
+        max_seq_len: Optional[int] = None,
+        prompt_template: Optional[PromptTemplate] = None,
+    ):
+        self._spm_model = SentencePieceBaseTokenizer(path)
+
+        self.special_tokens = (
+            special_tokens if special_tokens is not None else PHI3_SPECIAL_TOKENS
+        )
+
+        # Use custom EOS and pad ids instead of SentencePiece's
+        self.eos_id = self.special_tokens["<|endoftext|>"]
+        self.pad_id = self.special_tokens["<|endoftext|>"]
+
+        # During generation, stop when eos_id is encountered
+        self.stop_tokens = [self.eos_id]
+
+        self.max_seq_len = max_seq_len
+
+        self.prompt_template = prompt_template
+
+    @property
+    def vocab_size(self):
+        return self._spm_model.vocab_size
+
+    @property
+    def bos_id(self):
+        return self._spm_model.bos_id
+
+    def encode(
+        self,
+        text: str,
+        add_bos: bool = True,
+        add_eos: bool = True,
+        trim_leading_whitespace: bool = False,
+    ) -> List[int]:
+        return self._spm_model.encode(
+            text,
+            add_bos=add_bos,
+            add_eos=add_eos,
+            trim_leading_whitespace=trim_leading_whitespace,
+        )
+
+    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:
+        """Decode token IDs to strings.
+
+        Args:
+            ids (List[int]): The input token IDs to be decoded.
+            skip_special_tokens (bool): Whether to show or skip special tokens in the decoded string.
+                Default is True.
+
+        Returns:
+            str: The decoded text.
+        """
+        ids_for_decode = []
+        for token_id in ids:
+            # Filter out special tokens and the placeholder tokens added
+            # by the Phi3 team
+            if skip_special_tokens and (token_id >= 32_000 and token_id <= 32_064):
+                continue
+            else:
+                ids_for_decode.append(token_id)
+        return self._spm_model.decode(ids_for_decode)
+
+    def tokenize_messages(
+        self,
+        messages: List[Message],
+        *,
+        add_eos: bool = False,
+        ignore_system_prompt: bool = False,
+    ) -> Tuple[List[int], List[bool]]:
+        r"""Tokenize a list of messages one at a time then concatenate them,
+        returning a list of tokens and a list of masks.
+
+        Example:
+            >>> tokenizer = Phi3MiniTokenizer(tokenizer_path, max_seq_len)
+            >>> messages = [
+                Message(role="system", content="system message\n", masked=True),
+                Message(role="user", content="user prompt\n", masked=True),
+                Message(role="assistant", content="assistant response\n"),
+            ]
+
+            >>> # tokenize_messages encodes messages separately and concats
+            >>> tokenizer.tokenize_messages(messages)[0]
+            [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+
+            >>> # Same result as encoding the full string in one go
+            >>> tokenizer.encode(''.join([message.content for message in messages]))
+            [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+
+
+        Args:
+            messages (List[Message]): A list of messages, each containing role, content,
+                and masked attributes.
+            add_eos (bool): Whether to append EOS after assistant message, default to False
+            ignore_system_prompt (bool): Whether to ignore system prompt, defaults to False.
+
+        Raises:
+            ValueError: If the role is not "user", "assistant", or "system".
+
+        Returns:
+            Tuple[List[int], List[bool]]: The tokenized messages
+        """
+        templated_messages = (
+            self.prompt_template(messages)
+            if self.prompt_template is not None
+            else messages
+        )
+
+        start_of_turn = True
+        end_of_turn = False
+        tokenized_messages = []
+        mask = []
+
+        # The chat template in HF adds a bunch of newlines
+        new_line_token_id = self.encode("\n", add_bos=False, add_eos=False)
+
+        for message in templated_messages:
+            # Skip system prompt
+            if ignore_system_prompt and message.role == "system":
+                continue
+
+            # Prepend BOS on start of new turns
+            if start_of_turn:
+                tokenized_messages.append(self.bos_id)
+                mask.append(message.masked)
+
+            # Add special tokens
+            if message.role == "user":
+                tokenized_messages.append(self.special_tokens["<|user|>"])
+                mask.append(message.masked)
+            elif message.role == "assistant":
+                tokenized_messages.append(self.special_tokens["<|assistant|>"])
+                # If assistant message, this is the end of a turn
+                end_of_turn = True
+                mask.append(message.masked)
+            elif message.role == "system":
+                tokenized_messages.append(self.special_tokens["<|system|>"])
+                mask.append(message.masked)
+            else:
+                raise ValueError(
+                    f"Unknown role '{message.role}' for message: '{message.content}'"
+                )
+
+            # Add new line token
+            tokenized_messages.extend(new_line_token_id)
+            mask.extend([message.masked] * len(new_line_token_id))
+
+            # Tokenize current message, append with masks
+            tokens = []
+            for item in message.content:
+                if item["type"] == "text":
+                    tokens = tokens + self.encode(
+                        item["content"].rstrip(" "),
+                        add_bos=False,
+                        add_eos=False,
+                        trim_leading_whitespace=True,  # Always trim whitespace (just to match HF tokenizer implementation)
+                    )
+                else:
+                    raise RuntimeError(
+                        f"Unsupported message content type: {item['type']}"
+                    )
+
+            tokens = tokens + [self.special_tokens["<|end|>"]] + new_line_token_id
+            tokenized_messages.extend(tokens)
+            mask.extend([message.masked] * len(tokens))
+
+            # If assistant message, append EOS at end
+            if end_of_turn and add_eos:
+                tokenized_messages.append(self.eos_id)
+                mask.append(message.masked)
+                end_of_turn = False
+                start_of_turn = True
+            else:
+                start_of_turn = False
+
+            # Break out early if we reach max_seq_len
+            if self.max_seq_len and len(tokenized_messages) >= self.max_seq_len:
+                break
+
+        # Finally, truncate if necessary
+        if self.max_seq_len and len(tokenized_messages) >= self.max_seq_len:
+            tokenized_messages = truncate(
+                tokenized_messages, self.max_seq_len, self.eos_id if add_eos else None
+            )
+            mask = truncate(mask, self.max_seq_len, message.masked if add_eos else None)
+
+        return tokenized_messages, mask
+
+    def __call__(
+        self, sample: Mapping[str, Any], inference: bool = False
+    ) -> Mapping[str, Any]:
+        """
+        Apply ``tokenize_messages`` to the "messages" field in the sample.
+
+        Args:
+            sample (Mapping[str, Any]): A sample with a "messages" field containing
+                a List[Message] to tokenize
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            Mapping[str, Any]: The sample with added "tokens" and "mask" fields
+                and the "messages" field removed.
+            inference (bool): Whether the template is being used for inference or not.
+        """
+        messages = sample.pop("messages")
+        tokens, mask = self.tokenize_messages(messages)
+        sample["tokens"] = tokens
+        sample["mask"] = mask
+        return sample
Binary files marc_original/third_party/torchtune/torchtune/models/__pycache__/convert_weights.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/__pycache__/convert_weights.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/__pycache__/__init__.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/models/qwen2/_component_builders.py marc/third_party/torchtune/torchtune/models/qwen2/_component_builders.py
--- marc_original/third_party/torchtune/torchtune/models/qwen2/_component_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/qwen2/_component_builders.py	2025-02-20 17:49:30.566025916 -0500
@@ -0,0 +1,445 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import partial
+from typing import List
+from torchtune.modules.common_utils import reparametrize_as_dtype_state_dict_post_hook
+
+from torch import nn
+from torchtune.modules.transformer import TransformerDecoder
+from torchtune.models.qwen2._positional_embeddings import Qwen2RotaryPositionalEmbeddings
+
+from torchtune.modules import (
+    MultiHeadAttention,
+    FeedForward,
+    RMSNorm,
+    TransformerSelfAttentionLayer,
+    TiedLinear
+)
+
+
+from torchtune.modules.peft import DoRALinear, LORA_ATTN_MODULES, LoRALinear
+
+"""
+Component builders for the Qwen2 model and popular variants such as LoRA.
+
+torchtune provides composable building blocks. Builder functions help
+stitch these building blocks into higher-level components. This design has
+two benefits:
+- The building blocks themselves are very flexible. For example, ``MultiHeadAttention``
+can take either nn.Linear or nn.LoRALinear for ``q_proj``.
+- Builder functions expose a set of configurable params which keep the constructors of
+the building blocks simple.
+"""
+
+
+def qwen2(
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: float = 1_000_000.0,
+    tie_word_embeddings: bool = False,
+) -> TransformerDecoder:
+    """
+    Build the decoder associated with the Qwen2 model. This includes:
+    - Token embeddings
+    - num_layers number of TransformerSelfAttentionLayer blocks
+    - RMS Norm layer applied to the output of the transformer
+    - Final projection into token space
+
+    Args:
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        rope_base (float): the base period of the RoPE embeddings.
+        tie_word_embeddings (bool): whether the model's input and output word embeddings should be tied.
+
+    Returns:
+        TransformerDecoder: Instantiation of Qwen2 model.
+    """
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+
+    rope = Qwen2RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=True),
+        k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True),
+        v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True),
+        output_proj=nn.Linear(embed_dim, embed_dim, bias=False),
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    mlp = qwen2_mlp(dim=embed_dim, hidden_dim=intermediate_dim)
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+    if tie_word_embeddings:
+        output_proj = TiedLinear(tok_embeddings)
+    else:
+        output_proj = nn.Linear(embed_dim, vocab_size, bias=False)
+    return TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=head_dim,
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+
+def qwen2_mlp(dim: int, hidden_dim: int) -> FeedForward:
+    """
+    Build the MLP layer associated with the Qwen2 model.
+    """
+    gate_proj = nn.Linear(dim, hidden_dim, bias=False)
+    down_proj = nn.Linear(hidden_dim, dim, bias=False)
+    up_proj = nn.Linear(dim, hidden_dim, bias=False)
+    return FeedForward(gate_proj=gate_proj, down_proj=down_proj, up_proj=up_proj)
+
+
+def lora_qwen2(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    *,
+    # qwen2 args
+    vocab_size: int,
+    num_layers: int,
+    num_heads: int,
+    num_kv_heads: int,
+    embed_dim: int,
+    intermediate_dim: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    norm_eps: float = 1e-5,
+    rope_base: float = 1_000_000.0,
+    tie_word_embeddings: bool = False,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    # Quantization args
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Return a version of Qwen2 (an instance of :func:`~torchtune.models.qwen2.transformer.Qwen2TransformerDecoder`)
+    with LoRA applied based on the passed in configuration.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        vocab_size (int): number of tokens in vocabulary.
+        num_layers (int): number of layers in the transformer decoder.
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        embed_dim (int): embedding dimension for self-attention
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        intermediate_dim (Optional[int]): intermediate dimension for MLP. If not specified,
+            this is computed using :func:`~torchtune.modules.scale_hidden_dim_for_mlp`
+        norm_eps (float): epsilon in RMS norms.
+        rope_base (float): the base period of the RoPE embeddings.
+        tie_word_embeddings (bool): whether the model's input and output word embeddings should be tied.
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        quantize_base: (bool): Whether to quantize base model weights or not. Only applied to base
+            weights within linear layers LoRA is applied to. The final output linear projection is not
+            supported for quantization currently.
+
+    Returns:
+        TransformerDecoder: Instantiation of Qwen2 model with LoRA applied to
+        a subset of the attention projections in each layer.
+
+    Raises:
+        ValueError: if ``apply_lora_to_output`` and ``tie_word_embeddings``.
+
+    """
+
+    self_attn = lora_qwen2_self_attention(
+        lora_modules=lora_attn_modules,
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+        rope_base=rope_base,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+    if apply_lora_to_mlp:
+        mlp = lora_qwen2_mlp(
+            dim=embed_dim,
+            hidden_dim=intermediate_dim,
+            lora_rank=lora_rank,
+            lora_alpha=lora_alpha,
+            quantize_base=quantize_base,
+            use_dora=use_dora,
+            lora_dropout=lora_dropout,
+        )
+    else:
+        mlp = qwen2_mlp(dim=embed_dim, hidden_dim=intermediate_dim)
+
+    layer = TransformerSelfAttentionLayer(
+        attn=self_attn,
+        mlp=mlp,
+        sa_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+        mlp_norm=RMSNorm(dim=embed_dim, eps=norm_eps),
+    )
+
+    tok_embeddings = nn.Embedding(vocab_size, embed_dim)
+
+    if tie_word_embeddings:
+        if apply_lora_to_output:
+            raise ValueError(
+                "apply_lora_to_output is incompatible with tie_word_embeddings,"
+                " as there would be no output to apply lora to!"
+            )
+        output_proj = TiedLinear(tok_embeddings)
+    else:
+        # TODO: quantize_base is not applied to final output_proj currently.
+        adapter_cls = DoRALinear if use_dora else LoRALinear
+        output_proj = (
+            adapter_cls(embed_dim, vocab_size, rank=lora_rank, alpha=lora_alpha, dropout=lora_dropout)
+            if apply_lora_to_output
+            else nn.Linear(embed_dim, vocab_size, bias=False)
+        )
+    model = TransformerDecoder(
+        tok_embeddings=tok_embeddings,
+        layers=layer,
+        num_layers=num_layers,
+        max_seq_len=max_seq_len,
+        num_heads=num_heads,
+        head_dim=(embed_dim // num_heads),
+        norm=RMSNorm(embed_dim, eps=norm_eps),
+        output=output_proj,
+    )
+
+    if quantize_base:
+        # For QLoRA, we reparametrize 4-bit tensors to higher precision, and offload to CPU on the fly
+        # so as to not increase peak memory
+        model._register_state_dict_hook(
+            partial(
+                reparametrize_as_dtype_state_dict_post_hook,
+                # TODO this is clowny, figure out a better way to get what precision the rest
+                # of the model is in
+                dtype=tok_embeddings.weight.dtype,
+                offload_to_cpu=True,
+            )
+        )
+
+    return model
+
+
+def lora_qwen2_self_attention(
+    lora_modules: List[LORA_ATTN_MODULES],
+    *,
+    # MultiHeadAttention args
+    embed_dim: int,
+    num_heads: int,
+    num_kv_heads: int,
+    max_seq_len: int,
+    attn_dropout: float = 0.0,
+    rope_base: float = 1_000_000.0,
+    # LoRA args
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> MultiHeadAttention:
+    """
+    Return an instance of :func:`~torchtune.modules.MultiHeadAttention` with LoRA
+    applied to a subset of its linear layers
+
+    Args:
+        lora_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to. Options are ``{"q_proj", "k_proj", "v_proj",
+            "output_proj"}``.
+        embed_dim (int): embedding dimension for self-attention
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            `num_heads` % `num_kv_heads` == 0. For standard MHA set `num_kv_heads` == `num_heads`,
+            for GQA `num_kv_heads` < `num_heads`, and for MQA set `num_kv_heads` == 1.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        attn_dropout (float): dropout value passed onto scaled_dot_product_attention.
+            Default: 0.0
+        rope_base (float): the base period of the RoPE embeddings. Default: 1_000_000.0
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): LoRA dropout probability. Default: 0.0
+        quantize_base (bool): Whether to quantize base model parameters for linear layers
+            LoRA is being applied to. Default is ``False``.
+
+    Returns:
+        MultiHeadAttention: instantiation of self-attention module with LoRA
+        applied to a subset of Q, K, V, output projections.
+
+    Raises:
+        ValueError: If lora_modules arg is an empty list
+    """
+    if not lora_modules:
+        raise ValueError(f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules")
+
+    head_dim = embed_dim // num_heads
+    num_kv_heads = num_kv_heads if num_kv_heads else num_heads
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    q_proj = (
+        adapter_cls(
+            embed_dim,
+            num_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            use_bias=True,
+            quantize_base=quantize_base,
+        )
+        if "q_proj" in lora_modules
+        else nn.Linear(embed_dim, num_heads * head_dim, bias=True)
+    )
+    k_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            use_bias=True,
+            quantize_base=quantize_base,
+        )
+        if "k_proj" in lora_modules
+        else nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True)
+    )
+    v_proj = (
+        adapter_cls(
+            embed_dim,
+            num_kv_heads * head_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            use_bias=True,
+            quantize_base=quantize_base,
+        )
+        if "v_proj" in lora_modules
+        else nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True)
+    )
+    output_proj = (
+        adapter_cls(
+            embed_dim,
+            embed_dim,
+            rank=lora_rank,
+            alpha=lora_alpha,
+            dropout=lora_dropout,
+            quantize_base=quantize_base,
+        )
+        if "output_proj" in lora_modules
+        else nn.Linear(embed_dim, embed_dim, bias=False)
+    )
+    rope = Qwen2RotaryPositionalEmbeddings(dim=head_dim, max_seq_len=max_seq_len, base=rope_base)
+    self_attn = MultiHeadAttention(
+        embed_dim=embed_dim,
+        num_heads=num_heads,
+        num_kv_heads=num_kv_heads,
+        head_dim=head_dim,
+        q_proj=q_proj,
+        k_proj=k_proj,
+        v_proj=v_proj,
+        output_proj=output_proj,
+        pos_embeddings=rope,
+        kv_cache=None,
+        max_seq_len=max_seq_len,
+        attn_dropout=attn_dropout,
+    )
+    return self_attn
+
+
+def lora_qwen2_mlp(
+    *,
+    dim: int,
+    hidden_dim: int,
+    lora_rank: int,
+    lora_alpha: float,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> FeedForward:
+    adapter_cls = DoRALinear if use_dora else LoRALinear
+    gate_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    down_proj = adapter_cls(
+        in_dim=hidden_dim,
+        out_dim=dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    up_proj = adapter_cls(
+        in_dim=dim,
+        out_dim=hidden_dim,
+        rank=lora_rank,
+        alpha=lora_alpha,
+        dropout=lora_dropout,
+        quantize_base=quantize_base,
+    )
+    return FeedForward(
+        gate_proj=gate_proj,
+        down_proj=down_proj,
+        up_proj=up_proj,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/models/qwen2/_convert_weights.py marc/third_party/torchtune/torchtune/models/qwen2/_convert_weights.py
--- marc_original/third_party/torchtune/torchtune/models/qwen2/_convert_weights.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/qwen2/_convert_weights.py	2025-02-20 17:49:30.570025922 -0500
@@ -0,0 +1,117 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Dict
+
+import torch
+
+from torchtune.models.convert_weights import get_mapped_key
+
+# state dict key mappings from HF's format to torchtune's format
+_FROM_HF = {
+    "model.embed_tokens.weight": "tok_embeddings.weight",
+    "model.layers.{}.self_attn.q_proj.weight": "layers.{}.attn.q_proj.weight",
+    "model.layers.{}.self_attn.q_proj.bias": "layers.{}.attn.q_proj.bias",
+    "model.layers.{}.self_attn.k_proj.weight": "layers.{}.attn.k_proj.weight",
+    "model.layers.{}.self_attn.k_proj.bias": "layers.{}.attn.k_proj.bias",
+    "model.layers.{}.self_attn.v_proj.weight": "layers.{}.attn.v_proj.weight",
+    "model.layers.{}.self_attn.v_proj.bias": "layers.{}.attn.v_proj.bias",
+    "model.layers.{}.self_attn.o_proj.weight": "layers.{}.attn.output_proj.weight",
+    "model.layers.{}.self_attn.rotary_emb.inv_freq": None,
+    "model.layers.{}.mlp.gate_proj.weight": "layers.{}.mlp.w1.weight",
+    "model.layers.{}.mlp.up_proj.weight": "layers.{}.mlp.w3.weight",
+    "model.layers.{}.mlp.down_proj.weight": "layers.{}.mlp.w2.weight",
+    "model.layers.{}.input_layernorm.weight": "layers.{}.sa_norm.scale",
+    "model.layers.{}.post_attention_layernorm.weight": "layers.{}.mlp_norm.scale",
+    "model.norm.weight": "norm.scale",
+    "lm_head.weight": "output.weight",
+}
+
+
+QWEN2_TIED_KEY = "lm_head.weight"
+
+
+def qwen2_hf_to_tune(
+    state_dict: Dict[str, torch.Tensor],
+    num_heads: int = 32,
+    num_kv_heads: int = 32,
+    dim: int = 4096,
+    head_dim: int = None,
+    tie_word_embeddings: bool = False,
+) -> Dict[str, torch.Tensor]:
+    """
+    Convert a state dict from HF's format to TorchTune's format, which contains the weights
+    of a Qwen2 model.
+    State dicts from multiple checkpoint files should be consolidated into a single state dict
+    before calling this function.
+    The logic is identical to :func:`~torchtune.models.convert_weights.hf_to_tune`, but may not load
+    output projection weights.
+
+    Args:
+        state_dict (Dict[str, torch.Tensor]): State dict in HF's format.
+        num_heads (int): Number of heads in the model.
+        num_kv_heads (int): Number of heads in the key/value projection layers.
+        dim (int): Dimension of the model.
+        head_dim (int): Dimension of the head. If not provided, it will be calculated
+            as dim // num_heads.
+        tie_word_embeddings (bool): Whether the model's input and output word embeddings should be tied.
+
+    Returns:
+        Dict[str, torch.Tensor]: State dict in torchtune's format.
+    """
+    converted_state_dict = {}
+    if head_dim is None:
+        head_dim = dim // num_heads
+
+    for key, value in state_dict.items():
+        if (
+            tie_word_embeddings and QWEN2_TIED_KEY in key
+        ):  # Skip loading the output projection weights
+            continue
+        if "rotary_emb.inv_freq" in key:  # Skip loading the position embeddings
+            continue
+
+        new_key = get_mapped_key(key, _FROM_HF)
+        converted_state_dict[new_key] = value
+    return converted_state_dict
+
+
+def qwen2_tune_to_hf(
+    state_dict: Dict[str, torch.Tensor],
+    num_heads: int = 32,
+    num_kv_heads: int = 32,
+    dim: int = 4096,
+    head_dim: int = None,
+    tie_word_embeddings: bool = False,
+):
+    """
+    Convert a state dict from torchtune's format to HF's format. This function
+    doesn't handle any sharding or splitting of state dicts. It follows the
+    state_dict IN -> state_dict OUT pattern.
+
+    Args:
+        state_dict (Dict[str, torch.Tensor]): State dict in torchtune's format.
+        num_heads (int): Number of heads in the model.
+        num_kv_heads (int): Number of heads in the key/value projection layers.
+        dim (int): Dimension of the model.
+        head_dim (int): Dimension of the head. If not provided, it will be calculated
+            as dim // num_heads.
+        tie_word_embeddings (bool): Whether the model's input and output word embeddings should be tied.
+
+    Returns:
+        Dict[str, torch.Tensor]: State dict in HF's format.
+    """
+    converted_state_dict = {}
+    inverted_mapping_dict = {v: k for k, v in _FROM_HF.items()}
+
+    if head_dim is None:
+        head_dim = dim // num_heads
+
+    for key, value in state_dict.items():
+        new_key = get_mapped_key(key, inverted_mapping_dict)
+        converted_state_dict[new_key] = value
+
+    return converted_state_dict
diff -ruN marc_original/third_party/torchtune/torchtune/models/qwen2/__init__.py marc/third_party/torchtune/torchtune/models/qwen2/__init__.py
--- marc_original/third_party/torchtune/torchtune/models/qwen2/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/qwen2/__init__.py	2025-02-20 17:49:30.562025909 -0500
@@ -0,0 +1,35 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._component_builders import lora_qwen2, qwen2  # noqa
+from ._convert_weights import qwen2_hf_to_tune, qwen2_tune_to_hf  # noqa
+from ._model_builders import (
+    lora_qwen2_0_5b,
+    lora_qwen2_1_5b,
+    lora_qwen2_7b,
+    qwen2_0_5b,
+    qwen2_1_5b,
+    qwen2_7b,
+    qwen2_tokenizer,
+)
+from ._positional_embeddings import Qwen2RotaryPositionalEmbeddings
+from ._tokenizer import Qwen2Tokenizer
+
+__all__ = [
+    "qwen2_7b",
+    "qwen2_0_5b",
+    "qwen2_1_5b",
+    "qwen2_tokenizer",
+    "lora_qwen2_7b",
+    "lora_qwen2_0_5b",
+    "lora_qwen2_1_5b",
+    "qwen2",
+    "lora_qwen2",
+    "qwen2_hf_to_tune",
+    "qwen2_tune_to_hf",
+    "Qwen2RotaryPositionalEmbeddings",
+    "Qwen2Tokenizer",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/models/qwen2/_model_builders.py marc/third_party/torchtune/torchtune/models/qwen2/_model_builders.py
--- marc_original/third_party/torchtune/torchtune/models/qwen2/_model_builders.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/qwen2/_model_builders.py	2025-02-20 17:49:30.574025929 -0500
@@ -0,0 +1,297 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import List, Optional
+
+from torchtune.models.qwen2._component_builders import qwen2, lora_qwen2
+from torchtune.models.qwen2._tokenizer import Qwen2Tokenizer
+from torchtune.modules import TransformerDecoder
+from torchtune.modules.peft import LORA_ATTN_MODULES
+from torchtune.modules.tokenizers import parse_hf_tokenizer_json
+from torchtune.data._prompt_templates import _TemplateType
+from torchtune.data._prompt_templates import _get_prompt_template
+
+"""
+Model builders build specific instantiations using component builders. For example
+the qwen2_7b model builder uses the qwen2 component builder to create the
+qwen2 7B model.
+"""
+
+
+def qwen2_7b() -> TransformerDecoder:
+    """
+    Builder for creating a Qwen2 model initialized w/ the default 7B parameter values
+    from https://huggingface.co/Qwen/Qwen2-7B-Instruct
+
+    Returns:
+        TransformerDecoder: Instantiation of Qwen2 7B model
+    """
+    return qwen2(
+        vocab_size=152064,
+        num_layers=28,
+        num_heads=28,
+        num_kv_heads=4,
+        embed_dim=3584,
+        intermediate_dim=18944,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-06,
+        rope_base=1000000.0,
+    )
+
+
+def qwen2_0_5b() -> TransformerDecoder:
+    """
+    Builder for creating a Qwen2 model initialized w/ the default 0.5B parameter values
+    from https://huggingface.co/Qwen/Qwen2-0.5B-Instruct
+
+    Returns:
+        TransformerDecoder: Instantiation of Qwen2 0.5B model
+
+    Note:
+        Qwen2 0.5B and Qwen2 1.5B model builders will enable `tie_word_embeddings` by default
+        and returns an instance of `TransformerDecoder`.
+    """
+    return qwen2(
+        vocab_size=151936,
+        num_layers=24,
+        num_heads=14,
+        num_kv_heads=2,
+        embed_dim=896,
+        intermediate_dim=4864,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-06,
+        rope_base=1000000.0,
+        tie_word_embeddings=True,
+    )
+
+
+def qwen2_1_5b() -> TransformerDecoder:
+    """
+    Builder for creating a Qwen2 model initialized w/ the default 1.5B parameter values
+    from https://huggingface.co/Qwen/Qwen2-1.5B-Instruct
+
+    Returns:
+        TransformerDecoder: Instantiation of Qwen2 1.5B model
+
+    Note:
+        Qwen2 0.5B and Qwen2 1.5B model builders will enable `tie_word_embeddings` by default
+        and returns an instance of `TransformerDecoder`.
+    """
+    return qwen2(
+        vocab_size=151936,
+        num_layers=28,
+        num_heads=12,
+        num_kv_heads=2,
+        embed_dim=1536,
+        intermediate_dim=8960,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-06,
+        rope_base=1000000.0,
+        tie_word_embeddings=True,
+    )
+
+
+def qwen2_tokenizer(
+    path: str,
+    merges_file: str = None,
+    special_tokens_path: Optional[str] = None,
+    max_seq_len: Optional[int] = None,
+    prompt_template: Optional[_TemplateType] = "torchtune.data.ChatMLTemplate",
+    **kwargs,
+) -> Qwen2Tokenizer:
+    """
+    Tokenizer for Qwen2.
+
+    Args:
+        path (str): path to the vocab.json file.
+        merges_file (str): path to the merges.txt file.
+        special_tokens_path (Optional[str]): Path to ``tokenizer.json`` from Hugging Face
+            model files that contains all registered special tokens, or a local json file
+            structured similarly. Default is None to use the canonical Qwen2 special tokens.
+        max_seq_len (Optional[int]): A max sequence length to truncate tokens to.
+            Default: None
+        prompt_template (Optional[_TemplateType]): optional specified prompt template.
+            If a string, it is assumed to be the dotpath of a :class:`~torchtune.data.PromptTemplateInterface`
+            class. If a dictionary, it is assumed to be a custom prompt template mapping role to the
+            prepend/append tags. Default is :class:`~torchtune.models.llama2.Llama2ChatTemplate`.
+
+    Returns:
+        Qwen2Tokenizer: Instantiation of the Qwen2 tokenizer
+    """
+    special_tokens = parse_hf_tokenizer_json(special_tokens_path) if special_tokens_path is not None else None
+    template = _get_prompt_template(prompt_template) if prompt_template is not None else None
+    return Qwen2Tokenizer(path=path, merges_file=merges_file, special_tokens=special_tokens, max_seq_len=max_seq_len, prompt_template=template, **kwargs)
+
+
+def lora_qwen2_7b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    apply_lora_to_output: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Qwen2 7B model with LoRA enabled.
+
+    The Qwen2 defaults are the same as in :func:`~torchtune.models.qwen2.qwen2_7b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        apply_lora_to_output (bool): whether to apply LoRA to the model's final output projection.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Qwen2 7B model with LoRA applied
+    """
+    return lora_qwen2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=apply_lora_to_output,
+        vocab_size=152064,
+        num_layers=28,
+        num_heads=28,
+        num_kv_heads=4,
+        embed_dim=3584,
+        intermediate_dim=18944,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-6,
+        rope_base=1000000.0,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+def lora_qwen2_0_5b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Qwen2 0.5B model with LoRA enabled.
+
+    The Qwen2 defaults are the same as in :func:`~torchtune.models.qwen2.qwen2_0_5b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Qwen2 0.5B model with LoRA applied
+
+    Note:
+        Qwen2 0.5B and Qwen2 1.5B model builders will enable `tie_word_embeddings` by default
+        and returns an instance of `TransformerDecoder`.
+    """
+    return lora_qwen2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=False,
+        vocab_size=151936,
+        num_layers=24,
+        num_heads=14,
+        num_kv_heads=2,
+        embed_dim=896,
+        intermediate_dim=4864,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-6,
+        rope_base=1000000.0,
+        tie_word_embeddings=True,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
+
+
+def lora_qwen2_1_5b(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool = False,
+    lora_rank: int = 8,
+    lora_alpha: float = 16,
+    lora_dropout: float = 0.0,
+    use_dora: bool = False,
+    quantize_base: bool = False,
+) -> TransformerDecoder:
+    """
+    Builder for creating a Qwen2 1.5B model with LoRA enabled.
+
+    The Qwen2 defaults are the same as in :func:`~torchtune.models.qwen2.qwen2_1_5b`,
+    while LoRA default params are based on
+    https://github.com/tloen/alpaca-lora/blob/8bb8579e403dc78e37fe81ffbb253c413007323f/finetune.py#L41-L43.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether to apply LoRA to the MLP in each transformer layer.
+            Default: False
+        lora_rank (int): rank of each low-rank approximation
+        lora_alpha (float): scaling factor for the low-rank approximation
+        lora_dropout (float): dropout probability for the low-rank approximation. Default: 0.0
+        quantize_base (bool): Whether to quantize base model weights
+
+    Returns:
+        TransformerDecoder: Instantiation of Qwen2 1.5B model with LoRA applied
+
+    Note:
+        Qwen2 0.5B and Qwen2 1.5B model builders will enable `tie_word_embeddings` by default
+        and returns an instance of `TransformerDecoder`.
+    """
+    return lora_qwen2(
+        lora_attn_modules=lora_attn_modules,
+        apply_lora_to_mlp=apply_lora_to_mlp,
+        apply_lora_to_output=False,
+        vocab_size=151936,
+        num_layers=28,
+        num_heads=12,
+        num_kv_heads=2,
+        embed_dim=1536,
+        intermediate_dim=8960,
+        max_seq_len=32768,
+        attn_dropout=0.0,
+        norm_eps=1e-6,
+        rope_base=1000000.0,
+        tie_word_embeddings=True,
+        lora_rank=lora_rank,
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        use_dora=use_dora,
+        quantize_base=quantize_base,
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/models/qwen2/_positional_embeddings.py marc/third_party/torchtune/torchtune/models/qwen2/_positional_embeddings.py
--- marc_original/third_party/torchtune/torchtune/models/qwen2/_positional_embeddings.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/qwen2/_positional_embeddings.py	2025-02-20 17:49:30.578025936 -0500
@@ -0,0 +1,119 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional
+
+import torch
+
+from torch import nn
+
+
+class Qwen2RotaryPositionalEmbeddings(nn.Module):
+    """
+    RoPE Embeddings used in the Qwen2 model.
+    Ref: https://huggingface.co/Qwen/Qwen2-7B-Instruct
+
+    This class is not numerically equivalent to the RoPE Embedding module
+    used by Llama2 and Llama3.
+
+    Args:
+        dim (int): Embedding dimension. This is usually set to the dim of each
+            head in the attention module computed as ``embed_dim`` // ``num_heads``
+        max_seq_len (int): Maximum expected sequence length for the
+            model, if exceeded the cached freqs will be recomputed
+        base (float): The base for the geometric progression used to compute
+            the rotation angles
+    """
+
+    def __init__(
+        self,
+        dim: int,
+        max_seq_len: int = 4096,
+        base: float = 1_000_000.0,
+    ) -> None:
+        super().__init__()
+        self.dim = dim
+        self.base = base
+        self.max_seq_len = max_seq_len
+        self.rope_init()
+
+    def rope_init(self):
+        theta = 1.0 / (
+            self.base
+            ** (torch.arange(0, self.dim, 2)[: (self.dim // 2)].float() / self.dim)
+        )
+        self.register_buffer("theta", theta, persistent=False)
+        self.build_rope_cache(self.max_seq_len)
+
+    def build_rope_cache(self, max_seq_len: int = 4096) -> None:
+        # Create position indexes `[0, 1, ..., max_seq_len - 1]`
+        seq_idx = torch.arange(
+            max_seq_len, dtype=self.theta.dtype, device=self.theta.device
+        )
+
+        # Outer product of theta and position index; output tensor has
+        # a shape of [max_seq_len, dim // 2]
+        idx_theta = torch.einsum("i, j -> ij", seq_idx, self.theta).float()
+
+        # We cache the cos and sin embeddings instead of the IDs. This helps
+        # ensure we have correct behavior when training with bf16
+        # Size: [max_seq_len, (dim * 2)]
+        freqs = torch.cat([idx_theta, idx_theta], dim=-1)
+        cache = torch.cat([freqs.cos(), freqs.sin()], dim=-1)
+        self.register_buffer("cache", cache, persistent=False)
+
+    def forward(
+        self, x: torch.Tensor, input_pos: Optional[torch.Tensor] = None
+    ) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape
+                [b, s, n_h, h_d]
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape [b, s].
+                During inference, this indicates the position of the current token.
+                If none, assume the index of the token is its position id. Default is None.
+
+        Returns:
+            Tensor: output tensor with RoPE applied
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - s: sequence length
+            - n_h: num heads
+            - h_d: head dim
+
+        TODO: The implementation below can be made more efficient
+        for inference.
+        """
+        # input tensor has shape [b, s, n_h, h_d]
+        seq_len = x.size(1)
+        head_dim = x.size(-1)
+
+        # extract the values based on whether input_pos is set or not. When
+        # input_pos is provided, we're in inference mode
+        rope_cache = (
+            self.cache[:seq_len] if input_pos is None else self.cache[input_pos]
+        )
+
+        # reshape the cache for broadcasting
+        # tensor has shape [b, s, 1, h_d * 2] if packed samples,
+        # otherwise has shape [1, s, 1, h_d * 2]
+        rope_cache = rope_cache.view(-1, seq_len, 1, head_dim * 2)
+
+        # [b, s, 1, h_d]
+        cos = rope_cache[..., :head_dim].to(x.dtype)
+        sin = rope_cache[..., head_dim:].to(x.dtype)
+
+        x1 = x[..., : x.shape[-1] // 2]
+        x2 = x[..., x.shape[-1] // 2 :]
+        rotated = torch.cat((-x2, x1), dim=-1)
+
+        # cos: [b, s, 1, h_d]
+        # x: [b, s, n_h, h_d]
+        x_out = (x * cos) + (rotated * sin)
+        return x_out.type_as(x)
Binary files marc_original/third_party/torchtune/torchtune/models/qwen2/__pycache__/_component_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/qwen2/__pycache__/_component_builders.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/qwen2/__pycache__/_convert_weights.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/qwen2/__pycache__/_convert_weights.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/qwen2/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/qwen2/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/qwen2/__pycache__/_model_builders.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/qwen2/__pycache__/_model_builders.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/qwen2/__pycache__/_positional_embeddings.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/qwen2/__pycache__/_positional_embeddings.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/models/qwen2/__pycache__/_tokenizer.cpython-312.pyc and marc/third_party/torchtune/torchtune/models/qwen2/__pycache__/_tokenizer.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/models/qwen2/_tokenizer.py marc/third_party/torchtune/torchtune/models/qwen2/_tokenizer.py
--- marc_original/third_party/torchtune/torchtune/models/qwen2/_tokenizer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/models/qwen2/_tokenizer.py	2025-02-20 17:49:30.582025943 -0500
@@ -0,0 +1,410 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import json
+import unicodedata
+from functools import lru_cache
+from typing import Any, Dict, List, Mapping, Optional, Tuple
+
+import regex as re
+
+from torchtune.data import ChatMLTemplate, Message, PromptTemplate, truncate
+from torchtune.modules.tokenizers import ModelTokenizer
+
+PRETOKENIZE_REGEX = (
+    r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|"
+    r"[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"
+)
+
+QWEN2_SPECIAL_TOKENS = {
+    "<|endoftext|>": 151643,
+    "<|im_start|>": 151644,
+    "<|im_end|>": 151645,
+}
+
+
+ENDOFTEXT = "<|endoftext|>"
+IM_START = "<|im_start|>"
+IM_END = "<|im_end|>"
+
+DEFAULT_QWEN2_TOKENIZER_BPE_CACHE_SIZE = 151646
+
+
+@lru_cache()
+def bytes_to_unicode():
+    """
+    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoid mapping to whitespace/control
+    characters the bpe code barfs on.
+
+    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab
+    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for
+    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup
+    tables between utf-8 bytes and unicode strings.
+    """
+    bs = (
+        list(range(ord("!"), ord("~") + 1))
+        + list(range(ord(""), ord("") + 1))
+        + list(range(ord(""), ord("") + 1))
+    )
+    cs = bs[:]
+    n = 0
+    for b in range(2**8):
+        if b not in bs:
+            bs.append(b)
+            cs.append(2**8 + n)
+            n += 1
+    cs = [chr(n) for n in cs]
+    return dict(zip(bs, cs))
+
+
+def get_pairs(word):
+    """
+    Return set of symbol pairs in a word.
+
+    Word is represented as tuple of symbols (symbols being variable-length strings).
+    """
+    pairs = set()
+    prev_char = word[0]
+    for char in word[1:]:
+        pairs.add((prev_char, char))
+        prev_char = char
+    return pairs
+
+
+class Qwen2Tokenizer(ModelTokenizer):
+    """This class construct a Qwen2 tokenizer, based on GPT-2 byte-level BPE tokenization.
+
+    See <https://github.com/huggingface/transformers/blob/v4.40.1/src/transformers/models/qwen2/tokenization_qwen2.py>.
+
+    Args:
+        path (str): Path to vocab.json file.
+        merges_file (str): Path to merges.txt file.
+            merges.txt contains all BPE merge operations, and this file is required to split a single word into
+            byte-level BPE tokens.
+        special_tokens (Optional[Dict[str, int]]): Special tokens to add to the tokenizer. Default is None.
+        max_seq_len (Optional[int]): A max sequence length to truncate tokens to.
+            Default: None
+        prompt_template (Optional[PromptTemplate]): template used to format the messages based on their role. This is used
+            to add structured text around the actual messages. The structured text is used in three scenarios:
+
+            - Task-specific templates to gear models for a particular task that it will expect after training
+            - Model-specific templates that are required whenever the model is prompted, such as the [INST]
+              tags in Llama2 and in Mistral
+            - Community standardized templates, such as :class:`~torchtune.data.ChatMLTemplate`
+
+            The extra text will still get tokenized as normal text, not as special tokens.
+            Default is :class:`~torchtune.data.ChatMLTemplate`.
+        errors (str): Paradigm to follow when decoding bytes to UTF-8. Defaults to "replace".
+            See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.
+        unk_token (Optional[str]): The unknown token. A token that is not in the vocabulary cannot be converted
+            to an ID and is set to be this token instead. Defaults to ``<|endoftext|>``.
+        bos_token (Optional[str]): The beginning of sequence token. Defaults to None.
+        eos_token (str): The end of sequence token. Defaults to ``<|endoftext|>``.
+        pad_token (Optional[str]): The token used for padding. Defaults to ``<|endoftext|>``.
+        bpe_cache_size (int): BPE token cache size in Qwen2Tokenizer.
+            NOTE: large cache size will speed up tokenization, but the cache object will get really
+            large for long running processes (esp. for texts of language that do not use space between
+            word, e.g. Chinese); technically not a memory leak but appears as one.
+            By default, we set the cache size equals to size of the official Qwen2 tokenizer.
+
+    Example:
+        >>> tokenizer = Qwen2Tokenizer(path="/path/to/vocab.json", merges_file="/path/to/merges.txt")
+        >>> tokenized_text = tokenizer.encode("Hello world!")
+        >>> print(tokenized_text)
+        [39, 385, 78, 675, 0, 2000]
+    """
+
+    def __init__(
+        self,
+        path: str,
+        merges_file: str,
+        special_tokens: Optional[Dict[str, int]] = None,
+        max_seq_len: Optional[int] = None,
+        *,
+        prompt_template: Optional[PromptTemplate] = ChatMLTemplate(),
+        errors: str = "replace",
+        unk_token: Optional[str] = ENDOFTEXT,
+        bos_token: Optional[str] = None,
+        eos_token: str = ENDOFTEXT,
+        pad_token: Optional[str] = ENDOFTEXT,
+        bpe_cache_size: int = DEFAULT_QWEN2_TOKENIZER_BPE_CACHE_SIZE,
+    ):
+        with open(path, encoding="utf-8") as vocab_handle:
+            self.encoder = json.load(vocab_handle)
+
+        self.decoder = {v: k for k, v in self.encoder.items()}
+        self.errors = errors  # how to handle errors in decoding
+        self.byte_encoder = bytes_to_unicode()
+        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
+        bpe_merges = []
+        with open(merges_file, encoding="utf-8") as merges_handle:
+            for i, line in enumerate(merges_handle):
+                line = line.strip()
+                if (i == 0 and line.startswith("#version:")) or not line:
+                    continue
+                bpe_merges.append(tuple(line.split()))
+        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))
+
+        self._bpe = lru_cache(maxsize=bpe_cache_size)(self._bpe_without_cache)
+
+        self.pat = re.compile(PRETOKENIZE_REGEX)
+
+        self.special_tokens = (
+            special_tokens if special_tokens is not None else QWEN2_SPECIAL_TOKENS
+        )
+        self._special_tokens_reversed = {v: k for k, v in self.special_tokens.items()}
+
+        self.unk_id = None if unk_token is None else self.special_tokens[unk_token]
+        self.bos_id = None if bos_token is None else self.special_tokens[bos_token]
+        self.eos_id = None if eos_token is None else self.special_tokens[eos_token]
+        self.pad_id = None if pad_token is None else self.special_tokens[pad_token]
+        self.im_start_id = self.special_tokens[IM_START]
+        self.im_end_id = self.special_tokens[IM_END]
+        self.stop_tokens = [self.eos_id, self.im_end_id]
+
+        # Pattern for special tokens.
+        self._pattern_split_special_tokens = re.compile(
+            r"(\L<options>)", options=self.special_tokens.keys()
+        )
+
+        self.max_seq_len = max_seq_len
+
+        self.prompt_template = prompt_template
+
+    def _bpe_without_cache(self, token):
+        word = tuple(token)
+        pairs = get_pairs(word)
+
+        if not pairs:
+            return token
+
+        while True:
+            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))
+            if bigram not in self.bpe_ranks:
+                break
+            first, second = bigram
+            new_word = []
+            i = 0
+            while i < len(word):
+                try:
+                    j = word.index(first, i)
+                except ValueError:
+                    new_word.extend(word[i:])
+                    break
+                else:
+                    new_word.extend(word[i:j])
+                    i = j
+
+                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
+                    new_word.append(first + second)
+                    i += 2
+                else:
+                    new_word.append(word[i])
+                    i += 1
+            new_word = tuple(new_word)
+            word = new_word
+            if len(word) == 1:
+                break
+            else:
+                pairs = get_pairs(word)
+        word = " ".join(word)
+        return word
+
+    def _tokenize(self, text):
+        """Tokenize a string."""
+        bpe_tokens = []
+        for token in re.findall(self.pat, text):
+            token = "".join(
+                self.byte_encoder[b] for b in token.encode("utf-8")
+            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)
+            bpe_tokens.extend(bpe_token for bpe_token in self._bpe(token).split(" "))
+        return bpe_tokens
+
+    def _convert_token_to_id(self, token):
+        """Converts a token (str) in an id using the vocab."""
+        return self.encoder.get(token, self.unk_id)
+
+    def encode(
+        self, text: str, add_bos: bool = True, add_eos: bool = True
+    ) -> List[int]:
+        """
+        Encode a string into a list of token ids.
+
+        Args:
+            text (str): The string to encode.
+            add_bos (bool): (Optional) Whether to add the beginning of sequence token.
+            add_eos (bool): (Optional) Whether to add the end of sequence token.
+
+        Returns:
+            List[int]: The list of token ids.
+
+        Note:
+            This method follows
+            <https://github.com/huggingface/transformers/blob/v4.41.2/src/transformers/tokenization_utils.py#L541> and
+            <https://github.com/huggingface/transformers/blob/v4.41.2/src/transformers/models/qwen2/tokenization_qwen2.py#L262>.
+        """
+
+        text = unicodedata.normalize("NFC", text)
+
+        tokens = self._pattern_split_special_tokens.split(text)
+
+        tokenized_text = []
+        for token in tokens:
+            if not token:
+                continue
+            if token in self.special_tokens:
+                tokenized_text.append(token)
+            else:
+                tokenized_text.extend(self._tokenize(token))
+
+        # Convert tokenized text to token ids.
+        token_ids = []
+        if add_bos and self.bos_id is not None:
+            token_ids.append(self.bos_id)
+        for token in tokenized_text:
+            if token in self.special_tokens:
+                token_id = self.special_tokens[token]
+            else:
+                token_id = self._convert_token_to_id(token)
+            token_ids.append(token_id)
+        if add_eos and self.eos_id is not None:
+            token_ids.append(self.eos_id)
+
+        return token_ids
+
+    def _convert_id_to_token(self, index: int) -> str:
+        """Converts an index (integer) in a token (str) using the vocab."""
+        token = self._special_tokens_reversed.get(index, None)
+        if token is None:
+            return self.decoder.get(index)
+        return token
+
+    def _convert_tokens_to_string(self, tokens: List[str]) -> str:
+        """Converts a sequence of tokens (string) in a single string."""
+        text = "".join(tokens)
+        text = bytearray([self.byte_decoder[c] for c in text]).decode(
+            "utf-8", errors=self.errors
+        )
+        return text
+
+    def decode(
+        self,
+        token_ids: List[int],
+        skip_special_tokens: bool = False,
+    ) -> str:
+        """
+        Decode a list of token ids into a string.
+
+        Args:
+            token_ids (List[int]): The list of token ids.
+            skip_special_tokens (bool): Whether the special tokens should be removed from the decoded string.
+
+        Returns:
+            str: The decoded string.
+        """
+        sub_texts = []
+        current_sub_text = []
+        for token_id in token_ids:
+            token = self._convert_id_to_token(token_id)
+            if token_id in self._special_tokens_reversed:
+                if current_sub_text:
+                    string = self._convert_tokens_to_string(current_sub_text)
+                    if string:
+                        sub_texts.append(string)
+                    current_sub_text = []
+                if not skip_special_tokens:
+                    sub_texts.append(token)
+            else:
+                current_sub_text.append(token)
+        if current_sub_text:
+            sub_texts.append(self._convert_tokens_to_string(current_sub_text))
+
+        text = "".join(sub_texts)
+        return text
+
+    def tokenize_messages(
+        self,
+        messages: List[Message],
+        *,
+        add_eos: bool = True,
+    ) -> Tuple[List[int], List[bool]]:
+        """
+        Given a list of messages, return a list of tokens for the concatenated
+        and formatted messages.
+
+        Args:
+            messages (List[Message]): The message list to tokenize.
+            add_eos (bool): Wether to add the tokenizer's eos_id at the end of the
+                sequence of messages. Default is True.
+
+        Returns:
+            Tuple[List[int], List[bool]]: The list of token ids and the list of masks.
+
+        Raises:
+            RuntimeError: If a message contains non-text content
+        """
+        templated_messages = (
+            self.prompt_template(messages)
+            if self.prompt_template is not None
+            else messages
+        )
+
+        tokenized_messages = []
+        mask = []
+        for index, message in enumerate(templated_messages):
+            tokens = []
+            for item in message.content:
+                if item["type"] == "text":
+                    tokens = tokens + self.encode(
+                        item["content"],
+                        add_bos=False,
+                        add_eos=False,
+                    )
+                else:
+                    raise RuntimeError(
+                        f"Unsupported message content type: {item['type']}"
+                    )
+            tokenized_messages.extend(tokens)
+            mask.extend([message.masked] * len(tokens))
+
+            # If assistant message, append EOS at end
+            if message.role == "assistant" and add_eos:
+                tokenized_messages.append(self.eos_id)
+                mask.append(message.masked)
+
+            # Break out early if we reach max_seq_len
+            if self.max_seq_len and len(tokenized_messages) >= self.max_seq_len:
+                break
+
+        # Finally, truncate if necessary
+        if self.max_seq_len:
+            tokenized_messages = truncate(
+                tokenized_messages, self.max_seq_len, self.eos_id if add_eos else None
+            )
+            mask = truncate(mask, self.max_seq_len, True if add_eos else None)
+
+        return tokenized_messages, mask
+
+    def __call__(
+        self, sample: Mapping[str, Any], inference: bool = False
+    ) -> Mapping[str, Any]:
+        """
+        Apply ``tokenize_messages`` to the "messages" field in the sample.
+
+        Args:
+            sample (Mapping[str, Any]): A sample with a "messages" field containing
+                a List[Message] to tokenize
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            Mapping[str, Any]: The sample with added "tokens" and "mask" fields
+                and the "messages" field removed.
+            inference (bool): Whether the template is being used for inference or not.
+        """
+        messages = sample.pop("messages")
+        tokens, mask = self.tokenize_messages(messages)
+        sample["tokens"] = tokens
+        sample["mask"] = mask
+        return sample
diff -ruN marc_original/third_party/torchtune/torchtune/modules/attention.py marc/third_party/torchtune/torchtune/modules/attention.py
--- marc_original/third_party/torchtune/torchtune/modules/attention.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/attention.py	2025-02-20 17:49:30.590025955 -0500
@@ -0,0 +1,307 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+from typing import Optional
+
+import torch
+from torch import nn
+from torchtune.modules.attention_utils import _MaskType, _sdpa_or_flex_attention
+from torchtune.modules.kv_cache import KVCache
+
+logger = logging.getLogger(__name__)
+
+
+class MultiHeadAttention(nn.Module):
+    """Multi-headed attention layer with support for grouped query
+    attention (GQA) introduced in https://arxiv.org/abs/2305.13245v1.
+
+    GQA is a version of multiheaded attention (MHA) which uses fewer
+    key/value heads than query heads by grouping n query heads for each
+    key and value head. Multi-Query Attention is an extreme
+    version where we have a single key and value head shared by all
+    query heads.
+
+    Following is an example of MHA, GQA and MQA with num_heads = 4
+
+    (credit for the documentation:
+    `litgpt.Config <https://github.com/Lightning-AI/litgpt/blob/eda1aaaf391fd689664f95487ab03dc137e213fd/litgpt/config.py>`_).
+
+
+    ::
+
+                              
+         v  v  v  v       v      v               v 
+                              
+                                                      
+                              
+         k  k  k  k       k      k               k 
+                              
+                                  
+            
+         q  q  q  q    q  q  q  q    q  q  q  q 
+            
+            
+                MHA                    GQA                   MQA
+        n_kv_heads =4          n_kv_heads=2           n_kv_heads=1
+
+    Args:
+        embed_dim (int): embedding dimension for the model
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value
+        num_kv_heads (int): number of key and value heads. User should ensure
+            ``num_heads % num_kv_heads == 0``. For standard MHA set ``num_kv_heads == num_heads``,
+            for GQA ``num_kv_heads < num_heads``, and for MQA set ``num_kv_heads == 1``.
+        head_dim (int): dimension of each head, calculated by ``embed_dim // num_heads``.
+        q_proj (nn.Module): projection layer for query.
+        k_proj (nn.Module): projection layer for key.
+        v_proj (nn.Module): projection layer for value.
+        output_proj (nn.Module): projection layer for output.
+        pos_embeddings (Optional[nn.Module]): positional embeddings layer, e.g. RotaryPositionalEmbeddings.
+        q_norm (Optional[nn.Module]): normalization layer for query, e.g. RMSNorm. For decoding, this is applied
+            before updating from kv_cache. This means it will only support token wide normalization and not
+            batch or sequence wide normalization.
+        k_norm (Optional[nn.Module]): normalization layer for key, must be set if q_norm is.
+        kv_cache (Optional[KVCache]): KVCache object used to cache key and value
+        max_seq_len (int): maximum sequence length supported by the model.
+            This is needed to compute the RoPE Cache. Default: 4096.
+        is_causal (bool): sets the default mask to causal when no mask is provided
+        attn_dropout (float): dropout value passed onto the scaled_dot_product_attention function.
+            This argument is ignored if self.training is False. Default value is 0.0.
+
+    Raises:
+        ValueError: If ``num_heads % num_kv_heads != 0``
+        ValueError: If ``embed_dim % num_heads != 0``
+        ValueError: If ``attn_dropout < 0`` or ``attn_dropout > 1``
+        ValueError: if q_norm is defined without k_norm or vice versa
+    """
+
+    def __init__(
+        self,
+        *,
+        embed_dim: int,
+        num_heads: int,
+        num_kv_heads: int,
+        head_dim: int,
+        q_proj: nn.Module,
+        k_proj: nn.Module,
+        v_proj: nn.Module,
+        output_proj: nn.Module,
+        pos_embeddings: Optional[nn.Module] = None,
+        q_norm: Optional[nn.Module] = None,
+        k_norm: Optional[nn.Module] = None,
+        kv_cache: Optional[KVCache] = None,
+        max_seq_len: int = 4096,
+        is_causal: bool = True,
+        attn_dropout: float = 0.0,
+    ) -> None:
+        super().__init__()
+        if num_heads % num_kv_heads != 0:
+            raise ValueError(
+                f"num_heads ({num_heads}) must be divisible by "
+                f"num_kv_heads ({num_kv_heads})"
+            )
+
+        if embed_dim % num_heads != 0:
+            raise ValueError(
+                f"embed_dim ({embed_dim}) must be divisible by "
+                f"num_heads ({num_heads})"
+            )
+
+        if attn_dropout < 0 or attn_dropout > 1:
+            raise ValueError(f"attn_dropout ({embed_dim}) must be between 0.0 and 1.0")
+
+        if bool(q_norm) ^ bool(k_norm):
+            raise ValueError("q and k norm must be set together")
+
+        # Set attributes
+        self.num_heads = num_heads
+        self.num_kv_heads = num_kv_heads
+        self.embed_dim = embed_dim
+        self.attn_dropout = attn_dropout
+        self.head_dim = head_dim
+        self.max_seq_len = max_seq_len
+        self.is_causal = is_causal
+
+        # Set layers
+        self.kv_cache = kv_cache
+        self.q_proj = q_proj
+        self.k_proj = k_proj
+        self.v_proj = v_proj
+        self.output_proj = output_proj
+        self.q_norm = q_norm
+        self.k_norm = k_norm
+        self.pos_embeddings = pos_embeddings
+
+        # Use flex attention if supported and we are sample packing
+        self._attention_call = _sdpa_or_flex_attention()
+
+    def setup_cache(
+        self, batch_size: int, dtype: torch.dtype, max_seq_len: int
+    ) -> None:
+        """Setup key value caches for attention calculation. If called
+        after kv_cache is already setup, this will be skipped.
+
+        Args:
+            batch_size (int): batch size for the caches.
+            dtype (torch.dtype): dtype for the caches.
+            max_seq_len (int): maximum sequence length model will be run with.
+        """
+        # Don't overwrite user defined kv_cache from init
+        if self.kv_cache is not None:
+            logger.warning(
+                "Key value caches are already setup. You cannot call ``setup_caches()`` twice. Skipping."
+            )
+        else:
+            self.kv_cache = KVCache(
+                batch_size=batch_size,
+                max_seq_len=max_seq_len,
+                num_heads=self.num_heads,
+                head_dim=self.head_dim,
+                dtype=dtype,
+            )
+
+    def reset_cache(self):
+        """Reset the key value caches."""
+        if self.kv_cache is None:
+            raise RuntimeError(
+                "Key value caches are not setup. Call ``setup_caches()`` first."
+            )
+        self.kv_cache.reset()
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        y: Optional[torch.Tensor] = None,
+        *,
+        mask: Optional[_MaskType] = None,
+        input_pos: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape [b x s_x x d] for the query
+            y (Optional[torch.Tensor]): second input tensor with shape [b x s_y x d], is the input
+                for k and v. For self attention, x=y. Optional only with kv_cache enabled.
+            mask (Optional[_MaskType]): Used to mask the scores after the query-key multiplication
+                and before the softmax. Either:
+
+                A boolean tensor with shape ``[b x s x s]``, ``[b x s x self.encoder_max_cache_seq_len]``,
+                or ``[b x s x self.encoder_max_cache_seq_len]`` if using KV-cacheing with encoder/decoder layers.
+                A value of True in row ``i`` and column ``j`` means token ``i`` attends to token ``j``. A value of False means
+                token ``i`` does not attend to token ``j``. If no mask is specified, a causal mask
+                is used by default.
+
+                A :class:`~torch.nn.attention.flex_attention.BlockMask` for document masking in a packed sequence
+                created via `create_block_mask <https://pytorch.org/blog/flexattention/#mask-mods>`_. We  use
+                :func:`~torch.nn.attention.flex_attention.flex_attention` when computing attention with block masks.
+                Default is None.
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape [b x s].
+                During inference, this indicates the position of the current token.
+                If none, assume the index of the token is its position id. Default is None.
+
+        Raises:
+            ValueError: If no ``y`` input and ``kv_cache`` is not enabled.
+
+        Returns:
+            torch.Tensor: output tensor with attention applied
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - s_x: sequence length for x
+            - s_y: sequence length for y
+            - n_h: num heads
+            - n_kv: num kv heads
+            - d: embed dim
+            - h_d: head dim
+        """
+        # x has shape [b, s_x, d]
+        # y has shape [b, s_y, d]
+        b, s_x, _ = x.shape
+        s_y = y.shape[1] if y is not None else 0
+
+        # q has shape [b, s_x, num_heads * head_dim]
+        q = self.q_proj(x)
+
+        # number of queries per key/value
+        q_per_kv = self.num_heads // self.num_kv_heads
+        q = q.view(b, s_x, self.num_kv_heads * q_per_kv, self.head_dim)
+
+        # Apply positional embeddings
+        if self.pos_embeddings is not None:
+            q = self.pos_embeddings(q, input_pos=input_pos)
+
+        # [b, n_h, s_x, h_d]
+        q = q.transpose(1, 2)
+
+        # Normalize q
+        if self.q_norm is not None:
+            q = self.q_norm(q)
+
+        if y is None:
+            if self.kv_cache is None:
+                raise ValueError(
+                    "Must provide y input or use kv_cache to enable streaming decoding"
+                )
+            k = self.kv_cache.k_cache
+            v = self.kv_cache.v_cache
+        else:
+            # Update k and v shape, positional embeddings, and normalization
+
+            # k has shape [b, s_y, num_kv_heads * head_dim]
+            # v has shape [b, s_y, num_kv_heads * head_dim]
+            k = self.k_proj(y)
+            v = self.v_proj(y)
+
+            # Apply positional embeddings
+            # k: [b, s_y, n_kv, h_d]
+            k = k.view(b, s_y, -1, self.head_dim)
+            if self.pos_embeddings is not None:
+                k = self.pos_embeddings(k, input_pos=input_pos)
+
+            # View + expand + reshape bring num_kv_heads to num_heads for k and v
+            # to match q.
+
+            # k: [b, s_y, n_kv, 1, h_d]
+            # v: [b, s_y, n_kv, 1, h_d]
+            k = k.view(b, s_y, self.num_kv_heads, 1, self.head_dim)
+            v = v.view(b, s_y, self.num_kv_heads, 1, self.head_dim)
+
+            # If needed, expand the key and value tensors to have the same shape
+            # as the query tensor by copying values across the relevant dim
+            if self.num_heads != self.num_kv_heads:
+                k = k.expand(b, s_y, self.num_kv_heads, q_per_kv, self.head_dim)
+                v = v.expand(b, s_y, self.num_kv_heads, q_per_kv, self.head_dim)
+
+            # [b, s, n_h, h_d]
+            k = k.reshape(b, s_y, -1, self.head_dim)
+            v = v.reshape(b, s_y, -1, self.head_dim)
+
+            # [b, n_h, s, h_d]
+            k = k.transpose(1, 2)
+            v = v.transpose(1, 2)
+
+            # Normalize k
+            if self.k_norm is not None:
+                k = self.k_norm(k)
+
+            # Update key-value cache
+            if self.kv_cache is not None:
+                k, v = self.kv_cache.update(k, v)
+
+        output = self._attention_call(
+            q,
+            k,
+            v,
+            mask=mask,
+            dropout_p=self.attn_dropout if self.training else 0.0,
+            is_causal=self.kv_cache is None and mask is None and self.is_causal,
+        )
+
+        # reshape the output to be the same shape as the input
+        output = output.transpose(1, 2).contiguous().view(b, s_x, -1)
+        return self.output_proj(output)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/attention_utils.py marc/third_party/torchtune/torchtune/modules/attention_utils.py
--- marc_original/third_party/torchtune/torchtune/modules/attention_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/attention_utils.py	2025-02-20 17:49:30.594025962 -0500
@@ -0,0 +1,249 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+from typing import Callable, List, Optional, Union
+
+import torch
+
+from torch import nn
+from torchtune.utils._import_guard import _SUPPORTS_FLEX_ATTENTION
+from torchtune.utils._logging import get_logger, log_once
+
+_log: logging.Logger = get_logger()
+
+if _SUPPORTS_FLEX_ATTENTION:
+    from torch.nn.attention.flex_attention import (
+        BlockMask,
+        create_block_mask as create_block_causal_mask_flex,
+        flex_attention,
+    )
+
+    flex_attention_compiled = torch.compile(flex_attention, dynamic=False)
+
+    # We cannot do nested compile, but flex attention only has perf benefits
+    # when compiled. To insulate it from the compiler, we wrap it with
+    # compiler.disable so that it can be used regardless of whether the model
+    # is compiled or not, and flex attention always remains compiled.
+    @torch.compiler.disable(recursive=False)
+    def compile_friendly_flex_attention(
+        q: torch.Tensor,
+        k: torch.Tensor,
+        v: torch.Tensor,
+        block_mask: BlockMask,
+    ) -> torch.Tensor:
+        return flex_attention_compiled(q, k, v, block_mask=block_mask)
+
+    _MaskType = Union[torch.Tensor, BlockMask]
+else:
+    _MaskType = torch.Tensor
+
+
+def _get_document_ids_from_seq_lens(
+    seq_lens: List[torch.Tensor],
+) -> torch.Tensor:
+    """
+    Convert a batch tensor of seq lens into integer IDs denoting sample ownership.
+    For example, seq_lens = [2, 3, 1] would return [0, 0, 1, 1, 1, 2].
+
+    Args:
+        seq_lens (List[torch.Tensor]): Sequence lengths of samples in each pack in the batch,
+            shape (batch_size, n), where n is the max number of sequences in a pack and can vary
+            across packs.
+
+    Returns:
+        Tensor: Document IDs of shape (batch_size, max_seq_len).
+    """
+    batch_size = len(seq_lens)
+    batch_document_ids = []
+    for sample_idx in range(batch_size):
+        # We assume seq lens sum to max seq lens, so document_ids should be of
+        # shape (max_seq_len, )
+        document_ids = torch.cat(
+            [
+                torch.full((seq_len,), i, dtype=torch.long, device=seq_len.device)
+                for i, seq_len in enumerate(seq_lens[sample_idx])
+            ]
+        )
+        batch_document_ids.append(document_ids)
+    batch_document_ids = torch.stack(batch_document_ids)
+    return batch_document_ids
+
+
+def create_block_causal_mask(seq_lens: List[torch.Tensor]) -> torch.Tensor:
+    """
+    Given a batch tensor of seq lens defining the lengths of samples in each pack,
+    Construct a 2D block causal mask for each pack in the batch. For example, if
+    a single sample's seq_lens is [3, 2, 1], the mask would be::
+
+        mask = [
+            [1, 0, 0, 0, 0, 0],
+            [1, 1, 0, 0, 0, 0],
+            [1, 1, 1, 0, 0, 0],
+            [0, 0, 0, 1, 0, 0],
+            [0, 0, 0, 1, 1, 0],
+            [0, 0, 0, 0, 0, 1],
+        ]
+
+    Args:
+        seq_lens (List[torch.Tensor]): Sequence lengths of samples in each pack in the batch,
+            shape (batch_size, n), where n is the max number of sequences in a pack and can vary
+            across packs.
+
+
+    Returns:
+        Tensor: Block causal mask of shape (batch_size, max_seq_len, max_seq_len).
+    """
+    batch_block_attn_masks = []
+    batch_size = len(seq_lens)
+    for sample_idx in range(batch_size):
+        block_attn_masks = [
+            torch.tril(
+                torch.ones(seq_len, seq_len, dtype=torch.bool, device=seq_len.device)
+            )
+            for i, seq_len in enumerate(seq_lens[sample_idx])
+        ]
+
+        batch_block_attn_masks.append(torch.block_diag(*block_attn_masks))
+    return torch.stack(batch_block_attn_masks)
+
+
+def packed_block_causal_mask(
+    seq_lens: List[torch.Tensor],
+) -> _MaskType:
+    """
+    Create a block causal document mask for a batch of packed sequences. If on
+    torch version >= 2.5.0, this is done by creating a mask_mod function with the
+    block causal logic and passing this into :func:`torch.nn.attention.flex_attention.create_block_mask`.
+    The resultant BlockMask is a compressed representation of the full block causal
+    mask. If on an older version, a standard 2D block causal mask is created and returned.
+
+    Args:
+        seq_lens (List[torch.Tensor]): Sequence lengths of samples in each pack in the batch,
+            shape (batch_size, n), where n is the max number of sequences in a pack and can vary
+            across packs.
+
+    Returns:
+        _MaskType: BlockMask or Tensor if torch version < 2.5.0.
+    """
+    if _SUPPORTS_FLEX_ATTENTION:
+        document_ids = _get_document_ids_from_seq_lens(seq_lens)
+        batch_size, max_seq_len = document_ids.shape
+        document_ids = document_ids.to("cuda")
+
+        # Instead of passing a tensor mask, flex attention requires a mask_mod function
+        # that determines which elements of QK^T should be included in the attention
+        # computation prior to the softmax. For sample packing, we need both the
+        # logic for both causal mask and document mask. See PyTorch's official
+        # blog post for more details: https://pytorch.org/blog/flexattention/#mask-mods
+        def mask_mod(b, h, q_idx, kv_idx):
+            """
+            Defines the logic of a block causal mask by combining both a standard causal mask
+            and a block diagonal document mask.
+
+            See :func:`~torchtune.modules.attention_utils.create_block_causal_mask`
+            for an illustration.
+            """
+            causal_mask = q_idx >= kv_idx
+            document_mask = document_ids[b, q_idx] == document_ids[b, kv_idx]
+            return causal_mask & document_mask
+
+        return create_block_causal_mask_flex(
+            mask_mod,
+            batch_size,
+            None,
+            max_seq_len,
+            max_seq_len,
+            device="cuda",
+        )
+    else:
+        return create_block_causal_mask(seq_lens=seq_lens)
+
+
+def _sdpa_or_flex_attention() -> Callable:
+    """
+    Helper function to decide when to call flex attention or SDPA. It will use
+    flex attention if ALL of the following conditions are met, otherwise it will
+    default to SDPA:
+    - torch version >= 2.5.0
+    - we are sample packing, therefore mask is a BlockMask
+    - torch.cuda.get_device_capability() >= (7, 5)
+    """
+
+    if _SUPPORTS_FLEX_ATTENTION:
+
+        def _attention_call(
+            q: torch.Tensor,
+            k: torch.Tensor,
+            v: torch.Tensor,
+            mask: Optional[_MaskType],
+            dropout_p: float,
+            is_causal: bool,
+        ) -> torch.Tensor:
+
+            # Flex attention uses the BlockMask
+            # (https://github.com/pytorch/pytorch/blob/main/torch/nn/attention/flex_attention.py#L168)
+            # instead of a traditional boolean tensor mask. If this is passed in,
+            # we assume the user wants to use flex attention instead of traditional SDPA.
+            # This will use flash attention under the hood with support for custom masks.
+            # Currently, it is used when sample packing is enabled (see torchtune.datasets.PackedDataset)
+            if isinstance(mask, BlockMask):
+                log_once(
+                    _log,
+                    "Using flex attention for attention computation since a BlockMask was passed in.",
+                    level=logging.DEBUG,
+                )
+                if dropout_p > 0.0:
+                    raise ValueError(
+                        "Flex attention does not support dropout. Please set dropout to 0.0."
+                    )
+                return compile_friendly_flex_attention(
+                    q,
+                    k,
+                    v,
+                    block_mask=mask,
+                )
+            # If mask is a standard boolean tensor or None, then use SDPA
+            else:
+                # shape: [b, 1, s, s]
+                if mask is not None:
+                    mask = mask[:, None, :, :]
+
+                # Flash attention from https://pytorch.org/blog/accelerating-large-language-models/
+                return nn.functional.scaled_dot_product_attention(
+                    q,
+                    k,
+                    v,
+                    attn_mask=mask,
+                    dropout_p=dropout_p,
+                    is_causal=is_causal,
+                )
+
+    else:
+
+        def _attention_call(
+            q: torch.Tensor,
+            k: torch.Tensor,
+            v: torch.Tensor,
+            mask: Optional[_MaskType],
+            dropout_p: float,
+            is_causal: bool,
+        ) -> torch.Tensor:
+            # shape: [b, 1, s, s]
+            if mask is not None:
+                mask = mask[:, None, :, :]
+
+            # Flash attention from https://pytorch.org/blog/accelerating-large-language-models/
+            return nn.functional.scaled_dot_product_attention(
+                q,
+                k,
+                v,
+                attn_mask=mask,
+                dropout_p=dropout_p,
+                is_causal=is_causal,
+            )
+
+    return _attention_call
diff -ruN marc_original/third_party/torchtune/torchtune/modules/common_utils.py marc/third_party/torchtune/torchtune/modules/common_utils.py
--- marc_original/third_party/torchtune/torchtune/modules/common_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/common_utils.py	2025-02-20 17:49:30.598025968 -0500
@@ -0,0 +1,165 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import mmap
+import sys
+from collections import OrderedDict
+from functools import partial
+from typing import Any, Dict, Tuple
+
+import torch
+
+import torch.nn as nn
+from torch._subclasses.fake_tensor import FakeTensorConverter, FakeTensorMode
+from torchao.dtypes.nf4tensor import NF4Tensor
+
+_use_low_cpu_ram: bool = False
+
+
+def reparametrize_as_dtype_state_dict_post_hook(
+    model: nn.Module,
+    state_dict: Dict[str, Any],
+    *args: Tuple[Any, ...],
+    dtype: torch.dtype = torch.bfloat16,
+    offload_to_cpu: bool = True,
+    **kwargs: Dict[Any, Any],
+):
+    """
+    A state_dict hook that replaces NF4 tensors with their restored
+    higher-precision weight and optionally offloads the restored weight to CPU.
+    Use this hook to avoid increased peak GPU memory usage during checkpoint
+    save when training with QLoRA.
+
+    This function is meant to be used with PyTorch's ``nn.Module._register_state_dict_hook``, i.e.
+
+    >>> m = MyModule()
+    >>> m._register_state_dict_hook(reparametrize_as_dtype_state_dict_post_hook)
+
+    If the hook is registered per the above process, this hook will be called _after_ the module's
+    ``state_dict`` method is called. The hook will replace all ``NF4Tensor`` instances by unquantizing
+    them to the original dtype, and optionally offload the restored weight to CPU.
+
+    Args:
+        model (nn.Module): the model to take ``state_dict()`` on
+        state_dict (Dict[str, Any]): the state dict to modify
+        *args (Tuple[Any, ...]): Unused args passed when running this as a state_dict hook.
+        dtype (torch.dtype): the dtype to restore the weight to. Default is ``torch.bfloat16``.
+        offload_to_cpu (bool): whether to offload the restored weight to CPU. Default is ``True``.
+        **kwargs (Dict[Any, Any]): Unused keyword args passed when running this as a state_dict hook.
+    """
+    for k, v in state_dict.items():
+        if isinstance(v, NF4Tensor):
+            state_dict[k] = v.to(dtype)
+            if offload_to_cpu:
+                state_dict[k] = state_dict[k].cpu()
+
+
+def _low_ram_reparametrize_as_dtype_state_dict_post_hook(
+    model: nn.Module,
+    state_dict: Dict[str, Any],
+    *args: Tuple[Any, ...],
+    dtype: torch.dtype = torch.bfloat16,
+    offload_to_cpu: bool = True,
+    **kwargs: Dict[Any, Any],
+):
+    """
+    A state_dict hook that replaces NF4 tensors with their restored
+    higher-precision weight and optionally offloads the restored weight to CPU.
+    Use this hook to avoid increased peak GPU memory usage during checkpoint
+    save when training with QLoRA.
+
+    This hook is similar to ``reparametrize_as_dtype_state_dict_post_hook`` but uses
+    FakeTensor and mmap(2) to avoid CPU OOM on colab.
+
+    This function is meant to be used with PyTorch's ``nn.Module._register_state_dict_hook``, i.e.
+
+    >>> m = MyModule()
+    >>> m._register_state_dict_hook(reparametrize_as_dtype_state_dict_post_hook)
+
+    If the hook is registered per the above process, this hook will be called _after_ the module's
+    ``state_dict`` method is called. The hook will replace all ``NF4Tensor`` instances by unquantizing
+    them to the original dtype, and optionally offload the restored weight to CPU.
+
+    Args:
+        model (nn.Module): the model to take ``state_dict()`` on
+        state_dict (Dict[str, Any]): the state dict to modify
+        *args (Tuple[Any, ...]): Unused args passed when running this as a state_dict hook.
+        dtype (torch.dtype): the dtype to restore the weight to. Default is ``torch.bfloat16``.
+        offload_to_cpu (bool): whether to offload the restored weight to CPU. Default is ``True``.
+        **kwargs (Dict[Any, Any]): Unused keyword args passed when running this as a state_dict hook.
+    """
+    # Create a state dict of FakeTensors that matches the state_dict
+    mode = FakeTensorMode()
+    converter = FakeTensorConverter()
+    fake_state_dict = OrderedDict()
+    for k, v in state_dict.items():
+        if isinstance(v, NF4Tensor):
+            fake_state_dict[k] = converter.from_real_tensor(mode, v).to(dtype)
+        else:
+            fake_state_dict[k] = converter.from_real_tensor(mode, v)
+
+        if offload_to_cpu:
+            fake_state_dict[k] = fake_state_dict[k].cpu()
+
+    # Create a state_dict on disk with space reserved for storage bytes
+    # Then load with mmap and MAP_SHARED (can writeback to disk file)
+    dest_state_dict_path = "/tmp/fake_state_dict.pt"
+    with torch.serialization.skip_data(materialize_fake_tensors=True):
+        torch.save(fake_state_dict, dest_state_dict_path)
+    with torch.serialization.set_default_mmap_options(mmap.MAP_SHARED):
+        dest_state_dict = torch.load(dest_state_dict_path, mmap=True, weights_only=True)
+
+    # Do D2H and upcast one by one and since dest_state_dict is backed by mmap --> won't OOM
+    # even when there is no swap space (e.g. colab)
+    for k in state_dict.keys():
+        if isinstance(state_dict[k], NF4Tensor):
+            dest_state_dict[k].copy_(state_dict[k].to(dtype))
+        else:
+            dest_state_dict[k].copy_(state_dict[k])
+
+    # In place update original state_dict object. Although the private state dict
+    # post hook supports out of place behavior, the semantic actually buggy. We eventually want
+    # to use the public state_dict post hook which does not support out of place behavior.
+    for k in state_dict.keys():
+        state_dict[k] = dest_state_dict[k]
+
+
+def _register_reparametrize_state_dict_hooks(
+    module: nn.Module,
+    dtype: torch.dtype = torch.bfloat16,
+    offload_to_cpu: bool = True,
+):
+    """
+    Register the reparametrize state dict hooks to the module and its submodules.
+
+    This function is a wrapper that is meant to toggle between the low_cpu_ram
+    and regular versions of the ``reparametrize_as_dtype`` state dict hooks.
+
+    Args:
+        module (nn.Module): the module to register the hooks to.
+        dtype (torch.dtype): the dtype to restore the weight to. Default is ``torch.bfloat16``.
+        offload_to_cpu (bool): whether to offload the restored weight to CPU. Default is ``True``.
+
+    Raises:
+        RuntimeError: If the low RAM reparametrize hook is used on Windows or an incompatible torch version.
+    """
+    if _use_low_cpu_ram:
+        if torch.__version__ < "2.5.0.dev20240906":
+            raise RuntimeError(
+                "Low RAM reparametrize_as_dtype_state_dict_post_hook requires PyTorch 2.5.0.dev20240906 or later."
+            )
+        elif sys.platform == "win32":
+            # mmap.MAP_SHARED is not supported on Windows but this change targets colab.
+            raise RuntimeError(
+                "Low RAM reparametrize_as_dtype_state_dict_post_hook is not supported on Windows."
+            )
+        else:
+            hook = _low_ram_reparametrize_as_dtype_state_dict_post_hook
+    else:
+        hook = reparametrize_as_dtype_state_dict_post_hook
+    module._register_state_dict_hook(
+        partial(hook, dtype=dtype, offload_to_cpu=offload_to_cpu)
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/modules/feed_forward.py marc/third_party/torchtune/torchtune/modules/feed_forward.py
--- marc_original/third_party/torchtune/torchtune/modules/feed_forward.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/feed_forward.py	2025-02-20 17:49:30.602025975 -0500
@@ -0,0 +1,54 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+from typing import Optional
+
+import torch
+from torch import nn
+
+
+class FeedForward(nn.Module):
+    """This class implements the feed-forward network derived from Llama2.
+
+    Args:
+        gate_proj (nn.Module): Projection from input dim to hidden dim, fed through activation
+            and multiplied by up_proj.
+        down_proj (nn.Module): Final projection to output dim.
+        up_proj (Optional[nn.Module]): Projection from input dim to hidden dim, multiplied by
+            activation(gate_proj).
+        activation (nn.Module): Activation function to use. Default is nn.SiLU().
+    """
+
+    def __init__(
+        self,
+        *,
+        gate_proj: nn.Module,
+        down_proj: nn.Module,
+        up_proj: Optional[nn.Module] = None,
+        activation: nn.Module = nn.SiLU(),
+    ):
+        super().__init__()
+        self.w1 = gate_proj
+        self.w2 = down_proj
+        self.w3 = up_proj
+        self.activation = activation
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape ``(..., in_dim)``, where ``in_dim`` is the
+                input dimension of both ``gate_proj`` and ``up_proj``.
+
+        Returns:
+            torch.Tensor: output tensor with shape ``(..., out_dim)``, where ``out_dim`` is the \
+                output dimension of ``down_proj``.
+        """
+        h = self.activation(self.w1(x))
+        if self.w3 is not None:
+            h = h * self.w3(x)
+        h = self.w2(h)
+        return h
diff -ruN marc_original/third_party/torchtune/torchtune/modules/__init__.py marc/third_party/torchtune/torchtune/modules/__init__.py
--- marc_original/third_party/torchtune/torchtune/modules/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/__init__.py	2025-02-20 17:49:30.586025948 -0500
@@ -0,0 +1,46 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from .attention import MultiHeadAttention  # noqa
+from .attention_utils import create_block_causal_mask, packed_block_causal_mask
+from .common_utils import reparametrize_as_dtype_state_dict_post_hook
+from .feed_forward import FeedForward  # noqa
+from .kv_cache import KVCache  # noqa
+from .layer_norm import Fp32LayerNorm  # noqa
+from .low_precision import FrozenNF4Linear  # noqa
+from .lr_schedulers import get_cosine_schedule_with_warmup  # noqa
+from .position_embeddings import RotaryPositionalEmbeddings  # noqa
+from .rms_norm import RMSNorm  # noqa
+from .tanh_gate import TanhGate  # noqa
+from .tied_linear import TiedLinear  # noqa
+from .transformer import (  # noqa
+    TiedEmbeddingTransformerDecoder,
+    TransformerCrossAttentionLayer,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+from .vision_transformer import VisionTransformer
+
+__all__ = [
+    "MultiHeadAttention",
+    "TanhGate",
+    "FeedForward",
+    "FrozenNF4Linear",
+    "KVCache",
+    "RotaryPositionalEmbeddings",
+    "RMSNorm",
+    "TiedLinear",
+    "Fp32LayerNorm",
+    "VisionTransformer",
+    "TransformerDecoder",
+    "TiedEmbeddingTransformerDecoder",
+    "TransformerSelfAttentionLayer",
+    "TransformerCrossAttentionLayer",
+    "reparametrize_as_dtype_state_dict_post_hook",
+    "create_block_causal_mask",
+    "packed_block_causal_mask",
+    "get_cosine_schedule_with_warmup",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/modules/kv_cache.py marc/third_party/torchtune/torchtune/modules/kv_cache.py
--- marc_original/third_party/torchtune/torchtune/modules/kv_cache.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/kv_cache.py	2025-02-20 17:49:30.606025982 -0500
@@ -0,0 +1,116 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Tuple
+
+import torch
+from torch import nn
+
+
+class KVCache(nn.Module):
+    """
+    Standalone ``nn.Module`` containing a kv-cache to cache past key and values during inference.
+
+    Args:
+        batch_size (int): batch size model will be run with
+        max_seq_len (int): maximum sequence length model will be run with
+        num_heads (int): number of heads. We take num_heads instead of num_kv_heads because
+            the cache is created after we've expanded the key and value tensors to have the
+            same shape as the query tensor. See attention.py for more details
+        head_dim (int): per-attention head embedding dimension
+        dtype (torch.dtype): dtype for the caches
+    """
+
+    def __init__(
+        self,
+        batch_size: int,
+        max_seq_len: int,
+        num_heads: int,
+        head_dim: int,
+        dtype: torch.dtype,
+    ) -> None:
+        super().__init__()
+        cache_shape = (batch_size, num_heads, max_seq_len, head_dim)
+        self.register_buffer(
+            "k_cache", torch.zeros(cache_shape, dtype=dtype), persistent=False
+        )
+        self.register_buffer(
+            "v_cache", torch.zeros(cache_shape, dtype=dtype), persistent=False
+        )
+        self.register_buffer(
+            "cache_pos", torch.arange(0, cache_shape[2]), persistent=False
+        )
+        self.batch_size = batch_size
+
+    def reset(self) -> None:
+        """Reset the cache to zero."""
+        self.k_cache.zero_()
+        self.v_cache.zero_()
+        self.cache_pos -= self.size
+
+    @property
+    def size(self) -> int:
+        return self.cache_pos[0].item()
+
+    def update(
+        self, k_val: torch.Tensor, v_val: torch.Tensor
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        """Update KV cache with the new ``k_val``, ``v_val`` and return the updated cache.
+
+        Note:
+            When updating the KV cache, it is assumed that subsequent updates should update key-value
+            positions in consecutive sequence positions. If you wish to update cache values which have
+            already been filled, use ``.reset()``, which will reset the cache to the zero-th position.
+
+        Example:
+            >>> cache = KVCache(batch_size=2, max_seq_len=16, num_heads=4, head_dim=32, dtype=torch.bfloat16)
+            >>> keys, values = torch.ones((2, 4, 8, 32)), torch.ones((2, 4, 8, 32))
+            >>> cache.update(keys, values)
+            >>> # now positions 0 through 7 are filled
+            >>> cache.size
+            >>> 8
+            >>> keys, values = torch.ones((2, 4, 1, 32)), torch.ones((2, 4, 1, 32))
+            >>> cache.update(keys, values)
+            >>> # this will fill at position 8
+            >>> cache.size
+            >>> 9
+
+        Args:
+            k_val (torch.Tensor): Current key tensor with shape [B, H, S, D]
+            v_val (torch.Tensor): Current value tensor with shape [B, H, S, D]
+
+        Returns:
+            Tuple[torch.Tensor, torch.Tensor]: Updated key and value cache tensors, respectively.
+
+        Raises:
+            AssertionError: if the sequence length of ``k_val`` is longer than the maximum cache sequence length.
+            ValueError: if the batch size of the new key (or value) tensor is greater than the batch size
+                used during cache setup.
+        """
+        bsz, _, seq_len, _ = k_val.shape
+        if bsz > self.k_cache.shape[0]:
+            raise ValueError(
+                f"The current cache has been setup with a batch size of {self.k_cache.shape[0]}"
+                f", but found new key tensors with batch size {k_val.shape[0]}!"
+            )
+
+        assert (self.cache_pos[0] + seq_len) <= self.k_cache.shape[2]
+        k_out = self.k_cache
+        v_out = self.v_cache
+
+        k_out[:, :, self.cache_pos[:seq_len]] = k_val
+        v_out[:, :, self.cache_pos[:seq_len]] = v_val
+
+        # forward cache_pos seq_len positions along
+        # cache_pos starts at (0, 1, 2, 3, 4, 5, ...)
+        # an update of seq_len = 5 tokens brings it to
+        # (5, 6, 7, 8, 9, ...)
+        # this allows us to track the current position in the cache
+        # after the last update in a compile-friendly way without any dynamism
+        # e.g. relying on an int size tracker, or re-creating cache_pos every time
+        self.cache_pos += seq_len
+
+        return k_out, v_out
diff -ruN marc_original/third_party/torchtune/torchtune/modules/layer_norm.py marc/third_party/torchtune/torchtune/modules/layer_norm.py
--- marc_original/third_party/torchtune/torchtune/modules/layer_norm.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/layer_norm.py	2025-02-20 17:49:30.610025989 -0500
@@ -0,0 +1,37 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+from typing import Any
+
+import torch
+from torch import nn
+
+
+class Fp32LayerNorm(nn.LayerNorm):
+    """
+    Wrapper around :class:`~torch.nn.LayerNorm` to support mixed-precision training.
+    """
+
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): Input tensor.
+
+        Returns:
+            torch.Tensor: The normalized output tensor having the same shape as ``x``.
+        """
+        output = nn.functional.layer_norm(
+            x.float(),
+            self.normalized_shape,
+            self.weight.float() if self.weight is not None else None,
+            self.bias.float() if self.bias is not None else None,
+            self.eps,
+        )
+        return output.type_as(x)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/loss/ce_chunked_output_loss.py marc/third_party/torchtune/torchtune/modules/loss/ce_chunked_output_loss.py
--- marc_original/third_party/torchtune/torchtune/modules/loss/ce_chunked_output_loss.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/loss/ce_chunked_output_loss.py	2025-02-20 17:49:30.622026008 -0500
@@ -0,0 +1,83 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List
+
+import torch
+import torch.nn.functional as F
+
+
+class CEWithChunkedOutputLoss(torch.nn.Module):
+    """
+    Cross-entropy with chunked outputs that saves memory by only upcasting one chunk at a time.
+
+    Whenever the model is trained with bf16, before running CE, we have to upcast
+    it to fp32 for better accuracy and stability. When upcasting happens, the memory usage doubles.
+    Models like llama3 have large vocabulary size and, therefore, have a large output
+    tensor of shape ``(bsz, num_tokens, vocab_size)``. If we chunk on the token level, you can still compute
+    the cross entropy normally, but upcasting only one chunk at a time saves considerable memory.
+
+    The CE and upcasting have to be compiled together for better performance.
+    When using this class, we recommend using :func:`torch.compile` only on the method ``compute_cross_entropy``.
+    The gains from chunking won't be realized if you compile the entire class.
+
+    For more details, please refer to: https://github.com/pytorch/torchtune/pull/1390
+    """
+
+    def __init__(self, num_output_chunks: int = 8, ignore_index: int = -100):
+        super().__init__()
+        self.num_output_chunks = num_output_chunks
+        self.ignore_index = ignore_index
+
+    def compute_cross_entropy(
+        self, logits: torch.Tensor, labels: torch.Tensor
+    ) -> torch.Tensor:
+        """
+        Upcast logits to fp32 and compute cross entropy loss.
+        """
+        return F.cross_entropy(
+            logits.float(), labels, ignore_index=self.ignore_index, reduction="sum"
+        )
+
+    def forward(self, logits: List[torch.Tensor], labels: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            logits (List[torch.Tensor]): List of chunked logits of length
+                ``self.num_output_chunks``, where each chunk has shape
+                ``(batch_size, num_tokens / num_output_chunks, vocab_size)``.
+            labels (torch.Tensor): Ground truth labels of shape ``(batch_size, num_tokens)``.
+
+        Returns:
+            torch.Tensor: Cross entropy loss of shape (1,).
+
+        Example:
+            >>> loss_fn = ChunkedCrossEntropyLoss()
+            >>>
+            >>> h = torch.tensor([bsz, num_tokens, dim])
+            >>> output_chunks = [model.output(chunk) for chunk in h.chunk(num_chunks, dim=1)]
+            >>>
+            >>> labels = torch.tensor([bsz, num_tokens])
+            >>> loss = loss_fn(output_chunks, labels)
+        """
+
+        total_elements = (labels != self.ignore_index).sum()
+
+        # chunk and reshape labels (bsz, num_tokens, vocab) -> [(bsz*num_tokens/num_chunks, vocab)]
+        labels = [
+            target_chunk.reshape(-1)
+            for target_chunk in labels.chunk(self.num_output_chunks, dim=1)
+        ]
+        # reshape logits [(bsz, num_tokens/num_chunks, vocab)] -> [(bsz*num_tokens/num_chunks, vocab)]
+        logits = [
+            logit_chunk.reshape(-1, logit_chunk.size(-1)) for logit_chunk in logits
+        ]
+
+        # compute one chunk at a time
+        total_loss = 0.0
+        for logits_chunk, labels_chunk in zip(logits, labels):
+            total_loss += self.compute_cross_entropy(logits_chunk, labels_chunk)
+
+        return total_loss / total_elements
diff -ruN marc_original/third_party/torchtune/torchtune/modules/loss/__init__.py marc/third_party/torchtune/torchtune/modules/loss/__init__.py
--- marc_original/third_party/torchtune/torchtune/modules/loss/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/loss/__init__.py	2025-02-20 17:49:30.618026001 -0500
@@ -0,0 +1,14 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from .ce_chunked_output_loss import CEWithChunkedOutputLoss
+from .kd_losses import ForwardKLLoss, ForwardKLWithChunkedOutputLoss
+
+__all__ = [
+    "CEWithChunkedOutputLoss",
+    "ForwardKLLoss",
+    "ForwardKLWithChunkedOutputLoss",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/modules/loss/kd_losses.py marc/third_party/torchtune/torchtune/modules/loss/kd_losses.py
--- marc_original/third_party/torchtune/torchtune/modules/loss/kd_losses.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/loss/kd_losses.py	2025-02-20 17:49:30.626026015 -0500
@@ -0,0 +1,132 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List
+
+import torch
+import torch.nn.functional as F
+
+
+class ForwardKLLoss(torch.nn.Module):
+    """
+    The Kullback-Leibler divergence loss for valid indexes.
+    Implementation of https://github.com/jongwooko/distillm/blob/17c0f98bc263b1861a02d5df578c84aea652ee65/distillm/losses.py
+
+    Args:
+        ignore_index (int):  Specifies a target value that is ignored and does not contribute to the input gradient.
+            The loss is divided over non-ignored targets.
+            Default: -100.
+    """
+
+    def __init__(self, ignore_index: int = -100):
+        super().__init__()
+        self.ignore_index = ignore_index
+
+    def forward(
+        self,
+        student_logits: torch.Tensor,
+        teacher_logits: torch.Tensor,
+        labels: torch.Tensor,
+    ) -> torch.Tensor:
+        """
+        Args:
+            student_logits (torch.Tensor): logits from student model of shape
+                (batch_size*num_tokens, vocab_size).
+            teacher_logits (torch.Tensor): logits from teacher model of shape
+                (batch_size*num_tokens, vocab_size).
+            labels (torch.Tensor): Ground truth labels of shape
+                (batch_size, vocab_size).
+
+        Returns:
+            torch.Tensor: KL divergence loss of shape (1,).
+        """
+
+        teacher_prob = F.softmax(teacher_logits, dim=-1, dtype=torch.float32)
+        inf_mask = torch.isinf(student_logits)
+        student_logprob = F.log_softmax(student_logits, dim=-1, dtype=torch.float32)
+        prod_probs = torch.masked_fill(teacher_prob * student_logprob, inf_mask, 0)
+        x = torch.sum(prod_probs, dim=-1).view(-1)
+        mask = (labels != self.ignore_index).int()
+        if torch.sum(mask.view(-1), dim=0) == 0:
+            return torch.tensor(0.0, device=x.device)
+        return -torch.sum(x * mask.view(-1), dim=0) / torch.sum(mask.view(-1), dim=0)
+
+
+class ForwardKLWithChunkedOutputLoss(torch.nn.Module):
+    """
+    Forward KL with chunked outputs that saves memory by only upcasting one chunk at a time.
+
+    Since the model is trained with bf16, before computing KL divergence, we have to upcast
+    it to fp32 for better accuracy and stability. When upcasting happens, the memory usage doubles.
+    Models like llama3 have large vocabulary size and, therefore, have a large output
+    result (bsz, num_tokens, vocab_size). If we chunk on the token level, you can still compute
+    the cross entropy normally, but upcasting only one chunk at a time saves considerable memory.
+
+    Args:
+        num_output_chunks (int): Number of chunks to chunk the output into. Each chunk has shape
+            (batch_size, num_tokens / num_output_chunks, vocab_size).
+            Default: 8
+        ignore_index (int): Specifies a target value that is ignored and does not contribute to the input gradient.
+            The loss is divided over non-ignored targets.
+            Default: -100
+    """
+
+    def __init__(self, num_output_chunks: int = 8, ignore_index: int = -100):
+        super().__init__()
+        self.num_output_chunks = num_output_chunks
+        self.ignore_index = ignore_index
+        self.fkl_loss = ForwardKLLoss(ignore_index)
+
+    def forward(
+        self,
+        student_logits: List[torch.Tensor],
+        teacher_logits: List[torch.Tensor],
+        labels: torch.Tensor,
+    ) -> torch.Tensor:
+        """
+        Args:
+            student_logits (List[torch.Tensor]): List of chunked logits from student model of length
+                ``self.num_output_chunks``, where each chunk has shape
+                (batch_size, num_tokens / num_output_chunks, vocab_size).
+            teacher_logits (List[torch.Tensor]): List of chunked logits from teacher model of length
+                ``self.num_output_chunks``, where each chunk has shape
+                (batch_size, num_tokens / num_output_chunks, vocab_size).
+            labels (torch.Tensor): Ground truth labels of shape (batch_size, num_tokens).
+
+        Returns:
+            torch.Tensor: KL divergence loss of shape (1,).
+
+        Example:
+            >>> loss_fn = ForwardKLWithChunkedOutputLoss()
+            >>>
+            >>> h = torch.tensor([bsz, num_tokens, dim])
+            >>> output_chunks = [model.output(chunk) for chunk in h.chunk(num_chunks, dim=1)]
+            >>> teacher_chunks = [teacher_model.output(chunk) for chunk in h.chunk(num_chunks, dim=1)]
+            >>> labels = torch.tensor([bsz, num_tokens])
+            >>> loss = loss_fn(output_chunks, teacher_chunks, labels)
+        """
+
+        # reshape logits [(bsz, num_tokens/num_chunks, vocab)] -> [(bsz*num_tokens/num_chunks, vocab)]
+        teacher_logits = [
+            teacher_logits_chunk.reshape(-1, teacher_logits_chunk.size(-1))
+            for teacher_logits_chunk in teacher_logits
+        ]
+        student_logits = [
+            student_logits_chunk.reshape(-1, student_logits_chunk.size(-1))
+            for student_logits_chunk in student_logits
+        ]
+        # chunk and reshape labels (bsz, num_tokens, vocab) -> [(bsz*num_tokens/num_chunks, vocab)]
+        labels = [
+            target_chunk.reshape(-1)
+            for target_chunk in labels.chunk(self.num_output_chunks, dim=1)
+        ]
+        total_fkl_loss = 0.0
+        for student_chunk, teacher_chunk, label_chunk in zip(
+            student_logits, teacher_logits, labels
+        ):
+            total_fkl_loss += self.fkl_loss(student_chunk, teacher_chunk, label_chunk)
+
+        return total_fkl_loss / self.num_output_chunks
Binary files marc_original/third_party/torchtune/torchtune/modules/loss/__pycache__/ce_chunked_output_loss.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/loss/__pycache__/ce_chunked_output_loss.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/loss/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/loss/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/loss/__pycache__/kd_losses.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/loss/__pycache__/kd_losses.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/modules/low_precision/__init__.py marc/third_party/torchtune/torchtune/modules/low_precision/__init__.py
--- marc_original/third_party/torchtune/torchtune/modules/low_precision/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/low_precision/__init__.py	2025-02-20 17:49:30.630026021 -0500
@@ -0,0 +1,11 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from .nf4_linear import FrozenNF4Linear
+
+__all__ = [
+    "FrozenNF4Linear",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/modules/low_precision/nf4_linear.py marc/third_party/torchtune/torchtune/modules/low_precision/nf4_linear.py
--- marc_original/third_party/torchtune/torchtune/modules/low_precision/nf4_linear.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/low_precision/nf4_linear.py	2025-02-20 17:49:30.638026034 -0500
@@ -0,0 +1,60 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional
+
+import torch
+
+import torch.nn as nn
+from torchao.dtypes.nf4tensor import linear_nf4, to_nf4
+
+
+class FrozenNF4Linear(nn.Linear):
+    """
+    A linear layer similar to ``torch.nn.Linear`` but uses a quantized
+    NF4Tensor as its weight. This class also freezes its ``weight`` parameter
+    and is meant to be used as the base Linear layer for modeling
+    use cases such as QLoRA where base model parameters are frozen.
+    NOTE: biases are currently not supported.
+
+    Args:
+        in_dim (int): input dimension
+        out_dim (int): output dimension
+        device (Optional[torch.device]): device to use for the underlying weight. If ``None``, uses the default
+            device given by `torch.get_default_device()`.
+        **kwargs: any additional arguments to pass to the underlying Linear layer.
+
+    Raises:
+        RuntimeError: if ``bias`` is set to ``True``
+    """
+
+    def __init__(
+        self, in_dim: int, out_dim: int, device: Optional[torch.device] = None, **kwargs
+    ):
+        if "bias" in kwargs and kwargs.pop("bias"):
+            raise RuntimeError("FrozenNF4Linear does not currently support biases!")
+
+        super().__init__(in_dim, out_dim, device=device, bias=False, **kwargs)
+        self.weight.requires_grad_(False)
+        self.nf4_weight = to_nf4(self.weight)
+        # re-register self.weight as the nf4 weight, so that the nf4 weight
+        # shows up as expected in .parameters, state_dict, etc.
+        torch.utils.swap_tensors(
+            self.weight, torch.nn.Parameter(self.nf4_weight, requires_grad=False)
+        )
+
+    def forward(self, input: torch.Tensor) -> torch.Tensor:
+        """
+        Runs linear operation with input tensor as given by `input`. Computation happens in higher
+        precision, though only the nf4 weight is saved for backward for gradient computation to ensure
+        additional memory is not used.
+        Args:
+            input (torch.Tensor): input tensor
+
+        Returns:
+            Tensor: output tensor
+        """
+        return linear_nf4(input=input, weight=self.weight)
Binary files marc_original/third_party/torchtune/torchtune/modules/low_precision/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/low_precision/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/low_precision/__pycache__/nf4_linear.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/low_precision/__pycache__/nf4_linear.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/low_precision/__pycache__/_register_nf4_dispatch_ops.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/low_precision/__pycache__/_register_nf4_dispatch_ops.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/modules/low_precision/_register_nf4_dispatch_ops.py marc/third_party/torchtune/torchtune/modules/low_precision/_register_nf4_dispatch_ops.py
--- marc_original/third_party/torchtune/torchtune/modules/low_precision/_register_nf4_dispatch_ops.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/low_precision/_register_nf4_dispatch_ops.py	2025-02-20 17:49:30.634026028 -0500
@@ -0,0 +1,19 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+from torchao.dtypes.nf4tensor import implements as nf4_tensor_impl, to_nf4
+
+
+@nf4_tensor_impl([torch.ops.aten.clone.default])
+def clone(func, *args, **kwargs):
+    """
+    __torch_dispatch__ override that is called when cloning an NF4Tensor.
+    This is implemented by creating a new NF4Tensor with the unquantized weight
+    of the input tensor. Note that this is not an exact "clone" due to the loss
+    in precision.
+    """
+    return to_nf4(args[0][0].get_original_weight())
diff -ruN marc_original/third_party/torchtune/torchtune/modules/lr_schedulers.py marc/third_party/torchtune/torchtune/modules/lr_schedulers.py
--- marc_original/third_party/torchtune/torchtune/modules/lr_schedulers.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/lr_schedulers.py	2025-02-20 17:49:30.642026041 -0500
@@ -0,0 +1,61 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+
+import torch
+from torch.optim.lr_scheduler import LambdaLR
+from torchtune.utils._logging import deprecated
+
+
+@deprecated(
+    msg="Please use get_cosine_schedule_with_warmup from torchtune.training.lr_schedulers instead. \
+        "
+)
+def get_cosine_schedule_with_warmup(
+    optimizer: torch.optim.Optimizer,
+    num_warmup_steps: int,
+    num_training_steps: int,
+    num_cycles: float = 0.5,
+    last_epoch: int = -1,
+) -> LambdaLR:
+    """
+    Create a learning rate schedule that linearly increases the learning rate from
+    0.0 to lr over ``num_warmup_steps``, then decreases to 0.0 on a cosine schedule over
+    the remaining ``num_training_steps-num_warmup_steps`` (assuming ``num_cycles`` = 0.5).
+
+    This is based on the Hugging Face implementation
+    https://github.com/huggingface/transformers/blob/v4.23.1/src/transformers/optimization.py#L104.
+
+    Args:
+        optimizer (torch.optim.Optimizer): The optimizer for which to
+            schedule the learning rate.
+        num_warmup_steps (int): The number of steps for the warmup phase.
+        num_training_steps (int): The total number of training steps.
+        num_cycles (float): The number of waves in the cosine schedule. Defaults to 0.5
+            (decrease from the max value to 0 following a half-cosine).
+        last_epoch (int): The index of the last epoch when resuming training. Defaults to -1
+
+    Returns:
+        torch.optim.lr_scheduler.LambdaLR with the appropriate schedule.
+    """
+
+    def lr_lambda(current_step: int) -> float:
+        # linear warmup phase
+        if current_step < num_warmup_steps:
+            return current_step / max(1, num_warmup_steps)
+
+        # cosine
+        progress = (current_step - num_warmup_steps) / max(
+            1, num_training_steps - num_warmup_steps
+        )
+
+        cosine_lr_multiple = 0.5 * (
+            1.0 + math.cos(math.pi * num_cycles * 2.0 * progress)
+        )
+        return max(0.0, cosine_lr_multiple)
+
+    return LambdaLR(optimizer, lr_lambda, last_epoch)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/model_fusion/_fusion.py marc/third_party/torchtune/torchtune/modules/model_fusion/_fusion.py
--- marc_original/third_party/torchtune/torchtune/modules/model_fusion/_fusion.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/model_fusion/_fusion.py	2025-02-20 17:49:30.650026054 -0500
@@ -0,0 +1,455 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Dict, List, Optional, Union
+
+import torch
+from torch import nn
+from torchtune.modules import TransformerDecoder
+from torchtune.modules.model_fusion._fusion_utils import get_fusion_params
+from torchtune.modules.peft._utils import set_trainable_params
+
+
+class FusionLayer(nn.Module):
+    """Fusion layer as introduced in `Flamingo: a Visual Language Model for Few-Shot Learning <https://arxiv.org/abs/2204.14198>`_.
+
+    Deep Fusion model architectures combine pretrained encoder models with pretrained
+    language models by infusing the encoder outputs into the middle layers of the LLM.
+    This allows the language model to interpret the enocder outputs as text and
+    "understand" any modality for which you can train an encoder. To enable the language model
+    to adapt to the encoder outputs, the FusionLayer fuses a new learnable layer to an existing
+    decoder (language model) layer. This additional layer can take the encoder embeddings and
+    learn to combine them with the token embeddings from the decoder. The module supports fusing
+    the new layer before or after the original, in Flamingo the new layer is fused before the original.
+
+    The original layer is wrapped in FusionLayer such that it maintains its original state_dict
+    key and the pre-trained checkpoint isn't broken. The new layer parameters are available
+    through ``fusion_params`` to separately control if they're trainable or not.
+
+    Example:
+        >>> # Original decoder style transformer
+        >>> layer = nn.TransformerSelfAttentionLayer(...)
+        >>> model = TransformerDecoder(layers=layer, num_layers=32, ...)
+        >>>
+        >>> # Fuse a cross attention layer to each self attention layer to adapt for the encoder
+        >>> fusion_layer = nn.TransformerCrossAttentionLayer(...)
+        >>> fused_layer = FusionLayer(layer, fusion_layer)
+        >>> model = TransformerDecoder(layers=fused_layer, num_layers=32, ...)
+        >>>
+        >>> # Original decoder state_dict still works
+        >>> model.load_state_dict(..., strict=False)
+
+    Args:
+        layer (nn.Module): original decoder layer
+        fusion_layer (nn.Module): new fusion layer
+        fusion_first (bool): boolean to insert fusion layer before or after the decoder layer.
+    """
+
+    def __init__(
+        self, layer: nn.Module, fusion_layer: nn.Module, fusion_first: bool = True
+    ):
+        super().__init__()
+        self.layer = layer
+        self.fusion_layer = fusion_layer
+        self.fusion_first = fusion_first
+
+        # Keep FusionLayer wrappings out of the state_dict
+        self._register_state_dict_hook(FusionLayer._state_dict_hook)
+        self._register_load_state_dict_pre_hook(
+            FusionLayer._load_state_dict_hook, with_module=True
+        )
+        # TODO: Switch to register_load_state_dict_pre_hook and
+        # register_state_dict_pre_hook after PyTorch v2.5
+
+    def _state_dict_hook(self, state_dict, prefix, *args, **kwargs):
+        """Remove "layer" from the original layer in the state_dict
+        name. This keeps the orginal state dict name for the layer
+        from before fusing with the FusionLayer.
+
+        [!Note] This update changes the order of the OrderedDict
+        """
+        keys = list(state_dict.keys())
+        for key in keys:
+            local_key = key[len(prefix) :]
+            if local_key.startswith("layer"):
+                new_key = prefix + local_key.replace("layer.", "")
+                state_dict[new_key] = state_dict[key]
+                del state_dict[key]
+
+    def _load_state_dict_hook(self, state_dict, prefix, *args, **kwargs):
+        """Apply extra "layer" prefix to the state_dict key to
+        account for the FusionLayer wrapping.
+        """
+        keys = list(state_dict.keys())
+        for key in keys:
+            local_key = key[len(prefix) :]
+            if not local_key.startswith("fusion_layer"):
+                new_key = prefix + "layer." + local_key
+                state_dict[new_key] = state_dict[key]
+                del state_dict[key]
+
+    def setup_cache(
+        self,
+        batch_size: int,
+        dtype: torch.dtype,
+        *,
+        encoder_max_seq_len: int,
+        decoder_max_seq_len: int,
+    ) -> None:
+        """Setup key value cache for both layers.
+
+        Args:
+            batch_size (int): batch size for the caches.
+            dtype (torch.dtype): dtype for the caches.
+            encoder_max_seq_len (int): maximum cache sequence length for cross-attention layer.
+            decoder_max_seq_len (int): maximum cache sequence length for self-attention layer.
+        """
+        self.layer.setup_cache(
+            batch_size,
+            dtype,
+            encoder_max_seq_len=encoder_max_seq_len,
+            decoder_max_seq_len=decoder_max_seq_len,
+        )
+
+        self.fusion_layer.setup_cache(
+            batch_size,
+            dtype,
+            encoder_max_seq_len=encoder_max_seq_len,
+            decoder_max_seq_len=decoder_max_seq_len,
+        )
+
+    @property
+    def cache_enabled(self) -> bool:
+        """Check if the key value caches are setup."""
+        return self.layer.cache_enabled
+
+    def reset_cache(self):
+        """Reset both layers' key value caches."""
+        self.layer.reset_cache()
+        self.fusion_layer.reset_cache()
+
+    def fusion_params(self) -> List[str]:
+        """
+        Return parameters of fusion layer.
+        """
+        fusion_params = [
+            f"fusion_layer.{k}" for k, v in self.fusion_layer.named_parameters()
+        ]
+        return fusion_params
+
+    def forward(self, x: torch.Tensor, **kwargs: Dict) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape
+                [batch_size x seq_length x embed_dim]
+            **kwargs (Dict): all additional layer args
+
+        Returns:
+            Tensor: output tensor with same shape as input
+                [batch_size x seq_length x embed_dim]`
+
+        """
+        if self.fusion_first:
+            x = self.fusion_layer(x, **kwargs)
+            x = self.layer(x, **kwargs)
+        else:
+            x = self.layer(x, **kwargs)
+            x = self.fusion_layer(x, **kwargs)
+        return x
+
+
+class FusionEmbedding(nn.Module):
+    """Fusion embedding supports training additional special tokens while keeping
+    the original embedding frozen. When fusing new models with a language model,
+    there may be some additional tokens needed to support the fused language model. For
+    example, adding a vision encoder might necessitate additional tokens like ``<|image|>``
+    to indicate an images position in text and require learning an embedding for this token.
+    The FusionEmbedding keeps the original embeddings frozen while learning a much smaller
+    second embedding for the additional tokens. During forward this module routes
+    the tokens to the appropriate embedding table.
+
+    Use this as a drop-in replacement for :class:`torch.nn.Embedding` in your model.
+
+    Example:
+        >>> embedding = FusionEmbedding(vocab_size=100, fusion_vocab_size=10, embed_dim=128)
+        >>> model = TransformerDecoder(tok_embeddings=embedding, ...)
+        >>>
+        >>> # Original model state_dict still works
+        >>> model.load_state_dict(..., strict=False)
+
+    .. note::
+        This module assumes all tokens in the range [0, vocab_size) are part of the
+        original embedding table and all new tokens in the range
+        [vocab_size, vocab_size + fusion_vocab_size)
+
+    Args:
+        vocab_size (int): language model vocab size
+        fusion_vocab_size (int): additional tokens for the fused model
+        embed_dim (int): embedding dimension of the two embedding tables
+    """
+
+    def __init__(self, vocab_size: int, fusion_vocab_size: int, embed_dim: int) -> None:
+        super().__init__()
+        self.embedding = nn.Embedding(vocab_size, embed_dim)
+        self.fusion_embedding = nn.Embedding(fusion_vocab_size, embed_dim)
+        self.dim = embed_dim
+        self.num_embeddings = vocab_size + fusion_vocab_size
+        # TODO: Support merging the embeddings after finetuning
+
+        # Keep FusionLayer wrappings out of the state_dict
+        self._register_state_dict_hook(FusionEmbedding._state_dict_hook)
+        self._register_load_state_dict_pre_hook(
+            FusionEmbedding._load_state_dict_hook, with_module=True
+        )
+        # TODO: Switch to register_load_state_dict_pre_hook and
+        # register_state_dict_pre_hook after PyTorch v2.5
+
+    def _state_dict_hook(self, destination, prefix, keep_vars):
+        """Remove "embedding" from the original embedding in the state_dict
+        name. This keeps the orginal state dict name for the embedding
+        from before fusing with the FusionEmbedding.
+
+        [!Note] This update changes the order of the OrderedDict
+        """
+        key = prefix + "embedding.weight"
+        new_key = prefix + "weight"
+        destination[new_key] = destination[key]
+        del destination[key]
+
+    def _load_state_dict_hook(self, state_dict, prefix, *args, **kwargs):
+        """Apply extra "embedding" prefix to the state_dict key to
+        account for the FusionEmbedding wrapping.
+        """
+        key = prefix + "weight"
+        new_key = prefix + "embedding.weight"
+        state_dict[new_key] = state_dict[key]
+        del state_dict[key]
+
+    def fusion_params(self) -> List[str]:
+        """
+        Return fusion embedding parameters.
+        """
+        fusion_params = ["fusion_embedding.weight"]
+        return fusion_params
+
+    def _fused_embed(self, bs, seq_len):
+        """
+        Return an empty tensor the shape of the combined embedding.
+        """
+        device = self.embedding.weight.device
+        dtype = self.embedding.weight.dtype
+        return torch.empty(bs, seq_len, self.dim, device=device, dtype=dtype)
+
+    def forward(self, input: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            input (torch.Tensor): input integer tensor with shape
+                [batch_size x seq_length]
+
+        Returns:
+            Tensor: output tensor embedding with shape
+                [batch_size x seq_length x embed_dim]`
+
+        """
+        bs, seq_len = input.size()
+        vocab_size = self.embedding.num_embeddings
+
+        mask = input < vocab_size
+        # num_tokens = (input < vocab_size).sum()
+        tokens = torch.masked_select(input, mask)
+        # num_fusion_tokens = (input >= vocab_size).sum()
+        fusion_tokens = torch.masked_select(input, ~mask) - vocab_size
+
+        # [batch_size x num_tokens x embed_dim]
+        embeds = self.embedding(tokens)
+        # [batch_size x num_fusion_tokens x embed_dim]
+        fusion_embeds = self.fusion_embedding(fusion_tokens)
+
+        # [batch_size x seq_length x embed_dim]
+        out = self._fused_embed(bs, seq_len)
+        mask = mask.unsqueeze(-1).expand(bs, seq_len, self.dim)
+        out = out.masked_scatter(mask, embeds)
+        out = out.masked_scatter(~mask, fusion_embeds)
+        return out
+
+
+class DeepFusionModel(nn.Module):
+    """DeepFusion is a type of fused model architecture where a pretrained encoder is combined
+    with a pretrained decoder (LLM). This is a popular architecture for multimodal models, with
+    a full overview available in `The Evolution of Multimodal Model Architectures <https://arxiv.org/abs/2405.17927>`_.
+
+    This module has the same methods and forward signature as :class:`~torchtune.modules.TransformerDecoder` and can be used
+    interchangeably where :class:`~torchtune.modules.TransformerDecoder` is. It combines the encoder with the decoder as a
+    single module for checkpointing and finetuning. It is expected that the encoder and decoder
+    are already defined with any extra learnable ``fusion_params``: learnable parameters to help
+    adapt the pre-trained encoder to the pre-trained decoder.
+
+    Example:
+        >>> # decoder is a TransformerDecoder (e.g. llama3_8b) with fused cross attention layers
+        >>> embed = FusionEmbedding(...)
+        >>> layer = FusionLayer(
+        ...     layer=TransformerSelfAttentionLayer(...),
+        ...     fusion_layer=TransformerCrossAttentionLayer(...),
+        ... )
+        >>> decoder = TransformerDecoder(tok_embeddings=embed, layers=layer, num_layers=32, ...)
+        >>>
+        >>> # encoder is pre-trained encoder (e.g. clip_vit_224) with an added projection head
+        >>> projection_head = FeedForward(...)
+        >>> register_fusion_module(projection_head))
+        >>> encoder = nn.Sequential(clip_vit_224(), projection_head)
+        >>>
+        >>> # DeepFusionModel combines the encoder and decoder
+        >>> model = DeepFusionModel(decoder, encoder)
+        >>>
+        >>> # Load full fused checkpoints (e.g. a Flamingo checkpoint)
+        >>> model.load_state_dict(...)
+        >>>
+        >>> # Or load pretrained individual models (fusion_params are not loaded)
+        >>> model.encoder.load_state_dict(..., strict=False)
+        >>> model.decoder.load_state_dict(..., strict=False)
+        >>>
+        >>> # Forward pass
+        >>> output = model(tokens, mask, encoder_input, encoder_mask, input_pos)
+
+    Args:
+        decoder (TransformerDecoder): decoder module
+        encoder (nn.Module): encoder module
+        decoder_trainable (bool): whether to train or freeze the decoder. Default is False.
+        encoder_trainable (bool): whether to train or freeze the encoder. Default is False.
+        fusion_trainable (bool): whether to train the fusion parameters. Default is True.
+
+    """
+
+    def __init__(
+        self,
+        decoder: TransformerDecoder,
+        encoder: nn.Module,
+        *,
+        decoder_trainable: bool = False,
+        encoder_trainable: bool = False,
+        fusion_trainable: bool = True,
+    ):
+        super().__init__()
+        self.decoder = decoder
+        self.encoder = encoder
+
+        trainable_params = set()
+        if encoder_trainable:
+            trainable_params |= {
+                f"encoder.{n}" for n, p in self.encoder.named_parameters()
+            }
+        if decoder_trainable:
+            trainable_params |= {
+                f"decoder.{n}" for n, p in self.decoder.named_parameters()
+            }
+        if fusion_trainable:
+            trainable_params |= set(get_fusion_params(self))
+        else:
+            trainable_params -= set(get_fusion_params(self))
+        set_trainable_params(self, trainable_params)
+
+    def set_num_output_chunks(self, num_output_chunks: int) -> None:
+        """Used to save memory in combination with :class:`~torchtune.modules.loss.CEWithChunkedOutputLoss`.
+        This should be called before the first forward pass, in the recipe."""
+        self.decoder.set_num_output_chunks(num_output_chunks)
+
+    def setup_caches(
+        self,
+        batch_size: int,
+        dtype: torch.dtype,
+        *,
+        encoder_max_seq_len: int = None,
+        decoder_max_seq_len: int = None,
+    ):
+        """
+        Sets up key-value attention caches for inference for ``self.decoder``.
+        For each layer in ``self.decoder.layers``:
+        - :class:`torchtune.modules.TransformerSelfAttentionLayer` will use ``decoder_max_seq_len``.
+        - :class:`torchtune.modules.TransformerCrossAttentionLayer` will use ``encoder_max_seq_len``.
+        - :class:`torchtune.modules.fusion.FusionLayer` will use both ``decoder_max_seq_len`` and ``encoder_max_seq_len``.
+
+        Args:
+            batch_size (int): batch size for the caches.
+            dtype (torch.dtype): dtype for the caches.
+            encoder_max_seq_len (int): maximum encoder cache sequence length.
+            decoder_max_seq_len (int): maximum decoder cache sequence length.
+        """
+        self.decoder.setup_caches(
+            batch_size,
+            dtype,
+            encoder_max_seq_len=encoder_max_seq_len,
+            decoder_max_seq_len=decoder_max_seq_len,
+        )
+
+    def caches_are_enabled(self) -> bool:
+        """Check if the key value caches are setup."""
+        return self.decoder.caches_are_enabled()
+
+    def reset_caches(self):
+        """Reset the key value caches."""
+        self.decoder.reset_caches()
+
+    def forward(
+        self,
+        tokens: torch.Tensor,
+        *,
+        mask: Optional[torch.Tensor] = None,
+        encoder_input: Optional[Dict] = None,
+        encoder_mask: Optional[torch.Tensor] = None,
+        input_pos: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, List[torch.Tensor]]:
+        """
+        Args:
+            tokens (torch.Tensor): input tensor with shape ``[b x s]``
+            mask (Optional[torch.Tensor]): Optional boolean tensor which contains the attention mask
+                with shape ``[b x s x s]``. This is applied after the query-key multiplication and
+                before the softmax. A value of True in row i and column j means token i attends
+                to token j. A value of False means token i does not attend to token j. If no
+                mask is specified, a causal mask is used by default. Default is None.
+            encoder_input (Optional[Dict]): Optional input for the encoder.
+            encoder_mask (Optional[torch.Tensor]):  Boolean tensor defining a relational matrix between
+                tokens and encoder embeddings. A True value at position i,j means token i can attend
+                to embedding j in the decoder. Mask has shape ``[b x s x s_e]``. Default is None.
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape ``[b x s]``.
+                During inference, this indicates the position of the current token.
+                If none, assume the index of the token is its position id. Default is None.
+
+        Note: At the very first step of inference, when the model is provided with a prompt,
+        ``input_pos`` would contain the positions of all of the tokens in the prompt
+        (eg: ``torch.arange(prompt_length)``). This is because we will need to compute the
+        KV values for each position.
+
+        Returns:
+            Tensor: output tensor with shape ``[b x s x v]`` or a list of layer \
+                output tensors defined by ``output_hidden_states`` with the \
+                final output tensor appended to the list.
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - s: token sequence length
+            - s_e: encoder sequence length
+            - v: vocab size
+            - d: token embed dim
+            - d_e: encoder embed dim
+            - m_s: max seq len
+        """
+        # During decoding, encoder_input will only be provided
+        # for new inputs. Previous encoder outputs are cached
+        # in the decoder cache.
+        encoder_embed = None
+        if encoder_input is not None:
+            encoder_embed = self.encoder(**encoder_input)
+
+        output = self.decoder(
+            tokens=tokens,
+            mask=mask,
+            encoder_input=encoder_embed,
+            encoder_mask=encoder_mask,
+            input_pos=input_pos,
+        )
+        return output
diff -ruN marc_original/third_party/torchtune/torchtune/modules/model_fusion/_fusion_utils.py marc/third_party/torchtune/torchtune/modules/model_fusion/_fusion_utils.py
--- marc_original/third_party/torchtune/torchtune/modules/model_fusion/_fusion_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/model_fusion/_fusion_utils.py	2025-02-20 17:49:30.654026061 -0500
@@ -0,0 +1,69 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import functools
+from typing import Dict, List
+
+from torch import nn
+
+
+def register_fusion_module(module: nn.Module):
+    """Add the method fusion_params to an nn.Module that
+    marks all of the Modules parameters as fusion params.
+    This can be used for a layer or an entire model that is
+    added to combine two or more pretrained models.
+
+    For example, you might want to add a projection head
+    head onto an encoder to learn a projection from the
+    pre-trained encodings to the decoder's embedding space. This
+    is typical with both Deep Fusion and Early Fusion models.
+
+    Example:
+        >>> projection_head = FeedForward(...)
+        >>> register_fusion_module(projection_head))
+        >>> encoder = nn.Sequential(clip_vit_224(), projection_head)
+
+    Args:
+        module (nn.Module): module to add the fusion_params method to
+    """
+
+    def fusion_params(self) -> List[str]:
+        """
+        Return parameters of fusion layer.
+        """
+        return [k for k, v in self.named_parameters()]
+
+    module.fusion_params = functools.partial(fusion_params, module)
+
+
+def get_fusion_params(model: nn.Module) -> Dict[str, nn.Parameter]:
+    """
+    Return the subset of parameters from a model that correspond to fused
+    modules. Assumes that any fusion class has defined the
+    :func:`~torchtune.modules.model_fusion.FusionLayer.fusion_params` method.
+
+    Args:
+        model (nn.Module): Instance of model class containing some
+            fusion params.
+
+    Returns:
+        Dict[str, nn.Parameter]: the subset of model's state dict containing
+            only adapter parameters.
+
+    """
+    fusion_params = {}
+    for k, v in model.named_modules():
+        if hasattr(v, "fusion_params") and callable(v.fusion_params):
+            current_fusion_params = v.fusion_params()
+            for n, p in v.named_parameters(recurse=True):
+                if n in current_fusion_params:
+                    full_key = f"{k}.{n}" if k else n
+                    fusion_params.update({full_key: p})
+                    current_fusion_params.remove(n)
+            assert (
+                current_fusion_params == []
+            ), f"Fusion params {current_adapter_params} not converted"
+    return fusion_params
diff -ruN marc_original/third_party/torchtune/torchtune/modules/model_fusion/__init__.py marc/third_party/torchtune/torchtune/modules/model_fusion/__init__.py
--- marc_original/third_party/torchtune/torchtune/modules/model_fusion/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/model_fusion/__init__.py	2025-02-20 17:49:30.646026047 -0500
@@ -0,0 +1,16 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._fusion import DeepFusionModel, FusionEmbedding, FusionLayer
+from ._fusion_utils import get_fusion_params, register_fusion_module
+
+__all__ = [
+    "DeepFusionModel",
+    "FusionLayer",
+    "FusionEmbedding",
+    "register_fusion_module",
+    "get_fusion_params",
+]
Binary files marc_original/third_party/torchtune/torchtune/modules/model_fusion/__pycache__/_fusion.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/model_fusion/__pycache__/_fusion.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/model_fusion/__pycache__/_fusion_utils.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/model_fusion/__pycache__/_fusion_utils.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/model_fusion/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/model_fusion/__pycache__/__init__.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/modules/peft/dora.py marc/third_party/torchtune/torchtune/modules/peft/dora.py
--- marc_original/third_party/torchtune/torchtune/modules/peft/dora.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/peft/dora.py	2025-02-20 17:49:30.670026087 -0500
@@ -0,0 +1,164 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+from typing import List
+
+import torch
+import torch.nn.functional as F
+
+from torch import nn
+
+from torchao.dtypes.nf4tensor import linear_nf4, to_nf4
+from torchtune.modules.low_precision import _register_nf4_dispatch_ops  # noqa: F401
+from torchtune.modules.peft import AdapterModule
+
+
+class DoRALinear(nn.Module, AdapterModule):
+    """LoRA linear layer as introduced in `LoRA: Low-Rank Adaptation of Large Language Models <https://arxiv.org/abs/2106.09685>`_.
+
+    LoRA perturbs a given layer via a low-rank approximation where only
+    the rank decomposition matrices are trainable. In a linear layer instead of
+    :math:`x \\mapsto W_0x` a LoRALinear layer is defined as
+    :math:`x \\mapsto W_0x + (\\alpha / r)BAx`, where :math:`r` is the rank of
+    the matrices :math:`A` and :math:`B` and :math:`\\alpha` is a scaling factor.
+    As in the original implementation, we support dropout before multiplication
+    by the low-rank matrices.
+
+    Args:
+        in_dim (int): input dimension
+        out_dim (int): output dimension
+        rank (int): rank of the low-rank approximation
+        alpha (float): scaling factor for the low-rank approximation
+        dropout (float): dropout probability. Default: 0.0
+        use_bias (bool): whether to include bias in the original linear layer.
+            Default: False
+        quantize_base (bool): Whether to quantize base linear weight or not.
+            Default: False
+
+    Raises:
+        NotImplementedError: If use_bias is enabled.
+    """
+
+    def __init__(
+        self,
+        in_dim: int,
+        out_dim: int,
+        rank: int,
+        alpha: float,
+        dropout: float = 0.0,
+        use_bias: bool = False,
+        quantize_base: bool = False,
+    ):
+        super().__init__()
+        if use_bias:
+            raise NotImplementedError("DoRALinear does not support using bias")
+        self.in_dim = in_dim
+        self.out_dim = out_dim
+        self.scaling = alpha / rank
+        self._quantize_base = quantize_base
+        weight = self._create_weight()
+        self.register_parameter("weight", nn.Parameter(weight))
+
+        # 'self.disabled' is a flag showing whether to turn off DoRA adapters,
+        # this can be used in DPO for treating the dora adapters as the policy model
+        # and disabling it to treat the base model as the reference model
+        self.disabled = False
+
+        self.dropout = nn.Dropout(p=dropout) if dropout > 0.0 else nn.Identity()
+        self.lora_a = nn.Linear(in_features=in_dim, out_features=rank, bias=False)
+        self.lora_b = nn.Linear(in_features=rank, out_features=out_dim, bias=False)
+        self.magnitude = nn.Parameter(torch.empty(out_dim))
+        self.initialize_parameters()
+
+    def initialize_parameters(self):
+        # Initialize as in
+        # https://github.com/microsoft/LoRA/blob/4c0333854cb905966f8cc4e9a74068c1e507c7b7/loralib/layers.py#L119
+        _lora_a_init_params(self.lora_a)
+        _lora_b_init_params(self.lora_b)
+
+    def initialize_dora_magnitude(self):
+        """
+        DoRA initializes the magnitude vector such that its outputs are initially
+        identical to standard LoRA's outputs.
+        """
+        base_weight = self.weight.to(self.lora_a.weight.dtype)
+        lora_weight = self.lora_b.weight @ self.lora_a.weight
+        weight_norm = self._get_weight_norm(base_weight, lora_weight)
+        self.magnitude = nn.Parameter(weight_norm, requires_grad=True)
+
+    def _create_weight(self):
+        """
+        Creates a linear weight and bias tensor, using NF4 dtype if we're quantizing
+        (indicated via quantize_base=True).
+        """
+        in_dim, out_dim = self.in_dim, self.out_dim
+        linear = nn.Linear(in_features=in_dim, out_features=out_dim, bias=False)
+        weight = linear.weight if not self._quantize_base else to_nf4(linear.weight)
+        return weight
+
+    def _get_weight_norm(self, weight, lora_weight):
+        weight = weight + self.scaling * lora_weight
+        weight_norm = torch.linalg.norm(weight, dim=1).to(weight.dtype)
+        return weight_norm
+
+    def adapter_params(self) -> List[str]:
+        """
+        Return lora_a.weight and lora_b.weight as adapter params.
+        If bias is enabled, also return lora_a.bias and lora_b.bias.
+        """
+        adapter_params = ["lora_a.weight", "lora_b.weight", "magnitude"]
+        return adapter_params
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape ``(..., in_dim)``
+
+        Returns:
+            Tensor: output tensor with shape ``(..., out_dim)``
+        """
+        if self._quantize_base:
+            base_out = linear_nf4(input=x, weight=self.weight)
+        else:
+            base_out = F.linear(x, self.weight)
+        if self.disabled:
+            return base_out
+
+        x = self.dropout(x)
+
+        lora_out = self.lora_b(self.lora_a(x))
+        # Can't use raw matmul since FSDP hooks are attached to __call__
+        # Instead follow the approach in https://github.com/huggingface/peft/pull/1806
+        x_eye = torch.eye(
+            self.lora_a.weight.shape[1], device=self.lora_a.weight.device, dtype=x.dtype
+        )
+        lora_weight = self.lora_b(self.lora_a(x_eye)).T
+        magnitude = self.magnitude
+        weight = self.weight.to(x.dtype)
+        weight_norm = self._get_weight_norm(weight, lora_weight.detach())
+        weight_norm = weight_norm.detach()
+        mag_norm_scale = (magnitude / weight_norm).view(1, -1)
+
+        dora_out = (
+            mag_norm_scale - 1
+        ) * base_out + mag_norm_scale * lora_out * self.scaling
+
+        return dora_out + base_out
+
+
+def _lora_a_init_params(x: nn.Linear) -> None:
+    """
+    Initialize LoRA A weight to Kaiming uniform.
+    """
+    nn.init.kaiming_uniform_(x.weight, a=math.sqrt(5))
+
+
+def _lora_b_init_params(x: nn.Linear) -> None:
+    """
+    Initialize LoRA B weight to zeros.
+    """
+    nn.init.zeros_(x.weight)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/peft/__init__.py marc/third_party/torchtune/torchtune/modules/peft/__init__.py
--- marc_original/third_party/torchtune/torchtune/modules/peft/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/peft/__init__.py	2025-02-20 17:49:30.662026074 -0500
@@ -0,0 +1,35 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._utils import (  # noqa
+    AdapterModule,
+    disable_adapter,
+    get_adapter_params,
+    get_lora_module_names,
+    get_merged_lora_ckpt,
+    load_dora_magnitudes,
+    LORA_ATTN_MODULES,
+    set_trainable_params,
+    validate_missing_and_unexpected_for_lora,
+    validate_state_dict_for_lora,
+)
+from .dora import DoRALinear
+from .lora import LoRALinear
+
+
+__all__ = [
+    "DoRALinear",
+    "LoRALinear",
+    "AdapterModule",
+    "get_adapter_params",
+    "set_trainable_params",
+    "validate_missing_and_unexpected_for_lora",
+    "validate_state_dict_for_lora",
+    "load_dora_magnitudes",
+    "disable_adapter",
+    "get_merged_lora_ckpt",
+    "get_lora_module_names",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/modules/peft/lora.py marc/third_party/torchtune/torchtune/modules/peft/lora.py
--- marc_original/third_party/torchtune/torchtune/modules/peft/lora.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/peft/lora.py	2025-02-20 17:49:30.670026087 -0500
@@ -0,0 +1,146 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import math
+from typing import List
+
+import torch
+import torch.nn.functional as F
+
+from torch import nn
+
+from torchao.dtypes.nf4tensor import linear_nf4, to_nf4
+from torchtune.modules.low_precision import _register_nf4_dispatch_ops  # noqa: F401
+from torchtune.modules.peft import AdapterModule
+
+
+class LoRALinear(nn.Module, AdapterModule):
+    """LoRA linear layer as introduced in `LoRA: Low-Rank Adaptation of Large Language Models <https://arxiv.org/abs/2106.09685>`_.
+
+    LoRA perturbs a given layer via a low-rank approximation where only
+    the rank decomposition matrices are trainable. In a linear layer instead of
+    :math:`x \\mapsto W_0x` a LoRALinear layer is defined as
+    :math:`x \\mapsto W_0x + (\\alpha / r)BAx`, where :math:`r` is the rank of
+    the matrices :math:`A` and :math:`B` and :math:`\\alpha` is a scaling factor.
+    As in the original implementation, we support dropout before multiplication
+    by the low-rank matrices.
+
+    Args:
+        in_dim (int): input dimension
+        out_dim (int): output dimension
+        rank (int): rank of the low-rank approximation
+        alpha (float): scaling factor for the low-rank approximation
+        dropout (float): dropout probability. Default: 0.0
+        use_bias (bool): whether to include bias in the original linear layer.
+            Default: False
+        quantize_base (bool): Whether to quantize base linear weight or not.
+            Default: False
+    """
+
+    def __init__(
+        self,
+        in_dim: int,
+        out_dim: int,
+        rank: int,
+        alpha: float,
+        dropout: float = 0.0,
+        use_bias: bool = False,
+        quantize_base: bool = False,
+    ):
+        super().__init__()
+        self.in_dim = in_dim
+        self.rank = rank
+        self.alpha = alpha
+        self.out_dim = out_dim
+        self.use_bias = use_bias
+        self._quantize_base = quantize_base
+        weight, bias = self._create_weight_and_bias()
+        # 'self.disabled' is a flag showing whether to turn off LoRA adapters,
+        # this can be used in DPO for treating the lora adapters as the policy model
+        # and disabling it to treat the base model as the reference model
+        self.disabled = False
+        self.register_parameter("weight", nn.Parameter(weight))
+        self.register_parameter(
+            "bias", nn.Parameter(bias) if bias is not None else None
+        )
+        self.dropout = nn.Dropout(p=dropout) if dropout > 0.0 else nn.Identity()
+        self.lora_a = nn.Linear(in_features=in_dim, out_features=rank, bias=False)
+        self.lora_b = nn.Linear(in_features=rank, out_features=out_dim, bias=False)
+        self.merged = False
+        # Note: FSDP's meta device initialization contract assumes that a module's
+        # reset_parameters method only initializes its own parameters (i.e. no child
+        # params are initialized, as is done in initialize_parameters below).
+        # For that reason, we patch reset_parameters directly on lora_a and lora_b submodules
+        # when using meta device. This is done in
+        # torchtune.training.prepare_model_for_fsdp_with_meta_device.
+        # See this issue for more details: https://github.com/pytorch/pytorch/issues/104187.
+        # Without meta device, we only need the following:
+        self.initialize_parameters()
+
+    def initialize_parameters(self):
+        # Initialize as in
+        # https://github.com/microsoft/LoRA/blob/4c0333854cb905966f8cc4e9a74068c1e507c7b7/loralib/layers.py#L119
+        _lora_a_init_params(self.lora_a)
+        _lora_b_init_params(self.lora_b)
+
+    def _create_weight_and_bias(self):
+        """
+        Creates a linear weight and bias tensor, using NF4 dtype if we're quantizing
+        (indicated via quantize_base=True).
+        """
+        in_dim, out_dim, use_bias = self.in_dim, self.out_dim, self.use_bias
+        linear = nn.Linear(in_features=in_dim, out_features=out_dim, bias=use_bias)
+        weight = linear.weight if not self._quantize_base else to_nf4(linear.weight)
+        bias = None
+        if self.use_bias:
+            if self._quantize_base:
+                raise NotImplementedError(
+                    "Quantized LoRALinear does not support bias at the moment."
+                )
+            bias = linear.bias
+        return weight, bias
+
+    def adapter_params(self) -> List[str]:
+        """
+        Return lora_a.weight and lora_b.weight as adapter params.
+        If bias is enabled, also return lora_a.bias and lora_b.bias.
+        """
+        # NOTE: this function has to be updated if the names of "lora_a" and "lora_b"
+        # in this module change.
+        adapter_params = ["lora_a.weight", "lora_b.weight"]
+        return adapter_params
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape ``(..., in_dim)``
+
+        Returns:
+            torch.Tensor: output tensor with shape ``(..., out_dim)``
+
+        """
+        if self._quantize_base:
+            out = linear_nf4(input=x, weight=self.weight)
+        else:
+            out = F.linear(x, self.weight, self.bias)
+        if self.disabled:
+            return out
+        lora_out = self.lora_a(self.dropout(x))
+        lora_out = (self.alpha / self.rank) * self.lora_b(lora_out)
+        return out + lora_out
+
+
+def _lora_a_init_params(x: nn.Linear) -> None:
+    """
+    Initialize LoRA A weight to Kaiming uniform.
+    """
+    nn.init.kaiming_uniform_(x.weight, a=math.sqrt(5))
+
+
+def _lora_b_init_params(x: nn.Linear) -> None:
+    """
+    Initialize LoRA B weight to zeros.
+    """
+    nn.init.zeros_(x.weight)
Binary files marc_original/third_party/torchtune/torchtune/modules/peft/__pycache__/dora.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/peft/__pycache__/dora.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/peft/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/peft/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/peft/__pycache__/lora.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/peft/__pycache__/lora.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/peft/__pycache__/_utils.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/peft/__pycache__/_utils.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/modules/peft/_utils.py marc/third_party/torchtune/torchtune/modules/peft/_utils.py
--- marc_original/third_party/torchtune/torchtune/modules/peft/_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/peft/_utils.py	2025-02-20 17:49:30.666026080 -0500
@@ -0,0 +1,387 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import contextlib
+from typing import Any, Dict, Generator, List, Literal, Optional, Protocol, Set
+
+import torch
+from torch import nn
+
+# Modules from MultiHeadAttention that LoRA can be applied to
+LORA_ATTN_MODULES = Literal["q_proj", "k_proj", "v_proj", "output_proj"]
+
+
+class AdapterModule(Protocol):
+    """
+    Interface for an ``nn.Module`` containing adapter weights.
+    Note that an adapter module does not have to explicitly implement this protocol,
+    but it must define the ``adapter_params(self)`` method.
+    """
+
+    def adapter_params(self) -> List[str]:
+        """
+        Return a list of strings corresponding to the names of the ``nn.Parameter`` s in
+        the model coming from the adapter.
+        E.g. if an nn.Module has adapter ``self.proj = nn.Linear(in_dim, out_dim)``,
+        then adapter_params should return ``['proj.weight', 'proj.bias']``.
+
+        See LoRALinear's :func:`~torchtune.modules.peft.LoRALinear.adapter_params` for an example.
+        """
+        pass
+
+
+def get_adapter_params(model: nn.Module) -> Dict[str, nn.Parameter]:
+    """
+    Return the subset of parameters from a model that correspond to an adapter.
+    Assumes that any adapter class has defined the
+    :func:`~torchtune.modules.peft.AdapterModule.adapter_params` method.
+
+    Args:
+        model (nn.Module): Instance of model class containing some adapter params.
+
+    Returns:
+        Dict[str, nn.Parameter]: the subset of model's state dict containing
+        only adapter parameters.
+
+    """
+    adapter_params = {}
+    for k, v in model.named_modules():
+        if hasattr(v, "adapter_params") and callable(v.adapter_params):
+            current_adapter_params = v.adapter_params()
+            for n, p in v.named_parameters(recurse=True):
+                if n in current_adapter_params:
+                    full_key = f"{k}.{n}" if k else n
+                    adapter_params.update({full_key: p})
+                    current_adapter_params.remove(n)
+            assert (
+                current_adapter_params == []
+            ), f"Adapter params {current_adapter_params} not converted"
+    return adapter_params
+
+
+def set_trainable_params(model: nn.Module, adapter_params: Dict[str, Any]) -> None:
+    """
+    Set trainable parameters for an nn.Module based on a state dict of adapter parameters.
+
+    Args:
+        model (nn.Module): Instance of model class containing some adapter params.
+        adapter_params (Dict[str, Any]): State dict mapping adapter key names to their
+            respective nn.Parameters (i.e. outputs of :func:`~torchtune.modules.peft.get_adapter_params`.)
+
+    Returns:
+        None
+    """
+    for k, v in model.named_parameters():
+        v.requires_grad_(k in adapter_params)
+
+
+def get_lora_module_names(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool,
+    apply_lora_to_output: bool,
+) -> List[str]:
+    """
+    Return a list of the names of modules in the model that have LoRA applied. Note that
+    the names here are local to their modules and not the fully qualified names from the
+    model state dict.
+
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether LoRA is applied to each MLP linear.
+        apply_lora_to_output (bool): whether LoRA is applied to the final output projection.
+
+    Returns:
+        List[str]: list of module names in the model that have LoRA applied.
+    """
+    lora_module_keys = lora_attn_modules
+    if apply_lora_to_mlp:
+        lora_module_keys = lora_module_keys + ["w1", "w2", "w3"]
+    if apply_lora_to_output:
+        lora_module_keys.append("output")
+    return lora_module_keys
+
+
+def validate_state_dict_for_lora(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool,
+    apply_lora_to_output: bool,
+    full_model_state_dict_keys: List[str],
+    lora_state_dict_keys: Optional[List[str]] = None,
+    base_model_state_dict_keys: Optional[List[str]] = None,
+) -> None:
+    """
+    Validate that the state dict keys for a LoRA model are as expected.
+
+    (1) If lora_state_dict_keys are passed, this function will confirm that they match exactly the
+        LoRA param names from the full model (as determined by lora_modules).
+    (2) If base_model_state_dict_keys are passed, this function will confirm that they are exactly the
+        complement of the LoRA param names from the full model.
+    (3) If both lora_state_dict_keys and base_model_state_dict_keys are passed, this function will
+        confirm that the full model's params are exactly their disjoint union.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether LoRA is applied to each MLP linear.
+        apply_lora_to_output (bool): whether LoRA is applied to the final output projection.
+        full_model_state_dict_keys (List[str]): List of keys in the full model state dict.
+        lora_state_dict_keys (Optional[List[str]]): List of keys in the LoRA state dict.
+            If none, LoRA state dict keys will not be validated.
+        base_model_state_dict_keys (Optional[List[str]]): List of keys in the base model state dict.
+            If none, base model keys will not be validated.
+
+    Returns:
+        None
+
+    Raises:
+        AssertionError: If base model state dict is missing any non-LoRA params from the full model.
+        AssertionError: If LoRA state dict is missing any LoRA params from the full model.
+        AssertionError: If base model state dict has any LoRA params.
+        AssertionError: If LoRA state dict has any non-LoRA params.
+        AssertionError: If base model and LoRA state dicts have overlapping keys.
+        AssertionError: If full model state dict is missing keys from either base model or LoRA state dict.
+
+    """
+    lora_modules = get_lora_module_names(
+        lora_attn_modules, apply_lora_to_mlp, apply_lora_to_output
+    )
+    is_lora_param = lambda x: any(
+        [
+            ".".join([k, "lora"]) in x or ".".join([k, "magnitude"]) in x
+            for k in lora_modules
+        ]
+    )
+    for k in full_model_state_dict_keys:
+        if not is_lora_param(k):
+            if base_model_state_dict_keys is not None:
+                if k not in base_model_state_dict_keys:
+                    raise AssertionError(
+                        f"Missing non-LoRA key {k} from base model state dict"
+                    )
+            if lora_state_dict_keys is not None:
+                if k in lora_state_dict_keys:
+                    raise AssertionError(f"Non-LoRA key {k} found in LoRA state dict")
+        else:
+            if base_model_state_dict_keys is not None:
+                if k in base_model_state_dict_keys:
+                    raise AssertionError(f"LoRA key {k} found in base model state dict")
+            if lora_state_dict_keys is not None:
+                if k not in lora_state_dict_keys:
+                    raise AssertionError(f"Missing LoRA key {k} From LoRA state dict")
+
+    # Full model is disjoint union of base model and LoRA weights
+    if lora_state_dict_keys is not None and base_model_state_dict_keys is not None:
+        combined_state_dict_keys = set(lora_state_dict_keys).union(
+            base_model_state_dict_keys
+        )
+        shared_state_dict_keys = set(lora_state_dict_keys).intersection(
+            base_model_state_dict_keys
+        )
+        assert (
+            shared_state_dict_keys == set()
+        ), "Base model and LoRA state dict have overlapping keys"
+        assert combined_state_dict_keys == set(
+            full_model_state_dict_keys
+        ), "Extra keys not present in full model"
+
+
+def _get_lora_modules(state_dict: Dict[str, Any]) -> Set[str]:
+    """
+    Get the keys from a state dict that correspond to LoRALinear modules.
+
+    For example, if state_dict is the state dict of model and model.x.y.z is a
+    LoRALinear, this method will return "model.x.y.z", not
+    "model.x.y.z.lora_a.weight" or "model.x.y.z.lora_b.weight".
+
+    Args:
+        state_dict (Dict[str, Any]): State dict from a model.
+
+    Returns:
+        Set[str]: Set of keys in the state dict that correspond to LoRA modules.
+    """
+    lora_keys = [k for k in state_dict.keys() if "lora" in k or "magnitude" in k]
+    return set(
+        [
+            k.replace(".lora_a.weight", "")
+            .replace(".lora_b.weight", "")
+            .replace(".magnitude", "")
+            for k in lora_keys
+        ]
+    )
+
+
+@torch.no_grad
+def get_merged_lora_ckpt(
+    state_dict: Dict[str, Any],
+    rank: int,
+    alpha: float,
+) -> Dict[str, Any]:
+    """
+    Merge LoRA weights into the base model format for efficient inference.
+    NOTE: This function modifies state_dict inplace. If you do not want to do that,
+    make a copy prior to calling this function.
+
+    For every LoRA module in the state dict, this function will convert its
+    base weight then delete the LoRA-specific parameters.
+
+    Args:
+        state_dict (Dict[str, Any]): State dict from a model.
+        rank (int): The rank of LoRA matrices.
+        alpha (float): The alpha value used for scaling LoRA decompositions.
+
+    Returns:
+        Dict[str, Any]: The merged state dict.
+    """
+    lora_modules = _get_lora_modules(state_dict)
+    for module in lora_modules:
+        lora_a_weight = state_dict[f"{module}.lora_a.weight"]
+        lora_b_weight = state_dict[f"{module}.lora_b.weight"]
+        lora_magnitude = state_dict.get(f"{module}.magnitude", None)
+
+        # If magnitude is present, calculate merged DoRA weight
+        if lora_magnitude is not None:
+            base_weight = state_dict[f"{module}.weight"].to(lora_a_weight.dtype)
+
+            lora_weight = (alpha / rank) * lora_b_weight @ lora_a_weight
+            merged_weight = base_weight + lora_weight
+            weight_norm = torch.linalg.norm(base_weight + lora_weight, dim=1)
+            mag_norm_scale = (lora_magnitude / weight_norm).view(-1, 1)
+            merged_weight *= mag_norm_scale
+            state_dict[f"{module}.weight"] = merged_weight
+            del state_dict[f"{module}.magnitude"]
+
+        # Otherwise it is just vanilla LoRA
+        else:
+            state_dict[f"{module}.weight"] += (
+                (alpha / rank) * lora_b_weight @ lora_a_weight
+            )
+
+        del state_dict[f"{module}.lora_a.weight"]
+        del state_dict[f"{module}.lora_b.weight"]
+
+    return state_dict
+
+
+@contextlib.contextmanager
+def disable_adapter(model: nn.Module) -> Generator[None, None, None]:
+    """
+    Temporarily disable the adapters in a model. For example,
+    this can be used in DPO for treating the LoRA adapters as the policy model
+    and disabling it to treat the base model as the reference model.
+
+    This context manager goes through all modules in the provided neural network model,
+    and if a module has an ``adapter_params`` attribute that is callable and a ``disabled`` attribute,
+    it sets ``disabled`` to True. Then, the control is given back to caller. When exiting the context manager,
+    it sets ``disabled`` back to False for all modules that were temporarily disabled.
+
+    Args:
+        model (nn.Module): The model whose adapters are to be temporarily disabled.
+    Yields:
+        None: This function yields control back to the caller, with the adapters disabled.
+    Example:
+        >>> with disable_adapter(model):
+        ...     # Perform operations with adapters disabled
+        ...     pass
+
+    """
+    for _, module in model.named_modules():
+        if (
+            hasattr(module, "adapter_params")
+            and callable(module.adapter_params)
+            and hasattr(module, "disabled")
+        ):
+            module.disabled = True
+    try:
+        yield
+    finally:
+        for _, module in model.named_modules():
+            if (
+                hasattr(module, "adapter_params")
+                and callable(module.adapter_params)
+                and hasattr(module, "disabled")
+            ):
+                module.disabled = False
+
+
+def validate_missing_and_unexpected_for_lora(
+    lora_attn_modules: List[LORA_ATTN_MODULES],
+    apply_lora_to_mlp: bool,
+    apply_lora_to_output: bool,
+    base_missing: Optional[List[str]] = None,
+    base_unexpected: Optional[List[str]] = None,
+    lora_missing: Optional[List[str]] = None,
+    lora_unexpected: Optional[List[str]] = None,
+) -> None:
+    """
+    A more memory-efficient way to validate that LoRA state dict loading was done properly.
+
+    Similar to :func:`validate_state_dict_for_lora`, this function uses a model's LoRA config to
+    check that LoRA and/or base model weights are loaded into the full model correctly.
+    Unlike that function, this method relies only on the values of missing and unexpected
+    as returned by the load_state_dict API with strict=False. This allows us to do the
+    validation without any additional calls to .state_dict(), which use additional memory.
+
+    Args:
+        lora_attn_modules (List[LORA_ATTN_MODULES]): list of which linear layers
+            LoRA should be applied to in each self-attention block. Options are
+            ``{"q_proj", "k_proj", "v_proj", "output_proj"}``.
+        apply_lora_to_mlp (bool): whether LoRA is applied to each MLP linear.
+        apply_lora_to_output (bool): whether LoRA is applied to the final output projection.
+        base_missing (Optional[List[str]]): List of missing keys when loading base model weights.
+            Default: None
+        base_unexpected (Optional[List[str]]): List of unexpected keys when loading base model weights.
+            Default: None
+        lora_missing (Optional[List[str]]): List of missing keys when loading LoRA weights.
+            Default: None
+        lora_unexpected (Optional[List[str]]): List of unexpected keys when loading LoRA weights.
+            Default: None
+
+    Returns:
+        None
+
+    Raises:
+        AssertionError: if base_missing contains any base model keys.
+        AssertionError: if base_unexpected is nonempty.
+        AssertionError: if lora_missing contains any LoRA keys.
+        AssertionError: if lora_unexpected is nonempty.
+    """
+    lora_modules = get_lora_module_names(
+        lora_attn_modules, apply_lora_to_mlp, apply_lora_to_output
+    )
+    is_lora_param = lambda x: any(
+        [
+            ".".join([k, "lora"]) in x or ".".join([k, "magnitude"]) in x
+            for k in lora_modules
+        ]
+    )
+
+    if base_missing:
+        for k in base_missing:
+            if not is_lora_param(k):
+                raise AssertionError(f"Missing non-LoRA key {k} from base model dict")
+    if base_unexpected:
+        raise AssertionError("Unexpected key loading base model")
+    if lora_missing:
+        for k in lora_missing:
+            if is_lora_param(k):
+                raise AssertionError(f"Missing LoRA key {k} from adapter state dict")
+    if lora_unexpected:
+        raise AssertionError("Unexpected key loading adapter")
+
+
+def load_dora_magnitudes(model: nn.Module) -> None:
+    """
+    For DoRA magnitude we use setattr to move from meta device
+    """
+    dora_parents = {
+        n: p for n, p in model.named_modules() if hasattr(p, "adapter_params")
+    }
+    sd = {f"{n}.magnitude": p.magnitude for n, p in dora_parents.items()}
+    model.load_state_dict(sd, strict=False, assign=True)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/position_embeddings.py marc/third_party/torchtune/torchtune/modules/position_embeddings.py
--- marc_original/third_party/torchtune/torchtune/modules/position_embeddings.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/position_embeddings.py	2025-02-20 17:49:30.674026093 -0500
@@ -0,0 +1,127 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional
+
+import torch
+from torch import nn
+
+
+class RotaryPositionalEmbeddings(nn.Module):
+    """
+    This class implements Rotary Positional Embeddings (RoPE)
+    proposed in https://arxiv.org/abs/2104.09864.
+
+    Reference implementation (used for correctness verfication)
+    can be found here:
+    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80
+
+    In this implementation we cache the embeddings for each position upto
+    ``max_seq_len`` by computing this during init.
+
+    Args:
+        dim (int): Embedding dimension. This is usually set to the dim of each
+            head in the attention module computed as ``embed_dim // num_heads``
+        max_seq_len (int): Maximum expected sequence length for the
+            model, if exceeded the cached freqs will be recomputed
+        base (int): The base for the geometric progression used to compute
+            the rotation angles
+    """
+
+    def __init__(
+        self,
+        dim: int,
+        max_seq_len: int = 4096,
+        base: int = 10_000,
+    ) -> None:
+        super().__init__()
+        self.dim = dim
+        self.base = base
+        self.max_seq_len = max_seq_len
+        self.rope_init()
+
+    # TODO: delete this once all our recipes are moved off of FSDP1 since we
+    # no longer need to explicitly name our param init method reset_parameters
+    def reset_parameters(self):
+        self.rope_init()
+
+    def rope_init(self):
+        theta = 1.0 / (
+            self.base
+            ** (torch.arange(0, self.dim, 2)[: (self.dim // 2)].float() / self.dim)
+        )
+        self.register_buffer("theta", theta, persistent=False)
+        self.build_rope_cache(self.max_seq_len)
+
+    def build_rope_cache(self, max_seq_len: int = 4096) -> None:
+        # Create position indexes `[0, 1, ..., max_seq_len - 1]`
+        seq_idx = torch.arange(
+            max_seq_len, dtype=self.theta.dtype, device=self.theta.device
+        )
+
+        # Outer product of theta and position index; output tensor has
+        # a shape of [max_seq_len, dim // 2]
+        idx_theta = torch.einsum("i, j -> ij", seq_idx, self.theta).float()
+
+        # cache includes both the cos and sin components and so the output shape is
+        # [max_seq_len, dim // 2, 2]
+        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)
+        self.register_buffer("cache", cache, persistent=False)
+
+    def forward(
+        self, x: torch.Tensor, *, input_pos: Optional[torch.Tensor] = None
+    ) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape
+                ``[b, s, n_h, h_d]``
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape [b, s].
+                During inference, this indicates the position of the current token.
+                If none, assume the index of the token is its position id. Default is None.
+
+        Returns:
+            torch.Tensor: output tensor with shape ``[b, s, n_h, h_d]``
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - s: sequence length
+            - n_h: num heads
+            - h_d: head dim
+        """
+        # input tensor has shape [b, s, n_h, h_d]
+        seq_len = x.size(1)
+
+        # extract the values based on whether input_pos is set or not
+        rope_cache = (
+            self.cache[:seq_len] if input_pos is None else self.cache[input_pos]
+        )
+
+        # reshape input; the last dimension is used for computing the output.
+        # Cast to float to match the reference implementation
+        # tensor has shape [b, s, n_h, h_d // 2, 2]
+        xshaped = x.float().reshape(*x.shape[:-1], -1, 2)
+
+        # reshape the cache for broadcasting
+        # tensor has shape [b, s, 1, h_d // 2, 2] if packed samples,
+        # otherwise has shape [1, s, 1, h_d // 2, 2]
+        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2)
+
+        # tensor has shape [b, s, n_h, h_d // 2, 2]
+        x_out = torch.stack(
+            [
+                xshaped[..., 0] * rope_cache[..., 0]
+                - xshaped[..., 1] * rope_cache[..., 1],
+                xshaped[..., 1] * rope_cache[..., 0]
+                + xshaped[..., 0] * rope_cache[..., 1],
+            ],
+            -1,
+        )
+
+        # tensor has shape [b, s, n_h, h_d]
+        x_out = x_out.flatten(3)
+        return x_out.type_as(x)
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/attention.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/attention.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/attention_utils.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/attention_utils.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/common_utils.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/common_utils.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/feed_forward.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/feed_forward.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/kv_cache.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/kv_cache.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/layer_norm.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/layer_norm.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/lr_schedulers.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/lr_schedulers.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/position_embeddings.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/position_embeddings.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/rms_norm.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/rms_norm.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/tanh_gate.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/tanh_gate.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/tied_linear.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/tied_linear.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/transformer.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/transformer.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/__pycache__/vision_transformer.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/__pycache__/vision_transformer.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/modules/rms_norm.py marc/third_party/torchtune/torchtune/modules/rms_norm.py
--- marc_original/third_party/torchtune/torchtune/modules/rms_norm.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/rms_norm.py	2025-02-20 17:49:30.678026100 -0500
@@ -0,0 +1,44 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+
+from torch import nn
+
+
+class RMSNorm(nn.Module):
+    """
+    Implements Root Mean Square Normalization introduced in
+    https://arxiv.org/abs/1910.07467.
+
+    Reference implementation (used for correctness verification)
+    can be found here:
+    https://github.com/facebookresearch/llama/blob/main/llama/model.py
+
+    Args:
+        dim (int): embedding size
+        eps (float): small value to avoid division by zero. Default: 1e-6
+    """
+
+    def __init__(self, dim: int, eps: float = 1e-6) -> None:
+        super().__init__()
+        self.eps = eps
+        self.scale = nn.Parameter(torch.ones(dim))
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor to normalize
+
+        Returns:
+            torch.Tensor: The normalized and scaled tensor having the same shape as ``x``.
+        """
+        # computation is in fp32
+        x_fp32 = x.float()
+        x_normed = (
+            x_fp32 * torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + self.eps)
+        ).type_as(x)
+        return x_normed * self.scale
diff -ruN marc_original/third_party/torchtune/torchtune/modules/tanh_gate.py marc/third_party/torchtune/torchtune/modules/tanh_gate.py
--- marc_original/third_party/torchtune/torchtune/modules/tanh_gate.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/tanh_gate.py	2025-02-20 17:49:30.682026107 -0500
@@ -0,0 +1,27 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+
+from torch import nn
+
+
+class TanhGate(nn.Module):
+    """Implements a basic learnable gate to scale layer outputs"""
+
+    def __init__(self) -> None:
+        super().__init__()
+        self.scale = nn.Parameter(torch.zeros(1))
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor to gate
+
+        Returns:
+            torch.Tensor: The output tensor after gating. Has the same shape as ``x``.
+        """
+        return x * self.scale.tanh()
diff -ruN marc_original/third_party/torchtune/torchtune/modules/tied_linear.py marc/third_party/torchtune/torchtune/modules/tied_linear.py
--- marc_original/third_party/torchtune/torchtune/modules/tied_linear.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/tied_linear.py	2025-02-20 17:49:30.690026119 -0500
@@ -0,0 +1,43 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+class TiedLinear:
+    """
+    A tied linear layer, without bias, that shares the same weight as another linear layer.
+    This is useful for models that use tied weights, such as :func:`~torchtune.models.qwen2_0_5b`,
+    :func:`~torchtune.models.qwen2_1_5b` and all of the :func:`~torchtune.models.gemma` models.
+    It requires as input an nn.Module, instead of the weight of the module, so it
+    can work with FSDP. Otherwise, the memory reference will be lost after FSDP is applied.
+
+    Args:
+        tied_module (nn.Module): The module whose weight is shared. Only
+            the weight is used. The bias is ignored.
+    Raises:
+        AttributeError: If the provided module does not have an attribute 'weight'.
+    """
+
+    def __init__(self, tied_module: nn.Module):
+        self.tied_module = tied_module
+        if not hasattr(tied_module, "weight"):
+            raise AttributeError(
+                "Provided module does not have attribute 'weight'. Please check your tied_module."
+            )
+
+    def __call__(self, x: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): Input tensor. Should have shape ``(..., in_dim)``, where ``in_dim``
+                is the input dimension of the tied module.
+        Returns:
+            torch.Tensor: The output tensor, having shape ``(..., out_dim)``, where ``out_dim`` is \
+                the output dimension of the tied module.
+        """
+        return F.linear(x, self.tied_module.weight)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/tokenizers/__init__.py marc/third_party/torchtune/torchtune/modules/tokenizers/__init__.py
--- marc_original/third_party/torchtune/torchtune/modules/tokenizers/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/tokenizers/__init__.py	2025-02-20 17:49:30.694026126 -0500
@@ -0,0 +1,23 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._sentencepiece import SentencePieceBaseTokenizer
+from ._tiktoken import TikTokenBaseTokenizer
+from ._utils import (
+    BaseTokenizer,
+    ModelTokenizer,
+    parse_hf_tokenizer_json,
+    tokenize_messages_no_special_tokens,
+)
+
+__all__ = [
+    "SentencePieceBaseTokenizer",
+    "TikTokenBaseTokenizer",
+    "ModelTokenizer",
+    "BaseTokenizer",
+    "tokenize_messages_no_special_tokens",
+    "parse_hf_tokenizer_json",
+]
Binary files marc_original/third_party/torchtune/torchtune/modules/tokenizers/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/tokenizers/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/tokenizers/__pycache__/_sentencepiece.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/tokenizers/__pycache__/_sentencepiece.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/tokenizers/__pycache__/_tiktoken.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/tokenizers/__pycache__/_tiktoken.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/tokenizers/__pycache__/_utils.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/tokenizers/__pycache__/_utils.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/modules/tokenizers/_sentencepiece.py marc/third_party/torchtune/torchtune/modules/tokenizers/_sentencepiece.py
--- marc_original/third_party/torchtune/torchtune/modules/tokenizers/_sentencepiece.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/tokenizers/_sentencepiece.py	2025-02-20 17:49:30.698026133 -0500
@@ -0,0 +1,111 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional
+
+from sentencepiece import SentencePieceProcessor
+from torchtune.modules.tokenizers._utils import BaseTokenizer
+
+WHITESPACE_CHARS = [" ", "\n", "\t", "\r", "\v"]
+
+
+class SentencePieceBaseTokenizer(BaseTokenizer):
+    """
+    A light-weight wrapper around SentencePieceProcessor that additionally handles
+    trimming leading whitespaces.
+
+    Args:
+        path (str): Path to pretrained tokenizer file.
+
+    Examples:
+        >>> tokenizer = SentencePieceBaseTokenizer("/path/to/spm_model")
+        >>> tokenized_text = tokenizer.encode("Hello world!", add_bos=True, add_eos=True)
+        >>> print(tokenized_text)
+        [1, 31587, 29644, 102, 2]
+    """
+
+    def __init__(
+        self,
+        path: str,
+    ):
+        spm_model = SentencePieceProcessor()
+        spm_model.load(path)
+        self.spm_model = spm_model
+        self.vocab_size = spm_model.vocab_size()
+        self.bos_id = spm_model.bos_id()
+        self.eos_id = spm_model.eos_id()
+        self.pad_id = spm_model.pad_id()
+
+        # If the tokenizer does not encode whitespace,
+        # then we can more easily split strings
+        # on whitespace characters and encode them separately.
+        self.encodes_whitespace = any(
+            [self.spm_model.encode(c) for c in WHITESPACE_CHARS]
+        )
+
+    def encode(
+        self,
+        text: str,
+        add_bos: bool = True,
+        add_eos: bool = True,
+        trim_leading_whitespace: bool = False,
+        prefix: Optional[str] = None,
+    ) -> List[int]:
+        """Encode text into token IDs.
+
+        Args:
+            text (str): The input text to be encoded, unbatched.
+            add_bos (bool): Whether to prepend BOS to the input, defaults to True.
+            add_eos (bool): Whether to append EOS to the input, defaults to True.
+            trim_leading_whitespace (bool): Whether to trim leading whitespace from
+                underlying sentencepiece tokenization. Sentencepiece normally prepends
+                whitespace to any tokenized text, which can cause differences where
+                ``encode(s1) + encode(s2) != encode(s1 + s2)`` due to leading whitespace
+                added to s2. This will only trim leading whitespace if the underlying
+                ``SentencePieceProcessor`` encodes whitespace. Default: False
+            prefix (Optional[str]): Optional string to encode for trimming leading
+                whitespaces. Used only if trim_leading_whitespace=True. Default: None
+
+        Returns:
+            List[int]: The encoded token IDs.
+        """
+        # We typically trim leading whitespace on the next message when
+        # it is a continuation of the turn (i.e. not the first message)
+        # or the previous message did not end with a space. This is handled
+        # by the caller of this method. We only need to trim leading whitespace
+        # if the underlying SentencePieceProcessor encodes whitespace.
+        if trim_leading_whitespace and self.encodes_whitespace:
+            # Can define our own custom prefix depending on vocab if needed
+            if not hasattr(self, "prefix"):
+                self.prefix = prefix or "\n"
+                self.encoded_prefix = self.spm_model.encode(
+                    self.prefix, add_bos=False, add_eos=False
+                )
+            start_idx = len(self.encoded_prefix) + int(add_bos)
+            return self.spm_model.encode(
+                self.prefix + text,
+                add_bos=add_bos,
+                add_eos=add_eos,
+                out_type=int,
+            )[start_idx:]
+        else:
+            return self.spm_model.encode(
+                text,
+                add_bos=add_bos,
+                add_eos=add_eos,
+                out_type=int,
+            )
+
+    def decode(self, ids: List[int]) -> str:
+        """Decode token IDs to strings.
+
+        Args:
+            ids (List[int]): The input token IDs to be decoded.
+
+        Returns:
+            str: The decoded text.
+        """
+        return self.spm_model.decode(ids)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/tokenizers/_tiktoken.py marc/third_party/torchtune/torchtune/modules/tokenizers/_tiktoken.py
--- marc_original/third_party/torchtune/torchtune/modules/tokenizers/_tiktoken.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/tokenizers/_tiktoken.py	2025-02-20 17:49:30.702026139 -0500
@@ -0,0 +1,160 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Dict, Iterator, List
+
+from tiktoken import Encoding
+from tiktoken.load import load_tiktoken_bpe
+from torchtune.modules.tokenizers._utils import BaseTokenizer
+
+# Constants controlling encode logic
+MAX_ENCODE_CHARS = 400_000
+MAX_NO_WHITESPACE_CHARS = 25_000
+
+
+class TikTokenBaseTokenizer(BaseTokenizer):
+    """
+    A lightweight wrapper around tiktoken Encoding. This class additionally handles
+    breaking up the input text into substrings of a max length and splitting up long
+    repetitions to improve encode speed.
+
+    Args:
+        path (str): Path to pretrained tokenizer checkpoint file.
+        name (str): Name of the tokenizer (used by tiktoken for identification).
+        pattern (str): Regex pattern used to split input text into chunks before passing
+            to byte-pair encoding.
+        bos_id (int): beginning-of-sequence token id. This can be present or absent in ``special_tokens``.
+        eos_id (int): end-of-sequence token id. This can be present or absent in ``special_tokens``.
+        special_tokens (Dict[str, int]): Mapping of special tokens to their ids.
+
+    Examples:
+        >>> tokenizer = TikTokenBaseTokenizer("/path/to/tt_model")
+        >>> tokenized_text = tokenizer.encode("Hello world!", add_bos=True, add_eos=True)
+        >>> print(tokenized_text)
+        [1, 31587, 29644, 102, 2]
+    """
+
+    def __init__(
+        self,
+        path: str,
+        name: str,
+        pattern: str,
+        bos_id: int,
+        eos_id: int,
+        special_tokens: Dict[str, int],
+    ):
+        mergeable_ranks = load_tiktoken_bpe(path)
+        self.tt_model = Encoding(
+            name=name,
+            pat_str=pattern,
+            mergeable_ranks=mergeable_ranks,
+            special_tokens=special_tokens,
+        )
+        # Vocab size without special tokens
+        self.base_vocab_size = len(mergeable_ranks)
+        # Vocab size with special tokens
+        self.vocab_size = self.tt_model.n_vocab
+        self.bos_id = bos_id
+        self.eos_id = eos_id
+
+    def _split_long_repetitions(
+        self, s: str, max_consecutive_slice_len: int
+    ) -> Iterator[str]:
+        """
+        Split the string `s` so that each substring contains no more than `max_consecutive_slice_len`
+        consecutive whitespaces or consecutive non-whitespaces
+        """
+        current_slice_len = 0
+        current_slice_is_space = s[0].isspace() if len(s) > 0 else False
+        slice_start = 0
+
+        for i in range(len(s)):
+            is_now_space = s[i].isspace()
+
+            if current_slice_is_space ^ is_now_space:
+                current_slice_len = 1
+                current_slice_is_space = is_now_space
+            else:
+                current_slice_len += 1
+                if current_slice_len > max_consecutive_slice_len:
+                    yield s[slice_start:i]
+                    slice_start = i
+                    current_slice_len = 1
+        yield s[slice_start:]
+
+    def encode(
+        self,
+        text: str,
+        add_bos: bool = True,
+        add_eos: bool = True,
+    ) -> List[int]:
+        """
+        Encode a string into a list of token ids. Assumes that the string
+        contains no special tokens.
+
+        Args:
+            text (str): The string to encode.
+            add_bos (bool): Whether to add the tokenizer's bos_id to the encoded string.
+                Default True.
+            add_eos (bool): Whether to add the tokenizer's eos_id to the encoded string.
+                Default True.
+
+        Returns:
+            List[int]: The list of token ids.
+        """
+        substrs: List[str] = []
+        tokens = []
+        if not text:
+            return []
+        for i in range(0, len(text), MAX_ENCODE_CHARS):
+            substr = text[i : i + MAX_ENCODE_CHARS]
+            # See https://github.com/openai/tiktoken/issues/195
+            sliced_substr = self._split_long_repetitions(
+                substr, MAX_NO_WHITESPACE_CHARS
+            )
+            substrs.extend(sliced_substr)
+        for substr in substrs:
+            # allowed_special and disallowed_special are used by tiktoken to define
+            # how special tokens are encoded. Our setting here is to encode any
+            # special token as regular text and prevent tiktoken from raising errors.
+            # This means we should only call encode on strings not containing special tokens.
+            tokens.extend(
+                self.tt_model.encode(
+                    substr,
+                    allowed_special=set(),
+                    disallowed_special=(),
+                )
+            )
+        if add_bos:
+            tokens = [self.bos_id] + tokens
+        if add_eos:
+            tokens = tokens + [self.eos_id]
+        return tokens
+
+    def decode(
+        self,
+        token_ids: List[int],
+        truncate_at_eos: bool = True,
+    ) -> str:
+        """
+        Decode a list of token ids into a string.
+
+        Args:
+            token_ids (List[int]): The list of token ids.
+            truncate_at_eos (bool): Whether to truncate the string at the end of
+                sequence token. Default is True.
+
+        Returns:
+            str: The decoded string.
+        """
+        if truncate_at_eos:
+            try:
+                k = token_ids.index(self.eos_id)
+            except ValueError:
+                k = None
+            if k:
+                token_ids = token_ids[:k]
+        return self.tt_model.decode(token_ids)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/tokenizers/_utils.py marc/third_party/torchtune/torchtune/modules/tokenizers/_utils.py
--- marc_original/third_party/torchtune/torchtune/modules/tokenizers/_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/tokenizers/_utils.py	2025-02-20 17:49:30.706026146 -0500
@@ -0,0 +1,197 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import json
+from typing import Any, Dict, List, Optional, Protocol, Tuple
+
+from torchtune.data._messages import Message
+from torchtune.data._utils import truncate
+
+
+class BaseTokenizer(Protocol):
+    """
+    Abstract token encoding model that implements ``encode`` and ``decode`` methods.
+    See :class:`~torchtune.modules.tokenizers.SentencePieceBaseTokenizer` and
+    :class:`~torchtune.modules.tokenizers.TikTokenBaseTokenizer` for example implementations of this protocol.
+    """
+
+    def encode(self, text: str, **kwargs: Dict[str, Any]) -> List[int]:
+        """
+        Given a string, return the encoded list of token ids.
+
+        Args:
+            text (str): The text to encode.
+            **kwargs (Dict[str, Any]): kwargs.
+
+        Returns:
+            List[int]: The encoded list of token ids.
+        """
+        pass
+
+    def decode(self, token_ids: List[int], **kwargs: Dict[str, Any]) -> str:
+        """
+        Given a list of token ids, return the decoded text, optionally including special tokens.
+
+        Args:
+            token_ids (List[int]): The list of token ids to decode.
+            **kwargs (Dict[str, Any]): kwargs.
+
+        Returns:
+            str: The decoded text.
+        """
+        pass
+
+
+class ModelTokenizer(Protocol):
+    """
+    Abstract tokenizer that implements model-specific special token logic in
+    the ``tokenize_messages`` method. See :class:`~torchtune.models.llama3.Llama3Tokenizer`
+    for an example implementation of this protocol.
+    """
+
+    special_tokens: Dict[str, int]
+    max_seq_len: Optional[int]
+
+    def tokenize_messages(
+        self, messages: List[Message], **kwargs: Dict[str, Any]
+    ) -> Tuple[List[int], List[bool]]:
+        """
+        Given a list of messages, return a list of tokens and list of masks for
+        the concatenated and formatted messages.
+
+        Args:
+            messages (List[Message]): The list of messages to tokenize.
+            **kwargs (Dict[str, Any]): kwargs.
+
+        Returns:
+            Tuple[List[int], List[bool]]: The list of token ids and the list of masks.
+        """
+        pass
+
+
+def tokenize_messages_no_special_tokens(
+    tokenizer: ModelTokenizer,
+    messages: List[Message],
+    *,
+    bos_id: Optional[int] = None,
+    eos_id: Optional[int] = None,
+) -> Tuple[List[int], List[bool]]:
+    r"""Tokenize a list of messages one at a time then concatenate them,
+    returning a list of tokens and a list of masks. Does not add any special
+    tokens except for BOS and EOS (if provided). This serves as a common starting point for
+    model tokenizers that do not rely heavily on special tokens.
+
+    Examples:
+        >>> messages = [
+        ...     Message(role="system", content="system message\n", masked=True),
+        ...     Message(role="user", content="user prompt\n", masked=True),
+        ...     Message(role="assistant", content="assistant response\n"),
+        ... ]
+        # tokenize_messages encodes messages separately and concats
+        >>> tokens = tokenize_messages_no_special_tokens(
+        ...     tokenizer,
+        ...     messages,
+        ...     bos_id=tokenizer.bos_id,
+        ...     eos_id=tokenizer.eos_id,
+        ... )[0]
+        >>> print(tokens)
+        [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+        # Same result as encoding the full string in one go
+        >>> print(tokenizer.encode(''.join([message.content for message in messages])))
+        [1, 1788, 2643, 13, 1792, 9508, 13, 465, 22137, 2933, 2]
+
+
+    Args:
+        tokenizer (ModelTokenizer): Tokenizer to encode messages with.
+        messages (List[Message]): A list of messages, each containing role, content,
+            and masked attributes.
+        bos_id (Optional[int]): Beginning-of-sequence token id. If None, no BOS token will
+            be added. Default None.
+        eos_id (Optional[int]): End-of-sequence token id. If None, no EOS token will be added. Default None.
+
+    Returns:
+        Tuple[List[int], List[bool]]: The tokenized messages.
+
+    Raises:
+        RuntimeError: if any message in ``messages`` does not satisfy ``message['type'] == 'text'``.
+    """
+    start_of_turn = True
+    end_of_turn = False
+    prev_ends_with_space = False
+    max_seq_len = tokenizer.max_seq_len  # We define this on ModelTokenizer
+    tokenized_messages = []
+    mask = []
+    for message in messages:
+        # If assistant message, this is the end of a turn
+        end_of_turn = message.role == "assistant"
+
+        # Prepend BOS on start of new turns
+        if start_of_turn and bos_id is not None:
+            tokenized_messages.append(bos_id)
+            mask.append(message.masked)
+
+        # We want to trim leading whitespace on the next message when
+        # (a) it is a continuation of the turn (i.e. not the first message)
+        # (b) the vocabulary explicitly encodes whitespace characters (checked inside
+        #     the base tokenizer's encode method), and
+        # (c) the previous message did not end with a space
+        trim_leading_whitespace = (not start_of_turn) and not prev_ends_with_space
+
+        # Tokenize current message, append with masks
+        tokens = []
+        for item in message.content:
+            if item["type"] == "text":
+                tokens = tokens + tokenizer.encode(
+                    item["content"].rstrip(" "),
+                    add_bos=False,
+                    add_eos=False,
+                    trim_leading_whitespace=trim_leading_whitespace,
+                )
+            else:
+                raise RuntimeError(f"Unsupported message content type: {item['type']}")
+        prev_ends_with_space = item["content"].endswith(" ")
+        tokenized_messages.extend(tokens)
+        mask.extend([message.masked] * len(tokens))
+
+        # If assistant message, append EOS at end
+        if end_of_turn:
+            if eos_id is not None:
+                tokenized_messages.append(eos_id)
+                mask.append(message.masked)
+            end_of_turn = False
+            start_of_turn = True
+        else:
+            start_of_turn = False
+
+        # Break out early if we reach max_seq_len
+        if max_seq_len is not None and len(tokenized_messages) >= max_seq_len:
+            break
+
+    # Finally, truncate if necessary
+    if max_seq_len is not None:
+        tokenized_messages = truncate(tokenized_messages, max_seq_len, eos_id)
+        mask = truncate(
+            mask, max_seq_len, message.masked if eos_id is not None else None
+        )
+
+    return tokenized_messages, mask
+
+
+def parse_hf_tokenizer_json(tokenizer_json_path: str) -> Dict[str, int]:
+    """
+    Parse the ``tokenizer.json`` file from a Hugging Face model to extract the
+    special token str to id mapping.
+
+    Args:
+        tokenizer_json_path (str): Path to the ``tokenizer.json`` file.
+
+    Returns:
+        Dict[str, int]: The special token str to id mapping.
+    """
+    with open(tokenizer_json_path, "r") as f:
+        tokenizer_json = json.load(f)
+
+    return {token["content"]: token["id"] for token in tokenizer_json["added_tokens"]}
diff -ruN marc_original/third_party/torchtune/torchtune/modules/transformer.py marc/third_party/torchtune/torchtune/modules/transformer.py
--- marc_original/third_party/torchtune/torchtune/modules/transformer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/transformer.py	2025-02-20 17:49:30.710026153 -0500
@@ -0,0 +1,905 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import copy
+from typing import Callable, Dict, List, Optional, Union
+
+import torch
+import torch.nn.functional as F
+from torch import nn
+from torchtune.modules import MultiHeadAttention
+from torchtune.modules.attention_utils import _MaskType
+from torchtune.utils._logging import deprecated
+
+
+class TransformerSelfAttentionLayer(nn.Module):
+    """
+    Transformer layer derived from the Llama2 model. Normalization is applied before the attention **and** FF layer.
+
+    Args:
+        attn (MultiHeadAttention): Attention module.
+        mlp (nn.Module): Feed-forward module.
+        sa_norm (Optional[nn.Module]): Normalization to be applied before self-attention.
+        mlp_norm (Optional[nn.Module]): Normalization to be applied before the feed-forward layer.
+        sa_scale (Optional[nn.Module]): Module to scale self-attention output.
+        mlp_scale (Optional[nn.Module]): Module to scale the feed-forward output.
+    """
+
+    def __init__(
+        self,
+        attn: MultiHeadAttention,
+        mlp: nn.Module,
+        *,
+        sa_norm: Optional[nn.Module] = None,
+        mlp_norm: Optional[nn.Module] = None,
+        sa_scale: Optional[nn.Module] = None,
+        mlp_scale: Optional[nn.Module] = None,
+    ) -> None:
+        super().__init__()
+        self.attn = attn
+        self.mlp = mlp
+        self.sa_norm = sa_norm or nn.Identity()
+        self.mlp_norm = mlp_norm or nn.Identity()
+        self.sa_scale = sa_scale or nn.Identity()
+        self.mlp_scale = mlp_scale or nn.Identity()
+
+    def setup_cache(
+        self,
+        batch_size: int,
+        dtype: torch.dtype,
+        *,
+        encoder_max_seq_len: int,
+        decoder_max_seq_len: int,
+    ) -> None:
+        """Setup key value caches for attention calculation.
+
+        Args:
+            batch_size (int): batch size for the caches.
+            dtype (torch.dtype): dtype for the caches.
+            encoder_max_seq_len (int): this parameter is ignored in this layer.
+            decoder_max_seq_len (int): maximum cache sequence length.
+        """
+        self.attn.setup_cache(batch_size, dtype, max_seq_len=decoder_max_seq_len)
+
+    @property
+    def cache_enabled(self) -> bool:
+        """Check if the key value caches are setup."""
+        return self.attn.kv_cache is not None
+
+    def reset_cache(self):
+        """Reset the key value caches."""
+        self.attn.reset_cache()
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        *,
+        mask: Optional[_MaskType] = None,
+        input_pos: Optional[torch.Tensor] = None,
+        **kwargs: Dict,
+    ) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape
+                [batch_size x seq_length x embed_dim]
+            mask (Optional[_MaskType]): Used to mask the scores after the query-key multiplication
+                and before the softmax. Either:
+
+                A boolean tensor with shape ``[b x s x s]``, ``[b x s x self.encoder_max_cache_seq_len]``,
+                or ``[b x s x self.encoder_max_cache_seq_len]`` if using KV-cacheing with encoder/decoder layers.
+                A value of True in row ``i`` and column ``j`` means token ``i`` attends to token ``j``. A value of False means
+                token ``i`` does not attend to token ``j``. If no mask is specified, a causal mask
+                is used by default.
+
+                A :class:`~torch.nn.attention.flex_attention.BlockMask` for document masking in a packed sequence
+                created via `create_block_mask <https://pytorch.org/blog/flexattention/#mask-mods>`_. We  use
+                :func:`~torch.nn.attention.flex_attention.flex_attention` when computing attention with block masks.
+                Default is None.
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape [b x s].
+                During inference, this indicates the position of the current token.
+                If none, assume the index of the token is its position id. Default is None.
+            **kwargs (Dict): transformer layer inputs not relevant to self attention.
+
+        Returns:
+            torch.Tensor: output tensor with same shape as input
+                [batch_size x seq_length x embed_dim]
+        """
+        # Input tensor and attention output have the same shape
+        # [b, s, d]
+        # Norm applied before self-attention
+        h = self.sa_norm(x)
+        attn_out = self.attn(h, h, mask=mask, input_pos=input_pos)
+
+        # Residual connection; shape: [batch_size, seq_length, embed_dim]
+        h = self.sa_scale(attn_out) + x
+
+        # Norm applied before the feedforward layer
+        mlp_out = self.mlp(self.mlp_norm(h))
+
+        # Residual connection; shape: [batch_size, seq_length, embed_dim]
+        out = h + self.mlp_scale(mlp_out)
+        return out
+
+
+class TransformerCrossAttentionLayer(nn.Module):
+    """
+    Cross attention Transformer layer following the same conventions as the TransformerSelfAttentionLayer.
+    Normalization is applied before the attention **and** FF layer.
+
+    Args:
+        attn (MultiHeadAttention): Attention module.
+        mlp (nn.Module): Feed-forward module.
+        ca_norm (Optional[nn.Module]): Normalization to be applied before cross-attention.
+        mlp_norm (Optional[nn.Module]): Normalization to be applied before the feed-forward layer.
+        ca_scale (Optional[nn.Module]): Module to scale cross-attention output.
+        mlp_scale (Optional[nn.Module]): Module to scale the feed-forward output.
+
+    Raises:
+        AssertionError: if attn.pos_embeddings is set.
+    """
+
+    def __init__(
+        self,
+        attn: MultiHeadAttention,
+        mlp: nn.Module,
+        *,
+        ca_norm: Optional[nn.Module] = None,
+        mlp_norm: Optional[nn.Module] = None,
+        ca_scale: Optional[nn.Module] = None,
+        mlp_scale: Optional[nn.Module] = None,
+    ) -> None:
+        super().__init__()
+        if attn.pos_embeddings is not None:
+            raise AssertionError(
+                "Doesn't support positional embeddings for cross attention, \
+                because q and k are different sequences."
+            )
+        self.attn = attn
+        self.mlp = mlp
+        self.ca_norm = ca_norm or nn.Identity()
+        self.mlp_norm = mlp_norm or nn.Identity()
+        self.ca_scale = ca_scale or nn.Identity()
+        self.mlp_scale = mlp_scale or nn.Identity()
+
+    def setup_cache(
+        self,
+        batch_size: int,
+        dtype: torch.dtype,
+        *,
+        encoder_max_seq_len: int,
+        decoder_max_seq_len: int,
+    ) -> None:
+        """Setup key value caches for attention calculation.
+
+        Args:
+            batch_size (int): batch size for the caches.
+            dtype (torch.dtype): dtype for the caches.
+            encoder_max_seq_len (int): maximum cache sequence length.
+            decoder_max_seq_len (int): this parameter is ignored in this layer.
+        """
+        self.attn.setup_cache(batch_size, dtype, encoder_max_seq_len)
+
+    @property
+    def cache_enabled(self) -> bool:
+        """Check if the key value caches are setup."""
+        return self.attn.kv_cache is not None
+
+    def reset_cache(self):
+        """Reset the key value caches."""
+        self.attn.reset_cache()
+
+    def _skip_mask(self, mask: Optional[torch.Tensor]) -> Optional[torch.Tensor]:
+        """Some tokens in x may not attend to any encoder inputs
+        due to the cross attention mask (encoder_mask). This results in
+        a full row of the attention matrix being masked out.
+
+        In the example below, the word "the" is masked from every embedding.
+        The False value means a token can't attend to an embedding.
+
+        .. code-block:: text
+
+            |emb||emb||emb|
+        |The| F    F    F
+        |red| T    F    T
+        |car| F    T    T
+
+        This results in no inputs into the softmax layer which causes a NaN.
+        The skip mask is used to mask the outputs of attention and
+        mlp resulting in the token being skipped.
+
+        The above example would result in a skip mask of: [[True], [False], [False]]
+        which specifies which tokens to fully mask out.
+
+        """
+        # no skip_mask if no masking
+        if mask is None:
+            return None
+        # negate mask and convert to boolean mask
+        if mask.dtype == torch.bool:
+            mask = ~mask
+        else:
+            mask = torch.isneginf(mask)
+        # True where all elements in a row are True
+        mask = torch.all(mask, dim=-1, keepdim=True)
+        return mask
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        *,
+        encoder_input: Optional[torch.Tensor] = None,
+        encoder_mask: Optional[torch.Tensor] = None,
+        **kwargs: Dict,
+    ) -> torch.Tensor:
+        """
+        Args:
+            x (torch.Tensor): input tensor with shape
+                [batch_size x seq_length x embed_dim]
+            encoder_input (Optional[torch.Tensor]): Optional input embeds from the encoder. Shape
+                [batch_size x token_sequence x embed_dim]
+            encoder_mask (Optional[torch.Tensor]):  Boolean tensor defining a relational matrix between
+                tokens and encoder embeddings. A True value at position i,j means token i can attend
+                to embedding j in the decoder. Mask has shape [batch_size x token_sequence x embed_sequence].
+                Default is None.
+            **kwargs (Dict): transformer layer inputs not relevant to self attention.
+
+        Returns:
+            torch.Tensor: output tensor with same shape as input
+                [batch_size x seq_length x embed_dim]
+        """
+        # During decoding, it's possible encoder_input is None because the embeds
+        # are already stored in the kv cache.
+        empty_cache = not self.cache_enabled or self.attn.kv_cache.size == 0
+        # Skip cross attention when no secondary input as it's primary purpose
+        # is to attend between x and encoder_input.
+        if encoder_input is None and empty_cache:
+            return x
+
+        # A mask of tokens (x) with no encoder_input
+        skip_mask = self._skip_mask(encoder_mask)
+        if encoder_mask is not None:
+            # TODO: remove after PyTorch 2.5 is released
+            # This unmasks the skipped rows to avoid NaNs in SDPA Softmax backward
+            # This doesn't affect the output since outputs are masked out later
+            encoder_mask = encoder_mask.masked_fill(skip_mask, True)
+
+        # Input tensor and attention output have the same shape
+        # [b, s, d]
+        # Norm applied before self-attention
+        # TODO: Add support for sample packing and bring back input_pos
+        attn_out = self.attn(self.ca_norm(x), encoder_input, mask=encoder_mask)
+        if skip_mask is not None:
+            attn_out = attn_out.masked_fill(skip_mask, 0)
+
+        # Residual connection; shape: [batch_size, seq_length, embed_dim]
+        h = self.ca_scale(attn_out) + x
+
+        # Norm applied before the feedforward layer
+        mlp_out = self.mlp(self.mlp_norm(h))
+        if skip_mask is not None:
+            mlp_out = mlp_out.masked_fill(skip_mask, 0)
+
+        # Residual connection; shape: [batch_size, seq_length, embed_dim]
+        out = h + self.mlp_scale(mlp_out)
+        return out
+
+
+def _get_clones(module: nn.Module, n: int) -> nn.ModuleList:
+    """
+    Return a list of ``n`` identical layers.
+
+    Args:
+        module (nn.Module): module to be cloned
+        n (int): number of clones
+
+    Returns:
+        nn.ModuleList: list of ``n`` identical layers
+    """
+    # FIXME: copy.deepcopy() is not defined on nn.module
+    return nn.ModuleList([copy.deepcopy(module) for i in range(n)])
+
+
+class TransformerDecoder(nn.Module):
+    """
+    Transformer Decoder derived from the Llama2 architecture.
+
+    Args:
+        tok_embeddings (nn.Embedding): PyTorch embedding layer, to be used to move
+            tokens to an embedding space.
+        layers (Union[nn.Module, List[nn.Module], nn.ModuleList]): A single transformer Decoder layer, an
+            nn.ModuleList of layers or a list of layers. It is recommended to use an nn.ModuleList.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value. This is used to setup the
+            :func:`~torchtune.modules.KVCache`
+        head_dim (int): embedding dimension for each head in self-attention. This is used
+            to setup the :func:`~torchtune.modules.KVCache`
+        norm (nn.Module): Callable that applies normalization to the output of the decoder,
+            before final MLP.
+        output (Union[nn.Linear, Callable]): Callable that applies a linear transformation to the output of
+            the decoder.
+        num_layers (Optional[int]): Number of Transformer Decoder layers, only define when
+            layers is not a list.
+        output_hidden_states (Optional[List[int]]): List of layers (indices) to include in the output
+
+    Raises:
+        AssertionError: num_layers is set and layer is a list
+        AssertionError: num_layers is not set and layer is an nn.Module
+
+    Note:
+        Arg values are checked for correctness (eg: ``attn_dropout`` belongs to [0,1])
+        in the module where they are used. This helps reduces the number of raise
+        statements in code and improves readability.
+    """
+
+    def __init__(
+        self,
+        *,
+        tok_embeddings: nn.Embedding,
+        layers: Union[nn.Module, List[nn.Module], nn.ModuleList],
+        max_seq_len: int,
+        num_heads: int,
+        head_dim: int,
+        norm: nn.Module,
+        output: Union[nn.Linear, Callable],
+        num_layers: Optional[int] = None,
+        output_hidden_states: Optional[List[int]] = None,
+    ) -> None:
+        super().__init__()
+        if isinstance(layers, nn.ModuleList):
+            pass
+        elif isinstance(layers, list):
+            layers = nn.ModuleList(layers)
+        else:
+            if not isinstance(layers, nn.Module):
+                raise AssertionError("num_layers is defined, layers must be a module")
+            if num_layers is None:
+                raise AssertionError("num_layers is not defined, layers must be a list")
+            layers = _get_clones(layers, num_layers)
+
+        self.tok_embeddings = tok_embeddings
+        self.layers = layers
+        self.norm = norm
+        self.output = output
+        self.output_hidden_states = output_hidden_states or []
+        self.max_seq_len = max_seq_len
+        self.num_heads = num_heads
+        self.head_dim = head_dim
+        self.causal_mask = None
+        self.num_output_chunks = 0
+
+        # attributes for KV caches during inference
+        self.encoder_max_cache_seq_len = None
+        self.decoder_max_cache_seq_len = None
+
+    def set_num_output_chunks(self, num_output_chunks: int) -> None:
+        """Used to save memory in combination with :class:`~torchtune.modules.loss.CEWithChunkedOutputLoss`.
+        This should be called before the first forward pass, in the recipe."""
+        self.num_output_chunks = num_output_chunks
+
+    def setup_caches(
+        self,
+        batch_size: int,
+        dtype: torch.dtype,
+        *,
+        encoder_max_seq_len: Optional[int] = None,
+        decoder_max_seq_len: Optional[int] = None,
+    ):
+        """
+        Sets up key-value attention caches for inference. For each layer in ``self.layers``:
+        - :class:`~torchtune.modules.TransformerSelfAttentionLayer` will use ``decoder_max_seq_len``.
+        - :class:`~torchtune.modules.TransformerCrossAttentionLayer` will use ``encoder_max_seq_len``.
+        - :class:`~torchtune.modules.fusion.FusionLayer` will use both ``decoder_max_seq_len`` and ``encoder_max_seq_len``.
+
+        Args:
+            batch_size (int): batch size for the caches.
+            dtype (torch.dtype): dtype for the caches.
+            encoder_max_seq_len (Optional[int]): maximum encoder cache sequence length.
+            decoder_max_seq_len (Optional[int]): maximum decoder cache sequence length.
+        """
+
+        has_encoder_layers = any(
+            isinstance(m, TransformerCrossAttentionLayer) for m in self.modules()
+        )
+        has_decoder_layers = any(
+            isinstance(l, TransformerSelfAttentionLayer) for l in self.layers
+        )
+
+        if has_encoder_layers:
+            if encoder_max_seq_len is not None:
+                self.encoder_max_cache_seq_len = encoder_max_seq_len
+            else:
+                self.encoder_max_cache_seq_len = self.max_seq_len
+
+        if has_decoder_layers:
+            if decoder_max_seq_len is not None:
+                self.decoder_max_cache_seq_len = decoder_max_seq_len
+            else:
+                self.decoder_max_cache_seq_len = self.max_seq_len
+
+        for layer in self.layers:
+            layer.setup_cache(
+                batch_size,
+                dtype,
+                encoder_max_seq_len=self.encoder_max_cache_seq_len,
+                decoder_max_seq_len=self.decoder_max_cache_seq_len,
+            )
+
+    def caches_are_enabled(self) -> bool:
+        """Check if the key value caches are setup. This is useful to efficient inference."""
+        return self.layers[0].cache_enabled
+
+    def reset_caches(self):
+        """Reset the key value caches."""
+        if not self.caches_are_enabled():
+            raise RuntimeError(
+                "Key value caches are not setup. Call ``setup_caches()`` first."
+            )
+
+        for layer in self.layers:
+            layer.reset_cache()
+
+    @torch.compiler.disable
+    def chunked_output(self, last_hidden_state: torch.Tensor) -> List[torch.Tensor]:
+        """
+        Apply output projection in chunks. This should be applied in conjunction with
+        :class:`~torchtune.modules.loss.CEWithChunkedOutputLoss` as upcasting to fp32 is done there.
+
+        To use this method, you should first call
+        :func:`~torchtune.modules.TransformerDecoder.set_num_output_chunks`.
+
+        Args:
+            last_hidden_state (torch.Tensor): last hidden state of the decoder, having shape
+                [b, seq_len, embed_dim].
+
+        Returns:
+            List[torch.Tensor]: List of num_chunks output tensors, each with shape
+                [b, seq_len/num_chunks, out_dim], where out_dim is usually the vocab size.
+        """
+        return [
+            self.output(chunk)
+            for chunk in last_hidden_state.chunk(self.num_output_chunks, dim=1)
+        ]
+
+    def _validate_inputs(
+        self,
+        seq_len: int,
+        mask: Optional[torch.Tensor] = None,
+        encoder_input: Optional[torch.Tensor] = None,
+        encoder_mask: Optional[torch.Tensor] = None,
+        input_pos: Optional[torch.Tensor] = None,
+    ):
+        """
+        Validates inputs for ``forward``.
+        Args:
+            seq_len (int): Input tensor sequence length.
+            mask (Optional[torch.Tensor]): Attention mask used for inference and for sequence packing.
+            encoder_input (Optional[torch.Tensor]): Encoder input for cross-attention.
+            encoder_mask (Optional[torch.Tensor]): Encoder attention mask for cross-embedding attention.
+            input_pos (Optional[torch.Tensor]): Input tensor position IDs.
+
+        Raises:
+            ValueError: if seq_len of x is bigger than max_seq_len
+            ValueError: if the model has caches which have been setup with self-attention layers and ``mask`` is not provided.
+            ValueError: if the model has caches which have been setup with encoder layers and ``encoder_mask`` is not provided.
+            ValueError: if the model has caches which have been setup ``input_pos`` is not provided.
+        """
+
+        if seq_len > self.max_seq_len:
+            raise ValueError(
+                f"seq_len ({seq_len}) of input tensor should be smaller "
+                f"than max_seq_len ({self.max_seq_len})"
+            )
+
+        if self.caches_are_enabled():
+            if mask is None:
+                raise ValueError(
+                    "KV-caches for self-attention layers are setup for inference mode, causal masks must be provided!"
+                    " Use the `mask` arg to provide a causal mask."
+                )
+
+            if encoder_input is not None and encoder_mask is None:
+                raise ValueError(
+                    "KV-caches for cross-attention/fusion layers are setup for inference mode and you seem to be using"
+                    " encoder_input, causal masks must be provided! Use the `encoder_mask` arg to provide a causal mask."
+                )
+
+            if input_pos is None:
+                raise ValueError(
+                    "KV-caches are setup for inference mode, input positions must be provided!"
+                )
+
+    def forward(
+        self,
+        tokens: torch.Tensor,
+        *,
+        mask: Optional[_MaskType] = None,
+        encoder_input: Optional[torch.Tensor] = None,
+        encoder_mask: Optional[torch.Tensor] = None,
+        input_pos: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, List[torch.Tensor]]:
+        """
+        Args:
+            tokens (torch.Tensor): input tensor with shape ``[b x s]``
+            mask (Optional[_MaskType]): Used to mask the scores after the query-key multiplication
+                and before the softmax. This parameter is required during inference if caches have been setup.
+                Either:
+
+                A boolean tensor with shape ``[b x s x s]``, ``[b x s x self.encoder_max_cache_seq_len]``,
+                or ``[b x s x self.encoder_max_cache_seq_len]`` if using KV-cacheing with encoder/decoder layers.
+                A value of True in row ``i`` and column ``j`` means token ``i`` attends to token ``j``. A value of False means
+                token ``i`` does not attend to token ``j``. If no mask is specified, a causal mask
+                is used by default.
+
+                A :class:`~torch.nn.attention.flex_attention.BlockMask` for document masking in a packed sequence
+                created via `create_block_mask <https://pytorch.org/blog/flexattention/#mask-mods>`_. We  use
+                :func:`~torch.nn.attention.flex_attention.flex_attention` when computing attention with block masks.
+                Default is None.
+            encoder_input (Optional[torch.Tensor]): Optional input embeds from the encoder. Shape ``[b x s_e x d_e]``
+            encoder_mask (Optional[torch.Tensor]):  Boolean tensor defining a relational matrix between
+                tokens and encoder embeddings. A True value at position ``i,j`` means token ``i`` can attend
+                to embedding ``j`` in the decoder. Mask has shape ``[b x s x s_e]``. Default is None,
+                but this is required during inference if the model has been setup with any layers
+                which use encoder embeddings and caches have been setup.
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape ``[b x s]``.
+                During inference, this indicates the position of the current token.
+                This parameter is required during inference if caches have been setup. Default is None.
+
+        Returns:
+            Union[torch.Tensor, List[torch.Tensor]]: output tensor with shape ``[b x s x v]`` or a list of layer
+                output tensors defined by ``output_hidden_states`` with the
+                final output tensor appended to the list.
+
+        Note:
+            At the very first step of inference, when the model is provided with a prompt,
+            ``input_pos`` should contain the positions of all of the tokens in the prompt.
+            For a single-batch prompt, or a batch of prompts with identical lengths, this
+            will be ``torch.arange(prompt_length)``. For a batch of varying-length prompts,
+            shorter prompts are left-padded and position ids are correspondingly right-shifted,
+            thus positional ids should be of shape ``[b, padded_prompt_length]``.
+            This is because we will need to retrieve the positional embeddings for each input id.
+            In the subsequent steps, if the model has been setup with KV-caches, ``input_pos`` will contain
+            the position(s) of the current token(s) ``torch.tensor([padded_prompt_length])``. Otherwise,
+            ``input_pos`` will contain all the position ids up to the current token.
+
+        Shape notation:
+            - b: batch size
+            - s: token sequence length
+            - s_e: encoder sequence length
+            - v: vocab size
+            - d: token embed dim
+            - d_e: encoder embed dim
+            - m_s: max seq len
+        """
+        # input tensor of shape [b, s]
+        seq_len = tokens.shape[1]
+
+        self._validate_inputs(
+            seq_len,
+            mask=mask,
+            encoder_input=encoder_input,
+            encoder_mask=encoder_mask,
+            input_pos=input_pos,
+        )
+
+        # shape: [b, s, d]
+        h = self.tok_embeddings(tokens)
+
+        hidden = []
+        for i, layer in enumerate(self.layers):
+            if i in self.output_hidden_states:
+                hidden.append(h)
+            # shape: [b, s, d]
+            h = layer(
+                h,
+                mask=mask,
+                encoder_input=encoder_input,
+                encoder_mask=encoder_mask,
+                input_pos=input_pos,
+            )
+
+        # shape: [b, s, d]
+        h = self.norm(h)
+
+        if self.num_output_chunks > 0:
+            output = self.chunked_output(h)
+        else:
+            # shape: [b, seq_len, out_dim]
+            output = self.output(h).float()
+
+        # Output list if hidden states are requested, otherwise just the output
+        # TODO: always output a list to have a consistent output type
+        output = output if not hidden else [*hidden, output]
+        return output
+
+
+@deprecated(
+    msg="Please use torchtune.modules.TransformerDecoder instead. \
+If you need an example, see torchtune.models.qwen2._component_builders.py \
+on how to use torch.modules.TiedLinear for the output projection."
+)
+class TiedEmbeddingTransformerDecoder(nn.Module):
+    """
+    Transformer Decoder with tied embedding weight. A key difference between
+    this class and :class:`~torchtune.modules.TransformerDecoder`
+    is that the output projection is replaced with token embeddings weights.
+
+    Args:
+        tok_embeddings (nn.Embedding): PyTorch embedding layer, to be used to move
+            tokens to an embedding space.
+        layers (Union[nn.Module, List[nn.Module]]): Transformer Decoder layer or a list of layers.
+        max_seq_len (int): maximum sequence length the model will be run with, as used
+            by :func:`~torchtune.modules.KVCache`
+        num_heads (int): number of query heads. For MHA this is also the
+            number of heads for key and value. This is used to setup the
+            :func:`~torchtune.modules.KVCache`
+        head_dim (int): embedding dimension for each head in self-attention. This is used
+            to setup the :func:`~torchtune.modules.KVCache`
+        norm (nn.Module): Callable that applies normalization to the output of the decoder,
+            before final MLP.
+        num_layers (Optional[int]): Number of Transformer Decoder layers, only define when
+            layers is not a list.
+        output_hidden_states (Optional[List[int]]): List of layers (indices) to include in the output
+
+    Raises:
+        AssertionError: num_layers is set and layer is a list
+        AssertionError: num_layers is not set and layer is an nn.Module
+
+    Note:
+        Arg values are checked for correctness (eg: ``attn_dropout`` belongs to [0,1])
+        in the module where they are used. This helps reduces the number of raise
+        statements in code and improves readability.
+    """
+
+    def __init__(
+        self,
+        *,
+        tok_embeddings: nn.Embedding,
+        layers: Union[nn.Module, List[nn.Module]],
+        max_seq_len: int,
+        num_heads: int,
+        head_dim: int,
+        norm: nn.Module,
+        num_layers: Optional[int] = None,
+        output_hidden_states: Optional[List[int]] = None,
+    ) -> None:
+        super().__init__()
+        if num_layers is None:
+            if isinstance(layers, nn.Module):
+                raise AssertionError(
+                    "If num_layers is undefined, it is assumed that a list of layers is provided."
+                )
+            layers = nn.ModuleList(layers)
+        else:
+            if not isinstance(layers, nn.Module):
+                raise AssertionError("num_layers is defined, layers must be a module")
+            layers = _get_clones(layers, num_layers)
+
+        self.tok_embeddings = tok_embeddings
+        self.layers = layers
+        self.norm = norm
+        self.output_hidden_states = output_hidden_states or []
+        self.max_seq_len = max_seq_len
+        self.num_heads = num_heads
+        self.head_dim = head_dim
+        self.causal_mask = None
+        self.num_output_chunks = 0
+
+        # attributes for KV caches during inference
+        self.encoder_max_cache_seq_len = None
+        self.decoder_max_cache_seq_len = None
+
+    @torch.compiler.disable
+    def chunked_output(self, last_hidden_state: torch.Tensor) -> List[torch.Tensor]:
+        """
+        Apply output projection in chunks. This should be applied in conjunction with
+        :class:`~torchtune.modules.loss.CEWithChunkedOutputLoss` as upcasting to fp32 is done there.
+        To use this method, you should first call
+        :func:`~torchtune.modules.TiedEmbeddingTransformerDecoder.set_num_output_chunks`.
+        Args:
+            last_hidden_state (torch.Tensor): last hidden state of the decoder, having shape
+                [b, seq_len, embed_dim].
+        Returns:
+            List[torch.Tensor]: List of num_chunks output tensors, each with shape
+                [b, seq_len/num_chunks, out_dim], where out_dim is usually the vocab size.
+        """
+        return [
+            F.linear(chunk, self.tok_embeddings.weight)
+            for chunk in last_hidden_state.chunk(self.num_output_chunks, dim=1)
+        ]
+
+    def set_num_output_chunks(self, num_output_chunks: int) -> None:
+        """Used to save memory in combination with :class:`~torchtune.modules.loss.CEWithChunkedOutputLoss`.
+        This should be called before the first forward pass, in the recipe."""
+        self.num_output_chunks = num_output_chunks
+
+    def setup_caches(
+        self,
+        batch_size: int,
+        dtype: torch.dtype,
+        *,
+        encoder_max_seq_len: Optional[int] = None,
+        decoder_max_seq_len: Optional[int] = None,
+    ):
+        """
+        Sets up key-value attention caches for inference. For each layer in ``self.layers``:
+        - :class:`~torchtune.modules.TransformerSelfAttentionLayer` will use ``decoder_max_seq_len``.
+        - :class:`~torchtune.modules.TransformerCrossAttentionLayer` will use ``encoder_max_seq_len``.
+        - :class:`~torchtune.modules.fusion.FusionLayer` will use both ``decoder_max_seq_len`` and ``encoder_max_seq_len``.
+
+        Args:
+            batch_size (int): batch size for the caches.
+            dtype (torch.dtype): dtype for the caches.
+            encoder_max_seq_len (Optional[int]): maximum encoder cache sequence length.
+            decoder_max_seq_len (Optional[int]): maximum decoder cache sequence length.
+        """
+        has_encoder_layers = any(
+            isinstance(l, TransformerCrossAttentionLayer) for l in self.modules()
+        )
+        has_decoder_layers = any(
+            isinstance(l, TransformerSelfAttentionLayer) for l in self.layers
+        )
+        if has_encoder_layers:
+            if encoder_max_seq_len is not None:
+                self.encoder_max_cache_seq_len = encoder_max_seq_len
+            else:
+                self.encoder_max_cache_seq_len = self.max_seq_len
+
+        if has_decoder_layers:
+            if decoder_max_seq_len is not None:
+                self.decoder_max_cache_seq_len = decoder_max_seq_len
+            else:
+                self.decoder_max_cache_seq_len = self.decoder_max_cache_seq_len
+
+        for layer in self.layers:
+            layer.setup_cache(
+                batch_size,
+                dtype,
+                self.encoder_max_cache_seq_len,
+                self.decoder_max_cache_seq_len,
+            )
+
+    @property
+    def encoder_caches_are_enabled(self) -> bool:
+        """Checks if there are any :class:`~torchtune.modules.TransformerCrossAttentionLayer`,
+        or :class:`~torchtune.modules.fusion.FusionLayer` layers which have cache enabled.
+        """
+        return self.encoder_max_cache_seq_len is not None
+
+    @property
+    def decoder_caches_are_enabled(self) -> bool:
+        """Check if the key value caches are setup."""
+        return self.decoder_max_cache_seq_len is not None
+
+    def reset_caches(self):
+        """Reset the key value caches."""
+        if not (self.encoder_caches_are_enabled or self.decoder_caches_are_enabled):
+            raise RuntimeError(
+                "Key value caches are not setup. Call ``setup_caches()`` first."
+            )
+
+        for layer in self.layers:
+            layer.reset_cache()
+
+    def forward(
+        self,
+        tokens: torch.Tensor,
+        *,
+        mask: Optional[_MaskType] = None,
+        encoder_input: Optional[torch.Tensor] = None,
+        encoder_mask: Optional[torch.Tensor] = None,
+        input_pos: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, List[torch.Tensor]]:
+        """
+        Args:
+            tokens (torch.Tensor): input tensor with shape [b x s]
+            mask (Optional[_MaskType]): Used to mask the scores after the query-key multiplication
+                and before the softmax. Either a boolean tensor with shape [b x s x s] or a
+                :class:`~torch.nn.attention.flex_attention.BlockMask`. If a boolean tensor, a value
+                of True in row i and column j means token i attends to token j. A value of False means
+                token i does not attend to token j. If no mask is specified, a causal mask
+                is used by default. If a :class:`~torch.nn.attention.flex_attention.BlockMask` is passed
+                for document masking in a packed sequence via `create_block_mask
+                <https://pytorch.org/blog/flexattention/#mask-mods>`_, we use
+                :func:`~torch.nn.attention.flex_attention.flex_attention` when computing attention.
+                Default is None.
+            encoder_input (Optional[torch.Tensor]): Optional input embeds from the encoder. Shape [b x s_e x d_e]
+            encoder_mask (Optional[torch.Tensor]):  Boolean tensor defining a relational matrix between
+                tokens and encoder embeddings. A True value at position i,j means token i can attend
+                to embedding j in the decoder. Mask has shape [b x s x s_e]. Default is None.
+            input_pos (Optional[torch.Tensor]): Optional tensor which contains the position ids
+                of each token. During training, this is used to indicate the positions
+                of each token relative to its sample when packed, shape [b x s].
+                During inference, this indicates the position of the current token.
+                If none, assume the index of the token is its position id. Default is None.
+
+        Note: At the very first step of inference, when the model is provided with a prompt,
+        ``input_pos`` would contain the positions of all of the tokens in the prompt
+        (eg: ``torch.arange(prompt_length)``). This is because we will need to compute the
+        KV values for each position.
+
+        Returns:
+            Union[torch.Tensor, List[torch.Tensor]]: output tensor with shape [b x s x v] or a list of layer
+                output tensors defined by ``output_hidden_states`` with the
+                final output tensor appended to the list.
+
+        Raises:
+            ValueError: if seq_len of x is bigger than max_seq_len
+            ValueError: if a mask is provided and the model is in inference mode
+
+        Notation used for tensor shapes:
+            - b: batch size
+            - s: token sequence length
+            - s_e: encoder sequence length
+            - v: vocab size
+            - d: token embed dim
+            - d_e: encoder embed dim
+            - m_s: max seq len
+        """
+        # input tensor of shape [b, s]
+        bsz, seq_len = tokens.shape
+
+        if seq_len > self.max_seq_len:
+            raise ValueError(
+                f"seq_len ({seq_len}) of input tensor should be smaller "
+                f"than max_seq_len ({self.max_seq_len})"
+            )
+
+        # shape: [b, s, d]
+        h = self.tok_embeddings(tokens)
+
+        if self.decoder_caches_are_enabled:
+            if mask is None:
+                raise ValueError(
+                    "KV-caches for self-attention layers are setup for inference mode, masks must be provided!"
+                    " Use the `mask` arg to provide a mask."
+                )
+        if self.encoder_caches_are_enabled:
+            if encoder_mask is None:
+                raise ValueError(
+                    "KV-caches for cross-attention/fusion layers are setup for inference mode, encoder masks must be provided!"
+                    " Use the `encoder_mask` arg to provide an encoder mask."
+                )
+
+        if (
+            self.encoder_caches_are_enabled
+            or self.decoder_caches_are_enabled
+            and input_pos is None
+        ):
+            raise ValueError(
+                "KV-caches are setup for inference mode, input positions must be provided!"
+            )
+
+        hidden = []
+        for i, layer in enumerate(self.layers):
+            if i in self.output_hidden_states:
+                hidden.append(h)
+            # shape: [b, s, d]
+            h = layer(
+                h,
+                mask=mask,
+                encoder_input=encoder_input,
+                encoder_mask=encoder_mask,
+                input_pos=input_pos,
+            )
+
+        # shape: [b, s, d]
+        h = self.norm(h)
+
+        if self.num_output_chunks > 0:
+            output = self.chunked_output(h)
+        else:
+            # shape: [b, seq_len, out_dim]
+            output = F.linear(h, self.tok_embeddings.weight).float()
+
+        # Output list if hidden states are requested, otherwise just the output
+        # TODO: always output a list to have a consistent output type
+        output = output if not hidden else [*hidden, output]
+        return output
diff -ruN marc_original/third_party/torchtune/torchtune/modules/transforms/__init__.py marc/third_party/torchtune/torchtune/modules/transforms/__init__.py
--- marc_original/third_party/torchtune/torchtune/modules/transforms/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/transforms/__init__.py	2025-02-20 17:49:30.714026160 -0500
@@ -0,0 +1,13 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from torchtune.modules.transforms._transforms import Transform, VisionCrossAttentionMask
+
+
+__all__ = [
+    "Transform",
+    "VisionCrossAttentionMask",
+]
Binary files marc_original/third_party/torchtune/torchtune/modules/transforms/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/transforms/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/modules/transforms/__pycache__/_transforms.cpython-312.pyc and marc/third_party/torchtune/torchtune/modules/transforms/__pycache__/_transforms.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/modules/transforms/_transforms.py marc/third_party/torchtune/torchtune/modules/transforms/_transforms.py
--- marc_original/third_party/torchtune/torchtune/modules/transforms/_transforms.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/transforms/_transforms.py	2025-02-20 17:49:30.718026165 -0500
@@ -0,0 +1,188 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, List, Mapping, Optional, Protocol
+
+import torch
+
+
+class Transform(Protocol):
+    """
+    Loose interface for all data and model transforms. Transforms operate at the
+    sample level and perform operations on a sample dict, returning the updated dict.
+    For an example implementation of this protocol, see
+    :class:`~torchtune.modules.transforms.VisionCrossAttentionMask`.
+    """
+
+    def __call__(self, sample: Mapping[str, Any]) -> Mapping[str, Any]:
+        pass
+
+
+class VisionCrossAttentionMask(Transform):
+    """
+    Computes the cross-attention mask for text + image inputs. Text tokens that
+    participate in cross-attention with an image token will show True in the mask
+    and follow the interleaved structure laid out in Fig. 7 of the Flamingo paper
+    (https://arxiv.org/pdf/2204.14198):
+
+        (1) Text tokens immediately following the image token up until the next image token
+        (2) Consecutive image tokens attend to subsequent text tokens
+
+    ::
+
+                       
+        img1                                      
+                       
+                       
+        img2                                       
+                       
+                       
+        img3                                       
+                       
+            <img1> <img2>These  are   two  dogs. <img3> This   is    a    cat.
+
+
+
+    Resultant mask is constructed per image and is of shape (text_seq_len, image_seq_len),
+    where True indicates that the token outputted from the image encoder attends
+    to the token in the text sequence in cross-attention. A list of these masks
+    are returned with length equal to number of images in the sample.
+
+    Args:
+        tile_size (int): The size of the image tiles from the image transform
+        patch_size (int): The size of each patch. Used to divide the tiles into patches.
+            E.g. for patch_size = 40, a tile of shape (400, 400) will have 10x10 grid of patches
+            with shape (40, 40) each.
+        image_token_id (int): Token ID of the image special token.
+        max_num_tiles (Optional[int]): Maximum number of tiles in an image, used to
+            pad mask during inference. Defaults to None
+    """
+
+    def __init__(
+        self,
+        tile_size: int,
+        patch_size: int,
+        image_token_id: int,
+        max_num_tiles: Optional[int] = None,
+    ):
+        patch_grid_size = tile_size // patch_size
+        self.patches_per_tile = patch_grid_size**2
+        self.image_token_id = image_token_id
+        self.max_num_tiles = max_num_tiles
+
+    def _get_image_attention_intervals(self, tokens: List[int]) -> List[List[int]]:
+        """
+        Returns a list of lists of the form [start, end) where start is the index
+        of the current image token and end is the index of the next image token, exclusive.
+
+        Args:
+            tokens (List[int]): List of token IDs in the text sequence
+
+        Returns:
+            List[List[int]]: List of lists of the form [start, end) indicating
+                range of positions in text sequence that should attend to the image
+
+        Example:
+            >>> text = "<img1><img2>These are two dogs. <img3>This is a cat."
+            >>> image_token_id = 1
+            >>> tokens = [1, 1, 9673, 527, 1403, 12875, 13, 1, 1115, 374, 264, 8415]
+            >>> transform = VisionCrossAttentionMask(tile_size=400, patch_size=40, image_token_id=1)
+            >>> intervals = transform._get_image_attention_intervals(tokens)
+            >>> print(intervals)
+            [[0, 7], [1, 7], [7, 12]]
+        """
+        end = len(tokens)
+        vision_token_locations = [
+            i for i, token in enumerate(tokens) if token == self.image_token_id
+        ]
+        # Return empty list if there are no images
+        if len(vision_token_locations) == 0:
+            return []
+        # If there is only one image, it will attend to subsequent text until end
+        if len(vision_token_locations) == 1:
+            return [[vision_token_locations[0], end]]
+
+        # Construct intervals from previous image token to next image token
+        vision_masks = [
+            [tok_idx_prev, tok_idx_next]
+            # Offset by one to get consecutive indices
+            for tok_idx_prev, tok_idx_next in zip(
+                vision_token_locations[:-1], vision_token_locations[1:]
+            )
+        ]
+        # Last image will attend to subsequent text until end
+        vision_masks.append([vision_token_locations[-1], end])
+
+        # If there are consecutive vision tokens, they should all attend to the
+        # same subsequent text
+        last_mask_end = vision_masks[-1][1]
+        for vision_mask in vision_masks[::-1]:
+            if vision_mask[0] == vision_mask[1] - 1:
+                vision_mask[1] = last_mask_end
+            last_mask_end = vision_mask[1]
+        return vision_masks
+
+    def __call__(
+        self, sample: Mapping[str, Any], inference: bool = False
+    ) -> Mapping[str, Any]:
+        """
+        Generates the vision cross-attention mask for the given sample based on
+        the image token locations interleaved in the text sequence.
+
+        Args:
+            sample (Mapping[str, Any]): Sample dict containing the following keys:
+                - tokens (List[int]): List of token IDs in the text sequence. Number of
+                    image token IDs in the sequence must match the number of images.
+                - images (List[torch.Tensor]): List of image Tensors post-tiling of shape
+                    (n_tiles, c, h, w) each.
+            inference (bool): Whether the template is being used for inference or not.
+
+        Returns:
+            Mapping[str, Any]: sample with a new key encoder_mask, with a mask per image with shape
+                (text_seq_len, image_seq_len) where text_seq_len == len(tokens) and
+                image_seq_len == max_tiles * (patches_per_tile + 1). These masks get padded and concatenated
+                in the batch collator.
+
+        Raises:
+            RuntimeError: if the number of images in the batch does not match the number of image tokens in the batch.
+        """
+        tokens, images = sample["tokens"], sample["encoder_input"]["images"]
+        # One sample can have multiple images - verify the number of image tokens
+        # is the same
+        n_img = len(images)
+        intervals = self._get_image_attention_intervals(tokens)
+        if len(intervals) != n_img:
+            raise RuntimeError(
+                f"The number of image tokens ({len(intervals)}) does not match the number of images ({n_img})."
+            )
+
+        # Create mask for each individual image based on its number of tokens,
+        # which can vary based on number of tiles since they are not yet tile padded.
+        # The masks are padded and concatenated together in the batch collator
+        text_seq_len = len(tokens)
+        max_image_size = None
+        if inference and self.max_num_tiles is not None:
+            max_image_size = self.max_num_tiles * (self.patches_per_tile + 1)
+        masks = []
+        for image_num, interval in enumerate(intervals):
+            # Identify what part of text sequence should be attended
+            start, end = interval
+            # Compute this image's number of tokens based on num tiles, patches per tile
+            n_tiles = images[image_num].shape[0]
+            image_seq_len = n_tiles * (self.patches_per_tile + 1)  # +1 for CLS token
+            # Mask will be block of 1s at the corresponding interval in the text.
+            # It is not a causal block because all the image tokens correspond
+            # to a single image, so text tokens attend to all the image's tokens.
+            # The mask is text_seq_len x mask_image_size if defined, otherwise
+            # it uses current text/image sequence lengths.
+            mask = torch.zeros(
+                text_seq_len, max_image_size or image_seq_len, dtype=torch.bool
+            )
+            mask[start:end, :image_seq_len] = True
+            masks.append(mask)
+
+        sample.update({"encoder_mask": masks})
+        return sample
diff -ruN marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/get_canvas_best_fit.py marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/get_canvas_best_fit.py
--- marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/get_canvas_best_fit.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/get_canvas_best_fit.py	2025-02-20 17:49:30.726026179 -0500
@@ -0,0 +1,179 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+
+from collections import defaultdict
+from typing import List, Set, Tuple
+
+import torch
+
+logger = logging.getLogger(__name__)
+
+
+def get_canvas_best_fit(
+    image: torch.Tensor, possible_resolutions: torch.Tensor, resize_to_max_canvas: bool
+) -> Tuple[int, int]:
+    """
+    Determines the best canvas possible from a list of possible resolutions to
+    resize an image to, without distortion.
+
+    For each possible resolution, calculates the scaling factors for
+    width and height, and selects the smallest one, which is the limiting side.
+    E.g. if to match a canvas shape you have to upscale an image's height by 2x, and width by 1.5x,
+    then the maximum upscaling without distortion is min(2, 1.5) = 1.5.
+
+    If there are multiple canvases that satisfy the conditions,
+    we pick the one with the lowest area to minimize padding.
+
+    Args:
+        image (torch.Tensor): The image we want to fit into a canvas.
+        possible_resolutions (torch.Tensor): A tensor of shape (N, 2) where each
+            row represents a possible canvas.
+        resize_to_max_canvas (bool): If True, pick the canvas that allows maximum scaling.
+            If False, pick the canvas that minimizes downscaling, including no downscaling at all.
+
+    Returns:
+        Tuple[int, int]: The best resolution to fit the image into.
+
+    Examples:
+        >>> image = torch.rand(3, 200, 300)
+        >>> possible_resolutions = torch.tensor([
+        ...     [224, 672],
+        ...     [672, 224],
+        ...     [224, 448],
+        ...     [448, 224],
+        ...     [224, 224]
+        ... ])
+        >>> get_canvas_best_fit(image, possible_resolutions, resize_to_max_canvas=False)
+        (224, 448)
+
+        In the example above, we calculate the scaling factors for each possible resolution
+
+        >>> scale_height = torch.tensor([1.1200, 3.3600, 1.1200, 2.2400, 1.1200])
+        >>> scale_width = torch.tensor([2.2400, 0.7467, 1.4933, 0.7467, 0.7467])
+        >>> scales = torch.tensor([1.1200, 0.7467, 1.1200, 0.7467, 0.7467])
+
+        Two options have scaling_factor > 1, since resize_to_max_canvas is False, we pick the smallest
+
+        >>> upscaling_options = torch.tensor([1.1200, 1.1200])
+        >>> selected_scale = torch.tensor(1.1200)
+
+        There are two possible options, so we pick the one with the smallest area
+
+        >>> areas = torch.tensor([150528, 100352])  # for resolutions [672, 224] and [224, 448], respectively
+        >>> optimal_canvas = torch.tensor([224, 448])  # resolution with the smallest area
+    """
+
+    original_height, original_width = image.shape[-2:]
+
+    # possible resolutions heights/widths
+    target_heights, target_widths = (
+        possible_resolutions[:, 0],
+        possible_resolutions[:, 1],
+    )
+
+    # scaling factors to resize the image without distortion
+    scale_w = target_widths / original_width
+    scale_h = target_heights / original_height
+
+    # get limiting side scaling -> no distortion
+    scales = torch.where(scale_w > scale_h, scale_h, scale_w)
+
+    # filter only scales that allow upscaling
+    upscaling_options = scales[scales >= 1]
+    if len(upscaling_options) > 0:
+        if resize_to_max_canvas:
+            selected_scale = torch.max(upscaling_options)
+        else:
+            selected_scale = torch.min(upscaling_options)
+    else:
+        # no upscaling possible,
+        # get the minimum downscaling (max scale for scales<1)
+        downscaling_options = scales[scales < 1]
+        selected_scale = torch.max(downscaling_options)
+
+    # get all resolutions that support this scaling factor,
+    # e.g. you can upscale to 224x224, 224x448, 224x672 without distortion
+    chosen_canvas = possible_resolutions[scales == selected_scale]
+
+    # if there are multiple resolutions,
+    # get the one with minimum area to reduce padding
+    if len(chosen_canvas) > 1:
+        areas = chosen_canvas[:, 0] * chosen_canvas[:, 1]
+        optimal_idx = torch.argmin(areas)
+        optimal_canvas = chosen_canvas[optimal_idx]
+    else:
+        optimal_canvas = chosen_canvas[0]
+
+    return tuple(optimal_canvas.tolist())
+
+
+def find_supported_resolutions(
+    max_num_tiles: int, tile_size: int
+) -> List[Tuple[int, int]]:
+    """
+    Computes all combinations of resolutions, multiple of tile_size,
+    that contain up to max_num_tiles. Useful for when dividing an image into tiles.
+
+    For example, if we want at most 2 tiles per image, then we can support the
+    following resolutions: (1x1, 1x2, 2x1) * tile_size
+
+    Args:
+        max_num_tiles (int): Maximum number of tiles.
+        tile_size (int): Size of the side of the tile.
+
+    Returns:
+        List[Tuple[int, int]]: List of possible resolutions as tuples (height, width).
+
+    Examples:
+
+        >>> max_num_tiles = 4
+        >>> tile_size = 224
+        >>> find_supported_resolutions(max_num_tiles, tile_size)
+        [(224, 896), (448, 448), (224, 224), (896, 224), (224, 672), (672, 224), (224, 448), (448, 224)]
+    """
+
+    # create dictionary {aspect_ratio: [resolution1, ..., resolution n]}
+    # example {0.25: [(1,4)], 1.0: [(2,2), (1,1)], 4.0: [(4,1)]}
+    asp_dict = defaultdict(list)
+    for _tile_size in range(max_num_tiles, 0, -1):
+        factors = sorted(_get_factors(_tile_size))
+        asp_ratios = [(factor, _tile_size // factor) for factor in factors]
+        for height, width in asp_ratios:
+            ratio_float = height / width
+            asp_dict[ratio_float].append((height, width))
+
+    # get the resolutions multiplied by the tile_size
+    possible_resolutions = []
+    for ar, resolution in asp_dict.items():
+        for height, width in resolution:
+            possible_resolutions.append((height * tile_size, width * tile_size))
+
+    return possible_resolutions
+
+
+def _get_factors(n: int) -> Set[int]:
+    """
+    Calculate all factors of a given number, i.e. a divisor that leaves no remainder.
+
+    Args:
+        n (int): The number to find factors for.
+
+    Returns:
+        set: A set containing all factors of the number.
+
+    Examples:
+        >>> _get_factors(n=12)
+        {1, 2, 3, 4, 6, 12}
+    """
+    factors_set = set()
+
+    for i in range(1, int(n**0.5) + 1):
+        if n % i == 0:
+            factors_set.add(i)
+            factors_set.add(n // i)
+    return factors_set
diff -ruN marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/get_inscribed_size.py marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/get_inscribed_size.py
--- marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/get_inscribed_size.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/get_inscribed_size.py	2025-02-20 17:49:30.730026185 -0500
@@ -0,0 +1,78 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+
+import math
+
+from typing import Optional, Tuple
+
+logger = logging.getLogger(__name__)
+
+
+def get_inscribed_size(
+    image_size: Tuple[int], target_size: Tuple[int], max_size: Optional[int]
+) -> Tuple[int]:
+    """
+    Calculates the size of an image, if it was resized to be inscribed within the target_size.
+    It is upscaled or downscaled such that one size is equal to the target_size, and the second
+    size is less than or equal to the target_size.
+
+    The user can set max_size to limit upscaling along the larger dimension when target_size exceeds
+    the image_size.
+
+    Args:
+        image_size (Tuple[int]): The size of the image, in the form [height, width].
+        target_size (Tuple[int]): The desired resolution to fit the image into, in the format [height, width].
+        max_size (Optional[int]): The maximum size to upscale the image to.
+            If None, will upscale to target size.
+
+    Returns:
+        Tuple[int]: The resize dimensions for the image, in the format [height, width].
+
+    Examples:
+
+        Example 1: The image will be upscaled from (300, 800) to (448, 1194), since 448 is the limiting side.
+
+            >>> max_size = None
+            >>> image_size = (300, 800)
+            >>> target_size = (448, 1344)
+            >>> output = get_inscribed_size(image_size, target_size, max_size)
+
+        Example 2: The image will stay as is, since 800 > 600.
+
+            >>> max_size = 600
+            >>> image_size = (300, 800)
+            >>> target_size = (448, 1344)
+            >>> output = get_inscribed_size(image_size, target_size, max_size)
+
+        Example 3: The image will be downscaled from (500, 1000) to (224, 448).
+
+            >>> max_size = 600
+            >>> image_size = (500, 1000)
+            >>> target_size = (448, 488)
+            >>> output = get_inscribed_size(image_size, target_size, max_size)
+    """
+    assert len(image_size) == 2, "Image size must be a list of length 2."
+    assert len(target_size) == 2, "Canvas size must be a list of length 2."
+
+    image_height, image_width = image_size
+
+    # Bound target_size with max_size.
+    if max_size is not None:
+        target_height = min(max(image_height, max_size), target_size[0])
+        target_width = min(max(image_width, max_size), target_size[1])
+    else:
+        target_height, target_width = target_size
+
+    # Calculate the largest aspect ratio preserving size that fits target_size.
+    scale_h = target_height / image_height
+    scale_w = target_width / image_width
+
+    resize_height = min(math.floor(image_height * scale_w), target_height)
+    resize_width = min(math.floor(image_width * scale_h), target_width)
+
+    return (resize_height, resize_width)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/__init__.py marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/__init__.py
--- marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/__init__.py	2025-02-20 17:49:30.722026172 -0500
@@ -0,0 +1,5 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
diff -ruN marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/pad_dim_to_size.py marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/pad_dim_to_size.py
--- marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/pad_dim_to_size.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/pad_dim_to_size.py	2025-02-20 17:49:30.734026192 -0500
@@ -0,0 +1,46 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+import torch.nn.functional as F
+
+
+def pad_dim_to_size(
+    input: torch.Tensor, size: int, dim: int, *, fill: float = 0.0
+) -> torch.Tensor:
+    """
+    Pads the given dimension of the input to the given size.
+
+    Example:
+        >>> image = torch.rand(1, 4, 4)
+        >>> padded_image = pad_to_size(image, 3, dim=0)
+        >>> padded_image.shape
+        torch.Size([3, 4, 4])
+
+    Args:
+        input (torch.Tensor): Tensor to pad.
+        size (int): Size to pad to.
+        dim (int): Dimension to pad.
+        fill (float): Value to fill the padded region with. Default: 0.0
+
+    Returns:
+        torch.Tensor: Padded input.
+    """
+    pad_size = size - input.shape[dim]
+    assert (
+        pad_size >= 0
+    ), f"Tensor input shape {input.shape[dim]} is larger than given size {size}"
+
+    # Set up 0 padding for the entire tensor.
+    # Padding is in order W*H*C*N, with front and back for each dim.
+    # https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html
+    padding = [0] * 2 * input.dim()
+    # Find the pad_index: convert NCHW to WHCN, and only pad the back
+    # (not both sides).
+    pad_index = (input.dim() - dim) * 2 - 1
+    padding[pad_index] = pad_size
+    # Pad dim to size.
+    return F.pad(input, padding, value=fill)
diff -ruN marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/resize_with_pad.py marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/resize_with_pad.py
--- marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/resize_with_pad.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/resize_with_pad.py	2025-02-20 17:49:30.738026199 -0500
@@ -0,0 +1,169 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+
+import math
+from typing import Optional, Tuple
+
+import torch
+
+import torchvision
+from torchvision.transforms.v2 import functional as F
+
+logger = logging.getLogger(__name__)
+
+
+def resize_with_pad(
+    image: torch.Tensor,
+    target_size: Tuple[int, int],
+    resample: torchvision.transforms.InterpolationMode,
+    max_size: Optional[int] = None,
+) -> torch.Tensor:
+    """
+    Resizes and pads an image to target_size without causing distortion.
+    The user can set max_size to limit upscaling when target_size exceeds image_size.
+
+    Args:
+        image (torch.Tensor): The input image tensor in the format [..., H, W].
+        target_size (Tuple[int, int]): The desired resolution to fit the image into in the format [height, width].
+        resample (torchvision.transforms.InterpolationMode): Resampling method used when resizing images.
+            Supports torchvision.transforms.InterpolationMode.NEAREST, InterpolationMode.NEAREST_EXACT,
+            InterpolationMode.BILINEAR and InterpolationMode.BICUBIC.
+        max_size (Optional[int]): The maximum size to upscale the image to.
+            If None, will upscale up to target_size.
+
+    Returns:
+        torch.Tensor: The resized and padded image tensor in the format [..., H, W].
+
+    Examples:
+
+        Example 1: The image will be upscaled from (300, 800) to (448, 1194), since 448 is the limiting side,
+        and then padded from (448, 1194) to (448, 1344).
+
+            >>> max_size = None
+            >>> image = torch.rand([3, 300, 800])
+            >>> target_size = (448, 1344)
+            >>> resample = torchvision.transforms.InterpolationMode.BILINEAR
+            >>> output = resize_with_pad(image, target_size, resample, max_size)
+
+        Example 2: The image will stay as is, since 800 > 600, and then padded from (300, 800) to (448, 1344).
+
+            >>> max_size = 600
+            >>> image = torch.rand([3, 300, 800])
+            >>> target_size = (448, 1344)
+            >>> resample = torchvision.transforms.InterpolationMode.BILINEAR
+            >>> output = resize_with_pad(image, target_size, resample, max_size)
+
+        Example 3: The image will be downscaled from (500, 1000) to (224, 448),
+        and padded from (224, 448) to (448, 448).
+
+            >>> max_size = 600
+            >>> image = torch.rand([3, 500, 1000])
+            >>> target_size = (448, 488)
+            >>> resample = torchvision.transforms.InterpolationMode.BILINEAR
+            >>> output = resize_with_pad(image, target_size, resample, max_size)
+
+    """
+
+    image_height, image_width = image.shape[-2:]
+    image_size = (image_height, image_width)
+
+    # If target_size requires upscaling, we might want to limit the upscaling to max_size
+    if max_size is not None:
+        new_target_height = min(max(image_height, max_size), target_size[0])
+        new_target_width = min(max(image_width, max_size), target_size[1])
+        target_size_resize = (new_target_height, new_target_width)
+    else:
+        target_size_resize = target_size
+
+    # resize to target_size while preserving aspect ratio
+    new_size_preserving_aspect_ratio = _get_max_res_without_distortion(
+        image_size=image_size,
+        target_size=target_size_resize,
+    )
+
+    image = F.resize(
+        inpt=image,
+        size=list(new_size_preserving_aspect_ratio),
+        interpolation=resample,
+        antialias=True,
+    )
+
+    image = _pad_image_top_left(image=image, target_size=target_size)
+
+    return image
+
+
+def _pad_image_top_left(
+    image: torch.Tensor,
+    target_size: Tuple[int, int],
+) -> torch.Tensor:
+    """
+    Places the image at the top left of the canvas and pads with 0 the right and bottom
+    to fit to the target resolution. If target_size < image_size, it will crop the image.
+
+    Args:
+        image (torch.Tensor): The input image tensor in the format [..., H, W].
+        target_size (Tuple[int, int]): The desired resolution to fit the image into in the format [height, width].
+
+    Returns:
+        torch.Tensor: The padded image tensor in the format [..., H, W].
+    """
+
+    image_size = image.shape[-2:]
+
+    height, width = image_size
+    target_height, target_width = target_size
+
+    pad_x = target_width - width
+    pad_y = target_height - height
+
+    padding = [0, 0, pad_x, pad_y]
+    return F.pad(inpt=image, padding=padding)
+
+
+def _get_max_res_without_distortion(
+    image_size: Tuple[int, int],
+    target_size: Tuple[int, int],
+) -> Tuple[int, int]:
+    """
+    Determines the maximum resolution to which an image can be resized to without distorting its
+    aspect ratio, based on the target resolution.
+
+    For example, if image_size = (200,400) and target_size = (600,800),
+    scale_h = 600/200 = 3
+    scale_w = 800/400 = 2
+    So the maximum that we can upscale without distortion is min(scale_h, scale_w) = 2
+
+    Since scale_w is the limiting side, then new_w = target_w, and new_h = old_h*scale_w
+
+    Args:
+        image_size (Tuple[int, int]): The original resolution of the image.
+        target_size (Tuple[int, int]): The desired resolution to fit the image into.
+    Returns:
+        Tuple[int, int]: The optimal dimensions to which the image should be resized.
+    Examples:
+        >>> _get_max_res_without_distortion([200, 300], target_size = (450, 200))
+        (133, 200)
+        >>> _get_max_res_without_distortion([800, 600], target_size = (450, 1300))
+        (450, 337)
+    """
+
+    original_height, original_width = image_size
+    target_height, target_width = target_size
+
+    scale_w = target_width / original_width
+    scale_h = target_height / original_height
+
+    if scale_w < scale_h:
+        new_width = target_width
+        new_height = min(math.floor(original_height * scale_w), target_height)
+    else:
+        new_height = target_height
+        new_width = min(math.floor(original_width * scale_h), target_width)
+
+    return new_height, new_width
diff -ruN marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/tile_crop.py marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/tile_crop.py
--- marc_original/third_party/torchtune/torchtune/modules/transforms/vision_utils/tile_crop.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/transforms/vision_utils/tile_crop.py	2025-02-20 17:49:30.742026205 -0500
@@ -0,0 +1,59 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+
+import torch
+
+logger = logging.getLogger(__name__)
+
+
+def tile_crop(image: torch.Tensor, tile_size: int) -> torch.Tensor:
+    """
+    Divides a tensor into equally sized tiles. The tensor should be divisible by tile_size.
+
+    Args:
+        image (torch.Tensor): Input image to crop into tiles.
+        tile_size (int): Size of each tile.
+
+    Returns:
+        torch.Tensor: torch.Tensor of shape [num_tiles, channel_size, tile_size, tile_size]
+
+    Examples:
+        >>> image = torch.rand(3, 200, 300)
+        >>> tiles = tile_crop(image, tile_size=50)
+        >>> tiles.shape # 4x6 = 24 tiles
+        torch.Size([24, 3, 50, 50])
+
+        >>> image = torch.rand(3, 400, 600)
+        >>> tiles = tile_crop(image, tile_size=200)
+        >>> tiles.shape # 2x3 = 6 tiles
+        torch.Size([6, 3, 200, 200])
+    """
+
+    channel_size, height, width = image.shape
+
+    # assert sizes are divisible
+    assert (
+        height % tile_size == 0 and width % tile_size == 0
+    ), f"Image size {height}x{width} is not divisible by tile size {tile_size}"
+
+    # Reshape to split height and width into tile_size blocks
+    tiles_height = height // tile_size
+    tiles_width = width // tile_size
+
+    reshaped = image.view(channel_size, tiles_height, tile_size, tiles_width, tile_size)
+
+    # Transpose to bring tiles together
+    # We want [tiles_height, tiles_width, channel_size, tile_size, tile_size]
+    transposed = reshaped.permute(1, 3, 0, 2, 4)
+
+    # Flatten the tiles
+    tiles = transposed.contiguous().view(
+        tiles_height * tiles_width, channel_size, tile_size, tile_size
+    )
+
+    return tiles
diff -ruN marc_original/third_party/torchtune/torchtune/modules/vision_transformer.py marc/third_party/torchtune/torchtune/modules/vision_transformer.py
--- marc_original/third_party/torchtune/torchtune/modules/vision_transformer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/modules/vision_transformer.py	2025-02-20 17:49:30.746026212 -0500
@@ -0,0 +1,459 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional, Tuple
+
+import torch
+from torch import nn
+
+from torchtune.modules import Fp32LayerNorm
+from torchtune.modules.transformer import _get_clones
+
+
+class VisionTransformer(nn.Module):
+    """
+    Implementation of the ViT architecture (https://arxiv.org/abs/2010.11929),
+    with support for tile-cropped images, outputting of hidden layers and optional CLS projection.
+
+    ViT is a transformer architecture that takes in images and outputs N embedded tokens that
+    represent this image. Each image is divided into **patches** by a convolution.
+    These patches are flattened and subsequently treated as **tokens** by the transformer.
+
+    To further enhance the performance of ViT and avoid downscaling images, we support tile-cropped images,
+    which are images divided into **tiles** during the preprocessing stage. For example, instead of
+    downscaling an 800x400 image to fit 400x400, we may crop it into two 400x400 tiles,
+    if the ``tile_size=400``. For details on preprocessing, please refer to
+    :class:`torchtune.models.clip._transforms.CLIPImageTransform`.
+
+    Each of these tiles is further broken down into patches by a convolution operation. For example, if
+    your ``patch_size=40``, then each (400, 400) tile will become a grid of 10x10 patches, and your whole image will have
+    num_tiles * n_tokens -> num_tiles * (10x10 patches + 1 CLS token) -> num_tiles * 101.
+
+    Before the transformer layers, a CLS token is added to each tile as the first token.
+    In transformers, a token called CLS is a special token that is added to the beginning of each sequence.
+    This token can be used to represent the whole input, instead of using a pooling operation, for example.
+
+    To help the model "see" the whole image, we use positional embeddings. If your image
+    was tile-cropped, then you need to use tile positional embeddings:
+
+    - token_pos_embedding (tiled): :class:`torchtune.models.clip._position_embeddings.TiledTokenPositionalEmbedding`
+    - pre_tile_pos_embed: :class:`torchtune.models.clip._position_embeddings.TilePositionalEmbedding`
+    - post_tile_pos_embed: :class:`torchtune.models.clip._position_embeddings.TilePositionalEmbedding`
+
+    Otherwise, pre and post tile_pos_embed should be None and all you need is a simple
+    token positional embedding:
+
+    - token_pos_embedding (not tiled): :class:`torchtune.models.clip._position_embeddings.TokenPositionalEmbedding`
+
+    All images will be considered as a stack of tiles, even if your image was not tile-cropped. In such cases,
+    your image would be composed of a single tile.
+
+    In summary:
+
+    1) An image is broken down into tiles during preprocessing.
+    2) In the ViT, the tiles will be broken down into patches.
+    3) The patches will be flattened and transformed. We call them tokens, because that's how the transformer sees them.
+
+
+    Image: shape (8x8)
+
+    .. code-block:: text
+
+        |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |
+        |  9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |
+        | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 |
+        | 25 | 26 | 27 | 28 | 29 | 30 | 31 | 32 |
+        | 33 | 34 | 35 | 36 | 37 | 38 | 39 | 40 |
+        | 41 | 42 | 43 | 44 | 45 | 46 | 47 | 48 |
+        | 49 | 50 | 51 | 52 | 53 | 54 | 55 | 56 |
+        | 57 | 58 | 59 | 60 | 61 | 62 | 63 | 64 |
+
+    Tiles: shape (4,4,4) # (num_tiles, tile_size, tile_size)
+
+    .. code-block:: text
+
+        |  1 |  2 |  3 |  4 |    |  5 |  6 |  7 |  8 |
+        |  9 | 10 | 11 | 12 |    | 13 | 14 | 15 | 16 |
+        | 17 | 18 | 19 | 20 |    | 21 | 22 | 23 | 24 |
+        | 25 | 26 | 27 | 28 |    | 29 | 30 | 31 | 32 |
+
+        | 33 | 34 | 35 | 36 |    | 37 | 38 | 39 | 40 |
+        | 41 | 42 | 43 | 44 |    | 45 | 46 | 47 | 48 |
+        | 49 | 50 | 51 | 52 |    | 53 | 54 | 55 | 56 |
+        | 57 | 58 | 59 | 60 |    | 61 | 62 | 63 | 64 |
+
+    Patches: shape (4,4,2,2) # (num_tiles, num_patches_per_tile, patch_size, patch_size)
+
+    .. code-block:: text
+
+        |  1 |  2 |    |  3 |  4 |    |  5 |  6 |    |  7 |  8 |
+        |  9 | 10 |    | 11 | 12 |    | 13 | 14 |    | 15 | 16 |
+
+        | 17 | 18 |    | 19 | 20 |    | 21 | 22 |    | 23 | 24 |
+        | 25 | 26 |    | 27 | 28 |    | 29 | 30 |    | 31 | 32 |
+
+        | 33 | 34 |    | 35 | 36 |    | 37 | 38 |    | 39 | 40 |
+        | 41 | 42 |    | 43 | 44 |    | 45 | 46 |    | 47 | 48 |
+
+        | 49 | 50 |    | 51 | 52 |    | 53 | 54 |    | 55 | 56 |
+        | 57 | 58 |    | 59 | 60 |    | 61 | 62 |    | 63 | 64 |
+
+    token: shape (4, 4, 4) # (num_tiles, num_patches_per_tile, emb_dim)
+
+    .. code-block:: text
+
+        |  1 |  2 |  9 |  10 |    |  3 |  4 |  11 |  12 |    |  17 |  18 |  25 |  26 |    | 19 | 20 |  27 |  28 |
+        | ... continuation of data ...
+        | ... continuation of data ...
+        | 37 | 38 | 45 |  46 |    | 39 |  40 | 47 |  48 |    | 53 | 54 |  61 |  62 |    | 55 | 56 |  63 |  64 |
+
+    For the positional embeddings:
+
+    Same for every tile, different for every token.
+
+    - :class:`torchtune.models.clip._position_embeddings.TokenPositionalEmbedding`
+    - :class:`torchtune.models.clip._position_embeddings.TiledTokenPositionalEmbedding`
+
+    .. code-block:: text
+
+        |  1 |  2 |  3 |  4 |    |  1 |  2 |  3 |  4 |
+        |  9 | 10 | 11 | 12 |    |  9 | 10 | 11 | 12 |
+        | 17 | 18 | 19 | 20 |    | 17 | 18 | 19 | 20 |
+        | 25 | 26 | 27 | 28 |    | 25 | 26 | 27 | 28 |
+
+        |  1 |  2 |  3 |  4 |    |  1 |  2 |  3 |  4 |
+        |  9 | 10 | 11 | 12 |    |  9 | 10 | 11 | 12 |
+        | 17 | 18 | 19 | 20 |    | 17 | 18 | 19 | 20 |
+        | 25 | 26 | 27 | 28 |    | 25 | 26 | 27 | 28 |
+
+    Different for every tile, different for every token.
+
+    - :class:`torchtune.models.clip._position_embeddings.TiledTokenPositionalEmbedding`
+
+    .. code-block:: text
+
+        |  1 |  2 |    |  3 |  4 |    |  5 |  6 |    |  7 |  8 |
+        |  9 | 10 |    | 11 | 12 |    | 13 | 14 |    | 15 | 16 |
+
+        | 17 | 18 |    | 19 | 20 |    | 21 | 22 |    | 23 | 24 |
+        | 25 | 26 |    | 27 | 28 |    | 29 | 30 |    | 31 | 32 |
+
+        | 33 | 34 |    | 35 | 36 |    | 37 | 38 |    | 39 | 40 |
+        | 41 | 42 |    | 43 | 44 |    | 45 | 46 |    | 47 | 48 |
+
+        | 49 | 50 |    | 51 | 52 |    | 53 | 54 |    | 55 | 56 |
+        | 57 | 58 |    | 59 | 60 |    | 61 | 62 |    | 63 | 64 |
+
+    different for every tile, same for every token within a tile.
+
+    - :class:`torchtune.models.clip._position_embeddings.TilePositionalEmbedding`
+
+    .. code-block:: text
+
+        |  1 |  1 |  1 |  1 |    |  2 |  2 |  2 |  3 |
+        |  1 |  1 |  1 |  1 |    |  2 |  2 |  2 |  3 |
+        |  1 |  1 |  1 |  1 |    |  2 |  2 |  2 |  3 |
+        |  1 |  1 |  1 |  1 |    |  2 |  2 |  2 |  3 |
+
+        |  3 |  3 |  3 |  3 |    |  4 |  4 |  4 |  4 |
+        |  3 |  3 |  3 |  3 |    |  4 |  4 |  4 |  4 |
+        |  3 |  3 |  3 |  3 |    |  4 |  4 |  4 |  4 |
+        |  3 |  3 |  3 |  3 |    |  4 |  4 |  4 |  4 |
+
+    Args:
+        patch_size (int): The size of each patch. Used to divide the tiles into patches.
+            E.g. for ``patch_size=40``, a tile of shape (400, 400) will have 10x10 grid of patches.
+        tile_size (int): The size of your image tiles, if the image was tile-cropped in advance. Otherwise,
+            the size of the input image. In this case, the function will consider your image as a single tile.
+            with shape (40, 40) each.
+        num_layers (int): The number of transformer layers.
+        embed_dim (int): The dimensionality of each patch embedding (token).
+        layer (nn.Module): The transformer layer module.
+        token_pos_embedding (nn.Module): The token positional embedding module.
+        pre_tile_pos_embed (Optional[nn.Module]): The pre-tile positional embedding module. It should be
+            None if your image was not tile-cropped in advance.
+        post_tile_pos_embed (Optional[nn.Module]): The post-tile positional embedding module. It should be
+            None if your image was not tile-cropped in advance.
+        cls_projection (Optional[nn.Module]): The CLS projection module. It should take an input tensor
+            of shape (bsz * n_tiles, n_tokens, embed_dim) and output a tensor of shape
+            (bsz * n_tiles, cls_output_dim). If provided, only the CLS token projection will be
+            outputted, instead of all tokens.
+        out_indices (Optional[List[int]]): The indices of hidden layers to return.
+            If provided, it will return the intermediate results of the transformer layers
+            before they go through a next layer. For example, ``out_indices=[0,3]`` will
+            return the tokens before they go through the first and fourth layers.
+        in_channels (int): The number of image input channels.
+
+    Raises:
+        ValueError: If `tile_size` is not greater than 0.
+        ValueError: If `patch_size` is not greater than 0.
+        ValueError: If `len(out_indices)` is greater than `num_layers`.
+    """
+
+    def __init__(
+        self,
+        patch_size: int,
+        tile_size: int,
+        num_layers: int,
+        embed_dim: int,
+        layer: nn.Module,
+        token_pos_embedding: nn.Module,
+        pre_tile_pos_embed: Optional[nn.Module] = None,
+        post_tile_pos_embed: Optional[nn.Module] = None,
+        cls_projection: Optional[nn.Module] = None,
+        out_indices: Optional[List[int]] = None,
+        in_channels: int = 3,
+    ) -> None:
+        super().__init__()
+
+        if tile_size <= 0:
+            raise ValueError("tile_size must be > 0")
+        if patch_size <= 0:
+            raise ValueError("patch_size must be > 0")
+        if out_indices and (len(out_indices) > num_layers):
+            raise ValueError(
+                f"len(out_indices) must be <= num_layers. Got {out_indices=} and {num_layers=}"
+            )
+
+        # constants
+        patch_grid_size = tile_size // patch_size
+        self.patches_per_tile = patch_grid_size**2
+        self.out_indices = out_indices
+        if not out_indices:
+            self.out_indices = []
+
+        # input modules
+        self.pre_tile_pos_embed = pre_tile_pos_embed
+        self.post_tile_pos_embed = post_tile_pos_embed
+        self.token_pos_embedding = token_pos_embedding
+
+        self.cls_projection = cls_projection
+        self.layers = _get_clones(layer, num_layers)
+
+        # other modules
+        self.conv = nn.Conv2d(
+            in_channels=in_channels,
+            out_channels=embed_dim,
+            kernel_size=(patch_size, patch_size),
+            stride=(patch_size, patch_size),
+            bias=False,
+        )
+
+        self.ln_post = Fp32LayerNorm(embed_dim)
+        self.ln_pre = Fp32LayerNorm(embed_dim)
+
+        self.cls_token_embedding = CLSEmbedding(embed_dim)
+
+    def get_image_tokens_per_tile(self):
+        return self.patches_per_tile + 1  # +1 for CLS token
+
+    def forward(
+        self,
+        images: torch.Tensor,
+        aspect_ratio: Optional[torch.Tensor] = None,
+    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:
+        """
+        Processes images and returns the tokens and hidden states.
+
+        Multiple images per sample: we add a dimension n_imgs to the input. This is useful when a single
+        sample constains multiple images, for example:
+
+        - sample 1: "<image> what animal is this?"
+        - sample 2: "I like <image> more than <image>"
+
+        In this case, sample 1 has one image, and sample 2 has two images. max_n_imgs = max(2,1) = 2.
+        So your input should have shape (bsz=2, n_imgs=2, num_tiles, n_channels, tile_size, tile_size).
+
+        Notice that to batch it, you will have to pad n_imgs to max_n_imgs and max_num_tiles.
+
+        Args:
+            images (torch.Tensor): torch.Tensor with shape (bsz, n_imgs, n_tiles, n_channels, tile_size, tile_size).
+            aspect_ratio (Optional[torch.Tensor]): torch.Tensor with shape (bsz, n_imgs, 2). If all
+                images have a single tile, i.e. they were not tile-cropped, it should be None.
+                Used to calculate the positional embeddings for the tiles.
+
+        Returns:
+            Tuple[torch.Tensor, List[torch.Tensor]]: A tuple: (x, hidden_states),
+                where x is a torch.tensor of shape (bsz, n_imgs, n_tiles, n_tokens, embed_dim) and
+                hidden_states has shape is a list of len(out_indices) torch.tensor with shape
+                (bsz, n_imgs, n_tiles, n_tokens, embed_dim).
+
+        Raises:
+            ValueError: If aspect_ratio is None, but n_tiles > 1 in the batch.
+
+        Examples:
+
+            >>> from torchtune.modules.transforms.vision_utils.tile_crop import tile_crop
+            >>> from torchtune.modules import VisionTransformer
+            >>>
+            >>> num_channels = 3
+            >>> image_size = (800,400)
+            >>> tile_size = 400
+            >>> patch_size=40
+            >>> patch_grid_size = tile_size // patch_size
+            >>>
+            >>> # for details about preprocessing, please check
+            >>> # torchtune.models.clip._transforms.CLIPImageTransform
+            >>>
+            >>> # create a random image
+            >>> image = torch.rand(num_channels, image_size[0], image_size[1])
+            >>>
+            >>> # (num_tiles, nch, h, w) -> (2, 3, 400, 400)
+            >>> tile_cropped_image = tile_crop(image, tile_size)
+            >>> aspect_ratio = torch.tensor([2,1])
+            >>>
+            >>> # make it a batch of 1 image
+            >>> batch_image = tile_cropped_image.unsqueeze(0)
+            >>> batch_aspect_ratio = aspect_ratio.unsqueeze(0)
+            >>>
+            >>> # make it have only 1 image per sample
+            >>> batch_image = tile_cropped_image.unsqueeze(1)
+            >>> batch_aspect_ratio = aspect_ratio.unsqueeze(1)
+            >>>
+            >>> # For a detailed example, please check
+            >>> # torchtune.models.clip._position_embeddings.clip_vision_encoder
+            >>> # model = VisionTransformer(
+            ... #           out_indices = [1,2,3,4,5],
+            ... #           patch_size=40,
+            ... #           patch_grid_size = patch_grid_size,
+            ... #           embed_dim = 32,
+            ... #           num_layers = 6,
+            ... #           in_channels = num_channels,
+            ... #           ...)
+            >>>
+            >>> x, hidden_states = model(images = batch_image, aspect_ratio = batch_aspect_ratio)
+            >>>
+            >>> # (bsz, n_imgs, num_tiles, num_patches_per_tile + CLS token, embed_dim)
+            >>> print(x.shape)
+            torch.Size([1, 1, 2, 101, 32])
+            >>>
+            >>> # list with tensors of shape (bsz, n_imgs, num_tiles, num_patches_per_tile + CLS token, embed_dim)
+            >>> print(len(hidden_states))
+            5
+        """
+        hidden_states = []
+
+        # parse inputs
+        bsz, n_imgs, n_tiles, nch, w, h = images.shape
+        bsz_and_n_imgs = bsz * n_imgs
+
+        # if aspect_ratio is not provided, it defaults to one tile [1,1]
+        if aspect_ratio is None:
+            aspect_ratio = torch.ones(
+                (bsz_and_n_imgs, 2), dtype=torch.int, device=images.device
+            )
+            if n_tiles > 1:
+                raise ValueError(
+                    f"aspect_ratio was not provided, but found n_tiles>1 for {images.shape=}. Please provide aspect_ratio."
+                )
+
+        images = images.reshape(bsz_and_n_imgs * n_tiles, nch, w, h)
+        aspect_ratio = aspect_ratio.reshape(bsz_and_n_imgs, 2)
+
+        # patch embeddings (tokens)
+        # A tile becomes a grid of patch_grid_size X patch_grid_size patches
+        # these patches are flatenned, and called tokens from here on.
+
+        # out: (bsz * n_imgs * n_tiles, embed_dim, patch_grid_size, patch_grid_size)
+        x = self.conv(images)
+
+        # out: (bsz * n_imgs, n_tiles, n_tokens, embed_dim)
+        x = x.reshape(bsz_and_n_imgs, n_tiles, -1, self.patches_per_tile).permute(
+            0, 1, 3, 2
+        )
+        bsz_and_n_imgs, n_tiles, n_tokens, embed_dim = x.shape
+
+        # pre_tile_pos_embed
+        if self.pre_tile_pos_embed:
+            x = self.pre_tile_pos_embed(x, aspect_ratio)
+
+        # insert cls token
+        x = self.cls_token_embedding(x)
+        n_tokens += 1
+
+        # token_pos_embedding
+        x = self.token_pos_embedding(x, aspect_ratio)
+
+        # norm
+        x = self.ln_pre(x)
+
+        # transformer with optional hidden layer outputs
+        x = x.reshape(bsz_and_n_imgs, n_tiles * n_tokens, embed_dim)
+        for layer_idx, transformer_layer in enumerate(self.layers):
+            if layer_idx in self.out_indices:
+                h = x.reshape(bsz, n_imgs, n_tiles, n_tokens, embed_dim)
+                hidden_states.append(h)
+            x = transformer_layer(x)
+
+        # norm
+        x = self.ln_post(x)
+
+        # post_tile_pos_embed
+        if self.post_tile_pos_embed:
+            x = x.reshape(bsz_and_n_imgs, n_tiles, n_tokens, embed_dim)
+            x = self.post_tile_pos_embed(x, aspect_ratio)
+
+        # reshape output
+        x = x.reshape(bsz, n_imgs, n_tiles, n_tokens, embed_dim)
+
+        # cls token projection. n_tokens becomes 1
+        if self.cls_projection:
+            x = self.cls_projection(x)
+
+        return x, hidden_states
+
+
+class CLSEmbedding(nn.Module):
+    """
+    Adds a CLS token to every tile in an image.
+
+    Notice that tile is different from patch (token). An image is divided into tiles during pre-processing,
+    and patches are the outcome of the convolution in the ViT applied to each tile.
+
+    Args:
+        embed_dim (int): The dimensionality of the input patch embedding.
+    """
+
+    def __init__(self, embed_dim: int) -> None:
+        super().__init__()
+
+        scale = embed_dim**-0.5
+        self.weight = nn.Parameter(scale * torch.randn(embed_dim))
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+
+        # add 1 CLS token to every tile
+        bsz_and_n_imgs, n_tiles, n_tokens, embed_dim = x.shape
+        cls_emb = self.weight.broadcast_to(bsz_and_n_imgs, n_tiles, 1, embed_dim)
+        return torch.cat([cls_emb, x], dim=2)
+
+
+class CLSProjection(nn.Module):
+    """
+    Linear projection of the CLS token.
+
+    Args:
+        embed_dim (int): The dimensionality of the input patch embedding.
+        cls_output_dim (int): The dimensionality of the output projection.
+    """
+
+    def __init__(self, embed_dim: int, cls_output_dim: int) -> None:
+        super().__init__()
+
+        scale = embed_dim**-0.5
+        self.cls_output_dim = cls_output_dim
+        self.weight = nn.Parameter(scale * torch.randn(embed_dim, cls_output_dim))
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        bsz, n_imgs, n_tiles, n_tokens, embed_dim = x.shape
+        x = x.reshape(bsz * n_imgs * n_tiles, n_tokens, embed_dim)
+
+        # out: (bsz * n_tiles, cls_output_dim)
+        x = x[:, 0, :] @ self.weight
+
+        # num_tokens becomes 1 because we only return the CLS token projection
+        x = x.reshape(bsz, n_imgs, n_tiles, 1, self.cls_output_dim)
+        return x
Binary files marc_original/third_party/torchtune/torchtune/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/__pycache__/recipe_interfaces.cpython-312.pyc and marc/third_party/torchtune/torchtune/__pycache__/recipe_interfaces.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/__pycache__/_recipe_registry.cpython-312.pyc and marc/third_party/torchtune/torchtune/__pycache__/_recipe_registry.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/recipe_interfaces.py marc/third_party/torchtune/torchtune/recipe_interfaces.py
--- marc_original/third_party/torchtune/torchtune/recipe_interfaces.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/recipe_interfaces.py	2025-02-20 17:49:30.750026218 -0500
@@ -0,0 +1,87 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Protocol
+
+
+class FTRecipeInterface(Protocol):
+    """
+    This class provides a loose structure which every LLM fine-tuning recipe
+    should follow. Please note that the interface itself should not be a vehicle for
+    code reuse. torchtune strictly prohibits implementation inheritance in the codebase.
+
+    A few notes about the design and the need for this interface:
+    - This interface is meant to help recipe-writers organize their code in a way
+        which is easy to read, understand and extend. Minimizing code duplication is not
+        the goal. Recipe-writers are encouraged to copy-paste-modify.
+
+    - This interface is not meant to add constraints. If the interface comes in the
+        way of doing stuff, it needs to be updated or a new interface should be
+        written to support what might be a new "family" of recipes.
+    """
+
+    def load_checkpoint(self, **kwargs) -> None:
+        """
+        Responsible for loading ALL of the state for the recipe from the
+        checkpoint file, including state for the model, optimizer, dataloader and training
+        parameters such as the epoch and seed.
+        """
+        ...
+
+    def setup(self, **kwargs) -> None:
+        """
+        Responsible for setting up all of the components necessary for training. This includes
+        model, optimizer, loss function and dataloader.
+        """
+        ...
+
+    def train(self, **kwargs) -> None:
+        """
+        All of the training logic, including the core loop, loss computation, gradient
+        accumulation, and backward.
+        """
+        ...
+
+    def save_checkpoint(self, **kwargs) -> None:
+        """
+        Responsible for saving ALL of the state for the recipe,
+        including state for the model, optimizer, dataloader and training
+        parameters such as the epoch and seed.
+        """
+        ...
+
+    def cleanup(self, **kwargs) -> None:
+        """
+        Any cleaning up needed for the recipe.
+        """
+        ...
+
+
+class EvalRecipeInterface(Protocol):
+    """
+    This class provides a loose structure which every LLM evaluation recipe
+    should follow. Please note that the interface itself should not be a vehicle for
+    code reuse. torchtune strictly prohibits implementation inheritance in the codebase.
+    """
+
+    def load_checkpoint(self, **kwargs) -> None:
+        """
+        Responsible for loading ALL of the state for the recipe from the
+        checkpoint file.
+        """
+        ...
+
+    def setup(self, **kwargs) -> None:
+        """
+        Responsible for setting up all of the components necessary for evaluation.
+        """
+        ...
+
+    def evaluate(self, **kwargs) -> None:
+        """
+        All of the evaluation logic, including reporting.
+        """
+        ...
diff -ruN marc_original/third_party/torchtune/torchtune/_recipe_registry.py marc/third_party/torchtune/torchtune/_recipe_registry.py
--- marc_original/third_party/torchtune/torchtune/_recipe_registry.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/_recipe_registry.py	2025-02-20 17:49:30.206025324 -0500
@@ -0,0 +1,383 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from dataclasses import dataclass
+from typing import List
+
+
+@dataclass
+class Config:
+    name: str
+    file_path: str
+
+
+@dataclass
+class Recipe:
+    name: str
+    file_path: str
+    configs: List[Config]
+    supports_distributed: bool
+
+
+_ALL_RECIPES = [
+    Recipe(
+        name="full_finetune_single_device",
+        file_path="full_finetune_single_device.py",
+        configs=[
+            Config(
+                name="llama2/7B_full_low_memory",
+                file_path="llama2/7B_full_low_memory.yaml",
+            ),
+            Config(
+                name="code_llama2/7B_full_low_memory",
+                file_path="code_llama2/7B_full_low_memory.yaml",
+            ),
+            Config(
+                name="llama3/8B_full_single_device",
+                file_path="llama3/8B_full_single_device.yaml",
+            ),
+            Config(
+                name="llama3_1/8B_full_single_device",
+                file_path="llama3_1/8B_full_single_device.yaml",
+            ),
+            Config(
+                name="llama3_2/1B_full_single_device",
+                file_path="llama3_2/1B_full_single_device.yaml",
+            ),
+            Config(
+                name="llama3_2/3B_full_single_device",
+                file_path="llama3_2/3B_full_single_device.yaml",
+            ),
+            Config(
+                name="mistral/7B_full_low_memory",
+                file_path="mistral/7B_full_low_memory.yaml",
+            ),
+            Config(
+                name="phi3/mini_full_low_memory",
+                file_path="phi3/mini_full_low_memory.yaml",
+            ),
+            Config(
+                name="qwen2/7B_full_single_device",
+                file_path="qwen2/7B_full_single_device.yaml",
+            ),
+            Config(
+                name="qwen2/0.5B_full_single_device",
+                file_path="qwen2/0.5B_full_single_device.yaml",
+            ),
+            Config(
+                name="qwen2/1.5B_full_single_device",
+                file_path="qwen2/1.5B_full_single_device.yaml",
+            ),
+            Config(
+                name="llama3_2_vision/11B_full_single_device",
+                file_path="llama3_2_vision/11B_full_single_device.yaml",
+            ),
+        ],
+        supports_distributed=False,
+    ),
+    Recipe(
+        name="full_finetune_distributed",
+        file_path="full_finetune_distributed.py",
+        configs=[
+            Config(name="llama2/7B_full", file_path="llama2/7B_full.yaml"),
+            Config(name="llama2/13B_full", file_path="llama2/13B_full.yaml"),
+            Config(name="llama3/8B_full", file_path="llama3/8B_full.yaml"),
+            Config(name="llama3_1/8B_full", file_path="llama3_1/8B_full.yaml"),
+            Config(name="llama3_2/1B_full", file_path="llama3_2/1B_full.yaml"),
+            Config(name="llama3_2/3B_full", file_path="llama3_2/3B_full.yaml"),
+            Config(name="llama3/70B_full", file_path="llama3/70B_full.yaml"),
+            Config(name="llama3_1/70B_full", file_path="llama3_1/70B_full.yaml"),
+            Config(name="mistral/7B_full", file_path="mistral/7B_full.yaml"),
+            Config(name="gemma/2B_full", file_path="gemma/2B_full.yaml"),
+            Config(name="gemma/7B_full", file_path="gemma/7B_full.yaml"),
+            Config(name="phi3/mini_full", file_path="phi3/mini_full.yaml"),
+            Config(name="qwen2/7B_full", file_path="qwen2/7B_full.yaml"),
+            Config(name="qwen2/0.5B_full", file_path="qwen2/0.5B_full.yaml"),
+            Config(name="qwen2/1.5B_full", file_path="qwen2/1.5B_full.yaml"),
+            Config(
+                name="llama3_2_vision/11B_full",
+                file_path="llama3_2_vision/11B_full.yaml",
+            ),
+        ],
+        supports_distributed=True,
+    ),
+    Recipe(
+        name="lora_finetune_single_device",
+        file_path="lora_finetune_single_device.py",
+        configs=[
+            Config(
+                name="llama2/7B_lora_single_device",
+                file_path="llama2/7B_lora_single_device.yaml",
+            ),
+            Config(
+                name="llama2/7B_qlora_single_device",
+                file_path="llama2/7B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="code_llama2/7B_lora_single_device",
+                file_path="code_llama2/7B_lora_single_device.yaml",
+            ),
+            Config(
+                name="code_llama2/7B_qlora_single_device",
+                file_path="code_llama2/7B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="llama3/8B_lora_single_device",
+                file_path="llama3/8B_lora_single_device.yaml",
+            ),
+            Config(
+                name="llama3_1/8B_lora_single_device",
+                file_path="llama3_1/8B_lora_single_device.yaml",
+            ),
+            Config(
+                name="llama3/8B_qlora_single_device",
+                file_path="llama3/8B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="llama3_2/1B_lora_single_device",
+                file_path="llama3_2/1B_lora_single_device.yaml",
+            ),
+            Config(
+                name="llama3_2/3B_lora_single_device",
+                file_path="llama3_2/3B_lora_single_device.yaml",
+            ),
+            Config(
+                name="llama3/8B_dora_single_device",
+                file_path="llama3/8B_dora_single_device.yaml",
+            ),
+            Config(
+                name="llama3/8B_qdora_single_device",
+                file_path="llama3/8B_qdora_single_device.yaml",
+            ),
+            Config(
+                name="llama3_1/8B_qlora_single_device",
+                file_path="llama3_1/8B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="llama3_2/1B_qlora_single_device",
+                file_path="llama3_2/1B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="llama3_2/3B_qlora_single_device",
+                file_path="llama3_2/3B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="llama2/13B_qlora_single_device",
+                file_path="llama2/13B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="mistral/7B_lora_single_device",
+                file_path="mistral/7B_lora_single_device.yaml",
+            ),
+            Config(
+                name="mistral/7B_qlora_single_device",
+                file_path="mistral/7B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="gemma/2B_lora_single_device",
+                file_path="gemma/2B_lora_single_device.yaml",
+            ),
+            Config(
+                name="gemma/2B_qlora_single_device",
+                file_path="gemma/2B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="gemma/7B_lora_single_device",
+                file_path="gemma/7B_lora_single_device.yaml",
+            ),
+            Config(
+                name="gemma/7B_qlora_single_device",
+                file_path="gemma/7B_qlora_single_device.yaml",
+            ),
+            Config(
+                name="phi3/mini_lora_single_device",
+                file_path="phi3/mini_lora_single_device.yaml",
+            ),
+            Config(
+                name="phi3/mini_qlora_single_device",
+                file_path="phi3/mini_qlora_single_device.yaml",
+            ),
+            Config(
+                name="qwen2/7B_lora_single_device",
+                file_path="qwen2/7B_lora_single_device.yaml",
+            ),
+            Config(
+                name="qwen2/0.5B_lora_single_device",
+                file_path="qwen2/0.5B_lora_single_device.yaml",
+            ),
+            Config(
+                name="qwen2/1.5B_lora_single_device",
+                file_path="qwen2/1.5B_lora_single_device.yaml",
+            ),
+            Config(
+                name="llama3_2_vision/11B_lora_single_device",
+                file_path="llama3_2_vision/11B_lora_single_device.yaml",
+            ),
+        ],
+        supports_distributed=False,
+    ),
+    Recipe(
+        name="lora_dpo_single_device",
+        file_path="lora_dpo_single_device.py",
+        configs=[
+            Config(
+                name="llama2/7B_lora_dpo_single_device",
+                file_path="llama2/7B_lora_dpo_single_device.yaml",
+            ),
+        ],
+        supports_distributed=False,
+    ),
+    Recipe(
+        name="lora_dpo_distributed",
+        file_path="lora_dpo_distributed.py",
+        configs=[
+            Config(
+                name="llama2/7B_lora_dpo",
+                file_path="llama2/7B_lora_dpo.yaml",
+            ),
+        ],
+        supports_distributed=True,
+    ),
+    Recipe(
+        name="ppo_full_finetune_single_device",
+        file_path="ppo_full_finetune_single_device.py",
+        configs=[
+            Config(
+                name="mistral/7B_full_ppo_low_memory",
+                file_path="mistral/7B_full_ppo_low_memory.yaml",
+            ),
+        ],
+        supports_distributed=False,
+    ),
+    Recipe(
+        name="lora_finetune_distributed",
+        file_path="lora_finetune_distributed.py",
+        configs=[
+            Config(name="llama2/7B_lora", file_path="llama2/7B_lora.yaml"),
+            Config(name="llama2/13B_lora", file_path="llama2/13B_lora.yaml"),
+            Config(name="llama2/70B_lora", file_path="llama2/70B_lora.yaml"),
+            Config(
+                name="llama2/7B_qlora",
+                file_path="llama2/7B_qlora.yaml",
+            ),
+            Config(
+                name="llama2/70B_qlora",
+                file_path="llama2/70B_qlora.yaml",
+            ),
+            Config(name="llama3/8B_dora", file_path="llama3/8B_dora.yaml"),
+            Config(name="llama3/70B_lora", file_path="llama3/70B_lora.yaml"),
+            Config(name="llama3_1/70B_lora", file_path="llama3_1/70B_lora.yaml"),
+            Config(name="llama3/8B_lora", file_path="llama3/8B_lora.yaml"),
+            Config(name="llama3_1/8B_lora", file_path="llama3_1/8B_lora.yaml"),
+            Config(name="llama3_2/1B_lora", file_path="llama3_2/1B_lora.yaml"),
+            Config(name="llama3_2/3B_lora", file_path="llama3_2/3B_lora.yaml"),
+            Config(
+                name="llama3_1/405B_qlora",
+                file_path="llama3_1/405B_qlora.yaml",
+            ),
+            Config(name="mistral/7B_lora", file_path="mistral/7B_lora.yaml"),
+            Config(name="gemma/2B_lora", file_path="gemma/2B_lora.yaml"),
+            Config(name="gemma/7B_lora", file_path="gemma/7B_lora.yaml"),
+            Config(name="phi3/mini_lora", file_path="phi3/mini_lora.yaml"),
+            Config(name="qwen2/7B_lora", file_path="qwen2/7B_lora.yaml"),
+            Config(name="qwen2/0.5B_lora", file_path="qwen2/0.5B_lora.yaml"),
+            Config(name="qwen2/1.5B_lora", file_path="qwen2/1.5B_lora.yaml"),
+            Config(
+                name="llama3_2_vision/11B_lora",
+                file_path="llama3_2_vision/11B_lora.yaml",
+            ),
+        ],
+        supports_distributed=True,
+    ),
+    Recipe(
+        name="generate",
+        file_path="generate.py",
+        configs=[
+            Config(name="generation", file_path="generation.yaml"),
+        ],
+        supports_distributed=False,
+    ),
+    Recipe(
+        name="dev/generate_v2",
+        file_path="dev/generate_v2.py",
+        configs=[
+            Config(
+                name="llama2/generation_v2",
+                file_path="llama2/generation_v2.yaml",
+            ),
+            Config(
+                name="llama3_2_vision/generation_v2",
+                file_path="llama3_2_vision/generation_v2.yaml",
+            ),
+        ],
+        supports_distributed=False,
+    ),
+    Recipe(
+        name="eleuther_eval",
+        file_path="eleuther_eval.py",
+        configs=[
+            Config(name="eleuther_evaluation", file_path="eleuther_evaluation.yaml"),
+            Config(
+                name="llama3_2_vision/evaluation",
+                file_path="llama3_2_vision/evaluation.yaml",
+            ),
+            Config(
+                name="qwen2/evaluation",
+                file_path="qwen2/evaluation.yaml",
+            ),
+            Config(
+                name="gemma/evaluation",
+                file_path="gemma/evaluation.yaml",
+            ),
+            Config(
+                name="phi3/evaluation",
+                file_path="phi3/evaluation.yaml",
+            ),
+            Config(
+                name="mistral/evaluation",
+                file_path="mistral/evaluation.yaml",
+            ),
+        ],
+        supports_distributed=False,
+    ),
+    Recipe(
+        name="quantize",
+        file_path="quantize.py",
+        configs=[
+            Config(name="quantization", file_path="quantization.yaml"),
+        ],
+        supports_distributed=False,
+    ),
+    Recipe(
+        name="qat_distributed",
+        file_path="qat_distributed.py",
+        configs=[
+            Config(name="llama2/7B_qat_full", file_path="llama2/7B_qat_full.yaml"),
+            Config(name="llama3/8B_qat_full", file_path="llama3/8B_qat_full.yaml"),
+        ],
+        supports_distributed=True,
+    ),
+    Recipe(
+        name="knowledge_distillation_single_device",
+        file_path="knowledge_distillation_single_device.py",
+        configs=[
+            Config(
+                name="qwen2/knowledge_distillation_single_device",
+                file_path="qwen2/knowledge_distillation_single_device.yaml",
+            ),
+            Config(
+                name="llama3_2/knowledge_distillation_single_device",
+                file_path="llama3_2/knowledge_distillation_single_device.yaml",
+            ),
+        ],
+        supports_distributed=False,
+    ),
+]
+
+
+def get_all_recipes():
+    """List of recipes available from the CLI."""
+    return _ALL_RECIPES
diff -ruN marc_original/third_party/torchtune/torchtune/rlhf/__init__.py marc/third_party/torchtune/torchtune/rlhf/__init__.py
--- marc_original/third_party/torchtune/torchtune/rlhf/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/rlhf/__init__.py	2025-02-20 17:49:30.754026225 -0500
@@ -0,0 +1,37 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+from ._types import PPOStats, Trajectory
+from .rewards import (
+    estimate_advantages,
+    get_reward_penalty_mask,
+    get_rewards_ppo,
+    masked_mean,
+    masked_var,
+    whiten,
+)
+from .sequence_processing import (
+    get_batch_log_probs,
+    logits_to_logprobs,
+    truncate_sequence_at_first_stop_token,
+    truncate_sequence_for_logprobs,
+)
+
+__all__ = [
+    "truncate_sequence_at_first_stop_token",
+    "logits_to_logprobs",
+    "truncate_sequence_for_logprobs",
+    "get_reward_penalty_mask",
+    "estimate_advantages",
+    "get_rewards_ppo",
+    "whiten",
+    "masked_mean",
+    "masked_var",
+    "PPOStats",
+    "get_batch_log_probs",
+    "Trajectory",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/rlhf/loss/dpo.py marc/third_party/torchtune/torchtune/rlhf/loss/dpo.py
--- marc_original/third_party/torchtune/torchtune/rlhf/loss/dpo.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/rlhf/loss/dpo.py	2025-02-20 17:49:30.762026238 -0500
@@ -0,0 +1,233 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Tuple
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+class DPOLoss(nn.Module):
+    """
+    Direct Preference Optimization (DPO) Loss module: https://arxiv.org/abs/2305.18290
+    Simply stated from the paper:
+
+        Intuitively, the DPO update increases the relative log probability of preferred to dispreferred responses,
+        but it incorporates a dynamic, per-example importance weight that prevents
+        the model degeneration that we find occurs with a naive probability ratio objective.
+
+    Based on the implementation in HF's TRL library:
+    https://github.com/huggingface/trl/blob/5d1deb1445828cfd0e947cb3a7925b1c03a283fc/trl/trainer/dpo_trainer.py#L844
+
+    DPO retains similarities to PPO (https://arxiv.org/abs/2009.01325), where it optimizes a policy
+    (language) model to align with human preferences, and regularizes the loss function using a baseline
+    reference (the frozen, initial language model) to prevent over-fitting to the preference dataset.
+    It differs from PPO by optimizing the policy model directly using labelled preference data, rather
+    than using an additional reward model to provide feedback.
+    This significantly simplifies training and reduces compute overhead.
+
+    Args:
+        beta (float): Temperature parameter for the DPO loss, typically in the range of 0.1 to 0.5. Default is 0.1.
+        label_smoothing (float): Parameter encoding uncertainty about the labels. Default is 0.
+    """
+
+    def __init__(
+        self,
+        beta: float = 0.1,
+        label_smoothing: float = 0.0,
+    ):
+        super().__init__()
+        self.beta = beta
+        self.label_smoothing = label_smoothing
+
+    def forward(
+        self,
+        policy_chosen_logps: torch.Tensor,
+        policy_rejected_logps: torch.Tensor,
+        reference_chosen_logps: torch.Tensor,
+        reference_rejected_logps: torch.Tensor,
+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+        """
+        Compute the DPO loss for a batch of policy and reference model log probabilities.
+
+        Args:
+            policy_chosen_logps (torch.Tensor): Log probabilities of the policy model
+                for the chosen responses. Shape: (batch_size)
+            policy_rejected_logps (torch.Tensor): Log probabilities of the policy model
+                for the rejected responses. Shape: (batch_size)
+            reference_chosen_logps (torch.Tensor): Log probabilities of the reference model
+                for the chosen responses. Shape: (batch_size)
+            reference_rejected_logps (torch.Tensor): Log probabilities of the reference model
+                for the rejected responses. Shape: (batch_size)
+
+        Returns:
+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple of three tensors:
+                - losses: The DPO loss for each example in the batch.
+                - chosen_rewards: Rewards for the chosen responses.
+                - rejected_rewards: Rewards for the rejected responses.
+
+        """
+        pi_logratios = policy_chosen_logps - policy_rejected_logps
+        ref_logratios = reference_chosen_logps - reference_rejected_logps
+
+        logits = pi_logratios - ref_logratios
+
+        # The beta is a temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5.
+        # We ignore the reference model as beta -> 0. The label_smoothing parameter encodes our uncertainty about the labels and
+        # calculates a conservative DPO loss.
+        losses = (
+            -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)
+            - F.logsigmoid(-self.beta * logits) * self.label_smoothing
+        )
+
+        chosen_rewards = (
+            self.beta * (policy_chosen_logps - reference_chosen_logps).detach()
+        )
+        rejected_rewards = (
+            self.beta * (policy_rejected_logps - reference_rejected_logps).detach()
+        )
+
+        return losses, chosen_rewards, rejected_rewards
+
+
+class RSOLoss(nn.Module):
+    """
+    Statistical Rejection Sampling Optimization (RSO) or "hinge" loss module: https://arxiv.org/abs/2309.06657.
+    Intuition from the paper:
+
+        DPO is a logistic regression on human preference data, and SLiC (https://arxiv.org/abs/2305.10425) is almost
+        equivalent to a support vector machine (SVM) with hinge loss. [RSO] improve[s] SLiC as the SVM counter part of DPO.
+
+    Based on the implementation in HF's TRL library:
+    https://github.com/huggingface/trl/blob/4dce042a3863db1d375358e8c8092b874b02934b/trl/trainer/dpo_trainer.py#L1141
+
+    Args:
+        gamma (float): Equivalent temperature parameter (from DPO) for the RSO loss.
+    """
+
+    def __init__(
+        self,
+        gamma: float = 0.1,
+    ):
+        super().__init__()
+        self.gamma = gamma
+
+    def forward(
+        self,
+        policy_chosen_logps: torch.Tensor,
+        policy_rejected_logps: torch.Tensor,
+        reference_chosen_logps: torch.Tensor,
+        reference_rejected_logps: torch.Tensor,
+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+        """
+        Compute the RSO loss for a batch of policy and reference model log probabilities.
+
+        Args:
+            policy_chosen_logps (torch.Tensor): Log probabilities of the policy model
+                for the chosen responses. Shape: (batch_size)
+            policy_rejected_logps (torch.Tensor): Log probabilities of the policy model
+                for the rejected responses. Shape: (batch_size)
+            reference_chosen_logps (torch.Tensor): Log probabilities of the reference model
+                for the chosen responses. Shape: (batch_size)
+            reference_rejected_logps (torch.Tensor): Log probabilities of the reference model
+                for the rejected responses. Shape: (batch_size)
+
+        Returns:
+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple of three tensors:
+                - losses: The RSO loss for each example in the batch.
+                - chosen_rewards: Rewards for the chosen responses.
+                - rejected_rewards: Rewards for the rejected responses.
+
+        """
+        pi_logratios = policy_chosen_logps - policy_rejected_logps
+        ref_logratios = reference_chosen_logps - reference_rejected_logps
+
+        logits = pi_logratios - ref_logratios
+
+        losses = torch.relu(1 - self.gamma * logits)
+
+        chosen_rewards = (
+            self.gamma * (policy_chosen_logps - reference_chosen_logps).detach()
+        )
+        rejected_rewards = (
+            self.gamma * (policy_rejected_logps - reference_rejected_logps).detach()
+        )
+
+        return losses, chosen_rewards, rejected_rewards
+
+
+class SimPOLoss(nn.Module):
+    """
+    SimPO: Simple Preference Optimization with a Reference-Free Reward: https://arxiv.org/abs/2405.14734.
+    Intuition from the paper:
+
+        The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as
+        the implicit reward. Additionally, we introduce a target reward margin to the Bradley-Terry objective to
+        encourage a larger margin between the winning and losing responses, further enhancing the algorithm's performance.
+
+    Based on the TRL implementation:
+    https://github.com/huggingface/trl/blob/98ad01ddfd1e1b67ec018014b83cba40e0caea66/trl/trainer/cpo_trainer.py#L603
+
+    SimPO is pretty much identitcal to DPO but uses average logprobs to eliminate the need for a reference model to regularize
+    the policy during training. It also uses a target reward margin to guide the policy towards better responses.
+    This is kind of the same intuition as in :class:`~torchtune.rlhf.loss.IPOLoss`, but instead of optimizing against
+    a margin between the reference policy and policy models, we're optimizing against a margin between the chosen and
+    rejected responses.
+
+    Args:
+        beta (float): Equivalent temperature scaling parameter to DPO loss, typically in the range of 2.0 to 2.5. Default is 2.0.
+        gamma (float): Target reward margin hyperparameter, typically we have ``gamma in (0, 1.5]``.
+            Default is 0.5.
+        label_smoothing (float): Parameter encoding uncertainty about the labels. Default is 0.
+    """
+
+    def __init__(
+        self,
+        beta: float = 2.0,
+        gamma: float = 0.5,
+        label_smoothing: float = 0.0,
+    ):
+        super().__init__()
+        self.beta = beta
+        self.gamma = gamma
+        self.label_smoothing = label_smoothing
+
+    def forward(
+        self,
+        policy_chosen_logps: torch.Tensor,
+        policy_rejected_logps: torch.Tensor,
+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+        """
+        Compute the SimPO loss for a batch chosen and rejected average log probabilities.
+
+        Args:
+            policy_chosen_logps (torch.Tensor): Average log probabilities of the policy model
+                for the chosen responses with shape [b,].
+            policy_rejected_logps (torch.Tensor): Average log probabilities of the policy model
+                for the rejected responses with shape [b,].
+
+        Returns:
+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]; A tuple of three tensors with shape [b,]:
+                - losses: The SimPO loss for each example in the batch.
+                - chosen_rewards: Rewards for the chosen responses.
+                - rejected_rewards: Rewards for the rejected responses.
+        """
+
+        pi_logratios = policy_chosen_logps - policy_rejected_logps
+
+        gamma_logratios = self.gamma / self.beta
+        logits = pi_logratios - gamma_logratios
+
+        losses = (
+            -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)
+            - F.logsigmoid(-self.beta * logits) * self.label_smoothing
+        )
+
+        chosen_rewards = self.beta * (policy_chosen_logps).detach()
+        rejected_rewards = self.beta * (policy_rejected_logps).detach()
+
+        return losses, chosen_rewards, rejected_rewards
diff -ruN marc_original/third_party/torchtune/torchtune/rlhf/loss/__init__.py marc/third_party/torchtune/torchtune/rlhf/loss/__init__.py
--- marc_original/third_party/torchtune/torchtune/rlhf/loss/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/rlhf/loss/__init__.py	2025-02-20 17:49:30.762026238 -0500
@@ -0,0 +1,11 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+from .dpo import DPOLoss, RSOLoss, SimPOLoss
+from .ppo import PPOLoss
+
+__all__ = ["DPOLoss", "RSOLoss", "SimPOLoss", "PPOLoss"]
diff -ruN marc_original/third_party/torchtune/torchtune/rlhf/loss/ppo.py marc/third_party/torchtune/torchtune/rlhf/loss/ppo.py
--- marc_original/third_party/torchtune/torchtune/rlhf/loss/ppo.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/rlhf/loss/ppo.py	2025-02-20 17:49:30.766026245 -0500
@@ -0,0 +1,120 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional, Tuple
+
+import torch
+import torch.nn as nn
+from torchtune import rlhf
+
+
+class PPOLoss(nn.Module):
+    """
+    Proximal Policy Optimization (PPO) Loss module.
+    This implementation uses the following references:
+
+    https://arxiv.org/abs/1707.06347 eqn. 7
+
+    https://github.com/vwxyzjn/lm-human-preference-details/blob/ccc19538e817e98a60d3253242ac15e2a562cb49/lm_human_preference_details/train_policy_accelerate.py#L719
+
+    https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/ppo2/model.py#L68-L75
+
+
+    Args:
+        epsilon (float): clipping range for PPO update.
+        value_clip_range (float): clipping range for value function update.
+        value_coeff (float): coefficient for the value function loss contribution.
+    """
+
+    def __init__(
+        self,
+        epsilon: float = 0.1,
+        value_clip_range: float = 0.2,
+        value_coeff: float = 0.1,
+    ):
+        super().__init__()
+        self.epsilon = epsilon
+        self.value_clip_range = value_clip_range
+        self.value_coeff = value_coeff
+
+    def forward(
+        self,
+        pi_old_logprobs: torch.Tensor,
+        pi_logprobs: torch.Tensor,
+        advantages: torch.Tensor,
+        phi_old_values: torch.Tensor,
+        phi_values: torch.Tensor,
+        returns: torch.Tensor,
+        padding_masks: Optional[torch.Tensor] = None,
+        value_padding_masks: Optional[torch.Tensor] = None,
+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
+        """
+
+        Forward pass of the PPO loss module.
+
+        Args:
+            pi_old_logprobs (torch.Tensor): Log probabilities of the old policy.
+            pi_logprobs (torch.Tensor): Log probabilities of the current policy.
+            advantages (torch.Tensor): Advantage values.
+            phi_old_values (torch.Tensor): Value predictions of the old value function.
+            phi_values (torch.Tensor): Value predictions of the current value function.
+            returns (torch.Tensor): Return values.
+            padding_masks (Optional[torch.Tensor]): Padding token masks of the same shape as ``pi_logprobs``,
+                where True indicates the corresponding loss values should participage in policy loss calculation.
+            value_padding_masks (Optional[torch.Tensor]): Padding token masks of the same shape as ``pi_logprobs``,
+                where True indicates the corresponding loss values should participage in value loss calculation.
+
+        Returns:
+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: A tuple of five tensors:
+                - loss: The total PPO loss.
+                - policy_loss: The policy function loss.
+                - value_loss: The value function loss.
+                - ratios: The ratio between the current and old policy probabilities.
+                - clipfrac: The fraction of ratios that were clipped.
+
+        """
+        ratios = torch.exp(pi_logprobs - pi_old_logprobs)
+        clipped_ratios = torch.clamp(ratios, 1.0 - self.epsilon, 1.0 + self.epsilon)
+
+        policy_losses_clipped = -advantages * clipped_ratios
+        policy_losses_unclipped = -advantages * ratios
+
+        clipfrac = (policy_losses_clipped > policy_losses_unclipped).float()
+        clipfrac = (
+            clipfrac.mean()
+            if padding_masks is None
+            else rlhf.masked_mean(clipfrac, padding_masks)
+        )
+
+        policy_loss = torch.maximum(policy_losses_clipped, policy_losses_unclipped)
+        policy_loss = (
+            policy_loss.mean()
+            if padding_masks is None
+            else rlhf.masked_mean(policy_loss, padding_masks)
+        )
+
+        values_clipped = torch.clamp(
+            phi_values,
+            phi_old_values - self.value_clip_range,
+            phi_old_values + self.value_clip_range,
+        )
+        value_loss = torch.maximum(
+            (phi_values - returns) ** 2, (values_clipped - returns) ** 2
+        )
+        value_loss = (
+            0.5 * value_loss.mean()
+            if value_padding_masks is None
+            else 0.5 * rlhf.masked_mean(value_loss, value_padding_masks)
+        )
+
+        loss = policy_loss + (value_loss * self.value_coeff)
+        return (
+            loss,
+            policy_loss.detach(),
+            value_loss.detach(),
+            ratios.mean().detach(),
+            clipfrac.detach(),
+        )
Binary files marc_original/third_party/torchtune/torchtune/rlhf/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/rlhf/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/rlhf/__pycache__/rewards.cpython-312.pyc and marc/third_party/torchtune/torchtune/rlhf/__pycache__/rewards.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/rlhf/__pycache__/sequence_processing.cpython-312.pyc and marc/third_party/torchtune/torchtune/rlhf/__pycache__/sequence_processing.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/rlhf/__pycache__/_types.cpython-312.pyc and marc/third_party/torchtune/torchtune/rlhf/__pycache__/_types.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/rlhf/rewards.py marc/third_party/torchtune/torchtune/rlhf/rewards.py
--- marc_original/third_party/torchtune/torchtune/rlhf/rewards.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/rlhf/rewards.py	2025-02-20 17:49:30.770026251 -0500
@@ -0,0 +1,237 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional, Tuple
+
+import torch
+
+
+def get_reward_penalty_mask(
+    padding_masks: torch.Tensor,
+    seq_lens: torch.Tensor,
+    penalise_no_eos: bool = True,
+    min_response_length: int = None,
+) -> torch.Tensor:
+    """
+    Calculates a mask to penalise scores corresponding to sequences generated during PPO, where True indicates the score
+    at the corresponding position should be penalised.
+    This function assumes sequences have already been truncated at an EOS, if present, and padded to length,
+    e.g. by :func:`torchtune.rlhf.sequence_processing.truncate_sequence_at_first_stop_token`.
+
+    Scores are penalised such that:
+    - If ``min_response_length`` is set, scores for sequences with ``length < min_response_length`` are penalised.
+    - If ``penalise_no_eos`` is True, scores for sequences with no EOS token are penalised.
+
+    Args:
+        padding_masks (torch.Tensor): torch.Tensor where True indicates a padding token in the generated
+            sequence, and False otherwise. Shape: ``(b, response_len)``
+        seq_lens (torch.Tensor): The length of each generated sequence. Shape: ``(b,)``
+        penalise_no_eos (bool, optional): Whether to penalise sequences with no EOS token. Defaults to True.
+        min_response_length (int, optional): The minimum length of the response. If set, any responses is shorter
+            than this length will be penalised. Defaults to None.
+    Returns:
+        torch.Tensor: A mask tensor with shape ``(b,)`` where True indicates the corresponding score should be penalised.
+    """
+    reward_penalty_mask = torch.zeros_like(seq_lens).to(bool)
+
+    # since sequences will have been truncated at EOS, we can mask based on the presence of any padding tokens
+    if penalise_no_eos:
+        reward_penalty_mask = ~padding_masks.any(-1)
+
+    if min_response_length is not None:
+        reward_penalty_mask |= ~(seq_lens >= min_response_length)
+    return reward_penalty_mask
+
+
+def get_rewards_ppo(
+    scores: torch.Tensor,
+    logprobs: torch.Tensor,
+    ref_logprobs: torch.Tensor,
+    kl_coeff: float,
+    valid_score_idxs: Optional[torch.Tensor] = None,
+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+    """
+    Calculates PPO rewards for the given scores, logprobs, and reference logprobs.
+
+    Args:
+        scores (torch.Tensor): Reward model scores, shape ``(b,)``.
+        logprobs (torch.Tensor): Policy logprobs, shape ``(b, response_len)``.
+        ref_logprobs (torch.Tensor): Reference base model logprobs, shape ``(b, response_len)``.
+        kl_coeff (float): KL reward contribution coefficient.
+        valid_score_idxs (Optional[torch.Tensor]): A tensor of indexes for valid (non-padded) token predictions.
+            This is useful when calculating rewards for padded sequences, as scores and value estimates are defined
+            for the last valid predicted token. Shape: ``(b,)``. Default None.
+
+    Returns:
+        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple of tensors with shape ``(b, response_len)`` each:
+            - total_reward: total reward combining per-token kl rewards and reward model score.
+            - kl: kl divergence between policy and reference policy logprobs.
+            - kl_reward: kl divergence scaled by ``kl_coeff``.
+
+    Notation used for tensor shapes:
+        - b: batch size
+        - response_len: model response length
+    """
+
+    # 1. calculate kl between logprobs and reflogprobs
+    # 2. calculate kl reward using adaptive scaling value
+    # 3. calculate total reward by summing above
+    # return all
+    kl = logprobs - ref_logprobs
+    kl_reward = -kl_coeff * kl
+
+    total_reward = kl_reward.clone()
+
+    # adding reward to kl at final valid position
+    # https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L153
+
+    if valid_score_idxs is not None:
+        total_reward[
+            torch.arange(scores.shape[0], device=scores.device), valid_score_idxs
+        ] += scores
+    else:
+        total_reward[:, -1] += scores
+
+    return total_reward, kl, kl_reward
+
+
+def masked_mean(
+    x: torch.Tensor, mask: torch.Tensor, dim: Optional[int] = None
+) -> torch.Tensor:
+    """
+    Compute mean of tensor with masked values. Taken from https://github.com/huggingface/trl/blob/main/trl/core.py
+
+    Args:
+        x (torch.Tensor): The input tensor.
+        mask (torch.Tensor): The bool mask tensor, where True indicates the corresponding value in ``x``
+            should participate in the mean calculation.
+        dim (Optional[int]): The axis to calculate the mean over. Default None.
+
+    Returns:
+        torch.Tensor: The mean tensor.
+    """
+    return (x * mask).sum(dim=dim) / mask.sum(dim=dim)
+
+
+def masked_var(
+    x: torch.Tensor, mask: torch.Tensor, unbiased: bool = True
+) -> torch.Tensor:
+    """
+    Compute variance of tensor with masked values. Taken from https://github.com/huggingface/trl/blob/main/trl/core.py
+
+    Args:
+        x (torch.Tensor): The input tensor.
+        mask (torch.Tensor): The bool mask tensor, where True indicates the corresponding value in ``x``
+            should participate in the mean calculation.
+        unbiased (bool): Whether to use the unbiased variance.
+
+    Returns:
+        torch.Tensor: The variance tensor.
+
+    Raises:
+        ValueError: If the sum of the mask is zero.
+    """
+    mean = masked_mean(x, mask)
+    centered_values = x - mean
+    var = masked_mean(centered_values.pow(2), mask)
+    if unbiased:
+        mask_sum = mask.sum()
+        if mask_sum == 0:
+            raise ValueError(
+                "The sum of the mask is zero, which can happen when ``ppo_batch_size=1``;"
+                "try increase the ``ppo_batch_size`` or ``gradient_accumulation_steps``"
+            )
+        # note that if mask_sum == 1, then there is a division by zero issue
+        # to avoid it you just need to use a larger minibatch_size
+        bessel_correction = mask_sum / (mask_sum - 1)
+        var = var * bessel_correction
+    return var
+
+
+def whiten(
+    x: torch.Tensor, mask: Optional[torch.Tensor] = None, shift_mean: bool = True
+) -> torch.Tensor:
+    """
+    Whiten (normalises) values, optionally with masked values. Taken from https://github.com/huggingface/trl/blob/main/trl/core.py
+    Args:
+        x (torch.Tensor): The input tensor.
+        mask (Optional[torch.Tensor]): The bool mask tensor, where True indicates the corresponding value in ``x``
+            should participate in the mean calculation. Default None.
+        shift_mean (bool): Whether to shift normalised values by the mean.
+
+    Returns:
+        torch.Tensor: The whitened tensor.
+    """
+    if mask is not None:
+        mean = masked_mean(x, mask)
+        var = masked_var(x, mask) if mask.any() else x.var()
+    else:
+        mean, var = x.mean(), x.var()
+    whitened = (x - mean) * torch.rsqrt(var + 1e-8)
+    if shift_mean:
+        whitened += mean
+    return whitened
+
+
+def estimate_advantages(
+    values: torch.Tensor,
+    rewards: torch.Tensor,
+    gamma: float,
+    lmbda: float,
+    masks: Optional[torch.Tensor] = None,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+    Estimates the advantages and returns for the PPO algorithm using Generalized Advantage Estimation
+    https://arxiv.org/pdf/1506.02438.pdf
+
+    Args:
+        values (torch.Tensor): The predicted values for each state. Shape: ``(b, response_len)``
+        rewards (torch.Tensor): The rewards received at each time step. Shape: ``(b, response_len)``
+        gamma (float): The discount factor.
+        lmbda (float): The GAE-Lambda parameter.
+        masks (Optional[torch.Tensor]): A bool mask tensor, where True indicates the corresponding value in ``values``
+            should participate in the mean calculation. Default None.
+    Returns:
+        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the estimated advantages and returns.
+            - advantages (torch.Tensor): The estimated advantages. Shape: ``(b, response_len)``
+            - returns (torch.Tensor): The estimated returns. Shape: ``(b, response_len)``
+    Notation:
+        - b: batch size
+        - response_len: model response length
+    """
+
+    last_gae_lam = 0
+    advantages_reversed = []
+
+    response_length = values.shape[-1]
+
+    # estimate advantage for every predicted token position
+    for t in reversed(range(response_length)):
+        # value of the next state
+        next_values = values[:, t + 1] if t < response_length - 1 else 0.0
+        # exponentially discounted temporal difference error:
+        # delta_t = r_t + gamma * V(s_{t+1}) - V(s_t)
+        delta = rewards[:, t] + gamma * next_values - values[:, t]
+        # GAE-Lambda advantage discounting saved for the next iteration
+        # as A_t = delta_t + gamma * lambda * A_{t+1} + ...
+        last_gae_lam = delta + gamma * lmbda * last_gae_lam
+        advantages_reversed.append(last_gae_lam)
+
+    advantages = torch.stack(advantages_reversed[::-1], axis=1)
+
+    # returns are the expected value of taking action a_t at each timepoint over
+    # a trajectory. the value estimates v_t are the expected value over all actions
+    # over a trajectory - the advantage is the difference between the two
+    returns = advantages + values
+
+    # normalize advantages across the batch of trajectories to reduce variance
+    if masks is not None:
+        advantages = whiten(advantages, mask=masks)
+        advantages[~masks] = 0.0
+    else:
+        advantages = whiten(advantages)
+
+    return advantages, returns
diff -ruN marc_original/third_party/torchtune/torchtune/rlhf/sequence_processing.py marc/third_party/torchtune/torchtune/rlhf/sequence_processing.py
--- marc_original/third_party/torchtune/torchtune/rlhf/sequence_processing.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/rlhf/sequence_processing.py	2025-02-20 17:49:30.774026258 -0500
@@ -0,0 +1,159 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Tuple
+
+import torch
+import torch.nn.functional as F
+from torchtune import rlhf
+from torchtune.data import CROSS_ENTROPY_IGNORE_IDX
+
+
+def truncate_sequence_at_first_stop_token(
+    sequences: torch.Tensor, stop_tokens: torch.Tensor, fill_value: int = 0
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    """
+    Truncates sequence(s) after the first stop token and pads with ``fill_value``.
+
+    Args:
+        sequences (torch.Tensor): tensor of shape [batch_size, sequence_length] or [sequence_length].
+        stop_tokens (torch.Tensor): tensor containing stop tokens.
+        fill_value (int): value to pad the sequence with after the first stop token, usually ``pad_id``.
+
+    Returns:
+        Tuple[torch.Tensor, torch.Tensor]: A tuple of two tensors with the same shape as ``sequences``:
+            - padding_mask (torch.Tensor): a bool tensor where True indicates the token has been truncated.
+            - sequences (torch.Tensor) a tensor of truncated and padded sequences.
+
+    Example:
+        >>> stop_token_ids = torch.tensor([2, 869])
+        >>> fill_value = 0
+        >>> sequences = torch.tensor(
+        >>>     [
+        >>>         [869, 30, 869],
+        >>>         [2, 30, 869],
+        >>>         [869, 30, 2],
+        >>>         [50, 30, 869],
+        >>>         [13, 30, 2],
+        >>>         [13, 30, 5],
+        >>>         [13, 2, 20],
+        >>>         [13, 2, 2],
+        >>>         [2, 2, 2],
+        >>>     ]
+        >>> )
+        >>> eos_mask, truncated_sequences = rlhf.truncate_sequence_at_first_stop_token(
+        >>>     sequences, stop_token_ids, fill_value
+        >>> )
+        >>> eos_mask
+        >>> torch.tensor([
+        >>>         [False, True, True],
+        >>>         [False, True, True],
+        >>>         [False, True, True],
+        >>>         [False, False, False],
+        >>>         [False, False, False],
+        >>>         [False, False, False],
+        >>>         [False, False, True],
+        >>>         [False, False, True],
+        >>>         [False, True, True],
+        >>>     ]
+        >>> )
+        >>> truncated_sequences
+        >>> torch.tensor([
+        >>>         [869, 0, 0],
+        >>>         [2, 0, 0],
+        >>>         [869, 0, 0],
+        >>>         [50, 30, 869],
+        >>>         [13, 30, 2],
+        >>>         [13, 30, 5],
+        >>>         [13, 2, 0],
+        >>>         [13, 2, 0],
+        >>>         [2, 0, 0],
+        >>>     ]
+        >>> )
+    """
+    eos_mask = torch.isin(sequences, stop_tokens)
+    seq_lens = torch.cumsum(eos_mask, dim=1)
+    padding_mask = (seq_lens > 1) | ((seq_lens == 1) & ~eos_mask)
+    sequences[padding_mask] = fill_value
+    return padding_mask, sequences
+
+
+def logits_to_logprobs(
+    logits: torch.Tensor, sequences: torch.Tensor, temperature: float = 1.0
+) -> torch.Tensor:
+    """
+    Converts logits corresponding to a generated sequence to logprobs over the generated tokens.
+
+    Args:
+        logits (torch.Tensor): The logits tensor of shape [b, response_length, vocab_size].
+        sequences (torch.Tensor): The corresponding tokens of shape [b, response_length].
+        temperature (float): The temperature to scale the logits. Default 1.0
+    Returns:
+        torch.Tensor: The log probabilities corresponding to each token in ``sequences``. Shape [b, response_length].
+    """
+    return torch.gather(
+        F.log_softmax(logits / temperature, dim=-1),
+        2,
+        sequences.unsqueeze(-1),
+    ).squeeze(-1)
+
+
+def get_batch_log_probs(
+    logits: torch.FloatTensor,
+    labels: torch.LongTensor,
+    label_pad_token_id: int = CROSS_ENTROPY_IGNORE_IDX,
+    return_average_logprobs: bool = False,
+) -> torch.FloatTensor:
+    """
+    Calculate log probabilities based on provided logits and labels.
+
+    Args:
+        logits (torch.FloatTensor): direct logits output of the model of shape (b, s, v)
+        labels (torch.LongTensor): ground-truth labels to compute log probs with, shape (b, s).
+            Label tokens with a value of label_pad_token_id are ignored.
+        label_pad_token_id (int): token id to ignore in labels.
+        return_average_logprobs (bool): If True, return the average log probs across the sequence. Default
+            is False. See https://github.com/eric-mitchell/direct-preference-optimization/blob/f8b8c0f49dc92a430bae41585f9d467d3618fe2f/trainers.py#L96 # noqa
+
+    Returns:
+        Calculated log probs of shape (b, )
+
+    Raises:
+        ValueError: If logits and labels have different shapes.
+    """
+
+    if logits.shape[:-1] != labels.shape:
+        raise ValueError(
+            "Logits (batch and sequence length dim) and labels must have the same shape."
+        )
+
+    labels = labels[:, 1:].clone()
+    logits = logits[:, :-1, :]
+    loss_mask = labels != label_pad_token_id
+
+    labels[labels == label_pad_token_id] = 0
+    # take log-likelihood of the labels given our model
+    per_token_log_probs = logits_to_logprobs(logits, labels, temperature=1.0)
+
+    if return_average_logprobs:
+        return rlhf.masked_mean(per_token_log_probs, loss_mask, dim=-1)
+    else:
+        return (per_token_log_probs * loss_mask).sum(-1)
+
+
+def truncate_sequence_for_logprobs(
+    query_response_logits: torch.Tensor, context_length: int
+) -> torch.Tensor:
+    """
+    Truncates logits generated over a sequence for estimating logprobs over the tokens in the sequence.
+    This assumes the sequence is of the (query, response) format with length (context_length + response_length)
+    Args:
+        query_response_logits (torch.Tensor): The logits tensor of shape [b, context_length + response_length, vocab_size].
+        context_length (int): The length of the context.
+
+    Returns:
+        torch.Tensor: The truncated logits for the response with shape [b, response_length, vocab_size]."""
+    return query_response_logits[:, context_length - 1 : -1]
diff -ruN marc_original/third_party/torchtune/torchtune/rlhf/_types.py marc/third_party/torchtune/torchtune/rlhf/_types.py
--- marc_original/third_party/torchtune/torchtune/rlhf/_types.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/rlhf/_types.py	2025-02-20 17:49:30.758026232 -0500
@@ -0,0 +1,69 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import NamedTuple
+
+import torch
+
+
+class Trajectory(NamedTuple):
+    """
+    Contains a collection of tensors describing a generated trajectory during RLHF
+
+    Attributes:
+        query_responses (torch.Tensor): (query, response) pairs
+            shape [b, context_length + max_generated_tokens]
+        logprobs (torch.Tensor): log probabilities of the generated responses with shape [b, max_generated_tokens]
+        ref_logprobs (torch.Tensor): log probabilities of the generated responses using the reference policy
+            shape [b, max_generated_tokens]
+        values (torch.Tensor): value estimates of the generated responses with shape [b, max_generated_tokens]
+        masks (torch.Tensor): attention masks for input ids-generated responses pairs
+            shape [b, context_length + max_generated_tokens, context_length + max_generated_tokens]
+        position_ids (torch.Tensor): position IDs for input ids-generated responses pairs
+            shape [b, context_length + max_generated_tokens]
+        response_padding_masks (torch.Tensor): padding masks for the truncated and padded generated responses
+            shape [b, max_generated_tokens]
+        value_padding_masks (torch.Tensor): padding masks for the values with
+            shape [b, max_generated_tokens]
+        value_seq_idxs (torch.Tensor): indexes of the token
+            after the last valid (non-padding) token in the responses with shape [b]
+        scores (torch.Tensor): scores from the reward model with shape [b]
+        seq_lens (torch.Tensor): sequence lengths of truncated generated responses with shape [b]
+    """
+
+    query_responses: torch.Tensor
+    logprobs: torch.Tensor
+    ref_logprobs: torch.Tensor
+    values: torch.Tensor
+    masks: torch.Tensor
+    position_ids: torch.Tensor
+    response_padding_masks: torch.Tensor
+    value_padding_masks: torch.Tensor
+    value_seq_idxs: torch.Tensor
+    scores: torch.Tensor
+    seq_lens: torch.Tensor
+
+
+class PPOStats(NamedTuple):
+    """
+    Contains PPO loss statistics (metrics)
+
+    Attributes:
+        loss (torch.Tensor): The total PPO loss.
+        policy_loss (torch.Tensor): The policy function loss.
+        value_loss (torch.Tensor): The value function loss.
+        ratios (torch.Tensor): The ratio between the current and old policy probabilities.
+        clipfrac (torch.Tensor): The fraction of ratios that were clipped.
+        approx_policy_kls (torch.Tensor): Average estimated KL divergence between the policy before and after the optimisation step.
+
+    """
+
+    loss: torch.Tensor
+    policy_loss: torch.Tensor
+    value_loss: torch.Tensor
+    ratios: torch.Tensor
+    clipfrac: torch.Tensor
+    approx_policy_kls: torch.Tensor
diff -ruN marc_original/third_party/torchtune/torchtune/rlhf/utils/_convert_weights.py marc/third_party/torchtune/torchtune/rlhf/utils/_convert_weights.py
--- marc_original/third_party/torchtune/torchtune/rlhf/utils/_convert_weights.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/rlhf/utils/_convert_weights.py	2025-02-20 17:49:30.782026271 -0500
@@ -0,0 +1,126 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Dict
+
+import torch
+
+from torchtune.models.convert_weights import get_mapped_key
+
+_REWARD = {
+    "model.embed_tokens.weight": "tok_embeddings.weight",
+    "model.layers.{}.self_attn.q_proj.weight": "layers.{}.attn.q_proj.weight",
+    "model.layers.{}.self_attn.k_proj.weight": "layers.{}.attn.k_proj.weight",
+    "model.layers.{}.self_attn.v_proj.weight": "layers.{}.attn.v_proj.weight",
+    "model.layers.{}.self_attn.o_proj.weight": "layers.{}.attn.output_proj.weight",
+    "model.layers.{}.mlp.gate_proj.weight": "layers.{}.mlp.w1.weight",
+    "model.layers.{}.mlp.up_proj.weight": "layers.{}.mlp.w3.weight",
+    "model.layers.{}.mlp.down_proj.weight": "layers.{}.mlp.w2.weight",
+    "model.layers.{}.input_layernorm.weight": "layers.{}.sa_norm.scale",
+    "model.layers.{}.post_attention_layernorm.weight": "layers.{}.mlp_norm.scale",
+    "model.norm.weight": "norm.scale",
+    "score.weight": "output.weight",
+}
+
+
+def reward_hf_to_tune(
+    state_dict: Dict[str, torch.Tensor],
+    num_heads: int = 32,
+    num_kv_heads: int = 32,
+    dim: int = 4096,
+    head_dim: int = None,
+) -> Dict[str, torch.Tensor]:
+    """
+    Convert a state dict from HF's format to torchtune's format, which contains the weights
+    of a reward model (i.e. a classifier with a single class).
+    State dicts from multiple checkpoint files should be consolidated into a single state dict
+    before calling this function.
+    The logic is identical to :func:`~torchtune.models.convert_weights.hf_to_tune`, but with a different mapping.
+
+    Eg of HF-format state dict can be found in the ``Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback``
+    repo in HF.
+
+    Args:
+        state_dict (Dict[str, torch.Tensor]): State dict in HF's format.
+        num_heads (int): Number of heads in the model.
+        num_kv_heads (int): Number of heads in the key/value projection layers.
+        dim (int): Dimension of the model.
+        head_dim (int): Dimension of the head. If not provided, it will be calculated
+            as dim // num_heads.
+
+    Returns:
+        Dict[str, torch.Tensor]: State dict in torchtune's format.
+    """
+    converted_state_dict = {}
+    if head_dim is None:
+        head_dim = dim // num_heads
+
+    def _permute(t, n_heads):
+        return (
+            t.view(n_heads, 2, head_dim // 2, dim)
+            .transpose(1, 2)
+            .reshape((head_dim * n_heads), dim)
+        )
+
+    for key, value in state_dict.items():
+        # ignore output layer bias - these are not used in the reward model
+        # and some HF pipelines (e.g. TRL) may save them
+        if key == "score.bias":
+            continue
+        # Skip loading the position embeddings
+        if "rotary_emb.inv_freq" not in key:
+            new_key = get_mapped_key(key, _REWARD)
+        if "q_proj" in key:
+            value = _permute(value, num_heads)
+        elif "k_proj" in key:
+            value = _permute(value, num_kv_heads)
+        converted_state_dict[new_key] = value
+    return converted_state_dict
+
+
+def reward_tune_to_hf(
+    state_dict: Dict[str, torch.Tensor],
+    num_heads: int = 32,
+    num_kv_heads: int = 32,
+    dim: int = 4096,
+) -> Dict[str, torch.Tensor]:
+    """
+    Convert a state dict from torchtune's format to Hugging Face's format for a reward model.
+
+    This function takes a state dictionary in torchtune's format, which contains the weights of a reward model
+    (i.e. a classifier with a single class), and converts it into a format that can be loaded into a Hugging Face model.
+    The logic is identical to :func:`~torchtune.models.convert_weights.tune_to_hf`, but with a different mapping.
+
+    Args:
+        state_dict (Dict[str, torch.Tensor]): State dict in torchtune's format.
+        num_heads (int, optional): Number of heads in the model. Defaults to 32.
+        num_kv_heads (int, optional): Number of heads in the key/value projection layers. Defaults to 32.
+        dim (int, optional): Dimension of the model. Defaults to 4096.
+
+    Returns:
+        Dict[str, torch.Tensor]: State dict in Hugging Face's format.
+
+    """
+    converted_state_dict = {}
+    inverted_mapping_dict = {v: k for k, v in _REWARD.items()}
+    head_dim = dim // num_heads
+
+    def _permute(t, n_heads):
+        return (
+            t.view(n_heads, head_dim // 2, 2, dim)
+            .transpose(1, 2)
+            .reshape((head_dim * n_heads), dim)
+        )
+
+    for key, value in state_dict.items():
+        new_key = get_mapped_key(key, inverted_mapping_dict)
+        if "q_proj" in key:
+            value = _permute(value, num_heads)
+        elif "k_proj" in key:
+            value = _permute(value, num_kv_heads)
+        converted_state_dict[new_key] = value
+
+    return converted_state_dict
diff -ruN marc_original/third_party/torchtune/torchtune/rlhf/utils/__init__.py marc/third_party/torchtune/torchtune/rlhf/utils/__init__.py
--- marc_original/third_party/torchtune/torchtune/rlhf/utils/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/rlhf/utils/__init__.py	2025-02-20 17:49:30.778026264 -0500
@@ -0,0 +1,12 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._convert_weights import reward_hf_to_tune, reward_tune_to_hf  # noqa
+
+__all__ = [
+    "reward_hf_to_tune",
+    "reward_tune_to_hf",
+]
Binary files marc_original/third_party/torchtune/torchtune/rlhf/utils/__pycache__/_convert_weights.cpython-312.pyc and marc/third_party/torchtune/torchtune/rlhf/utils/__pycache__/_convert_weights.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/rlhf/utils/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/rlhf/utils/__pycache__/__init__.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/training/_activation_offloading.py marc/third_party/torchtune/torchtune/training/_activation_offloading.py
--- marc_original/third_party/torchtune/torchtune/training/_activation_offloading.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/_activation_offloading.py	2025-02-20 17:49:30.790026285 -0500
@@ -0,0 +1,329 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional
+from warnings import warn
+
+import psutil
+import torch
+import torchao
+from torch.autograd.graph import saved_tensors_hooks
+from torchao.dtypes.nf4tensor import NF4Tensor
+
+
+class OffloadActivations(saved_tensors_hooks):
+    """Context manager under which activation tensors created in the forward pass will be offloaded.
+
+    Enable the memory efficiency technique of activation offloading, where activations bigger than
+    min_offload_size bytes will be offloaded to CPU in the forward and brought back in the backward.
+    This is in contrast to maintaining the activation on GPU VRAM throughout the program.
+
+    This manager contains the option of using one additional CUDA stream to handle the communication
+    between CUDA and CPU, which is intended to overlap with the default computation stream to improve
+    runtime. We designed synchronization with a few heuristics for optimizing the tradeoff between
+    runtime vs memory usage.
+
+    Args:
+        use_pin_memory (bool): Whether or not the offloaded Tensor will be placed in pinned
+            memory on the CPU. Pinned memory allows the Tensor to be moved back onto GPU more quickly
+            but is a limited resource. Default: True.
+
+        use_streams (Optional[bool]): Whether or not to use streams for performance optimization where
+            the communications get overlapped with the computation. Requires a torch build
+            after torch-2.5.0.dev20240907. Default: True if a later torch build is found, else False.
+
+        max_fwd_stash_size (int): The maximum size of the forward stash, or the maximum number of
+            consecutive activations to keep alive during the forward pass. This number must be at
+            least 1. Keeping alive more activations will potentially allow more overlap between the
+            communication and compute streams at the cost of increasing memory usage. Keeping alive
+            fewer activations will conserve memory, but may cause poor overlap between the streams,
+            increasing runtime. Default: 5.
+
+        min_offload_size (int): The minimum number of bytes a Tensor must be in order to qualify
+            for offloading. If the tensor is too small, we do not want to waste bandwidth and resources
+            moving it to CPU and back. Default: 1024 bytes.
+
+    Raises:
+        ValueError: if max_fwd_stash_size is not at least 1.
+        RuntimeError: if use_streams but torch installation is earlier than torch-2.5.0.dev20240907
+
+    Example:
+        >>> with OffloadActivations():
+        >>>     logits = model(inputs)
+        >>> loss = ...
+        >>> loss.backward()
+    """
+
+    def __init__(
+        self,
+        use_pin_memory: bool = True,
+        use_streams: Optional[bool] = None,
+        max_fwd_stash_size: int = 5,
+        min_offload_size: int = 1024,
+    ) -> None:
+        if use_streams is None:
+            # Default to True if an acceptable torch is installed (later nightly/version or from source)
+            self.use_streams = torch.__version__ >= "2.5.0.dev20240907"
+        else:
+            self.use_streams = use_streams
+
+        self.min_tensor_size_bytes = (
+            min_offload_size  # we don't want to bother with small tensors
+        )
+        self.tracker = (
+            {}
+        )  # tensor_id => (new_tensor, if_modified)  ---> track what saved/offloaded tensors are where
+        self.tensor_id: int = 0
+        self.is_first_forward_call = True
+        self.is_first_backward_call = True
+        self.is_first_forward_pass = True
+
+        # managing cpu memory
+        self.use_pin_memory: bool = use_pin_memory
+        self.virtual_memory_safe_pct = (
+            60  # we should not exceed this percentage of memory
+        )
+
+        self.s0 = torch.cuda.default_stream()  # comp stream
+
+        # for streaming
+        if self.use_streams:
+            if torch.__version__ < "2.5.0.dev20240907":
+                raise RuntimeError(
+                    "OffloadActivations with use_streams=True requires PyTorch 2.5.0.dev20240907 or later."
+                )
+            self.s1 = torch.cuda.Stream()  # comms stream
+            self.fwd_stash = {}  # tensor_id => (activation, ev1)
+            if max_fwd_stash_size < 1:
+                raise ValueError(
+                    f"max_fwd_stash_size should be at least 1 but is {max_fwd_stash_size}"
+                )
+            self.max_fwd_stash_size = max_fwd_stash_size
+            self.bwd_tensor_stash = {}  # tensor_id => activation
+            self.bwd_ev_stash = {}  # tensor_id => ev0
+            self.curr_graph_id = None
+            self.curr_autograd_node = None
+
+        # -------- platform util functions -------- #
+        def verify_sufficient_virtual_memory():
+            curr_pct = get_cpu_ram_pct()
+            if curr_pct > self.virtual_memory_safe_pct:
+                warn(
+                    f"***** WARNING: {curr_pct=}% > {self.virtual_memory_safe_pct=}% of virtual memory used"
+                )
+
+        def get_cpu_ram_pct() -> float:
+            # get the percentage of memory used by the system
+            return psutil.virtual_memory().percent
+
+        def get_tensor_id() -> int:
+            # create a unique id for each tensor we are managing
+            self.tensor_id += 1
+            return self.tensor_id
+
+        def get_num_bytes_tensor(x: torch.Tensor) -> int:
+            # get the number of bytes in a tensor, for memory management purposes
+            return (
+                x.element_size() * x.nelement()
+            )  # x.element_size() * x._base_storage().nbytes()
+
+        # -------- core pack / unpack work -------- #
+        def pack_tensor(activation: torch.Tensor) -> int:
+            # activations are passed in during forward pass - from here we take over and return a unique id
+            if self.is_first_forward_call:
+                assert (
+                    len(self.tracker) == 0
+                ), "backward pass should have cleared tracker of all tensors"
+
+                # set training phase trackers
+                self.is_first_forward_call = False
+                self.is_first_backward_call = True
+
+            # query for basic tensor info
+            num_bytes = get_num_bytes_tensor(activation)
+            tensor_id = get_tensor_id()
+
+            # only offload hefty bois
+            if num_bytes >= self.min_tensor_size_bytes:
+                if self.use_streams:
+                    # First, sync back and dereference previously offloaded tensors
+                    # as the offloading should be done sufficiently long ago.
+                    for id in [k for k in self.fwd_stash.keys()]:
+                        if id <= tensor_id - self.max_fwd_stash_size:
+                            _, ev = self.fwd_stash[id]
+                            self.s0.wait_event(ev)
+                            del self.fwd_stash[id]
+                        else:
+                            break
+
+                    # Sync in, offload, and add an event to sync back later
+                    self.s1.wait_stream(self.s0)
+
+                stream = self.s1 if self.use_streams else self.s0
+                with torch.cuda.stream(stream):
+                    try:
+                        cpu_tensor = torch.empty_like(
+                            activation, pin_memory=self.use_pin_memory, device="cpu"
+                        )
+                    except NotImplementedError as e:
+                        if (
+                            isinstance(activation, NF4Tensor)
+                            and torchao.__version__ < "0.6.0.dev20240917"
+                        ):
+                            raise RuntimeError(
+                                "Offloading NF4Tensors requires torchao-0.6.0.dev20240917 or later"
+                            ) from e
+                        raise e
+                    cpu_tensor.copy_(activation, non_blocking=True)
+                    self.tracker[tensor_id] = (
+                        cpu_tensor,
+                        True,
+                    )  # True = (in future) modified
+
+                if self.use_streams:
+                    event = self.s1.record_event()
+
+                    # Stash to keep activation alive til s1 is done
+                    self.fwd_stash[tensor_id] = (activation, event)
+            else:
+                self.tracker[tensor_id] = (
+                    activation,
+                    False,
+                )  # False = not modified, tensor is as is
+
+            return tensor_id
+
+        def unpack_tensor_single_stream(unpack_tensor_id: int) -> torch.Tensor:
+            # backward pass - we are called with the tensor_id, which
+            # we will use to retrieve the saved/offloaded tensor
+            if self.is_first_backward_call:
+                if self.is_first_forward_pass:
+                    self.is_first_forward_pass = False
+                    if self.use_pin_memory:
+                        verify_sufficient_virtual_memory()
+
+                self.is_first_backward_call = False
+                self.is_first_forward_call = True
+
+            assert (
+                unpack_tensor_id in self.tracker
+            ), f"untracked tensor with id {unpack_tensor_id}"
+
+            maybe_gpu_tensor, modified = self.tracker[unpack_tensor_id]
+            if modified:
+                gpu_tensor = maybe_gpu_tensor.to("cuda", non_blocking=True)
+                maybe_gpu_tensor = gpu_tensor
+
+            # clear tensor from tracking
+            del self.tracker[unpack_tensor_id]
+            return maybe_gpu_tensor
+
+        def unpack_tensor_with_streams(unpack_tensor_id: int) -> torch.Tensor:
+            # backward pass - we are called with the tensor_id, which
+            # we will use to retrieve the saved/offloaded tensor
+            if self.is_first_backward_call:
+                self.curr_graph_id = torch._C._current_graph_task_id()
+
+                def wait_and_del_remaining_references() -> None:
+                    for id in [k for k in self.bwd_tensor_stash.keys()]:
+                        event = self.bwd_ev_stash[id]
+                        self.s1.wait_event(event)
+                        del self.bwd_tensor_stash[id]
+
+                # Register a callback to the end of autograd to clean everything up
+                torch.autograd.variable.Variable._execution_engine.queue_callback(
+                    wait_and_del_remaining_references
+                )
+
+                if self.is_first_forward_pass:
+                    self.is_first_forward_pass = False
+                    if self.use_pin_memory:
+                        verify_sufficient_virtual_memory()
+
+                self.is_first_backward_call = False
+                self.is_first_forward_call = True
+
+            assert (
+                unpack_tensor_id in self.tracker
+            ), f"untracked tensor with id {unpack_tensor_id}"
+
+            maybe_gpu_tensor, modified = self.tracker[unpack_tensor_id]
+            if modified:
+                # Get data on the current autograd node
+                graph_id = torch._C._current_graph_task_id()
+                node = torch._C._current_autograd_node()
+                prev_node_ids = []
+
+                # If we're on a new node, mark prev node's tensors to be freed later
+                if graph_id == self.curr_graph_id and self.curr_autograd_node != node:
+                    self.curr_autograd_node = node
+                    prev_node_ids = [id for id in self.bwd_tensor_stash.keys()]
+
+                brought_back_from_cpu = True
+                if unpack_tensor_id in self.fwd_stash:
+                    maybe_gpu_tensor = self.fwd_stash[unpack_tensor_id][0]
+                    brought_back_from_cpu = False
+                else:
+                    # Kick off the process to bring tensors back
+                    with torch.cuda.stream(self.s1):
+                        gpu_tensor = maybe_gpu_tensor.to("cuda", non_blocking=True)
+                        maybe_gpu_tensor = gpu_tensor
+
+                    # Tell comp stream to wait for the info to be loaded before executing
+                    self.s0.wait_stream(self.s1)
+
+                    # Stash the tensor to keep memory alive until compute stream is complete
+                    self.bwd_tensor_stash[unpack_tensor_id] = maybe_gpu_tensor
+
+                def hook(outputs, inputs):
+                    # create events for the current node inputs/outputs if they were streamed in
+                    if brought_back_from_cpu:
+                        event = self.s0.record_event()
+                        self.bwd_ev_stash[unpack_tensor_id] = event
+
+                    # if there are still things in the fwd_stash, get rid of them as we're in bwd now
+                    for id in [k for k in self.fwd_stash.keys()]:
+                        _, ev = self.fwd_stash[id]
+                        self.s0.wait_event(ev)
+                        del self.fwd_stash[id]
+
+                    # wait on prev node's events and del those
+                    for id in prev_node_ids:
+                        event = self.bwd_ev_stash[id]
+                        self.s1.wait_event(event)
+                        del self.bwd_tensor_stash[id]
+
+                    return outputs
+
+                node.register_hook(hook)
+
+            # clear tensor from tracking
+            del self.tracker[unpack_tensor_id]
+            return maybe_gpu_tensor
+
+        unpack_tensor = (
+            unpack_tensor_with_streams
+            if self.use_streams
+            else unpack_tensor_single_stream
+        )
+        super().__init__(pack_tensor, unpack_tensor)
+
+
+class NoOpManager(saved_tensors_hooks):
+    """
+    A saved_tensors_hook manager used to disable any other saved_tensors_hook manager
+    applied before. This relies on the behavior that only the most recently registered
+    saved_tensors_hook will run.
+
+    One example usage is to opt a local region of code out of activations offloading,
+    which is usually applied globally to best track state.
+    """
+
+    def __init__(self) -> None:
+        def noop(tensor):
+            return tensor
+
+        super().__init__(noop, noop)
diff -ruN marc_original/third_party/torchtune/torchtune/training/activations.py marc/third_party/torchtune/torchtune/training/activations.py
--- marc_original/third_party/torchtune/torchtune/training/activations.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/activations.py	2025-02-20 17:49:30.806026310 -0500
@@ -0,0 +1,90 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional, Union
+
+from torch import nn
+from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
+    checkpoint_wrapper as ptd_checkpoint_wrapper,
+    CheckpointImpl,
+)
+from torch.utils.checkpoint import checkpoint
+
+
+# Uses PTD FSDP AC wrapper
+# currently selective per layer checkpointing are supported
+def checkpoint_wrapper(module, ac_mode, ac_style):
+
+    if ac_mode == "full":
+        return ptd_checkpoint_wrapper(
+            module,
+            checkpoint_impl=CheckpointImpl.NO_REENTRANT,
+            checkpoint_fn=checkpoint,
+            use_reentrant=False,
+            preserve_rng_state=False,
+        )
+
+    # selective layer checkpointing...some checks in case we receive '2' or 2...
+    elif ac_mode == "selective":
+        """enables selective checkpointing of candidate layers.
+        Usage:
+        'selective_ac_option' with a positive 'int' value in config controls which layers to checkpoint.
+        1 == checkpointing every one (all).
+        2 == checkpoint every 2nd one
+        """
+        every_x_layer = int(ac_style)
+
+        if not (every_x_layer >= 0):
+            raise ValueError(
+                f"Selective layer AC policy (every_x_layer) expects a positive integer, received {every_x_layer}"
+            )
+
+        checkpoint_wrapper.__dict__.setdefault("_count", 0)
+
+        checkpoint_wrapper._count += 1
+        if not every_x_layer or checkpoint_wrapper._count % every_x_layer == 0:
+            return ptd_checkpoint_wrapper(
+                module,
+                checkpoint_impl=CheckpointImpl.NO_REENTRANT,
+                checkpoint_fn=checkpoint,
+                use_reentrant=False,
+                preserve_rng_state=False,
+            )
+        # skip activation checkpointing and store activations for this layer
+        else:
+            return module
+
+    else:
+        raise NotImplementedError(
+            "Unknown AC type or AC config. Only selective op and selective layer ac implemented currently."
+        )
+
+
+def apply_selective_activation_checkpointing(
+    model: nn.Module,
+    ac_mode: str,
+    ac_option: Optional[Union[int, str]],
+) -> None:
+    """Utility to setup activation checkpointing and wrap the model for checkpointing.
+
+    Args:
+        model (nn.Module): Model to setup activation checkpointing.
+        ac_mode (str): Activation checkpointing mode. ['none', 'full', 'selective']
+        ac_option (Optional[Union[int, str]]): Activation checkpointing option. If ac_mode is
+            "selective", ac_option can be an integer or a string representing the number of layers
+            to checkpoint. If ac_mode is "selective" and ac_option is "op", then selective op ac is run.
+            If ac_mode is "none" or "full", ac_option is ignored.
+    """
+
+    for layer_id, transformer_block in enumerate(model.layers):
+        if ac_mode in ("full", "selective"):
+
+            transformer_block = checkpoint_wrapper(
+                transformer_block,
+                ac_mode,
+                ac_option,
+            )
+        model.layers[layer_id] = transformer_block
diff -ruN marc_original/third_party/torchtune/torchtune/training/checkpointing/_checkpointer.py marc/third_party/torchtune/torchtune/training/checkpointing/_checkpointer.py
--- marc_original/third_party/torchtune/torchtune/training/checkpointing/_checkpointer.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/checkpointing/_checkpointer.py	2025-02-20 17:49:30.814026324 -0500
@@ -0,0 +1,906 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import gc
+import json
+import os
+
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Protocol, Union
+
+import torch
+from safetensors.torch import save_file
+from torchtune import training
+
+from torchtune.models import convert_weights
+from torchtune.models.phi3._convert_weights import phi3_hf_to_tune, phi3_tune_to_hf
+from torchtune.models.qwen2._convert_weights import qwen2_hf_to_tune, qwen2_tune_to_hf
+from torchtune.rlhf.utils import reward_hf_to_tune, reward_tune_to_hf
+from torchtune.training.checkpointing._utils import (
+    FormattedCheckpointFiles,
+    get_path,
+    ModelType,
+    safe_torch_load,
+    save_config,
+)
+from torchtune.utils._logging import get_logger, log_rank_zero
+
+logger = get_logger("DEBUG")
+
+
+class _CheckpointerInterface(Protocol):
+    """
+    Interface implemented by Checkpointers in torchtune.
+
+    torchtune checkpointers are designed to be composable components which can be plugged
+    into any training recipe. Each checkpointer supports a specific set of models and training
+    scenarios making these easy to understand, debug and extend. For example, the
+    ``FullModelCheckpointer``s are used for loading and saving all of the model weights.
+    This checkpointer can be used for Full-Finetuning scenarios or PEFT where the output is a
+    merged checkpoint. In case the current suite of checkpointers are inadequate,
+    users are encouraged to implement their own and contribute back to torchtune.
+
+    torchtune is also designed to be "state-dict invariant". This means the checkpointer
+    ensures that the output checkpoint has the same format as the original checkpoint i.e.
+    the output checkpoint has the same keys split across the same number of files as the original
+    checkpoint. Being "state-dict invariant" allows users to seamlessly use torchtune checkpoints
+    with their favorite post-training tools from the open-source ecosystem without writing
+    torchtune-specific convertors. To be "state-dict invariant", the ``load_checkpoint`` and
+    ``save_checkpoint`` methods make use of the weight convertors available in
+    ``torchtune/models/<model_folder>``.
+
+    torchtune Checkpointers support two checkpointing scenarios:
+        * End-of-training Checkpointing. The model weights at the end of a completed training
+            run are written out to file. The checkpointer ensures that the output checkpoint
+            files have the same keys as the input checkpoint file used to begin training. The
+            checkpointer also ensures that the keys are partitioned across the same number of
+            files as the original checkpoint. This ensures that the original metadata files can
+            be used as is, and the output checkpoint can be used with any tool that understands
+            the original checkpoint format. This includes popular inference engines such as
+            ``llama.cpp`` and ``gpt-fast``. The output state dict has the following format:
+            {
+                "key_1": weight
+                ...
+            }
+
+
+        Mid-training Chekpointing. In addition to the model checkpoint files, we output an
+            additional "recipe_state.pt" file for intermediate checkpoints. These are currently
+            output at the end of each epoch, and contain information such as optimizer state,
+            number of epochs completed etc which is needed to correctly resume a previously
+            interrupted training run. The recipe is responsible for constructing the state dict
+            with the information it needs. The checkpointer extracts the model state dict
+            (key = "model") and writes everything else out to "recipe_state.pt". To prevent us
+            from flooding ``output_dir`` with checkpoint files, the recipe state is overwritten
+            at the end of each epoch. The output state dicts have the following formats:
+
+            Model:
+                {
+                    "key_1": weight
+                    ...
+                }
+
+            Recipe State:
+                {
+                    "optimizer": ...,
+                    "epoch": ...,
+                    ...
+                }
+
+    """
+
+    def load_checkpoint(self, **kwargs) -> Dict[str, Any]:
+        ...
+
+    def save_checkpoint(self, state_dict: Dict[str, Any], **kwargs) -> None:
+        ...
+
+
+class FullModelTorchTuneCheckpointer(_CheckpointerInterface):
+    """
+    Checkpointer which reads and writes checkpoints in a format compatible with
+    torchtune. No conversion of weights is required.
+
+    Currently this supports reading a single checkpoint file only. This will likely change as
+    we add support for larger models.
+
+    Args:
+        checkpoint_dir (str): Directory containing the checkpoint files
+        checkpoint_files (List[str]): List of checkpoint files to load. Since the checkpointer takes care
+            of sorting by file ID, the order in this list does not matter
+        model_type (ModelType): Model type of the model for which the checkpointer is being loaded
+        output_dir (str): Directory to save the checkpoint files
+        adapter_checkpoint (Optional[str]): Path to the adapter weights. Default is None
+        recipe_checkpoint (Optional[str]): Path to the recipe state checkpoint file. Default is None
+        resume_from_checkpoint (bool): If True, the checkpointer will load the additional checkpoint files to
+            resume training from a previous run. Default is False
+
+    Raises:
+        ValueError: If more than one checkpoint file is provided
+        ValueError: If the checkpoint file does not have a .pt extension
+        ValueError: If ``resume_from_checkpoint`` is True but ``recipe_checkpoint`` is None
+
+
+    """
+
+    def __init__(
+        self,
+        checkpoint_dir: str,
+        checkpoint_files: List[str],
+        model_type: ModelType,
+        output_dir: str,
+        adapter_checkpoint: Optional[str] = None,
+        recipe_checkpoint: Optional[str] = None,
+        resume_from_checkpoint: bool = False,
+    ) -> None:
+        # Fail fast if ``checkpoint_files`` is invalid
+        if len(checkpoint_files) != 1:
+            raise ValueError(
+                "Currently we only support reading from a single torchtune checkpoint file. "
+                f"Got {len(checkpoint_files)} files instead."
+            )
+
+        self._checkpoint_dir = Path(checkpoint_dir)
+        self._checkpoint_path = get_path(self._checkpoint_dir, checkpoint_files[0])
+
+        if not self._checkpoint_path.suffix == ".pt":
+            raise ValueError(
+                f"Checkpoint file {self._checkpoint_path} is not a valid checkpoint file. "
+                "Checkpointer expects a valid .pt file."
+            )
+
+        self._adapter_checkpoint = (
+            get_path(self._checkpoint_dir, adapter_checkpoint)
+            if adapter_checkpoint
+            else None
+        )
+
+        self._resume_from_checkpoint = resume_from_checkpoint
+        self._model_type = model_type
+        self._output_dir = Path(output_dir)
+
+        # recipe_checkpoint contains the recipe state. This should be available if
+        # resume_from_checkpoint is True
+        self._recipe_checkpoint = None
+        if self._resume_from_checkpoint:
+            if recipe_checkpoint is None:
+                raise ValueError(
+                    "If resume_from_checkpoint is True, recipe_checkpoint file must be provided."
+                )
+            self._recipe_checkpoint = get_path(self._checkpoint_dir, recipe_checkpoint)
+
+    def load_checkpoint(self, weights_only: bool = True) -> Dict[str, Any]:
+        """
+        Load torchtune checkpoint from file. Currently only loading from a single file is supported.
+
+        The output state_dict has the following format, with keys other than "model" only present if
+        ``resume_from_checkpoint`` is True:
+
+        >>>     {
+        >>>         "model": {
+        >>>             "key_1": weight
+        >>>             ...
+        >>>         },
+        >>>         "optimizer": {...},
+        >>>         ...
+        >>>     }
+
+        Args:
+            weights_only (bool): flag passed down to torch.load. We expose this, because quantized models
+                cannot be loaded with weights_only=True
+
+        Returns:
+            Dict[str, Any]: state_dict from the input checkpoint
+        """
+        state_dict: Dict[str:Any] = {}
+        state_dict[training.MODEL_KEY] = safe_torch_load(
+            self._checkpoint_path, weights_only=weights_only
+        )
+
+        if self._adapter_checkpoint:
+            adapter_state_dict = safe_torch_load(self._adapter_checkpoint)
+            state_dict[training.ADAPTER_KEY] = adapter_state_dict
+
+        if self._resume_from_checkpoint:
+            recipe_state = safe_torch_load(self._recipe_checkpoint, mmap=False)
+            state_dict.update(recipe_state)
+        return state_dict
+
+    def save_checkpoint(
+        self,
+        state_dict: Dict[str, Any],
+        epoch: int,
+        intermediate_checkpoint: bool = False,
+        adapter_only: bool = False,
+    ) -> None:
+        """
+        Save torchtune checkpoint to file. If ``intermediate_checkpoint`` is True, an additional
+        checkpoint file ``recipe_state.pt`` is created in ``_output_dir`` which contains the recipe
+        state. The output state dicts have the following formats:
+
+        >>> # Model
+        >>> {
+        >>>     "key_1": weight
+        >>>     ...
+        >>> }
+        >>>
+        >>> # Recipe state
+        >>> {
+        >>>     "optimizer": ...,
+        >>>     "epoch": ...,
+        >>>     ...
+        >>> }
+
+        Args:
+            state_dict (Dict[str, Any]): State dict with model and (optionally) recipe state
+            epoch (int): Current epoch number. This is added to the checkpoint file name to ensure
+                we're not overwriting intermediate checkpoint files
+            intermediate_checkpoint (bool): If True, save an additional checkpoint file with the
+                recipe state
+            adapter_only (bool): If True, only save the adapter weights. Default is False
+
+
+        Raises:
+            ValueError: if ``adapter_only`` is True and adapter checkpoint not found in state_dict.
+        """
+        self._output_dir.mkdir(exist_ok=True)
+
+        # Output file is always a .pt file with the epoch number in the name
+        if not adapter_only:
+            checkpoint_file = Path.joinpath(
+                self._output_dir, f"torchtune_model_{epoch}"
+            ).with_suffix(".pt")
+            torch.save(state_dict[training.MODEL_KEY], checkpoint_file)
+            logger.info(
+                "Model checkpoint of size "
+                f"{os.path.getsize(checkpoint_file) / 1000**3:.2f} GB "
+                f"saved to {checkpoint_file}"
+            )
+
+        if training.ADAPTER_KEY in state_dict:
+            output_path = Path.joinpath(
+                self._output_dir, f"adapter_{epoch}"
+            ).with_suffix(".pt")
+            torch.save(state_dict[training.ADAPTER_KEY], output_path)
+            logger.info(
+                "Adapter checkpoint of size "
+                f"{os.path.getsize(output_path) / 1000**3:.2f} GB "
+                f"saved to {output_path}"
+            )
+        elif adapter_only:
+            raise ValueError(
+                "Adapter checkpoint not found in state_dict. Please ensure that the state_dict contains adapter weights."
+            )
+
+        # If the recipe state needs to be output, first remove the model state dict
+        if intermediate_checkpoint:
+            _ = state_dict.pop(training.MODEL_KEY)
+            _ = state_dict.pop(training.ADAPTER_KEY, None)
+            _ = state_dict.pop(training.ADAPTER_CONFIG, None)
+            output_path = Path.joinpath(self._output_dir, "recipe_state.pt")
+            torch.save(state_dict, output_path)
+            logger.info(
+                "Recipe checkpoint of size "
+                f"{os.path.getsize(output_path) / 1000**3:.2f} GB "
+                f"saved to {output_path}"
+            )
+        else:
+            logger.info("Saving final epoch checkpoint.")
+            if adapter_only:
+                logger.info(
+                    "Please note that you have set adapter_only=True, so only adapter weights will be saved."
+                    "You need to merge the adapter weights into your base model for further use. "
+                    f"See {self.__class__.__name__}.save_checkpoint for more details."
+                )
+            else:
+                logger.info(
+                    "The full model checkpoint, including all weights and configurations, has been saved successfully."
+                    "You can now use this checkpoint for further training or inference."
+                )
+
+
+class FullModelHFCheckpointer(_CheckpointerInterface):
+    """
+    Checkpointer which reads and writes checkpoints in HF's format. For LoRA models this includes
+    saving checkpoints in a format that can be loaded into PEFT via e.g. ``from_pretrained``. Examples include
+    the Llama-2-7b-hf model from the meta-llama repo (https://huggingface.co/meta-llama/Llama-2-7b-hf).
+
+    Note:
+        HF checkpoint names are usually ordered by ID (eg: 0001_of_0003, 0002_of_0003, etc.) To ensure \
+        we read the files in the right order, we sort the checkpoint file names before reading.
+
+    Note:
+        Checkpoint conversion to and from HF's format requires access to model params which are \
+        read directly from the ``config.json`` file. This helps ensure we either load the weights \
+        correctly or error out in case of discrepancy between the HF checkpoint file and torchtune's \
+        model implementations.
+
+    Args:
+        checkpoint_dir (str): Directory containing the checkpoint files
+        checkpoint_files (Union[List[str], Dict[str, str]]): List of checkpoint files to load. Since the checkpointer takes care
+            of sorting by file ID, the order in this list does not matter. TODO: update this
+        model_type (ModelType): Model type of the model for which the checkpointer is being loaded
+        output_dir (str): Directory to save the checkpoint files
+        adapter_checkpoint (Optional[str]): Path to the adapter weights. Default is None
+        recipe_checkpoint (Optional[str]): Path to the recipe state checkpoint file. Default is None
+        resume_from_checkpoint (bool): If True, the checkpointer will load the additional checkpoint files to
+            resume training from a previous run. Default is False
+        safe_serialization (bool): If True, the checkpointer will save the checkpoint file using `safetensors`
+
+    Raises:
+        ValueError: If ``resume_from_checkpoint`` is True but ``recipe_checkpoint`` is None
+    """
+
+    def __init__(
+        self,
+        checkpoint_dir: str,
+        checkpoint_files: Union[List[str], Dict[str, str]],
+        model_type: ModelType,
+        output_dir: str,
+        adapter_checkpoint: Optional[str] = None,
+        recipe_checkpoint: Optional[str] = None,
+        resume_from_checkpoint: bool = False,
+        safe_serialization: bool = False,
+    ) -> None:
+        self._checkpoint_dir = Path(checkpoint_dir)
+
+        if not isinstance(checkpoint_files, List):
+            formatted_checkpoint_files = FormattedCheckpointFiles.from_dict(
+                checkpoint_files
+            )
+            checkpoint_files = formatted_checkpoint_files.build_checkpoint_filenames()
+        self._checkpoint_paths = self._validate_hf_checkpoint_files(checkpoint_files)
+        self._adapter_checkpoint = (
+            get_path(self._checkpoint_dir, adapter_checkpoint)
+            if adapter_checkpoint
+            else None
+        )
+
+        self._model_type = ModelType[model_type]
+        self._output_dir = Path(output_dir)
+        self._resume_from_checkpoint = resume_from_checkpoint
+        self._safe_serialization = safe_serialization
+
+        # weight_map contains the state_dict key -> checkpoint file mapping so we can correctly
+        # parition the state dict into output checkpoint files. This is updated during checkpoint
+        # load
+        self._weight_map: Dict[str, str] = None
+
+        # the config.json file contains model params needed for state dict conversion
+        self._config = json.loads(
+            Path.joinpath(self._checkpoint_dir, "config.json").read_text()
+        )
+
+        # save config.json to output_dir
+        save_config(self._output_dir, self._config)
+
+        # recipe_checkpoint contains the recipe state. This should be available if
+        # resume_from_checkpoint is True
+        self._recipe_checkpoint = None
+        if self._resume_from_checkpoint:
+            if recipe_checkpoint is None:
+                raise ValueError(
+                    "If resume_from_checkpoint is True, recipe_checkpoint file must be provided."
+                )
+            self._recipe_checkpoint = get_path(self._checkpoint_dir, recipe_checkpoint)
+
+    def _validate_hf_checkpoint_files(self, checkpoint_files: List[str]) -> List[Path]:
+        """
+        Validates that the checkpoint files exist and sorts based on ID.
+        """
+        checkpoint_paths: List[Path] = []
+        for f in checkpoint_files:
+            checkpoint_path = get_path(self._checkpoint_dir, f)
+            checkpoint_paths.append(checkpoint_path)
+        return sorted(checkpoint_paths)
+
+    def load_checkpoint(self) -> Dict[str, Any]:
+        """
+        Load HF checkpoint from file.
+
+        The keys and weights from across all checkpoint files are merged into a single state_dict.
+        We preserve the "state_dict key" <-> "checkpoint file" mapping in weight_map so we can
+        write the state dict correctly in ``save_checkpoint``.
+
+        Before returning, the model state dict is converted to a torchtune-compatible format using
+        the appropriate convert_weights function (depending on ``self._model_type``).
+
+        Returns:
+            state_dict (Dict[str, Any]): torchtune checkpoint state dict
+
+        Raises:
+            ValueError: If the values in the input state_dict are not Tensors
+        """
+        self._weight_map = {}
+
+        # merged state_dict contains keys and weights from all the checkpoint files
+        merged_state_dict: Dict[str, torch.Tensor] = {}
+
+        # converted_state_dict is the final state_dict passed to the recipe after the
+        # keys are converted into the torchtune format. This optionally also contains
+        # the recipe state and adapter weights
+        converted_state_dict: Dict[str, Dict[str, torch.Tensor]] = {}
+
+        # _checkpoint_paths are already sorted so simply enumerate to generate the right id
+        for cpt_idx, cpt_path in enumerate(self._checkpoint_paths):
+            state_dict = safe_torch_load(cpt_path)
+            for key, value in state_dict.items():
+                # Ensure that the state dict is a flat dict of keys and tensors. Breaking this assumption
+                # will break recipe code
+                if not isinstance(value, torch.Tensor):
+                    raise ValueError(
+                        f"Expected all values in the state dict to be torch.Tensor. "
+                        f"Found {type(value)} instead."
+                    )
+                # idx is written in the 4 digit format (eg: 0001, 0002, etc.)
+                self._weight_map[key] = f"{cpt_idx + 1:04}"
+            merged_state_dict.update(state_dict)
+
+            # delete the state_dict to free up memory; TODO check if this del is needed
+            del state_dict
+            gc.collect()
+        if self._model_type == ModelType.PHI3_MINI:
+            log_rank_zero(
+                logger=logger,
+                msg="Converting Phi-3 Mini weights from HF format."
+                "Note that conversion of adapter weights into PEFT format is not supported.",
+            )
+            converted_state_dict[training.MODEL_KEY] = phi3_hf_to_tune(
+                merged_state_dict
+            )
+        elif self._model_type == ModelType.REWARD:
+            converted_state_dict[training.MODEL_KEY] = reward_hf_to_tune(
+                merged_state_dict,
+                num_heads=self._config["num_attention_heads"],
+                num_kv_heads=self._config["num_key_value_heads"],
+                dim=self._config["hidden_size"],
+            )
+        elif self._model_type == ModelType.QWEN2:
+            converted_state_dict[training.MODEL_KEY] = qwen2_hf_to_tune(
+                merged_state_dict,
+                num_heads=self._config["num_attention_heads"],
+                num_kv_heads=self._config["num_key_value_heads"],
+                dim=self._config["hidden_size"],
+                tie_word_embeddings=self._config["tie_word_embeddings"],
+            )
+        elif self._model_type == ModelType.LLAMA3_VISION:
+            from torchtune.models.llama3_2_vision._convert_weights import (
+                llama3_vision_hf_to_tune,
+            )
+
+            text_config = self._config.get("text_config", {})
+            vision_config = self._config.get("vision_config", {})
+            converted_state_dict[training.MODEL_KEY] = llama3_vision_hf_to_tune(
+                merged_state_dict,
+                num_heads=text_config["num_attention_heads"],
+                num_kv_heads=text_config["num_key_value_heads"],
+                dim=text_config["hidden_size"],
+                head_dim=text_config.get("head_dim", None),
+                vocab_size=text_config["vocab_size"],
+                cross_attention_layers=text_config.get("cross_attention_layers", None),
+                encoder_dim=vision_config["hidden_size"],
+                tile_size=vision_config["image_size"],
+                num_tiles=vision_config["max_num_tiles"],
+                supported_aspect_ratios=vision_config.get(
+                    "supported_aspect_ratios", None
+                ),
+            )
+        else:
+            converted_state_dict[training.MODEL_KEY] = convert_weights.hf_to_tune(
+                merged_state_dict,
+                num_heads=self._config["num_attention_heads"],
+                num_kv_heads=self._config["num_key_value_heads"],
+                dim=self._config["hidden_size"],
+                head_dim=self._config.get("head_dim", None),
+            )
+
+        if self._adapter_checkpoint:
+            adapter_state_dict = safe_torch_load(self._adapter_checkpoint)
+            converted_state_dict[training.ADAPTER_KEY] = adapter_state_dict
+
+        if self._resume_from_checkpoint:
+            recipe_state = safe_torch_load(self._recipe_checkpoint, mmap=False)
+            converted_state_dict.update(recipe_state)
+        return converted_state_dict
+
+    def save_checkpoint(
+        self,
+        state_dict: Dict[str, Any],
+        epoch: int,
+        intermediate_checkpoint: bool = False,
+        adapter_only: bool = False,
+    ) -> None:
+        """
+        Save HF checkpoint to file. If ``intermediate_checkpoint`` is True, an additional
+        checkpoint file ``recipe_state.pt`` is created in ``_output_dir`` which contains the recipe
+        state.
+
+        The state_dict is first converted back to the HF format and then partitioned based on the
+        ``_weight_map`` into separate checkpoint files.
+
+        Args:
+            state_dict (Dict[str, Any]): Checkpoint state dict to be written out to file
+            epoch (int): Epoch number. Used to create the checkpoint file name
+            intermediate_checkpoint (bool): If True, an additional checkpoint files for recipe state
+                and (if applicable) adapter weights are created. Default is False
+            adapter_only (bool): If True, only save the adapter weights. Default is False
+
+        Raises:
+            ValueError: if ``adapter_only`` is True and adapter checkpoint not found in state_dict.
+        """
+        self._output_dir.mkdir(exist_ok=True)
+
+        # convert the state_dict back to hf format; do this inplace
+        if not adapter_only:
+            if self._model_type == ModelType.PHI3_MINI:
+                state_dict[training.MODEL_KEY] = phi3_tune_to_hf(
+                    state_dict[training.MODEL_KEY]
+                )
+            elif self._model_type == ModelType.REWARD:
+                state_dict[training.MODEL_KEY] = reward_tune_to_hf(
+                    state_dict[training.MODEL_KEY],
+                    num_heads=self._config["num_attention_heads"],
+                    num_kv_heads=self._config["num_key_value_heads"],
+                    dim=self._config["hidden_size"],
+                )
+            elif self._model_type == ModelType.QWEN2:
+                state_dict[training.MODEL_KEY] = qwen2_tune_to_hf(
+                    state_dict[training.MODEL_KEY],
+                    num_heads=self._config["num_attention_heads"],
+                    num_kv_heads=self._config["num_key_value_heads"],
+                    dim=self._config["hidden_size"],
+                    tie_word_embeddings=self._config["tie_word_embeddings"],
+                )
+            elif self._model_type == ModelType.LLAMA3_VISION:
+                from torchtune.models.llama3_2_vision._convert_weights import (
+                    llama3_vision_tune_to_hf,
+                )
+
+                text_config = self._config.get("text_config", {})
+                vision_config = self._config.get("vision_config", {})
+                state_dict[training.MODEL_KEY] = llama3_vision_tune_to_hf(
+                    state_dict[training.MODEL_KEY],
+                    num_heads=text_config["num_attention_heads"],
+                    num_kv_heads=text_config["num_key_value_heads"],
+                    dim=text_config["hidden_size"],
+                    head_dim=text_config.get("head_dim", None),
+                    vocab_size=text_config["vocab_size"],
+                    cross_attention_layers=text_config.get(
+                        "cross_attention_layers", None
+                    ),
+                    encoder_dim=vision_config["hidden_size"],
+                    tile_size=vision_config["image_size"],
+                    num_tiles=vision_config["max_num_tiles"],
+                    supported_aspect_ratios=vision_config.get(
+                        "supported_aspect_ratios", None
+                    ),
+                )
+            else:
+                state_dict[training.MODEL_KEY] = convert_weights.tune_to_hf(
+                    state_dict[training.MODEL_KEY],
+                    num_heads=self._config["num_attention_heads"],
+                    num_kv_heads=self._config["num_key_value_heads"],
+                    dim=self._config["hidden_size"],
+                    head_dim=self._config.get("head_dim", None),
+                )
+
+            # split the state_dict into separate dicts, one for each output checkpoint file
+            split_state_dicts: Dict[str, Dict[str, torch.Tensor]] = {}
+            for key, weight in state_dict[training.MODEL_KEY].items():
+                cpt_idx = self._weight_map[key]
+                if cpt_idx not in split_state_dicts:
+                    split_state_dicts[cpt_idx] = {}
+                split_state_dicts[cpt_idx].update({key: weight})
+
+            # write the partitioned state dicts to the right checkpoint file
+            for cpt_idx, model_state_dict in split_state_dicts.items():
+                if not self._safe_serialization:
+                    output_path = Path.joinpath(
+                        self._output_dir, f"hf_model_{cpt_idx}_{epoch}"
+                    ).with_suffix(".pt")
+                    torch.save(model_state_dict, output_path)
+                else:
+                    output_path = Path.joinpath(
+                        self._output_dir,
+                        f"model-0{cpt_idx}-of-0{list(split_state_dicts.keys())[-1]}_{epoch}",
+                    ).with_suffix(".safetensors")
+                    save_file(model_state_dict, output_path, metadata={"format": "pt"})
+                logger.info(
+                    "Model checkpoint of size "
+                    f"{os.path.getsize(output_path) / 1000**3:.2f} GB "
+                    f"saved to {output_path}"
+                )
+
+        if training.ADAPTER_KEY in state_dict:
+            # Save torchtune format adapter weights even if we save PEFT format
+            # This way we can resume no matter what (and memory footprint of adapter weights is small)
+            output_path = Path.joinpath(
+                self._output_dir, f"adapter_{epoch}"
+            ).with_suffix(".pt")
+            torch.save(state_dict[training.ADAPTER_KEY], output_path)
+            logger.info(
+                "Adapter checkpoint of size "
+                f"{os.path.getsize(output_path) / 1000**3:.2f} GB "
+                f"saved to {output_path}"
+            )
+
+            if self._model_type == ModelType.PHI3_MINI:
+                logger.warning(
+                    "Saving Phi-3 Mini adapter weights to PEFT format is not supported, saving to torchtune format instead"
+                )
+            else:
+                state_dict[
+                    training.ADAPTER_KEY
+                ] = convert_weights.tune_to_peft_adapter_weights(
+                    state_dict[training.ADAPTER_KEY],
+                    num_heads=self._config["num_attention_heads"],
+                    num_kv_heads=self._config["num_key_value_heads"],
+                    dim=self._config["hidden_size"],
+                    head_dim=self._config.get("head_dim", None),
+                )
+                peft_output_path = Path.joinpath(
+                    self._output_dir, "adapter_model"
+                ).with_suffix(".bin")
+                torch.save(state_dict[training.ADAPTER_KEY], peft_output_path)
+                logger.info(
+                    "Adapter checkpoint of size "
+                    f"{os.path.getsize(output_path) / 1000**3:.2f} GB "
+                    f"saved to {peft_output_path}"
+                )
+        elif adapter_only:
+            raise ValueError(
+                "Adapter checkpoint not found in state_dict. Please ensure that the state_dict contains adapter weights."
+            )
+
+        if training.ADAPTER_CONFIG in state_dict:
+            if self._model_type == ModelType.PHI3_MINI:
+                logger.warning(
+                    "PEFT integration for Phi-3 Mini is not supported, skipping adapter config save"
+                )
+            else:
+                state_dict[
+                    training.ADAPTER_CONFIG
+                ] = convert_weights.tune_to_peft_adapter_config(
+                    state_dict[training.ADAPTER_CONFIG]
+                )
+                output_path = Path.joinpath(self._output_dir, "adapter_config.json")
+                with open(output_path, "w") as f:
+                    json.dump(state_dict[training.ADAPTER_CONFIG], f)
+                logger.info(
+                    "Adapter checkpoint of size "
+                    f"{os.path.getsize(output_path) / 1000**3:.2f} GB "
+                    f"saved to {output_path}"
+                )
+
+        # If the recipe state needs to be output, first remove the model state dict
+        # and if it exists, remove the adapter state dict as well
+        if intermediate_checkpoint:
+            _ = state_dict.pop(training.MODEL_KEY, None)
+            _ = state_dict.pop(training.ADAPTER_KEY, None)
+            _ = state_dict.pop(training.ADAPTER_CONFIG, None)
+            output_path = Path.joinpath(self._output_dir, "recipe_state.pt")
+            torch.save(state_dict, output_path)
+            logger.info(
+                "Recipe checkpoint of size "
+                f"{os.path.getsize(output_path) / 1000**3:.2f} GB "
+                f"saved to {output_path}"
+            )
+        else:
+            logger.info("Saving final epoch checkpoint.")
+            if adapter_only:
+                logger.info(
+                    "Please note that you have set adapter_only=True, so only adapter weights will be saved."
+                    "You need to merge the adapter weights into your base model for further use. "
+                    f"See {self.__class__.__name__}.save_checkpoint for more details."
+                )
+            else:
+                logger.info(
+                    "The full model checkpoint, including all weights and configurations, has been saved successfully."
+                    "You can now use this checkpoint for further training or inference."
+                )
+
+
+class FullModelMetaCheckpointer(_CheckpointerInterface):
+    """
+    Checkpointer which reads and writes checkpoints in Meta's format. Examples include
+    the Llama-2-7b model from the meta-llama repo (https://huggingface.co/meta-llama/Llama-2-7b)
+
+    Currently we support reading from a single checkpoint file only. Support for reading from
+    sharded checkpoints is WIP.
+
+    Args:
+        checkpoint_dir (str): Directory containing the checkpoint files
+        checkpoint_files (List[str]): List of checkpoint files to load. Currently this checkpointer only
+            supports loading a single checkpoint file.
+        model_type (ModelType): Model type of the model for which the checkpointer is being loaded
+        output_dir (str): Directory to save the checkpoint files
+        adapter_checkpoint (Optional[str]): Path to the adapter weights. Default is None
+        recipe_checkpoint (Optional[str]): Path to the recipe state checkpoint file. Default is None
+        resume_from_checkpoint (bool): If True, the checkpointer will load the additional checkpoint files to
+            resume training from a previous run. Default is False
+
+    Raises:
+        ValueError: If ``checkpoint_files`` is not a list of length 1
+        ValueError: If ``resume_from_checkpoint`` is True but ``recipe_checkpoint`` is None
+    """
+
+    def __init__(
+        self,
+        checkpoint_dir: str,
+        checkpoint_files: List[str],
+        model_type: ModelType,
+        output_dir: str,
+        adapter_checkpoint: Optional[str] = None,
+        recipe_checkpoint: Optional[str] = None,
+        resume_from_checkpoint: bool = False,
+    ) -> None:
+        # Fail fast if ``checkpoint_files`` is invalid
+        if len(checkpoint_files) != 1:
+            raise ValueError(
+                "Currently we only support reading from a single torchtune checkpoint file. "
+                f"Got {len(checkpoint_files)} files instead."
+            )
+
+        self._checkpoint_dir = Path(checkpoint_dir)
+        self._checkpoint_path = get_path(self._checkpoint_dir, checkpoint_files[0])
+
+        self._adapter_checkpoint = (
+            get_path(self._checkpoint_dir, adapter_checkpoint)
+            if adapter_checkpoint
+            else None
+        )
+
+        self._resume_from_checkpoint = resume_from_checkpoint
+        self._model_type = ModelType[model_type]
+        self._output_dir = Path(output_dir)
+
+        # recipe_checkpoint contains the recipe state. This should be available if
+        # resume_from_checkpoint is True
+        self._recipe_checkpoint = None
+        if self._resume_from_checkpoint:
+            if recipe_checkpoint is None:
+                raise ValueError(
+                    "If resume_from_checkpoint is True, recipe_checkpoint file must be provided."
+                )
+            self._recipe_checkpoint = get_path(self._checkpoint_dir, recipe_checkpoint)
+
+    def load_checkpoint(self) -> Dict[str, Any]:
+        """
+        Load Meta checkpoint from file. Currently only loading from a single file is supported.
+        """
+        state_dict: Dict[str:Any] = {}
+        model_state_dict = safe_torch_load(self._checkpoint_path)
+        if self._model_type == ModelType.LLAMA3_VISION:
+            from torchtune.models.llama3_2_vision._convert_weights import (
+                llama3_vision_meta_to_tune,
+            )
+
+            state_dict[training.MODEL_KEY] = llama3_vision_meta_to_tune(
+                model_state_dict
+            )
+        else:
+            state_dict[training.MODEL_KEY] = convert_weights.meta_to_tune(
+                model_state_dict
+            )
+
+        # llama3_2 has tied weights, so we need to remove the output.weight key
+        if self._model_type == ModelType.LLAMA3_2:
+            logger.info(
+                "Identified model_type = Llama3_2. Ignoring output.weight in"
+                " checkpoint in favor of the tok_embedding.weight"
+                " tied weights."
+            )
+            state_dict[training.MODEL_KEY].pop("output.weight")
+
+        if self._adapter_checkpoint:
+            adapter_state_dict = safe_torch_load(self._adapter_checkpoint)
+            state_dict[training.ADAPTER_KEY] = adapter_state_dict
+
+        if self._resume_from_checkpoint:
+            recipe_state = safe_torch_load(self._recipe_checkpoint, mmap=False)
+            state_dict.update(recipe_state)
+        return state_dict
+
+    def save_checkpoint(
+        self,
+        state_dict: Dict[str, Any],
+        epoch: int,
+        intermediate_checkpoint: bool = False,
+        adapter_only: bool = False,
+    ) -> None:
+        """
+        Save Meta checkpoint to file. If ``intermediate_checkpoint`` is True, an additional
+        checkpoint file ``recipe_state.pt`` is created in ``_output_dir`` which contains the recipe
+        state.
+
+        Args:
+            state_dict (Dict[str, Any]): Checkpoint state dict to be written out to file
+            epoch (int): Epoch number. Used to create the checkpoint file name
+            intermediate_checkpoint (bool): If True, an additional checkpoint files for recipe state
+                and (if applicable) adapter weights are created. Default is False
+            adapter_only (bool): If True, only save the adapter weights. Default is False
+
+        Raises:
+            ValueError: if ``adapter_only`` is True and adapter checkpoint not found in state_dict.
+        """
+        self._output_dir.mkdir(exist_ok=True)
+
+        if not adapter_only:
+            model_state_dict = state_dict[training.MODEL_KEY]
+            if self._model_type == ModelType.LLAMA3_VISION:
+                from torchtune.models.llama3_2_vision._convert_weights import (
+                    llama3_vision_tune_to_meta,
+                )
+
+                state_dict[training.MODEL_KEY] = llama3_vision_tune_to_meta(
+                    model_state_dict
+                )
+            else:
+                # llama3_2 has tied weights, so we need to add the output.weight key
+                if (
+                    self._model_type == ModelType.LLAMA3_2
+                    and "output.weight" not in model_state_dict
+                ):
+                    model_state_dict["output.weight"] = model_state_dict[
+                        "tok_embeddings.weight"
+                    ]
+
+                state_dict[training.MODEL_KEY] = convert_weights.tune_to_meta(
+                    model_state_dict
+                )
+
+            # Output file is always a .pt file with the epoch number in the name
+            checkpoint_file = Path.joinpath(
+                self._output_dir, f"meta_model_{epoch}"
+            ).with_suffix(".pt")
+            torch.save(state_dict[training.MODEL_KEY], checkpoint_file)
+            logger.info(
+                "Model checkpoint of size "
+                f"{os.path.getsize(checkpoint_file) / 1000**3:.2f} GB "
+                f"saved to {checkpoint_file}"
+            )
+
+        if training.ADAPTER_KEY in state_dict:
+            output_path = Path.joinpath(
+                self._output_dir, f"adapter_{epoch}"
+            ).with_suffix(".pt")
+            torch.save(state_dict[training.ADAPTER_KEY], output_path)
+            logger.info(
+                "Adapter checkpoint of size "
+                f"{os.path.getsize(output_path) / 1000**3:.2f} GB "
+                f"saved to {output_path}"
+            )
+        elif adapter_only:
+            raise ValueError(
+                "Adapter checkpoint not found in state_dict. Please ensure that the state_dict contains adapter weights."
+            )
+
+        # If the recipe state needs to be output, first remove the model state dict
+        # and if it exists, remove the adapter state dict as well
+        if intermediate_checkpoint:
+            _ = state_dict.pop(training.MODEL_KEY)
+            _ = state_dict.pop(training.ADAPTER_KEY, None)
+            _ = state_dict.pop(training.ADAPTER_CONFIG, None)
+            output_path = Path.joinpath(self._output_dir, "recipe_state.pt")
+            torch.save(state_dict, output_path)
+            logger.info(
+                "Recipe checkpoint of size "
+                f"{os.path.getsize(output_path) / 1000**3:.2f} GB "
+                f"saved to {output_path}"
+            )
+        else:
+            logger.info("Saving final epoch checkpoint.")
+            if adapter_only:
+                logger.info(
+                    "Please note that you have set adapter_only=True, so only adapter weights will be saved."
+                    "You need to merge the adapter weights into your base model for further use. "
+                    f"See {self.__class__.__name__}.save_checkpoint for more details."
+                )
+            else:
+                logger.info(
+                    "The full model checkpoint, including all weights and configurations, has been saved successfully."
+                    "You can now use this checkpoint for further training or inference."
+                )
diff -ruN marc_original/third_party/torchtune/torchtune/training/checkpointing/__init__.py marc/third_party/torchtune/torchtune/training/checkpointing/__init__.py
--- marc_original/third_party/torchtune/torchtune/training/checkpointing/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/checkpointing/__init__.py	2025-02-20 17:49:30.810026317 -0500
@@ -0,0 +1,53 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from typing import Union
+
+from torchtune.training.checkpointing._checkpointer import (
+    FullModelHFCheckpointer,
+    FullModelMetaCheckpointer,
+    FullModelTorchTuneCheckpointer,
+)
+from torchtune.training.checkpointing._utils import (
+    ADAPTER_CONFIG,
+    ADAPTER_KEY,
+    EPOCHS_KEY,
+    FormattedCheckpointFiles,
+    MAX_STEPS_KEY,
+    MODEL_KEY,
+    ModelType,
+    OPT_KEY,
+    RNG_KEY,
+    SEED_KEY,
+    STEPS_KEY,
+    TOTAL_EPOCHS_KEY,
+    update_state_dict_for_classifier,
+)
+
+Checkpointer = Union[
+    FullModelHFCheckpointer,
+    FullModelMetaCheckpointer,
+    FullModelTorchTuneCheckpointer,
+]
+
+__all__ = [
+    "FullModelHFCheckpointer",
+    "FullModelMetaCheckpointer",
+    "FullModelTorchTuneCheckpointer",
+    "ModelType",
+    "Checkpointer",
+    "update_state_dict_for_classifier",
+    "ADAPTER_CONFIG",
+    "ADAPTER_KEY",
+    "EPOCHS_KEY",
+    "MAX_STEPS_KEY",
+    "MODEL_KEY",
+    "OPT_KEY",
+    "RNG_KEY",
+    "SEED_KEY",
+    "STEPS_KEY",
+    "TOTAL_EPOCHS_KEY",
+    "FormattedCheckpointFiles",
+]
Binary files marc_original/third_party/torchtune/torchtune/training/checkpointing/__pycache__/_checkpointer.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/checkpointing/__pycache__/_checkpointer.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/checkpointing/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/checkpointing/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/checkpointing/__pycache__/_utils.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/checkpointing/__pycache__/_utils.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/training/checkpointing/_utils.py marc/third_party/torchtune/torchtune/training/checkpointing/_utils.py
--- marc_original/third_party/torchtune/torchtune/training/checkpointing/_utils.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/checkpointing/_utils.py	2025-02-20 17:49:30.818026331 -0500
@@ -0,0 +1,275 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import json
+import string
+from enum import Enum
+from pathlib import Path
+from typing import Any, Dict, Iterable, Tuple
+from warnings import warn
+
+import torch
+from safetensors import safe_open
+
+"""
+Keys used during checkpoint load and checkpoint save.
+"""
+
+# adapter config containing info about LoRA modules, rank, alpha
+ADAPTER_CONFIG = "adapter_config"
+# key used for adapter weights such as LoRA weights
+ADAPTER_KEY = "adapter"
+# number of epochs completed thus far
+EPOCHS_KEY = "epochs_run"
+MAX_STEPS_KEY = "max_steps_per_epoch"
+MODEL_KEY = "model"
+OPT_KEY = "optimizer"
+SEED_KEY = "seed"
+# total number of epochs for training; resumed training runs for
+# (total_epochs - epochs_run) number of epochs
+TOTAL_EPOCHS_KEY = "total_epochs"
+# number of steps completed thus far - for PPO
+STEPS_KEY = "steps_run"
+# rng state for ensuring correct training resuming in PPO
+RNG_KEY = "rng_state"
+
+
+class ModelType(Enum):
+    """ModelType is used by the checkpointer to distinguish between different model architectures.
+
+    If you are adding a new model that follows a different format than those in the repo already,
+    you can add a new ModelType to gate on weight conversion logic unique to that model.
+
+    Attributes:
+        GEMMA (str): Gemma family of models. See :func:`~torchtune.models.gemma.gemma`
+        LLAMA2 (str): Llama2 family of models. See :func:`~torchtune.models.llama2.llama2`
+        LLAMA3 (str): Llama3 family of models. See :func:`~torchtune.models.llama3.llama3`
+        LLAMA3_2 (str): Llama3.2 family of models. See :func:`~torchtune.models.llama3_2.llama3_2`
+        LLAMA3_VISION (str): LLama3 vision family of models. See :func:`~torchtune.models.llama3_2_vision.llama3_2_vision_decoder`
+        MISTRAL (str): Mistral family of models. See :func:`~torchtune.models.mistral.mistral`
+        PHI3_MINI (str): Phi-3 family of models. See :func:`~torchtune.models.phi3.phi3`
+        REWARD (str): A Llama2, Llama3, or Mistral model with a classification head projecting
+            to a single class for reward modelling.
+            See :func:`~torchtune.models.mistral.mistral_reward_7b` or :func:`~torchtune.models.llama2.llama2_reward_7b`
+        QWEN2 (str): Qwen2 family of models. See :func:`~torchtune.models.qwen2.qwen2`
+
+    Example:
+        >>> # Usage in a checkpointer class
+        >>> def load_checkpoint(self, ...):
+        >>>     ...
+        >>>     if self._model_type == MY_NEW_MODEL:
+        >>>         state_dict = my_custom_state_dict_mapping(state_dict)
+    """
+
+    GEMMA: str = "gemma"
+    LLAMA2: str = "llama2"
+    LLAMA3: str = "llama3"
+    LLAMA3_2: str = "llama3_2"
+    LLAMA3_VISION: str = "llama3_vision"
+    MISTRAL: str = "mistral"
+    PHI3_MINI: str = "phi3_mini"
+    REWARD: str = "reward"
+    QWEN2: str = "qwen2"
+
+
+class FormattedCheckpointFiles:
+    """
+    This class gives a more concise way to represent a list of filenames of the format ``file_{i}_of_{n_files}.pth``.
+
+    Args:
+        filename_format (str): Format string for the filename. Must have exactly two placeholders, e.g.
+            ``file_{}_of_{}.pth``.
+        max_filename (str): Maximum filename in the list. Should be a string representation of an integer,
+            possibly with leading zeroes.
+    """
+
+    def __init__(
+        self,
+        filename_format: str,
+        max_filename: str,
+    ):
+        self.filename_format = filename_format
+        self.max_filename = max_filename
+        self._validate_filename_format()
+
+    @classmethod
+    def from_dict(cls, d: dict) -> "FormattedCheckpointFiles":
+        if "filename_format" not in d or "max_filename" not in d:
+            raise ValueError(
+                "Must pass 'filename_format' and 'max_filename' keys to generate checkpoint filenames"
+            )
+        return cls(
+            filename_format=d["filename_format"],
+            max_filename=d["max_filename"],
+        )
+
+    def _validate_filename_format(self):
+        n_format_placeholders = [
+            x[1]
+            for x in string.Formatter().parse(self.filename_format)
+            if x[1] is not None
+        ]
+        if len(n_format_placeholders) != 2:
+            raise ValueError(
+                "Filename format string must have exactly two placeholders, e.g. 'file_{i}_of_{n_files}.pth'"
+            )
+
+    def build_checkpoint_filenames(self):
+        """
+        Builds a list of checkpoint filenames from the filename format and max filename.
+
+        Returns:
+            List[str]: List of checkpoint filenames.
+
+        Example:
+            >>> # Example usage
+            >>> f = FormattedCheckpointFiles(filename_format="file_{}_of_{}.safetensors", max_filename="00003")
+            >>> f.build_checkpoint_filenames()
+            >>> ['file_00001_of_00003.safetensors', 'file_00002_of_00003.safetensors', 'file_00003_of_00003.safetensors']
+        """
+        num_files = int(self.max_filename)
+        return [
+            self.filename_format.format(
+                str(i + 1).zfill(len(self.max_filename)),
+                self.max_filename,
+            )
+            for i in range(num_files)
+        ]
+
+
+def get_path(input_dir: Path, filename: str, missing_ok: bool = False) -> Path:
+    """
+    Utility to recover and validate the path for a given file within a given directory.
+
+    Args:
+        input_dir (Path): Directory containing the file
+        filename (str): Name of the file
+        missing_ok (bool): Whether to raise an error if the file is missing.
+
+    Returns:
+        Path: Path to the file
+
+    Raises:
+        ValueError: If the file is missing and missing_ok is False.
+    """
+    if not input_dir.is_dir():
+        raise ValueError(f"{input_dir} is not a valid directory.")
+
+    file_path = Path.joinpath(input_dir, filename)
+
+    # If missing_ok is False, raise an error if the path is invalid
+    if not missing_ok and not file_path.is_file():
+        raise ValueError(f"No file with name: {filename} found in {input_dir}.")
+    return file_path
+
+
+def safe_torch_load(
+    checkpoint_path: Path, weights_only: bool = True, mmap: bool = True
+) -> Dict[str, Any]:
+    """
+    Utility to load a checkpoint file onto CPU in a safe manner. Provides separate handling for
+    safetensors files.
+
+    Args:
+        checkpoint_path (Path): Path to the checkpoint file.
+        weights_only (bool): Whether to load only tensors, primitive types, and dictionaries
+            (passthrough to torch.load). Default: True
+        mmap (bool): Whether to mmap from disk into CPU memory. Default: True
+
+    Returns:
+        Dict[str, Any]: State dict from the checkpoint file.
+
+    Raises:
+        ValueError: If the checkpoint file is not found or cannot be loaded.
+    """
+    try:
+        # convert the path into a string since pathlib Path and mmap don't work
+        # well together
+        is_safetensors_file = (
+            True if str(checkpoint_path).endswith(".safetensors") else False
+        )
+        if is_safetensors_file:
+            result = {}
+            with safe_open(checkpoint_path, framework="pt", device="cpu") as f:
+                for k in f.keys():
+                    result[k] = f.get_tensor(k)
+            state_dict = result
+        else:
+            state_dict = torch.load(
+                str(checkpoint_path),
+                map_location="cpu",
+                mmap=mmap,
+                weights_only=weights_only,
+            )
+    except Exception as e:
+        raise ValueError(f"Unable to load checkpoint from {checkpoint_path}. ") from e
+    return state_dict
+
+
+def save_config(path: Path, config: Dict[str, Any]) -> None:
+    """
+    Save a configuration dictionary to a file.
+
+    Args:
+        path (Path): Path to save the configuration file.
+        config (Dict[str, Any]): Configuration dictionary to save.
+    """
+    if not path.is_dir():
+        path.mkdir(exist_ok=True)
+    file_path = Path.joinpath(path, "config.json")
+    if not file_path.exists():
+        with open(file_path, "w") as f:
+            json.dump(config, f)
+
+
+def update_state_dict_for_classifier(
+    state_dict: Dict[str, torch.Tensor],
+    model_named_parameters: Iterable[Tuple[str, torch.nn.Parameter]],
+    force_override: bool = False,
+):
+    """
+    Validates the state dict for checkpoint loading for a classifier model.
+    To be used prior to a call to ``model.load_state_dict(state_dict)``.
+    This function will overwrite the ``output.weight`` in the state-dict
+    to be loaded with the ``output.weight`` in the model if the shapes
+    for the ``output.weight`` do not match. You may also wish to override this behaviour,
+    for example, if ``num_classes`` for your checkpoint and model are the same.
+
+    Concretely, when fine-tuning a classifier model from the checkpoint of a base language model
+    which has ``output.weight`` of shape ``[vocab_dim, embed_dim]``, we overwrite
+    the ``output.weight`` in the state-dict to be loaded with the randomly initialized
+    ``[num_classes, embed_dim]`` weight in the model. This is done in-place.
+
+    Args:
+        state_dict (Dict[str, torch.Tensor]): state dict to be loaded into the classifier model.
+        model_named_parameters (Iterable[Tuple[str, torch.nn.Parameter]]): model named parameters
+            from ``model.named_parameters()``.
+        force_override (bool): Whether to replace ``output.weight`` in ``state_dict`` with the model's
+            ``output.weight``, even if the shapes match.
+    Notes:
+        - ``output.bias`` will be ignored if present in ``state_dict``
+        - This function will always replace the ``output.weight`` in ``state_dict``,
+            if ``output.weight != model.output.weight``.
+
+    Raises:
+        AssertionError: if ``state_dict`` does not contain ``output.weight``.
+        AssertionError: if ``model_named_parameters`` does not contain ``output.weight``.
+
+    """
+    output_weight = dict(model_named_parameters).get("output.weight", None)
+    if "output.weight" not in state_dict:
+        raise AssertionError(
+            "Expected output.weight in state_dict, but it wasn't found."
+        )
+    if output_weight is None:
+        raise AssertionError(
+            "Expected output.weight in model_named_parameters, but it wasn't found."
+        )
+    if "output.bias" in state_dict:
+        warn("Found output.bias in state dict - this will not be used!")
+        state_dict.pop("output.bias")
+    if state_dict["output.weight"].shape[0] != output_weight.shape[0] or force_override:
+        state_dict["output.weight"] = output_weight
diff -ruN marc_original/third_party/torchtune/torchtune/training/_compile.py marc/third_party/torchtune/torchtune/training/_compile.py
--- marc_original/third_party/torchtune/torchtune/training/_compile.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/_compile.py	2025-02-20 17:49:30.794026290 -0500
@@ -0,0 +1,86 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+from typing import Union
+
+import torch
+from torch import nn
+
+from torchtune.modules import (
+    TransformerCrossAttentionLayer,
+    TransformerDecoder,
+    TransformerSelfAttentionLayer,
+)
+from torchtune.modules.loss import CEWithChunkedOutputLoss
+from torchtune.modules.model_fusion import DeepFusionModel
+from torchtune.utils import get_logger, torch_version_ge
+
+log = get_logger("INFO")
+
+
+def compile_model(
+    model: Union[TransformerDecoder, DeepFusionModel],
+    verbose: bool = True,
+) -> None:
+    """
+    Utility to compile a transformer model inplace. On PyTorch nightlies we use per-layer compile
+    to reduce compile times. Otherwise we compile the full model, which takes longer.
+
+    Args:
+        model (Union[TransformerDecoder, DeepFusionModel]): A model to compile.
+            Can be a TransformerDecoder or DeepFusionModel; in the latter case only
+            the model's decoder will be compiled.
+        verbose (bool): Whether to log compile info. Default: True
+    Returns:
+        None
+
+    """
+    backend = os.environ.get("TORCH_COMPILE_BACKEND", "inductor")
+    if isinstance(model, DeepFusionModel):
+        model = model.decoder
+    if torch_version_ge("2.5.0"):
+        if verbose:
+            log.info("Compiling model layers with torch.compile...")
+        for m in reversed(list(model.modules())):
+            if isinstance(m, TransformerSelfAttentionLayer) or isinstance(
+                m, TransformerCrossAttentionLayer
+            ):
+                m.compile(backend=backend)
+    else:
+        if verbose:
+            log.info(
+                """
+                Compiling full model with torch.compile...
+                For faster compile times via per-layer compile, please run on PyTorch nightlies.
+                """
+            )
+        model.compile(backend=backend)
+
+
+def compile_loss(loss: nn.Module, verbose: bool = True) -> None:
+    """
+    Utility to compile and return loss function. If the loss function is chunked cross-entropy,
+    we only compile the upcast + cross-entropy calculation, not the chunking. For other losses
+    we compile the entire loss function.
+
+    Args:
+        loss (nn.Module): A loss function to compile.
+        verbose (bool): Whether to log compile info. Default: True
+    Returns:
+        loss (nn.Module): loss with either entire module compiled or (in the case of
+            CEWithChunkedOutputLoss) only the upcast and cross-entropy calculation compiled.
+    """
+    backend = os.environ.get("TORCH_COMPILE_BACKEND", "inductor")
+    if verbose:
+        log.info("Compiling loss with torch.compile...")
+    if isinstance(loss, CEWithChunkedOutputLoss):
+        loss.compute_cross_entropy = torch.compile(
+            loss.compute_cross_entropy, backend=backend
+        )
+    else:
+        loss = torch.compile(loss, backend=backend)
+    return loss
diff -ruN marc_original/third_party/torchtune/torchtune/training/_distributed.py marc/third_party/torchtune/torchtune/training/_distributed.py
--- marc_original/third_party/torchtune/torchtune/training/_distributed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/_distributed.py	2025-02-20 17:49:30.798026297 -0500
@@ -0,0 +1,623 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+import logging
+import os
+from itertools import chain
+from typing import Any, Callable, cast, Dict, List, Optional, Set, Tuple, Type
+
+import torch
+import torch.distributed as dist
+from torch import nn
+
+from torch.distributed._composable.fsdp import CPUOffloadPolicy, fully_shard
+from torch.distributed._tensor import distribute_tensor, DTensor
+from torch.distributed._tensor.placement_types import DTensorSpec, TensorMeta
+from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
+    _CHECKPOINT_WRAPPED_MODULE,
+)
+from torch.distributed.checkpoint.state_dict import _init_optim_state
+from torch.distributed.fsdp import ShardingStrategy
+from torch.distributed.fsdp.wrap import ModuleWrapPolicy
+from torch.optim import Optimizer
+from torchao.dtypes.nf4tensor import NF4Tensor, to_nf4
+from torchtune.modules import TransformerDecoder
+from torchtune.modules.peft import DoRALinear, LoRALinear
+from torchtune.modules.peft.lora import _lora_a_init_params, _lora_b_init_params
+from torchtune.utils import get_logger
+
+from torchtune.utils._device import get_device
+
+_log: logging.Logger = get_logger()
+
+
+FSDPPolicyType: Type = Callable[[nn.Module, bool, int], bool]
+
+FSDPPolicyType.__doc__ = """
+
+A datatype for a function that can be used as an FSDP wrapping policy.
+In particular, this type denotes a function that can accept an nn.Module, a boolean flag, and an integer
+and return a boolean indicating whether the module should be wrapped with FSDP. Objects of this type can
+be directly passed into PyTorch FSDP's ``auto_wrap_policy`` argument to specify how FSDP wraps submodules.
+
+The below function serves as an example of creating and returning a function that obeys the contract of
+``FSDPPolicyType``::
+
+    def get_fsdp_policy(module: nn.Module, modules_to_wrap: Set[Type], min_num_params: int):
+
+        def my_fsdp_policy(module: nn.Module, modules_to_wrap: Set[Type], recurse: bool, min_num_params: int) -> bool:
+            if recurse:
+                return True
+            # Wrap layers that are of type in ``modules_to_wrap`` and layers with more than min_num_params
+
+            return isinstance(module, tuple(modules_to_wrap)) or sum(p.numel() for p in module.parameters()) > 1000
+
+        return functools.partial(my_fsdp_policy, modules_to_wrap=modules_to_wrap)
+
+Please see documentation of ``auto_wrap_policy`` at https://pytorch.org/docs/stable/fsdp.html for additional details.
+
+"""
+
+_valid_distributed_single_node_nnodes = ["1:1", "1"]
+
+
+def _get_sharding_strategy(strategy: str) -> ShardingStrategy:
+    """Helper function to convert sharding strategy strings to ShardingStrategy enum."""
+    return getattr(ShardingStrategy, strategy)
+
+
+def is_distributed() -> bool:
+    """Check if all environment variables required to initialize torch.distributed are set
+    and distributed is properly installed. This indicates a distributed run.
+    https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization
+
+    Checks the following conditions:
+
+    * torch.distributed is available
+    * master port and master address environment variables are set
+    * world size is >1
+    * rank environment variable is set
+
+    Returns:
+        bool: True if all of the above conditions hold, False otherwise.
+    """
+    port = os.environ.get("MASTER_PORT", "")
+    addr = os.environ.get("MASTER_ADDR", "")
+    size = int(os.environ.get("WORLD_SIZE", 1))
+    rank = int(os.environ.get("RANK", -1))
+    avlb = dist.is_available()
+    return bool(port and addr and size >= 1 and rank >= 0 and avlb)
+
+
+def _broadcast_tensor(tensor: torch.Tensor, src: int = 0) -> torch.Tensor:
+    """Broadcasts a tensor from a source to all other processes.
+
+    Args:
+        tensor (torch.Tensor): torch.Tensor to broadcast.
+        src (int, optional): Source rank. Defaults to 0.
+
+    Returns:
+        torch.Tensor: Broadcasted tensor.
+    """
+    if dist.is_available() and dist.is_initialized():
+        device = tensor.device
+        if dist.get_backend() == "nccl":
+            tensor = tensor.to(get_device("cuda"))
+        dist.broadcast(tensor, src=src, group=None)
+        return tensor.to(device)
+    else:
+        return tensor
+
+
+def init_distributed(**kwargs: Dict[str, Any]) -> bool:
+    """Initialize process group required for ``torch.distributed``.
+
+    Args:
+        **kwargs (Dict[str, Any]): Additional arguments to pass to torch.distributed.init_process_group.
+
+    Returns:
+        bool: True if torch.distributed is initialized.
+
+    Raises:
+        RuntimeError: If torch.distributed is already initialized.
+    """
+    if is_distributed():
+        if dist.is_initialized():
+            raise RuntimeError("torch.distributed already initialized.")
+        dist.init_process_group(**kwargs)
+        return True
+    else:
+        return False
+
+
+def set_torch_num_threads() -> None:
+    """
+    Sets the number of threads used by torch to utilize all physical CPU
+    cores for intra-op parallelism. Currently, this function sets num_threads
+    to be the number of physical CPU cores divided by the number of GPUs as we
+    use one process per GPU, and this avoids CPU oversubscription. Note that this is
+    currently a rough approximation, and doesn't take into account environments where
+    things like CPU affinity is set.
+    """
+    num_threads = os.cpu_count() // (
+        torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1
+    )
+    torch.set_num_threads(num_threads)
+    _log.info(f"Set intra op parallelism no. of threads to {num_threads}")
+
+
+def get_world_size_and_rank() -> Tuple[int, int]:
+    """Function that gets the current world size (aka total number
+    of ranks) and rank number of the current process in the default process group.
+
+    Returns:
+        Tuple[int, int]: world size, rank
+    """
+    if dist.is_available() and dist.is_initialized():
+        return torch.distributed.get_world_size(), torch.distributed.get_rank()
+    else:
+        return 1, 0
+
+
+def validate_no_params_on_meta_device(model: nn.Module) -> None:
+    """
+    Utility to validate that model has no params or buffers on meta device.
+    If a meta param or buffer is found, an error indicating the param name will
+    be raised.
+
+    Args:
+        model (nn.Module): model to check for meta params
+
+    Raises:
+        RuntimeError: If meta params or buffers exist in model
+    """
+    for n, p in chain(model.named_parameters(), model.named_buffers()):
+        if p.is_meta:
+            raise RuntimeError(f"Unexpected param or buffer {n} on meta device.")
+
+
+def contains_fsdp(model: nn.Module) -> bool:
+    """
+    Checks if the model contains FSDP.
+
+    Args:
+        model (nn.Module): Model to check.
+
+    Returns:
+        bool: True if the model contains FSDP, False otherwise.
+    """
+    return any(
+        isinstance(m, torch.distributed.fsdp.FullyShardedDataParallel)
+        for m in model.modules()
+    )
+
+
+def _dummy_reset_params(x: nn.Module) -> None:
+    """
+    Dummy method for patching no-op reset_parameters() when using
+    FSDP with meta device.
+    """
+    return
+
+
+def prepare_model_for_fsdp_with_meta_device(model: nn.Module) -> nn.Module:
+    """
+    Dynamically define reset_parameters on every submodule of the model. For LoRA models,
+    ensure that the FSDP contract of reset_parameters only modifying a module's directly-owned
+    parameters is satisfied. More details here: https://github.com/pytorch/pytorch/issues/104187.
+
+    Args:
+        model (nn.Module): model class to prepare for usage with FSDP and meta device.
+
+    Returns:
+        nn.Module: Model with reset_parameters defined on every submodule.
+        In the case of a LoRA model, we override the default reset_parameters of nn.Linear.
+
+    Raises:
+        RuntimeError: if model contains submodule with non-callable attribute reset_parameters
+    """
+    for k, v in model.named_modules():
+        # If the module does not have reset_parameters defined, we define
+        # a no-op reset_parameters method to satisfy FSDP's contract.
+        reset_params = getattr(v, "reset_parameters", None)
+
+        if reset_params is not None and not callable(reset_params):
+            raise RuntimeError(
+                f"Cannot override existing reset_parameters variable for FSDP init in {k}"
+            )
+
+        if reset_params is None:
+            v.reset_parameters = _dummy_reset_params.__get__(v)
+
+        # This will define reset_parameters for LoRA weight initialization
+        # directly on any LoRALinear submodules lora_a and lora_b.
+        if isinstance(v, LoRALinear) or isinstance(v, DoRALinear):
+            v.lora_a.reset_parameters = _lora_a_init_params.__get__(v.lora_a)
+            v.lora_b.reset_parameters = _lora_b_init_params.__get__(v.lora_b)
+
+    return model
+
+
+def lora_fsdp_wrap_policy(modules_to_wrap: Set[Type]) -> FSDPPolicyType:
+    """
+    A default policy for wrapping models trained with LoRA using FSDP.
+
+    FSDP's default behavior is to allocate gradients at the level of FSDP-wrapped modules.
+    This means that if any parameter in a given FSDP-wrapped module requires gradients, then memory will be
+    allocated for gradients for the entire module.
+
+    In the case of LoRA, where only the adapters are trainable, this means that
+    we need to wrap the adapter submodules in their own FSDP units to
+    maximize memory savings. After this is done, model will also be hierarchically wrapped
+    based on nn.Module types specified in ``modules_to_wrap``.
+
+    Args:
+        modules_to_wrap (Set[Type]): nn.Module types to recursively wrap
+
+    Returns:
+        FSDPPolicyType: Wrapping policy that can be passed into ``FullyShardedDataParallel``. Please see
+        documentation for :const:`~torchtune.utils.FSDPPolicyType` for additional details.
+    """
+
+    def lora_wrap_fsdp(module: nn.Module, recurse: bool, **kwargs):
+        if recurse:
+            return True
+
+        # Assumes lora_a and lora_b are nn.Linears that are the
+        # only trainable modules in the entire network. Wraps
+        # these in separate FSDP unit to work around FSDP allocating
+        # extra gradient memory when wrapped with other modules.
+        if hasattr(module, "weight") and module.weight.requires_grad:
+            return True
+
+        return isinstance(module, tuple(modules_to_wrap))
+
+    return lora_wrap_fsdp
+
+
+def load_from_full_model_state_dict(
+    model: "FSDPModule",  # noqa
+    full_sd: Dict[str, Any],
+    device: torch.device,
+    is_rank_zero: bool,
+    strict: bool = False,
+    cpu_offload: bool = False,
+):
+    """
+    Converting full state dict into a sharded state dict
+    and loading it into FSDP model
+    - 'full' means plain tensor
+    - 'sharded' means `DTensor` where reach rank has a shard of the plain tensor
+    - `is_rank_zero` matters if only rank 0 pass in non-empty `full_sd` and
+       we need to broadcast from rank 0
+    """
+    meta_sharded_sd = model.state_dict()
+    sharded_sd = {}
+    for param_name, full_tensor in full_sd.items():
+        sharded_meta_param = meta_sharded_sd.get(param_name)
+        full_tensor = full_tensor.to(sharded_meta_param.dtype).to(device)
+        if isinstance(sharded_meta_param._local_tensor, NF4Tensor):
+            full_tensor = to_nf4(full_tensor)
+            # replicating logic from `_fsdp_param.py`` `_init_sharded_param`
+            # otherwise `distribute_tensor(DTensor(local=NF4))`
+            # requires dispatching `c10d.scatter_``
+            # long-term solution is `swap_tensor`
+            mesh = sharded_meta_param.device_mesh
+            if mesh.ndim > 1:
+                raise NotImplementedError(f"only support 1D FSDP but got {mesh.ndim=}")
+            shard_mesh_dim = 0
+            shard_world_size = mesh.size(shard_mesh_dim)
+            shard_rank = cast(
+                torch.distributed.ProcessGroup, mesh.get_group(shard_mesh_dim)
+            ).rank()
+            chunk = list(torch.chunk(full_tensor, shard_world_size, dim=0))[shard_rank]
+            sharded_param = full_tensor.new_zeros(chunk.size())
+            sharded_param[: chunk.size(0)].copy_(chunk)
+
+            # TODO: change to from_local API (need to add view support for NF4)
+            sharded_tensor = DTensor(
+                local_tensor=sharded_param,
+                spec=DTensorSpec(
+                    mesh=sharded_meta_param.device_mesh,
+                    placements=sharded_meta_param.placements,
+                    tensor_meta=TensorMeta(
+                        shape=sharded_meta_param.size(),
+                        dtype=sharded_meta_param.dtype,
+                        stride=sharded_meta_param.stride(),
+                    ),
+                ),
+                requires_grad=sharded_meta_param.requires_grad,
+            )
+
+        else:
+            sharded_tensor = distribute_tensor(
+                full_tensor,
+                sharded_meta_param.device_mesh,
+                sharded_meta_param.placements,
+            )
+        if cpu_offload:
+            sharded_tensor = sharded_tensor.cpu()
+        sharded_sd[param_name] = nn.Parameter(sharded_tensor)
+    # choose `assign=True` since we cannot call `copy_` on meta tensor
+    return model.load_state_dict(sharded_sd, strict=strict, assign=True)
+
+
+def get_full_model_state_dict(
+    model: "FSDPModule",  # noqa
+    is_rank_zero: bool,
+    device: Optional[torch.device] = None,
+    trainable_only: bool = False,
+) -> Dict[str, Any]:
+    """
+    Converting sharded state dict into a full state dict on CPU
+    Returning non-empty result on rank0 to avoid peaking CPU memory
+
+    Args:
+        model (FSDPModule): wrapped module
+        is_rank_zero (bool): flag to check if the process is on rank 0
+        device (Optional[torch.device]): device to use for sharded tensors. Default: None
+        trainable_only (bool): flag to check if only trainable parameters should be returned. Default: False
+
+    Raises:
+        AssertionError: if the model contains NF4Tensor and the model is not wrapped with FSDP
+
+    Returns:
+        Dict[str, Any]: State dict on CPU
+    """
+    # [Warning] FSDPModel.state_dict converts all Parameter Tensors to DTensors
+    sharded_sd = model.state_dict()
+    cpu_state_dict = {}
+    has_nf4 = any(
+        isinstance(param._local_tensor, NF4Tensor) for param in model.parameters()
+    )
+    if has_nf4:
+        from torch.distributed._composable.fsdp.fully_shard import FSDPModule
+
+        # Iterating from lowerer modules to higher
+        # Unsharding lora adapters before unsharding transformer block
+        for module_name, module in reversed(list(model.named_modules())):
+            if not isinstance(module, FSDPModule):
+                continue
+            module.unshard(async_op=False)
+            if is_rank_zero:
+                module_name = module_name.replace(f".{_CHECKPOINT_WRAPPED_MODULE}", "")
+                for local_fqn, param in module.named_parameters():
+                    local_fqn = local_fqn.replace(f".{_CHECKPOINT_WRAPPED_MODULE}", "")
+                    if len(module_name) > 0:
+                        full_fqn = module_name + "." + local_fqn
+                    else:
+                        full_fqn = local_fqn
+                    if trainable_only and not param.requires_grad:
+                        # skip trainable params when trainable_only is True
+                        continue
+                    if full_fqn in cpu_state_dict:
+                        # Iterate over every param in every module bottoms-up
+                        # When lower TransformerBlock gets unsharded,
+                        # we insert  (full_fqn, full_tensor) into cpu_state_dict.
+                        # When higher Transformer gets unsharded, we avoid updating
+                        # params from lower TransformerBlockonly again. Instead, only updating
+                        # tok_embeddings etc that belongs to Transformer
+                        continue
+                    if isinstance(param, NF4Tensor):
+                        # upcasting NF4 to original dtype
+                        param = param.to(param.dtype)
+                    if isinstance(param, DTensor):
+                        raise AssertionError(
+                            f"Internal error: expect unsharded {full_fqn} in plain torch.Tensor but got DTensor."
+                            " Might be a bug in get_full_model_state_dict"
+                        )
+                    cpu_state_dict[full_fqn] = param.cpu()
+            module.reshard()
+    else:
+        for param_name, sharded_param in sharded_sd.items():
+            if sharded_param.is_cpu:
+                assert device is not None and device.type == "cuda", (
+                    f"Expect cuda but got device={device}. "
+                    "Please call get_full_model_state_dict(..., device=self._device),"
+                    " so DTensor can communicate over NCCL."
+                )
+                sharded_param = sharded_param.to(device)
+            full_param = sharded_param.full_tensor()
+            if is_rank_zero:
+                cpu_state_dict[param_name] = full_param.cpu()
+            else:
+                del full_param
+    return cpu_state_dict
+
+
+def get_full_optimizer_state_dict(
+    opt: Optimizer,
+    is_rank_zero: bool,
+    device: Optional[torch.device] = None,
+) -> Dict[str, Any]:
+    """
+    Converting optimizer state from sharded to full
+    For example, "exp_avg" in AdamW is `DTensor`,
+    "exp_avg.full_tensor()" converts it to plain tensor on rank 0
+    Returning non-empty cpu state dict on rank 0
+    """
+    sharded_sd = opt.state_dict()
+    sharded_state = sharded_sd["state"]
+    full_state = {}
+    for group_id, sharded_group in sharded_state.items():
+        group_state = {}
+        for attr, sharded_tensor in sharded_group.items():
+            # "exp_avg" in AdamW is `DTensor`
+            if isinstance(sharded_tensor, DTensor):
+                if sharded_tensor.is_cpu:
+                    assert device is not None and device.type == "cuda", (
+                        f"Expect cuda but got device={device}. "
+                        "Please call get_full_optimizer_state_dict(..., device=self._device),"
+                        " so DTensor can communicate over NCCL."
+                    )
+                    sharded_tensor = sharded_tensor.to(device)
+                full_tensor = sharded_tensor.full_tensor()
+            else:
+                # "step" in AdamW is plain tensor
+                full_tensor = sharded_tensor
+            if is_rank_zero:
+                group_state[attr] = full_tensor.cpu()
+            else:
+                del full_tensor
+        if is_rank_zero:
+            full_state[group_id] = group_state
+        else:
+            del group_state
+    if is_rank_zero:
+        return {
+            "param_groups": sharded_sd["param_groups"],
+            "state": full_state,
+        }
+    else:
+        return {}
+
+
+def load_from_full_optimizer_state_dict(
+    opt: Optimizer,
+    full_sd: Dict[str, Any],
+    device: torch.device,
+) -> Dict[str, Any]:
+    """
+    Converting full optimizer state to sharded state dict
+    and loading it into optimizer
+    """
+    PARAMS = "params"  # noqa: N806
+    _init_optim_state(opt)
+    param_groups = opt.state_dict()["param_groups"]
+    state = opt.state_dict()["state"]
+
+    full_param_groups = full_sd["param_groups"]
+    full_state = full_sd["state"]
+
+    for param_group, full_param_group in zip(param_groups, full_param_groups):
+        for key, value in full_param_group.items():
+            if key == PARAMS:
+                continue
+            param_group[key] = value
+        for pid, full_pid in zip(param_group[PARAMS], full_param_group[PARAMS]):
+            if pid not in state:
+                continue
+            param_state = state[pid]
+            full_param_state = full_state[full_pid]
+            for attr, full_tensor in full_param_state.items():
+                sharded_tensor = param_state[attr]
+                if isinstance(sharded_tensor, DTensor):
+                    # exp_avg is DTensor
+                    param_state[attr] = distribute_tensor(
+                        full_tensor,
+                        sharded_tensor.device_mesh,
+                        sharded_tensor.placements,
+                    )
+                else:
+                    # step is plain tensor
+                    param_state[attr] = full_tensor
+    opt.load_state_dict(
+        {
+            "param_groups": param_groups,
+            "state": state,
+        }
+    )
+
+
+def get_full_finetune_fsdp_wrap_policy(
+    memory_efficient_fsdp_wrap: bool, modules_to_wrap: Set[Type]
+) -> FSDPPolicyType:
+    """
+    Retrieves an FSDP wrapping policy based on the specified flags ``memory_efficient_fsdp_wrap`` and
+    ``modules_to_wrap``. Specifically, if ``memory_efficient_fsdp_wrap`` is set to ``True``, the returned
+    policy will wrap the model's token embedding and output projection in addition to the modules specified
+    to maximize memory savings.
+
+    Args:
+        memory_efficient_fsdp_wrap (bool): If ``True``, will also wrap embedding and output projection layers with FSDP.
+        modules_to_wrap (Set[Type]): Set of module types to wrap.
+
+    Note:
+        ``memory_efficient_fsdp_wrap`` memory improvements have currently only been verified on llama3 workloads
+        where they provide ~15% memory improvement (when used alongside AC memory efficient wrapping). Other workloads
+        have not been verified and may not see the same improvements.
+
+    Returns:
+        FSDPPolicyType: Wrapping policy that can be passed into ``FullyShardedDataParallel`` as the ``auto_wrap_policy``
+        argument. Please see documentation for :const:`~torchtune.utils.FSDPPolicyType` for additional details.
+    """
+    if memory_efficient_fsdp_wrap:
+        return _memory_efficient_wrap_policy(modules_to_wrap=modules_to_wrap)
+    else:
+        return ModuleWrapPolicy(modules_to_wrap)
+
+
+def _memory_efficient_wrap_policy(modules_to_wrap: Set[Type]) -> FSDPPolicyType:
+    """
+    A default policy for memory efficient wrapping for full finetuning using FSDP. Specifically,
+    this will wrap the model's token embedding and output projection into their own FSDP units to
+    maximize memory savings. This helps especially if these layers are particularly large,
+    such as due to a large embedding size.
+    After this is done, model will also be hierarchically wrapped
+    based on nn.Module types specified in ``modules_to_wrap``. This function assumes that the
+    input model has an attribute ``output`` that is a nn.Linear which is the model's output projection.
+    Args:
+        modules_to_wrap (Set[Type]): nn.Module types to recursively wrap
+    Returns:
+        FSDPPolicyType: Wrapping policy that can be passed into ``FullyShardedDataParallel``.
+    """
+    modules_to_wrap.add(torch.nn.Embedding)
+
+    def llama3_wrap(module: nn.Module, recurse: bool, **kwargs):
+        # Label that output_proj should be wrapped individually.
+        if isinstance(module, TransformerDecoder):
+            module.output._wrap = True
+        if recurse:
+            return True
+
+        # Wrap output_proj individually.
+        if getattr(module, "_wrap", False):
+            return True
+
+        return isinstance(module, tuple(modules_to_wrap))
+
+    return llama3_wrap
+
+
+def shard_model(
+    model: TransformerDecoder,
+    shard_conditions: List[Callable[[str, nn.Module], bool]],
+    *,
+    cpu_offload: bool,
+    reshard_after_forward: bool = True,
+) -> None:
+    """
+    Utility to shard a model with FSDP using the PyTorch Distributed fully_shard API.
+
+    This method will over the model's named modules from the bottom-up and apply shard modules
+    based on whether they meet any of the criteria from shard_conditions.
+
+    Args:
+        model (TransformerDecoder): Model to shard with FSDP.
+        shard_conditions (List[Callable[[str, nn.Module], bool]]): A list of functions to determine
+            which modules to shard with FSDP. Each function should take module name (relative to root)
+            and the module itself, returning True if FSDP should shard the module and False otherwise.
+            If any of shard_conditions return True for a given module, it will be sharded by FSDP.
+        cpu_offload (bool): If set to True, FSDP will offload parameters, gradients, and optimizer
+            states to CPU.
+        reshard_after_forward (bool): Whether to reshard parameters and buffers after
+            the forward pass. Setting this to True corresponds to the FULL_SHARD sharding strategy
+            from FSDP1, while setting it to False corresponds to the SHARD_GRAD_OP sharding strategy.
+
+    """
+    fsdp_kwargs = {"reshard_after_forward": reshard_after_forward}
+    if cpu_offload:
+        fsdp_kwargs["offload_policy"] = CPUOffloadPolicy()
+
+    # Shard the model with FSDP, iterating in reverse to start with
+    # lowest-level modules first
+    for n, m in reversed(list(model.named_modules())):
+        if any([shard_condition(n, m) for shard_condition in shard_conditions]):
+            fully_shard(m, **fsdp_kwargs)
+
+    # Finally shard the entire model to account for any stragglers
+    fully_shard(model, **fsdp_kwargs)
diff -ruN marc_original/third_party/torchtune/torchtune/training/__init__.py marc/third_party/torchtune/torchtune/training/__init__.py
--- marc_original/third_party/torchtune/torchtune/training/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/__init__.py	2025-02-20 17:49:30.786026278 -0500
@@ -0,0 +1,132 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+from torchtune.training._activation_offloading import NoOpManager, OffloadActivations
+from torchtune.training._compile import compile_loss, compile_model
+from torchtune.training._distributed import (
+    contains_fsdp,
+    FSDPPolicyType,
+    get_full_finetune_fsdp_wrap_policy,
+    get_full_model_state_dict,
+    get_full_optimizer_state_dict,
+    get_world_size_and_rank,
+    init_distributed,
+    is_distributed,
+    load_from_full_model_state_dict,
+    load_from_full_optimizer_state_dict,
+    lora_fsdp_wrap_policy,
+    prepare_model_for_fsdp_with_meta_device,
+    set_torch_num_threads,
+    shard_model,
+    validate_no_params_on_meta_device,
+)
+from torchtune.training._profiler import (
+    DEFAULT_PROFILE_DIR,
+    DEFAULT_PROFILER_ACTIVITIES,
+    DEFAULT_SCHEDULE,
+    DEFAULT_TRACE_OPTS,
+    DummyProfiler,
+    PROFILER_KEY,
+    setup_torch_profiler,
+)
+from torchtune.training.activations import apply_selective_activation_checkpointing
+from torchtune.training.checkpointing import (
+    ADAPTER_CONFIG,
+    ADAPTER_KEY,
+    Checkpointer,
+    EPOCHS_KEY,
+    FormattedCheckpointFiles,
+    FullModelHFCheckpointer,
+    FullModelMetaCheckpointer,
+    FullModelTorchTuneCheckpointer,
+    MAX_STEPS_KEY,
+    MODEL_KEY,
+    ModelType,
+    OPT_KEY,
+    RNG_KEY,
+    SEED_KEY,
+    STEPS_KEY,
+    TOTAL_EPOCHS_KEY,
+    update_state_dict_for_classifier,
+)
+from torchtune.training.lr_schedulers import get_cosine_schedule_with_warmup
+from torchtune.training.memory import (
+    cleanup_before_training,
+    create_optim_in_bwd_wrapper,
+    get_memory_stats,
+    log_memory_stats,
+    OptimizerInBackwardWrapper,
+    register_optim_in_bwd_hooks,
+    set_activation_checkpointing,
+)
+from torchtune.training.pooling import get_unmasked_sequence_lengths
+from torchtune.training.precision import (
+    get_dtype,
+    set_default_dtype,
+    validate_expected_param_dtype,
+)
+from torchtune.training.quantization import get_quantizer_mode
+from torchtune.training.seed import set_seed
+
+__all__ = [
+    "apply_selective_activation_checkpointing",
+    "get_dtype",
+    "set_default_dtype",
+    "validate_expected_param_dtype",
+    "FullModelHFCheckpointer",
+    "FullModelMetaCheckpointer",
+    "FullModelTorchTuneCheckpointer",
+    "ModelType",
+    "Checkpointer",
+    "update_state_dict_for_classifier",
+    "ADAPTER_CONFIG",
+    "ADAPTER_KEY",
+    "EPOCHS_KEY",
+    "MAX_STEPS_KEY",
+    "MODEL_KEY",
+    "OPT_KEY",
+    "RNG_KEY",
+    "SEED_KEY",
+    "STEPS_KEY",
+    "TOTAL_EPOCHS_KEY",
+    "get_quantizer_mode",
+    "get_cosine_schedule_with_warmup",
+    "cleanup_before_training",
+    "create_optim_in_bwd_wrapper",
+    "get_memory_stats",
+    "log_memory_stats",
+    "OptimizerInBackwardWrapper",
+    "register_optim_in_bwd_hooks",
+    "set_activation_checkpointing",
+    "init_distributed",
+    "is_distributed",
+    "get_world_size_and_rank",
+    "set_torch_num_threads",
+    "shard_model",
+    "prepare_model_for_fsdp_with_meta_device",
+    "validate_no_params_on_meta_device",
+    "contains_fsdp",
+    "FSDPPolicyType",
+    "get_full_finetune_fsdp_wrap_policy",
+    "lora_fsdp_wrap_policy",
+    "get_full_model_state_dict",
+    "get_full_optimizer_state_dict",
+    "load_from_full_model_state_dict",
+    "load_from_full_optimizer_state_dict",
+    "set_seed",
+    "get_unmasked_sequence_lengths",
+    "DEFAULT_PROFILE_DIR",
+    "DEFAULT_PROFILER_ACTIVITIES",
+    "DEFAULT_SCHEDULE",
+    "DEFAULT_TRACE_OPTS",
+    "DummyProfiler",
+    "PROFILER_KEY",
+    "setup_torch_profiler",
+    "compile_loss",
+    "compile_model",
+    "NoOpManager",
+    "OffloadActivations",
+    "FormattedCheckpointFiles",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/training/lr_schedulers.py marc/third_party/torchtune/torchtune/training/lr_schedulers.py
--- marc_original/third_party/torchtune/torchtune/training/lr_schedulers.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/lr_schedulers.py	2025-02-20 17:49:30.822026336 -0500
@@ -0,0 +1,56 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+
+import torch
+from torch.optim.lr_scheduler import LambdaLR
+
+
+def get_cosine_schedule_with_warmup(
+    optimizer: torch.optim.Optimizer,
+    num_warmup_steps: int,
+    num_training_steps: int,
+    num_cycles: float = 0.5,
+    last_epoch: int = -1,
+) -> LambdaLR:
+    """
+    Create a learning rate schedule that linearly increases the learning rate from
+    0.0 to lr over ``num_warmup_steps``, then decreases to 0.0 on a cosine schedule over
+    the remaining ``num_training_steps-num_warmup_steps`` (assuming ``num_cycles`` = 0.5).
+
+    This is based on the Hugging Face implementation
+    https://github.com/huggingface/transformers/blob/v4.23.1/src/transformers/optimization.py#L104.
+
+    Args:
+        optimizer (torch.optim.Optimizer): The optimizer for which to
+            schedule the learning rate.
+        num_warmup_steps (int): The number of steps for the warmup phase.
+        num_training_steps (int): The total number of training steps.
+        num_cycles (float): The number of waves in the cosine schedule. Defaults to 0.5
+            (decrease from the max value to 0 following a half-cosine).
+        last_epoch (int): The index of the last epoch when resuming training. Defaults to -1
+
+    Returns:
+        torch.optim.lr_scheduler.LambdaLR with the appropriate schedule.
+    """
+
+    def lr_lambda(current_step: int) -> float:
+        # linear warmup phase
+        if current_step < num_warmup_steps:
+            return current_step / max(1, num_warmup_steps)
+
+        # cosine
+        progress = (current_step - num_warmup_steps) / max(
+            1, num_training_steps - num_warmup_steps
+        )
+
+        cosine_lr_multiple = 0.5 * (
+            1.0 + math.cos(math.pi * num_cycles * 2.0 * progress)
+        )
+        return max(0.0, cosine_lr_multiple)
+
+    return LambdaLR(optimizer, lr_lambda, last_epoch)
diff -ruN marc_original/third_party/torchtune/torchtune/training/memory.py marc/third_party/torchtune/torchtune/training/memory.py
--- marc_original/third_party/torchtune/torchtune/training/memory.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/memory.py	2025-02-20 17:49:30.826026343 -0500
@@ -0,0 +1,299 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import gc
+import logging
+
+from typing import Any, Callable, Dict, Set, Type, Union
+
+import torch
+from torch import nn
+from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
+    apply_activation_checkpointing,
+)
+from torch.distributed.fsdp.wrap import ModuleWrapPolicy
+from torch.optim.lr_scheduler import LRScheduler
+from torchtune.utils import get_logger
+
+_log: logging.Logger = get_logger()
+
+ACWrapPolicyType: Type = Union[Set[Type], Callable[[nn.Module, bool, int], bool]]
+
+
+def set_activation_checkpointing(
+    model: nn.Module, auto_wrap_policy: ACWrapPolicyType, **kwargs
+) -> None:
+    """Utility to apply activation checkpointing to the passed-in model.
+
+    Args:
+        model (nn.Module): Model to apply activation checkpointing to.
+        auto_wrap_policy (ACWrapPolicyType): Policy to wrap module.
+            This can either be a set of ``nn.Module`` types, in which case, modules of the specified type(s)
+            will be wrapped individually with activation checkpointing, or a ``callable`` policy describing
+            how to wrap the model with activation checkpointing. For more information on authoring custom
+            policies, please see this tutorial:
+            https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html#transformer-wrapping-policy.
+        **kwargs: additional arguments to pass to ``torch.distributed`` activation checkpointing.
+    """
+    if isinstance(auto_wrap_policy, set):
+        auto_wrap_policy = ModuleWrapPolicy(auto_wrap_policy)
+    apply_activation_checkpointing(model, auto_wrap_policy=auto_wrap_policy, **kwargs)
+
+
+def cleanup_before_training() -> None:
+    """
+    Call gc collect, empty CUDA cache, and reset peak memory stats.
+    """
+    gc.collect()
+    torch.cuda.empty_cache()
+    torch.cuda.reset_peak_memory_stats()
+
+
+class OptimizerInBackwardWrapper:
+    """
+    A bare-bones class meant for checkpoint save and load for optimizers running
+    in backward. Usage is limited to the following:
+
+    Note:
+        This wrapper is only meant to be used for single-device use cases.
+        Distributed use cases such as FSDP, which require specialized optimizer state checkpointing, are not supported.
+
+    Args:
+        optim_map (Dict[str, torch.optim.Optimizer]): Mapping from parameter names to optimizers.
+
+    Example:
+        >>> optim_dict = {
+        >>>     p: config.instantiate(cfg_optimizer, [p])
+        >>>     for p in self._model.parameters()
+        >>> }
+        >>>
+        >>> # Save checkpoint
+        >>> ckpt = OptimizerInBackwardWrapper(optim_dict).state_dict()
+        >>> torch.save("/tmp/optim_ckpt", ckpt)
+        >>>
+        >>> # Load checkpoint
+        >>> placeholder_optim_dict = {
+        >>>     p: config.instantiate(cfg_optimizer, [p])
+        >>>     for p in self._model.parameters()
+        >>> }
+        >>>
+        >>> wrapper = OptimInBackwardWrapper(placeholder_optim_dict)
+        >>>
+        >>> # load_state_dict expects a dict produced by this class's
+        >>> # state_dict method.
+        >>> wrapper.load_state_dict(torch.load("/tmp/optim_ckpt"))
+        >>> # placeholder_optim_dict now has updated optimizer states.
+
+    """
+
+    def __init__(self, optim_map: Dict[str, torch.optim.Optimizer]):
+        self.optim_map = optim_map
+        self.lr_scheduler = None
+
+    def state_dict(self) -> Dict[str, Any]:
+        """
+        Returns a state dict mapping parameter names to optimizer states. This
+        state_dict is only loadable by this same class.
+
+        Returns:
+            Dict[str, Any]: state dict mapping parameter names to optimizer states.
+        """
+        return {p: opt.state_dict() for p, opt in self.optim_map.items()}
+
+    def load_state_dict(self, optim_ckpt_map: Dict[str, Any]):
+        """
+        Load optimizer states from a state dict produced by this class's
+        state_dict method.
+
+        Args:
+            optim_ckpt_map (Dict[str, Any]): state dict mapping parameter names to optimizer states.
+
+        Raises:
+            RuntimeError: If the optimizer state dict does not contain all the expected parameters.
+        """
+        params_covered = set()
+        for param_name in optim_ckpt_map.keys():
+            if param_name not in self.optim_map:
+                raise RuntimeError(
+                    f"Trying to load optimizer state for unexpected param {param_name}"
+                )
+            self.optim_map[param_name].load_state_dict(optim_ckpt_map[param_name])
+            params_covered.add(param_name)
+        # Ensure all params have been loaded into, report missing params
+        missing_params = set(self.optim_map.keys()) - params_covered
+        if missing_params:
+            raise RuntimeError(
+                f"Expected to load optimizer state for params {missing_params}!"
+            )
+
+    def get_optim_key(self, key: str) -> Any:
+        """
+        Returns value of key from an arbitrary optimizer running in backward. Note that
+        this assumes all optimizer in backwards have the same value for the key, i.e.,
+        are initialized with the same hyperparameters.
+        """
+        return list(self.optim_map.values())[0].param_groups[0][key]
+
+    def set_lr_scheduler(self, lr_scheduler: LRScheduler) -> None:
+        """
+        Sets the learning rate scheduler and modifies its step method to update all optimizers.
+
+        Args:
+            lr_scheduler (LRScheduler): The learning rate scheduler to use.
+        """
+        self.lr_scheduler = lr_scheduler
+        original_step = self.lr_scheduler.step
+
+        def custom_step(epoch=None):
+            if epoch is None:
+                original_step()
+            else:
+                original_step(epoch)
+            new_lr = self.lr_scheduler.get_last_lr()[0]
+            for opt in self.optim_map.values():
+                for param_group in opt.param_groups:
+                    param_group["lr"] = new_lr
+
+        self.lr_scheduler.step = custom_step
+
+    def step_lr_scheduler(self, epoch: int = None):
+        """
+        Steps the learning rate scheduler if it exists.
+
+        Args:
+            epoch (int, optional): The current epoch number. Defaults to None.
+
+        Raises:
+            RuntimeError: If the LR scheduler has not been set.
+        """
+        if self.lr_scheduler:
+            self.lr_scheduler.step(epoch)
+        else:
+            raise RuntimeError(
+                "LR scheduler has not been set. Call set_lr_scheduler first."
+            )
+
+    def get_last_lr(self) -> float:
+        """
+        Gets the last learning rate from the scheduler if it exists.
+
+        Returns:
+            float: The last learning rate.
+
+        Raises:
+            RuntimeError: If the LR scheduler has not been set.
+        """
+        if self.lr_scheduler:
+            return self.lr_scheduler.get_last_lr()[0]
+        else:
+            raise RuntimeError(
+                "LR scheduler has not been set. Call set_lr_scheduler first."
+            )
+
+
+def create_optim_in_bwd_wrapper(
+    model: torch.nn.Module, optim_dict: Dict[torch.nn.Parameter, torch.optim.Optimizer]
+) -> OptimizerInBackwardWrapper:
+    """
+    Create a wrapper for optimizer step running in backward.
+
+    Args:
+        model (torch.nn.Module): Model that contains parameters that are being optimized. For now,
+            it is assumed that all parameters being optimized belong to a single top-level model.
+            ``named_parameters`` attribute of ``model`` will be accessed to look up parameter names for
+            parameters being optimized.
+        optim_dict (Dict[torch.nn.Parameter, torch.optim.Optimizer]): Mapping from
+            parameters to optimizers.
+
+    Returns:
+        ``OptimizerInBackwardWrapper``: Wrapper for optimizer states running in backward.
+    """
+    return OptimizerInBackwardWrapper(
+        {n: optim_dict[p] for n, p in model.named_parameters()}
+    )
+
+
+def register_optim_in_bwd_hooks(
+    model: torch.nn.Module, optim_dict: Dict[torch.nn.Parameter, torch.optim.Optimizer]
+) -> None:
+    """
+    Register hooks for optimizer step running in backward.
+
+    When fusing the optimizer step into backward, we need to call ``.step()`` on the optimizer
+    for a given parameter as soon as its gradient is ready. This utility registers post-accumulate-grad
+    hooks on all parameters in the model to achieve this.
+
+    Args:
+        model (torch.nn.Module): Model whose parameters will be optimized. Note that currently
+            hooks for ALL parameters in the model will be registered.
+        optim_dict (Dict[torch.nn.Parameter, torch.optim.Optimizer]): Mapping from
+            parameters to optimizers.
+    """
+
+    def optim_step(param) -> None:
+        optim_dict[param].step()
+        optim_dict[param].zero_grad()
+
+    for p in model.parameters():
+        p.register_post_accumulate_grad_hook(optim_step)
+
+
+def get_memory_stats(device: torch.device, reset_stats: bool = True) -> dict:
+    """
+    Computes a memory summary for the passed in device. If ``reset_stats`` is ``True``, this will
+    also reset CUDA's peak memory tracking. This is useful to get data around relative use of peak
+    memory (e.g. peak memory during model init, during forward, etc) and optimize memory for
+    individual sections of training.
+
+    Args:
+        device (torch.device): Device to get memory summary for. Only CUDA devices are supported.
+        reset_stats (bool): Whether to reset CUDA's peak memory tracking.
+
+    Returns:
+        Dict[str, float]: A dictionary containing the peak memory active, peak memory allocated,
+        and peak memory reserved. This dict is useful for logging memory stats.
+
+    Raises:
+        ValueError: If the passed-in device is not CUDA.
+    """
+    if device.type != "cuda":
+        raise ValueError(
+            f"Logging memory stats is only supported on CUDA devices, got {device}"
+        )
+
+    peak_memory_active = torch.cuda.memory_stats().get("active_bytes.all.peak", 0) / (
+        1024**3
+    )
+    peak_mem_alloc = torch.cuda.max_memory_allocated(device) / (1024**3)
+    peak_mem_reserved = torch.cuda.max_memory_reserved(device) / (1024**3)
+
+    if reset_stats:
+        torch.cuda.reset_peak_memory_stats(device)
+
+    memory_stats = {
+        "peak_memory_active": peak_memory_active,
+        "peak_memory_alloc": peak_mem_alloc,
+        "peak_memory_reserved": peak_mem_reserved,
+    }
+    return memory_stats
+
+
+def log_memory_stats(stats: Dict[str, float]) -> None:
+    """
+    Logs a dict containing memory stats to the logger. ``stats`` should contain the fields
+    ``peak_memory_active``, ``peak_memory_alloc``, and ``peak_memory_reserved`` as
+    returned by :func:`torchtune.training.get_memory_stats`.
+
+    Args:
+        stats (Dict[str, float]): A dictionary containing the peak memory active, peak memory
+            allocated, and peak memory reserved stats.
+    """
+    _log.info(
+        "Memory stats after model init:"
+        f"\n\tGPU peak memory allocation: {stats['peak_memory_alloc']:.2f} GiB"
+        f"\n\tGPU peak memory reserved: {stats['peak_memory_reserved']:.2f} GiB"
+        f"\n\tGPU peak memory active: {stats['peak_memory_active']:.2f} GiB"
+    )
diff -ruN marc_original/third_party/torchtune/torchtune/training/metric_logging.py marc/third_party/torchtune/torchtune/training/metric_logging.py
--- marc_original/third_party/torchtune/torchtune/training/metric_logging.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/metric_logging.py	2025-02-20 17:49:30.826026343 -0500
@@ -0,0 +1,449 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import os
+import sys
+import time
+from pathlib import Path
+
+from typing import Any, Dict, List, Mapping, Optional, Union
+
+import torch
+
+from numpy import ndarray
+from omegaconf import DictConfig, OmegaConf
+from torchtune.training._distributed import get_world_size_and_rank
+
+from torchtune.utils import get_logger
+from typing_extensions import Protocol
+
+Scalar = Union[torch.Tensor, ndarray, int, float]
+
+log = get_logger("DEBUG")
+
+
+class MetricLoggerInterface(Protocol):
+    """Abstract metric logger."""
+
+    def log(
+        self,
+        name: str,
+        data: Scalar,
+        step: int,
+    ) -> None:
+        """Log scalar data.
+
+        Args:
+            name (str): tag name used to group scalars
+            data (Scalar): scalar data to log
+            step (int): step value to record
+        """
+        pass
+
+    def log_config(self, config: DictConfig) -> None:
+        """Logs the config
+
+        Args:
+            config (DictConfig): config to log
+        """
+        pass
+
+    def log_dict(self, payload: Mapping[str, Scalar], step: int) -> None:
+        """Log multiple scalar values.
+
+        Args:
+            payload (Mapping[str, Scalar]): dictionary of tag name and scalar value
+            step (int): step value to record
+        """
+        pass
+
+    def close(self) -> None:
+        """
+        Close log resource, flushing if necessary.
+        Logs should not be written after `close` is called.
+        """
+        pass
+
+
+class DiskLogger(MetricLoggerInterface):
+    """Logger to disk.
+
+    Args:
+        log_dir (str): directory to store logs
+        filename (Optional[str]): optional filename to write logs to.
+            Default: None, in which case log_{unixtimestamp}.txt will be used.
+        **kwargs: additional arguments
+
+    Warning:
+        This logger is not thread-safe.
+
+    Note:
+        This logger creates a new file based on the current time.
+    """
+
+    def __init__(self, log_dir: str, filename: Optional[str] = None, **kwargs):
+        self.log_dir = Path(log_dir)
+        self.log_dir.mkdir(parents=True, exist_ok=True)
+        if not filename:
+            unix_timestamp = int(time.time())
+            filename = f"log_{unix_timestamp}.txt"
+        self._file_name = self.log_dir / filename
+        self._file = open(self._file_name, "a")
+        print(f"Writing logs to {self._file_name}")
+
+    def path_to_log_file(self) -> Path:
+        return self._file_name
+
+    def log(self, name: str, data: Scalar, step: int) -> None:
+        self._file.write(f"Step {step} | {name}:{data}\n")
+        self._file.flush()
+
+    def log_dict(self, payload: Mapping[str, Scalar], step: int) -> None:
+        self._file.write(f"Step {step} | ")
+        for name, data in payload.items():
+            self._file.write(f"{name}:{data} ")
+        self._file.write("\n")
+        self._file.flush()
+
+    def __del__(self) -> None:
+        self._file.close()
+
+    def close(self) -> None:
+        self._file.close()
+
+
+class StdoutLogger(MetricLoggerInterface):
+    """Logger to standard output."""
+
+    def log(self, name: str, data: Scalar, step: int) -> None:
+        print(f"Step {step} | {name}:{data}")
+
+    def log_dict(self, payload: Mapping[str, Scalar], step: int) -> None:
+        print(f"Step {step} | ", end="")
+        for name, data in payload.items():
+            print(f"{name}:{data} ", end="")
+        print("\n", end="")
+
+    def __del__(self) -> None:
+        sys.stdout.flush()
+
+    def close(self) -> None:
+        sys.stdout.flush()
+
+
+class WandBLogger(MetricLoggerInterface):
+    """Logger for use w/ Weights and Biases application (https://wandb.ai/).
+    For more information about arguments expected by WandB, see https://docs.wandb.ai/ref/python/init.
+
+    Args:
+        project (str): WandB project name. Default is `torchtune`.
+        entity (Optional[str]): WandB entity name. If you don't specify an entity,
+            the run will be sent to your default entity, which is usually your username.
+        group (Optional[str]): WandB group name for grouping runs together. If you don't
+            specify a group, the run will be logged as an individual experiment.
+        log_dir (Optional[str]): WandB log directory. If not specified, use the `dir`
+            argument provided in kwargs. Else, use root directory.
+        **kwargs: additional arguments to pass to wandb.init
+
+    Example:
+        >>> from torchtune.training.metric_logging import WandBLogger
+        >>> logger = WandBLogger(project="my_project", entity="my_entity", group="my_group")
+        >>> logger.log("my_metric", 1.0, 1)
+        >>> logger.log_dict({"my_metric": 1.0}, 1)
+        >>> logger.close()
+
+    Raises:
+        ImportError: If ``wandb`` package is not installed.
+
+    Note:
+        This logger requires the wandb package to be installed.
+        You can install it with `pip install wandb`.
+        In order to use the logger, you need to login to your WandB account.
+        You can do this by running `wandb login` in your terminal.
+    """
+
+    def __init__(
+        self,
+        project: str = "torchtune",
+        entity: Optional[str] = None,
+        group: Optional[str] = None,
+        log_dir: Optional[str] = None,
+        **kwargs,
+    ):
+        try:
+            import wandb
+        except ImportError as e:
+            raise ImportError(
+                "``wandb`` package not found. Please install wandb using `pip install wandb` to use WandBLogger."
+                "Alternatively, use the ``StdoutLogger``, which can be specified by setting metric_logger_type='stdout'."
+            ) from e
+        self._wandb = wandb
+
+        # Use dir if specified, otherwise use log_dir.
+        self.log_dir = kwargs.pop("dir", log_dir)
+
+        _, self.rank = get_world_size_and_rank()
+
+        if self._wandb.run is None and self.rank == 0:
+            # we check if wandb.init got called externally,
+            run = self._wandb.init(
+                project=project,
+                entity=entity,
+                group=group,
+                dir=self.log_dir,
+                **kwargs,
+            )
+
+        if self._wandb.run:
+            self._wandb.run._label(repo="torchtune")
+
+        # define default x-axis (for latest wandb versions)
+        if getattr(self._wandb, "define_metric", None):
+            self._wandb.define_metric("global_step")
+            self._wandb.define_metric("*", step_metric="global_step", step_sync=True)
+
+        self.config_allow_val_change = kwargs.get("allow_val_change", False)
+
+    def log_config(self, config: DictConfig) -> None:
+        """Saves the config locally and also logs the config to W&B. The config is
+        stored in the same directory as the checkpoint. You can
+        see an example of the logged config to W&B in the following link:
+        https://wandb.ai/capecape/torchtune/runs/6053ofw0/files/torchtune_config_j67sb73v.yaml
+
+        Args:
+            config (DictConfig): config to log
+        """
+        if self._wandb.run:
+            resolved = OmegaConf.to_container(config, resolve=True)
+            self._wandb.config.update(
+                resolved, allow_val_change=self.config_allow_val_change
+            )
+            try:
+                output_config_fname = Path(
+                    os.path.join(
+                        config.checkpointer.checkpoint_dir,
+                        "torchtune_config.yaml",
+                    )
+                )
+                OmegaConf.save(config, output_config_fname)
+
+                log.info(f"Logging {output_config_fname} to W&B under Files")
+                self._wandb.save(
+                    output_config_fname, base_path=output_config_fname.parent
+                )
+
+            except Exception as e:
+                log.warning(
+                    f"Error saving {output_config_fname} to W&B.\nError: \n{e}."
+                    "Don't worry the config will be logged the W&B workspace"
+                )
+
+    def log(self, name: str, data: Scalar, step: int) -> None:
+        if self._wandb.run:
+            self._wandb.log({name: data, "global_step": step})
+
+    def log_dict(self, payload: Mapping[str, Scalar], step: int) -> None:
+        if self._wandb.run:
+            self._wandb.log({**payload, "global_step": step})
+
+    def __del__(self) -> None:
+        # extra check for when there is an import error
+        if hasattr(self, "_wandb") and self._wandb.run:
+            self._wandb.finish()
+
+    def close(self) -> None:
+        if self._wandb.run:
+            self._wandb.finish()
+
+
+class TensorBoardLogger(MetricLoggerInterface):
+    """Logger for use w/ PyTorch's implementation of TensorBoard (https://pytorch.org/docs/stable/tensorboard.html).
+
+    Args:
+        log_dir (str): torch.TensorBoard log directory
+        organize_logs (bool): If `True`, this class will create a subdirectory within `log_dir` for the current
+            run. Having sub-directories allows you to compare logs across runs. When TensorBoard is
+            passed a logdir at startup, it recursively walks the directory tree rooted at logdir looking for
+            subdirectories that contain tfevents data. Every time it encounters such a subdirectory,
+            it loads it as a new run, and the frontend will organize the data accordingly.
+            Recommended value is `True`. Run `tensorboard --logdir my_log_dir` to view the logs.
+        **kwargs: additional arguments
+
+    Example:
+        >>> from torchtune.training.metric_logging import TensorBoardLogger
+        >>> logger = TensorBoardLogger(log_dir="my_log_dir")
+        >>> logger.log("my_metric", 1.0, 1)
+        >>> logger.log_dict({"my_metric": 1.0}, 1)
+        >>> logger.close()
+
+    Note:
+        This utility requires the tensorboard package to be installed.
+        You can install it with `pip install tensorboard`.
+        In order to view TensorBoard logs, you need to run `tensorboard --logdir my_log_dir` in your terminal.
+    """
+
+    def __init__(self, log_dir: str, organize_logs: bool = True, **kwargs):
+        from torch.utils.tensorboard import SummaryWriter
+
+        self._writer: Optional[SummaryWriter] = None
+        _, self._rank = get_world_size_and_rank()
+
+        # In case organize_logs is `True`, update log_dir to include a subdirectory for the
+        # current run
+        self.log_dir = (
+            os.path.join(log_dir, f"run_{self._rank}_{time.time()}")
+            if organize_logs
+            else log_dir
+        )
+
+        # Initialize the log writer only if we're on rank 0.
+        if self._rank == 0:
+            self._writer = SummaryWriter(log_dir=self.log_dir)
+
+    def log(self, name: str, data: Scalar, step: int) -> None:
+        if self._writer:
+            self._writer.add_scalar(name, data, global_step=step, new_style=True)
+
+    def log_dict(self, payload: Mapping[str, Scalar], step: int) -> None:
+        for name, data in payload.items():
+            self.log(name, data, step)
+
+    def __del__(self) -> None:
+        if self._writer:
+            self._writer.close()
+            self._writer = None
+
+    def close(self) -> None:
+        if self._writer:
+            self._writer.close()
+            self._writer = None
+
+
+class CometLogger(MetricLoggerInterface):
+    """Logger for use w/ Comet (https://www.comet.com/site/).
+    Comet is an experiment tracking tool that helps ML teams track, debug,
+    compare, and reproduce their model training runs.
+
+    For more information about arguments expected by Comet, see
+    https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#for-the-experiment.
+
+    Args:
+        api_key (Optional[str]): Comet API key. It's recommended to configure the API Key with `comet login`.
+        workspace (Optional[str]): Comet workspace name. If not provided, uses the default workspace.
+        project (Optional[str]): Comet project name. Defaults to Uncategorized.
+        experiment_key (Optional[str]): The key for comet experiment to be used for logging. This is used either to
+            append data to an Existing Experiment or to control the ID of new experiments (for example to match another
+            ID). Must be an alphanumeric string whose length is between 32 and 50 characters.
+        mode (Optional[str]): Control how the Comet experiment is started.
+
+            * ``"get_or_create"``: Starts a fresh experiment if required, or persists logging to an existing one.
+            * ``"get"``: Continue logging to an existing experiment identified by the ``experiment_key`` value.
+            * ``"create"``: Always creates of a new experiment, useful for HPO sweeps.
+        online (Optional[bool]): If True, the data will be logged to Comet server, otherwise it will be stored locally
+            in an offline experiment. Default is ``True``.
+        experiment_name (Optional[str]): Name of the experiment. If not provided, Comet will auto-generate a name.
+        tags (Optional[List[str]]): Tags to associate with the experiment.
+        log_code (bool): Whether to log the source code. Defaults to True.
+        **kwargs (Dict[str, Any]): additional arguments to pass to ``comet_ml.start``. See
+            https://www.comet.com/docs/v2/api-and-sdk/python-sdk/reference/Experiment-Creation/#comet_ml.ExperimentConfig
+
+    Example:
+        >>> from torchtune.training.metric_logging import CometLogger
+        >>> logger = CometLogger(project_name="my_project", workspace="my_workspace")
+        >>> logger.log("my_metric", 1.0, 1)
+        >>> logger.log_dict({"my_metric": 1.0}, 1)
+        >>> logger.close()
+
+    Raises:
+        ImportError: If ``comet_ml`` package is not installed.
+
+    Note:
+        This logger requires the comet_ml package to be installed.
+        You can install it with ``pip install comet_ml``.
+        You need to set up your Comet.ml API key before using this logger.
+        You can do this by calling ``comet login`` in your terminal.
+        You can also set it as the `COMET_API_KEY` environment variable.
+    """
+
+    def __init__(
+        self,
+        api_key: Optional[str] = None,
+        workspace: Optional[str] = None,
+        project: Optional[str] = None,
+        experiment_key: Optional[str] = None,
+        mode: Optional[str] = None,
+        online: Optional[bool] = None,
+        experiment_name: Optional[str] = None,
+        tags: Optional[List[str]] = None,
+        log_code: bool = True,
+        **kwargs: Dict[str, Any],
+    ):
+        try:
+            import comet_ml
+        except ImportError as e:
+            raise ImportError(
+                "``comet_ml`` package not found. Please install comet_ml using `pip install comet_ml` to use CometLogger."
+                "Alternatively, use the ``StdoutLogger``, which can be specified by setting metric_logger_type='stdout'."
+            ) from e
+
+        _, self.rank = get_world_size_and_rank()
+
+        # Declare it early so further methods don't crash in case of
+        # Experiment Creation failure due to mis-named configuration for
+        # example
+        self.experiment = None
+
+        if self.rank == 0:
+            self.experiment = comet_ml.start(
+                api_key=api_key,
+                workspace=workspace,
+                project=project,
+                experiment_key=experiment_key,
+                mode=mode,
+                online=online,
+                experiment_config=comet_ml.ExperimentConfig(
+                    log_code=log_code, tags=tags, name=experiment_name, **kwargs
+                ),
+            )
+
+    def log(self, name: str, data: Scalar, step: int) -> None:
+        if self.experiment is not None:
+            self.experiment.log_metric(name, data, step=step)
+
+    def log_dict(self, payload: Mapping[str, Scalar], step: int) -> None:
+        if self.experiment is not None:
+            self.experiment.log_metrics(payload, step=step)
+
+    def log_config(self, config: DictConfig) -> None:
+        if self.experiment is not None:
+            resolved = OmegaConf.to_container(config, resolve=True)
+            self.experiment.log_parameters(resolved)
+
+            # Also try to save the config as a file
+            try:
+                self._log_config_as_file(config)
+            except Exception as e:
+                log.warning(f"Error saving Config to disk.\nError: \n{e}.")
+                return
+
+    def _log_config_as_file(self, config: DictConfig):
+        output_config_fname = Path(
+            os.path.join(
+                config.checkpointer.checkpoint_dir,
+                "torchtune_config.yaml",
+            )
+        )
+        OmegaConf.save(config, output_config_fname)
+
+        self.experiment.log_asset(
+            output_config_fname, file_name="torchtune_config.yaml"
+        )
+
+    def close(self) -> None:
+        if self.experiment is not None:
+            self.experiment.end()
+
+    def __del__(self) -> None:
+        self.close()
diff -ruN marc_original/third_party/torchtune/torchtune/training/pooling.py marc/third_party/torchtune/torchtune/training/pooling.py
--- marc_original/third_party/torchtune/torchtune/training/pooling.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/pooling.py	2025-02-20 17:49:30.830026350 -0500
@@ -0,0 +1,49 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+import torch
+
+
+def get_unmasked_sequence_lengths(mask: torch.Tensor) -> torch.Tensor:
+    """
+    Returns the sequence lengths for each batch element, excluding masked tokens.
+
+    Args:
+        mask (torch.Tensor): Boolean mask with shape [b x s], where True indicates a value to be masked out
+            This is usually a mask for padding tokens, where True indicates a padding token.
+
+    Returns:
+        Tensor: Sequence indices logits with shape [b]
+
+    Shape notation:
+        - b = batch size
+        - s = sequence length
+
+    Example:
+        >>> input_ids = torch.tensor([
+        ...        [2, 4, 0, 0],
+        ...        [2, 4, 6, 0],
+        ...        [2, 4, 6, 9]
+        ...    ])
+        >>> mask = input_ids == 0
+        >>> mask
+        tensor([[False, False,  True,  True],
+                [False, False, False,  True],
+                [False, False, False, False]])
+        >>> get_unmasked_sequence_lengths(mask)
+        tensor([1, 2, 3])
+
+    """
+    # calculate per-batch-element sequence lengths by finding last valid tokens
+    if mask.any():
+        sequence_lengths = (
+            (~mask).sum(-1).sub(1).clip(0).to(mask.device, dtype=torch.long)
+        )
+    else:
+        sequence_lengths = torch.full(
+            (mask.shape[0],), mask.shape[1] - 1, dtype=torch.long, device=mask.device
+        )
+
+    return sequence_lengths
diff -ruN marc_original/third_party/torchtune/torchtune/training/precision.py marc/third_party/torchtune/torchtune/training/precision.py
--- marc_original/third_party/torchtune/torchtune/training/precision.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/precision.py	2025-02-20 17:49:30.834026356 -0500
@@ -0,0 +1,162 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import contextlib
+from typing import Dict, Generator, Iterable, Optional, Tuple
+
+import torch
+
+from torchtune.utils import get_logger
+
+log = get_logger()
+
+PRECISION_STR_TO_DTYPE: Dict[str, torch.dtype] = {
+    "fp16": torch.float16,
+    "bf16": torch.bfloat16,
+    "fp32": torch.float32,
+    "fp64": torch.float64,
+}
+
+
+def _set_float32_precision(precision: str = "high") -> None:
+    """Sets the precision of float32 matrix multiplications and convolution operations.
+
+    For more information, see the PyTorch docs:
+    - https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html
+    - https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.allow_tf32
+
+    Args:
+        precision (str): The setting to determine which datatypes to use for matrix multiplication and convolution operations.
+    """
+    if not torch.cuda.is_available():  # Not relevant for non-CUDA devices
+        return
+    # set precision for matrix multiplications
+    torch.set_float32_matmul_precision(precision)
+    # set precision for convolution operations
+    if precision == "highest":
+        torch.backends.cudnn.allow_tf32 = False
+    else:
+        torch.backends.cudnn.allow_tf32 = True
+
+
+def verify_bf16_support() -> bool:
+    """
+    Check that bf16 is available on this hardware. Requirements:
+        - CUDA is available and supports bf16
+            - CUDA version >= 11
+            - CUDA compute capability >= 8
+        - NCCL is available and version >= 2.10
+        - MPS is available and torch was built with MPS
+
+    Returns:
+        bool: True if bf16 is available, False otherwise.
+
+    """
+    cuda_support = (
+        torch.cuda.is_available()
+        and torch.cuda.is_bf16_supported()
+        and torch.distributed.is_nccl_available()
+        and torch.cuda.nccl.version() >= (2, 10)
+    )
+    mps_support = torch.backends.mps.is_available() and torch.backends.mps.is_built()
+    return cuda_support or mps_support
+
+
+def get_dtype(
+    dtype: Optional[str] = None, device: Optional[torch.device] = None
+) -> torch.dtype:
+    """Get the torch.dtype corresponding to the given precision string. If no string is passed,
+    we will default to torch.float32.
+
+    Note:
+        If bf16 precision is requested with a CUDA device, we verify whether the device indeed supports
+        bf16 kernels. If not, a ``RuntimeError`` is raised.
+
+    Args:
+        dtype (Optional[str]): The precision dtype. Default: ``None``, in which we default to torch.float32
+        device (Optional[torch.device]): Device in use for training. Only CUDA and CPU
+            devices are supported. If a CUDA device is passed in, additional checking is done
+            to ensure that the device supports the requested precision. Default: ``None``, in which case
+            a CUDA device is assumed.
+    Raises:
+        ValueError: if precision isn't supported by the library
+        RuntimeError: if bf16 precision is requested but not available on this hardware.
+
+    Returns:
+        torch.dtype: The corresponding torch.dtype.
+
+    """
+
+    # None defaults to float32
+    if dtype is None:
+        return torch.float32
+
+    # Convert to torch.dtype
+    torch_dtype = PRECISION_STR_TO_DTYPE.get(dtype, dtype)
+
+    # dtype must be one of the supported precisions
+    if torch_dtype not in PRECISION_STR_TO_DTYPE.values():
+        raise ValueError(
+            f"Dtype {torch_dtype} must be one of {', '.join(list(PRECISION_STR_TO_DTYPE.keys()))} for finetuning."
+        )
+
+    if (
+        torch_dtype == torch.bfloat16
+        and device != torch.device("cpu")
+        and not verify_bf16_support()
+    ):
+        raise RuntimeError(
+            "bf16 precision was requested but not available on this hardware. Please use fp32 precision instead."
+        )
+
+    return torch_dtype
+
+
+@contextlib.contextmanager
+def set_default_dtype(dtype: torch.dtype) -> Generator[None, None, None]:
+    """
+    Context manager to set torch's default dtype.
+
+    Args:
+        dtype (torch.dtype): The desired default dtype inside the context manager.
+
+    Returns:
+        ContextManager: context manager for setting default dtype.
+
+    Example:
+        >>> with set_default_dtype(torch.bfloat16):
+        >>>     x = torch.tensor([1, 2, 3])
+        >>>     x.dtype
+        torch.bfloat16
+
+
+    """
+    old_dtype = torch.get_default_dtype()
+    torch.set_default_dtype(dtype)
+    try:
+        yield
+    finally:
+        torch.set_default_dtype(old_dtype)
+
+
+def validate_expected_param_dtype(
+    named_params: Iterable[Tuple[str, torch.nn.Parameter]], dtype: torch.dtype
+) -> None:
+    """
+    Validates that all input parameters have the expected dtype.
+
+    Args:
+        named_params (Iterable[Tuple[str, torch.nn.Parameter]]): Iterable of named parameters.
+        dtype (torch.dtype): Expected dtype.
+
+    Raises:
+        ValueError: If any parameter has a different dtype than `dtype`.
+    """
+    for name, param in named_params:
+        if param.dtype != dtype:
+            raise ValueError(
+                f"Parameter {name} has dtype {param.dtype}, but expected {dtype}"
+            )
diff -ruN marc_original/third_party/torchtune/torchtune/training/_profiler.py marc/third_party/torchtune/torchtune/training/_profiler.py
--- marc_original/third_party/torchtune/torchtune/training/_profiler.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/_profiler.py	2025-02-20 17:49:30.802026304 -0500
@@ -0,0 +1,384 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+import datetime
+import os
+import time
+from functools import partial
+from pathlib import Path
+from typing import Optional, Tuple
+
+import torch
+import torch.distributed
+
+from omegaconf import DictConfig
+from torch._C._profiler import _ExperimentalConfig
+from torch.profiler import tensorboard_trace_handler
+from torchtune.training import get_world_size_and_rank
+
+from torchtune.utils import get_logger
+
+log = get_logger("INFO")
+
+PROFILER_KEY = "profiler"
+DEFAULT_PROFILER_ACTIVITIES = {
+    torch.profiler.ProfilerActivity.CPU,
+    torch.profiler.ProfilerActivity.CUDA,
+}
+
+DEFAULT_SCHEDULE: dict = {
+    "wait_steps": 5,
+    "warmup_steps": 5,
+    "active_steps": 2,
+    "num_cycles": 1,
+}
+
+DEFAULT_TRACE_OPTS: dict = {
+    "profile_memory": False,
+    "with_stack": False,
+    "record_shapes": True,
+    "with_flops": False,
+}
+
+DEFAULT_PROFILE_DIR: str = "profiler_output"
+
+
+def _warn(msg: str):
+    _, rank = get_world_size_and_rank()
+    if rank == 0:
+        log.warning(msg)
+
+
+def trace_handler(
+    prof: torch.profiler.profile,
+    output_dir: str,
+    metric: str = "self_cuda_time_total",
+    row_limit: int = 25,
+):
+    """
+    Handles export of artifacts from ``torch.profiler.profile``.
+
+    The following artifacts are exported:
+    - chrome / tensorboard trace - viewable through tensorboard or perfetto.dev / chrome::/tracing
+    - trace event table
+    - memory timeline and snapshot.pickle if ``profile_memory``
+    - stacks if ``with_stack`` (note that ``profile_memory`` requires ``with_stack`` to be ``True``),
+    viewable as a flamegraph see (https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.export_stacks).
+
+    Notes:
+    - Each profiling cycle is exported as a sub-directory in output_dir
+        - E.g., profiling in 5-step cycle (wait=2, warmup=2, active=1, repeat=0) will result in
+        sub-directories iteration_5, iteration_10, etc.
+    - If profiling in a distributed setting, each artifact will be prefixed with rank.
+    - Memory timeline is only exported for rank 0 (error if exporting from multiple ranks on single node)
+
+    See profiler documentation (https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile) for more details
+
+    Args:
+        prof (torch.profiler.profile): instance of torch profiler to use
+        output_dir (str):  directory to store artifacts
+        metric (str): metric to order trace event table by, see ``torch.profiler.profile.key_averages().table`` for
+        row_limit (int): number of rows to display in trace event table
+
+    """
+    world_size, rank = get_world_size_and_rank()
+    curr_trace_dir_name = "iteration_" + str(prof.step_num)
+    curr_trace_dir = os.path.join(output_dir, curr_trace_dir_name)
+    if not os.path.exists(curr_trace_dir):
+        os.makedirs(curr_trace_dir, exist_ok=True)
+
+    # Export chrome / tensorboard trace
+    if rank == 0:
+        log.info(f"Dumping traces at step {prof.step_num}")
+    begin = time.monotonic()
+
+    # Use tensorboard trace handler rather than directly exporting chrome traces since
+    # tensorboard doesn't seem to be able to parse traces with prof.export_chrome_trace
+
+    now = datetime.datetime.now()
+
+    exporter = tensorboard_trace_handler(
+        curr_trace_dir,
+        worker_name=f"r0-{now.year}-{now.month}-{now.day}-{now.hour}-{now.minute}",
+        use_gzip=True,
+    )
+    exporter(prof)
+
+    if rank == 0:
+        log.info(f"Finished dumping traces in {time.monotonic() - begin:.2f} seconds")
+
+    # Memory timeline sometimes fails to export
+    if prof.profile_memory:
+        if rank == 0:
+            try:
+                prof.export_memory_timeline(
+                    f"{curr_trace_dir}/rank{rank}_memory-timeline.html"
+                )
+            except Exception as e:
+                log.warn(f" Failed to export memory timeline: {e}")
+
+            torch.cuda.memory._dump_snapshot(
+                f"{curr_trace_dir}/rank{rank}_memory_snapshot.pickle"
+            )
+
+    # Dump stack traces
+    if prof.with_stack:
+        prof.export_stacks(f"{curr_trace_dir}/rank{rank}_stacks.txt", metric=metric)
+
+    # Export event averages
+    key_avgs = prof.key_averages(
+        group_by_input_shape=prof.record_shapes, group_by_stack_n=5
+    ).table(sort_by=metric, row_limit=row_limit)
+    with open(f"{curr_trace_dir}/rank{rank}_key_averages.txt", "w") as f:
+        print(key_avgs, file=f)
+    if rank == 0:
+        log.info(f"Saving profiling results to {curr_trace_dir}")
+
+    # TODO: Is this necessary?
+    # see https://github.com/pytorch/torchtitan/blob/3050098dcee4901d88c712f9e8e9703d1735a29b/torchtitan/profiling.py#L48
+    if world_size > 1:
+        torch.distributed.barrier()
+
+
+class DummyProfiler:
+    """
+    Drop-in replacement for torch.profiler.profile that functions as a nullcontext / object
+    with no-op methods for ``start``, ``stop``, and ``step``.
+
+    This is helpful for instrumenting profiling in a recipe without requiring changes to the
+    code independent of whether profiling is on / off.
+
+    E.g.,
+    ```
+        profiler = DummyProfiler()
+        #profiler = torch.profiler.profile()
+
+        # Below is same regardless of profiler object type
+        with profiler as prof:
+            for epoch in epochs:
+                for batch in batches:
+                    train.step()
+                    prof.step()
+
+    """
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *args):
+        pass
+
+    def start(self):
+        pass
+
+    def stop(self):
+        pass
+
+    def step(self):
+        pass
+
+
+def setup_torch_profiler(
+    enabled: bool = False,
+    cpu: bool = True,
+    cuda: bool = True,
+    profile_memory: bool = DEFAULT_TRACE_OPTS["profile_memory"],
+    with_stack: bool = DEFAULT_TRACE_OPTS["with_stack"],
+    record_shapes: bool = DEFAULT_TRACE_OPTS["record_shapes"],
+    with_flops: bool = DEFAULT_TRACE_OPTS["with_flops"],
+    # `torch.profiler.schedule` args - note we defer setting these to enable more fine-grained
+    # warnings within this setup function
+    wait_steps: Optional[int] = None,
+    warmup_steps: Optional[int] = None,
+    active_steps: Optional[int] = None,
+    num_cycles: Optional[int] = None,
+    output_dir: Optional[str] = None,
+) -> Tuple[torch.profiler.profile, DictConfig]:
+    """
+    Sets up :class:`~torch.profiler.profile` and returns the profiler config with post-setup updates.
+
+    The profiler config can be provided in configs under the ``profiler`` key with the following layout:
+
+    .. code-block:: yaml
+
+        profiler:
+          _component_: torchtune.training.setup_torch_profiler
+          enabled: bool
+          # Output directory of trace artifacts
+          output_dir: str
+
+          # torch.profiler.ProfilerActivity types to trace
+          cpu: bool
+          cuda: bool
+
+          # Trace options
+          profile_memory: bool
+          with_stack: bool
+          record_shapes: bool
+          with_flops: bool
+
+          # torch.profiler.schedule args
+          wait_steps: int
+          warmup_steps: int
+          active_steps: int
+          num_cycles: int
+
+    The profiler schedule updates with respect to an optimizer step (e.g., if
+    ``gradient_accumulation = 2``, then the profiler will step every 2 batches).
+
+    Sensible defaults will be chosen if the config is missing options:
+
+    - If no activities are specified, profiler will default to CPU + CUDA
+    - If no schedule is specified, profiler will default to ``DEFAULT_SCHEDULE``
+    - Certain options will be overridden (``with_stack`` and ``record_shapes``) \
+    depending on requirements of other options (e.g., ``profile_memory`` requires \
+    ``with_stack`` and ``record_shapes``).
+
+
+    Note:
+        - Enabling the profiler will result in training speed reduction.
+        - Setting ``profile_memory: True`` will generate large trace files.
+        - The profiler schedule is context dependent. Calling ``profiler.step()`` \
+        at each batch iteration but **outside** the gradient accumulation scope will \
+        ``step`` the profiler each forward / backward step. Calling ``profiler.step()`` \
+        each batch iteration but **within** the gradient accumulation scope  will ``step`` \
+        the profiler each optimizer update step such that each ``step`` contains multiple \
+        forward / backward passes.
+
+    Args:
+        enabled (bool): Enable pytorch profiler. Default is False.
+        cpu (bool): Enable cpu profiling. Default is True.
+        cuda (bool): Enable cuda profiling. Default is True.
+        profile_memory (bool): Profile memory usage. Default is False.
+        with_stack (bool): Profile stack. Default is False.
+        record_shapes (bool): Record shapes. Default is True.
+        with_flops (bool): Profile flops. Default is False.
+        wait_steps (Optional[int]): Wait time in steps. Maps to ``wait`` kwarg of ``torch.profiler.schedule``.
+        warmup_steps (Optional[int]): Warmup time in steps. Maps to ``warmup`` kwarg of ``torch.profiler.schedule``.
+        active_steps (Optional[int]): Active time in steps. Maps to ``active`` kwarg of ``torch.profiler.schedule``.
+        num_cycles (Optional[int]): Number of profiling cycles. Maps to ``repeat`` kwarg of ``torch.profiler.schedule``.
+        output_dir (Optional[str]): Tracing file output path.
+
+    Returns:
+        Tuple[torch.profiler.profile, DictConfig]
+    """
+
+    if not enabled:
+        _warn(" Profiling disabled.")
+        return DummyProfiler(), DictConfig({"enabled": False})
+
+    # Set up profiler activities
+    activities = []
+    if cpu:
+        activities.append(torch.profiler.ProfilerActivity.CPU)
+    if cuda:
+        activities.append(torch.profiler.ProfilerActivity.CUDA)
+    if len(activities) == 0:
+        _warn("No activities specified, defaulting to CPU + CUDA")
+        activities = DEFAULT_PROFILER_ACTIVITIES
+        cpu = cuda = True
+
+    # Check for schedule
+    # 1) If no schedule is provided, set to DEFAULT_SCHEDULE
+    # 2) else check for missing keys and warn if any are missing, setting these to defaults
+    # Note that this might result in code duplication if these checks are already done in the `recipe`
+    # However, we retain this checks in the case that the _setup_profiler section of the `recipe` does not implement these checks
+
+    # Set up profiler schedule
+    use_default_schedule = not any(
+        [
+            wait_steps is not None,
+            warmup_steps is not None,
+            active_steps is not None,
+            num_cycles is not None,
+        ]
+    )
+
+    # Use default schedule if None, else validate that schedule is valid and can be passed to `instantiate`
+    if use_default_schedule:
+        schedule_args = DEFAULT_SCHEDULE
+        _warn(
+            " No schedule found in config, defaulting to {}".format(
+                ", ".join(f"{k} = {schedule_args[k]}" for k in schedule_args.keys())
+            )
+        )
+    else:
+        schedule_args = {
+            "wait_steps": wait_steps,
+            "warmup_steps": warmup_steps,
+            "active_steps": active_steps,
+            "num_cycles": num_cycles,
+        }
+        missing_keys = [k for k in schedule_args.keys() if schedule_args[k] is None]
+        if len(missing_keys) > 0:
+            for k in missing_keys:
+                schedule_args[k] = DEFAULT_SCHEDULE[k]
+            _warn(
+                " Missing keys in torch profiler schedule {}: defaulting to {}".format(
+                    ", ".join(missing_keys),
+                    ", ".join(f"{k} = {schedule_args[k]}" for k in missing_keys),
+                )
+            )
+    schedule = torch.profiler.schedule(
+        wait=schedule_args["wait_steps"],
+        warmup=schedule_args["warmup_steps"],
+        active=schedule_args["active_steps"],
+        repeat=schedule_args["num_cycles"],
+    )
+
+    # profile_memory requires with_stack and record_shapes, hence we override these if profile_memory is True
+    # See torch.profiler.profiler._memory_profile
+    if profile_memory:
+        _warn(
+            "`profile_memory` requires `with_stack` and `record_shapes`, these will be enabled since `profile_memory` is True"
+        )
+    with_stack = with_stack or profile_memory
+    record_shapes = record_shapes or profile_memory
+    # experimental config is needed to export stacks: see https://github.com/pytorch/pytorch/issues/100253
+    experimental_config = _ExperimentalConfig(verbose=True) if with_stack else None
+
+    # Handle exporting of trace, memory timeline and other profiler artifacts
+    if output_dir is None:
+        _warn(
+            f" No output directory found in profiler config, defaulting to {DEFAULT_PROFILE_DIR}"
+        )
+        output_dir = DEFAULT_PROFILE_DIR
+
+    output_dir = Path(output_dir)
+    output_dir.mkdir(parents=True, exist_ok=True)
+    output_dir = str(output_dir)
+
+    # trace_handler manages the export of profiler artifacts
+    # this callback will be triggered after **each** profiling cycle
+    callback = partial(trace_handler, output_dir=output_dir)
+
+    profiler = torch.profiler.profile(
+        activities=activities,
+        profile_memory=profile_memory,
+        with_stack=with_stack,
+        record_shapes=record_shapes,
+        with_flops=with_flops,
+        schedule=schedule,
+        experimental_config=experimental_config,
+        on_trace_ready=callback,
+    )
+
+    profiler_cfg = DictConfig(
+        {
+            "enabled": enabled,
+            "output_dir": output_dir,
+            "cpu": cpu,
+            "cuda": cuda,
+            "profile_memory": profile_memory,
+            "with_stack": with_stack,
+            "record_shapes": record_shapes,
+            "with_flops": with_flops,
+            **schedule_args,
+        }
+    )
+
+    return (profiler, profiler_cfg)
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/_activation_offloading.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/_activation_offloading.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/activations.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/activations.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/_compile.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/_compile.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/_distributed.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/_distributed.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/lr_schedulers.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/lr_schedulers.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/memory.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/memory.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/metric_logging.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/metric_logging.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/pooling.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/pooling.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/precision.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/precision.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/_profiler.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/_profiler.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/quantization.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/quantization.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/training/__pycache__/seed.cpython-312.pyc and marc/third_party/torchtune/torchtune/training/__pycache__/seed.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/training/quantization.py marc/third_party/torchtune/torchtune/training/quantization.py
--- marc_original/third_party/torchtune/torchtune/training/quantization.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/quantization.py	2025-02-20 17:49:30.838026363 -0500
@@ -0,0 +1,169 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Callable, Optional
+
+from torchtune.utils._import_guard import _USE_NEW_TENSOR_CORE_TILED_LAYOUT_API
+
+if _USE_NEW_TENSOR_CORE_TILED_LAYOUT_API:
+    from torchao.dtypes import TensorCoreTiledLayout
+else:
+    from torchao.dtypes import TensorCoreTiledLayoutType as TensorCoreTiledLayout
+
+from torchao.quantization import (
+    int4_weight_only,
+    int8_dynamic_activation_int4_weight,
+    quantize_,
+)
+from torchao.quantization.prototype.qat import (
+    disable_4w_fake_quant,
+    disable_8da4w_fake_quant,
+    enable_4w_fake_quant,
+    enable_8da4w_fake_quant,
+    Int4WeightOnlyQATQuantizer,
+    Int8DynActInt4WeightQATQuantizer,
+)
+from torchao.quantization.prototype.qat._module_swap_api import (
+    disable_4w_fake_quant_module_swap,
+    disable_8da4w_fake_quant_module_swap,
+    enable_4w_fake_quant_module_swap,
+    enable_8da4w_fake_quant_module_swap,
+    Int4WeightOnlyQATQuantizerModuleSwap,
+    Int8DynActInt4WeightQATQuantizerModuleSwap,
+)
+
+
+__all__ = [
+    "get_quantizer_mode",
+    "Int4WeightOnlyQuantizer",
+    "Int4WeightOnlyQATQuantizer",
+    "Int4WeightOnlyQATQuantizerModuleSwap",
+    "Int8DynActInt4WeightQuantizer",
+    "Int8DynActInt4WeightQATQuantizer",
+    "Int8DynActInt4WeightQATQuantizerModuleSwap",
+]
+
+
+_quantizer_to_mode = {}
+_quantizer_mode_to_disable_fake_quant = {}
+_quantizer_mode_to_enable_fake_quant = {}
+
+
+# ========================================================
+# int8 dynamic activations + int4 weight tensor subclass |
+# ========================================================
+
+
+class Int8DynActInt4WeightQuantizer:
+    """
+    Quantizer for applying int8 per token dynamic activation + int4
+    per group weight quantization to linear layers in the model.
+    """
+
+    def __init__(self, groupsize: int = 256):
+        self.groupsize = groupsize
+
+    def quantize(self, model):
+        quantize_fn = int8_dynamic_activation_int4_weight(self.groupsize)
+        quantize_(model, quantize_fn)
+        return model
+
+
+_quantizer_to_mode[Int8DynActInt4WeightQuantizer] = "8da4w"
+_quantizer_to_mode[Int8DynActInt4WeightQATQuantizer] = "8da4w-qat"
+_quantizer_mode_to_disable_fake_quant["8da4w-qat"] = disable_8da4w_fake_quant
+_quantizer_mode_to_enable_fake_quant["8da4w-qat"] = enable_8da4w_fake_quant
+
+
+# ==================
+# int4 weight only |
+# ==================
+
+
+class Int4WeightOnlyQuantizer:
+    """
+    Quantizer for applying int4 per group weight only quantization
+    to linear layers in the model using the efficient tinygemm kernel.
+    """
+
+    def __init__(self, groupsize: int = 128, inner_k_tiles: int = 8):
+        self.groupsize = groupsize
+        self.inner_k_tiles = inner_k_tiles
+
+    def quantize(self, model):
+        layout_type = TensorCoreTiledLayout(self.inner_k_tiles)
+        quantize_fn = int4_weight_only(self.groupsize, layout_type)
+        quantize_(model, quantize_fn)
+        return model
+
+
+_quantizer_to_mode[Int4WeightOnlyQuantizer] = "4w"
+_quantizer_to_mode[Int4WeightOnlyQATQuantizer] = "4w-qat"
+_quantizer_mode_to_disable_fake_quant["4w-qat"] = disable_4w_fake_quant
+_quantizer_mode_to_enable_fake_quant["4w-qat"] = enable_4w_fake_quant
+
+
+# =============
+# module swap |
+# =============
+
+# Note: QAT tensor subclass implementation in torchao only works
+# with FSDP2 today. For other distribution strategies like DDP and
+# FSDP1, users will need to fall back to the old module swap flow.
+
+# int4 weight-only
+_quantizer_to_mode[Int4WeightOnlyQATQuantizerModuleSwap] = "4w-qat-module-swap"
+_quantizer_mode_to_disable_fake_quant[
+    "4w-qat-module-swap"
+] = disable_4w_fake_quant_module_swap
+_quantizer_mode_to_enable_fake_quant[
+    "4w-qat-module-swap"
+] = enable_4w_fake_quant_module_swap
+
+# int8 dynamic activations + int4 weight
+_quantizer_to_mode[Int8DynActInt4WeightQATQuantizerModuleSwap] = "8da4w-qat-module-swap"
+_quantizer_mode_to_disable_fake_quant[
+    "8da4w-qat-module-swap"
+] = disable_8da4w_fake_quant_module_swap
+_quantizer_mode_to_enable_fake_quant[
+    "8da4w-qat-module-swap"
+] = enable_8da4w_fake_quant_module_swap
+
+
+def get_quantizer_mode(quantizer: Optional[Callable]) -> Optional[str]:
+    """Given a quantizer object, returns a string that specifies the type of quantization.
+
+    For example, in the case of int4 weight only quantization, we'll return "4w".
+    If the quantizer is not recognized as a known quantizer, we'll return None.
+
+    Currently supported:
+
+    - :class:`~torchao.quantization.quant_api.Int8DynActInt4WeightQuantizer`: "8da4w" (requires ``torch>=2.3.0``)
+    - :class:`~torchao.quantization.prototype.qat.Int8DynActInt4WeightQATQuantizer`: "8da4w-qat" (requires ``torch>=2.4.0``)
+
+    Args:
+        quantizer (Optional[Callable]): A callable object that implements the `quantize` method.
+
+    Returns:
+        Optional[str]: The quantization mode.
+    """
+    return _quantizer_to_mode.get(type(quantizer), None)
+
+
+def _get_disable_fake_quant(quantizer_mode: str) -> Callable:
+    """Given a quantizer mode, return the corresponding function for disabling fake
+    quantize in a model prepared by the quantizer.
+    If the quantizer is not recognized as a known QAT quantizer, return None.
+    """
+    return _quantizer_mode_to_disable_fake_quant.get(quantizer_mode, None)
+
+
+def _get_enable_fake_quant(quantizer_mode: str) -> Callable:
+    """Given a quantizer mode, return the corresponding function for enabling fake
+    quantize in a model prepared by the quantizer.
+    If the quantizer is not recognized as a known QAT quantizer, return None.
+    """
+    return _quantizer_mode_to_enable_fake_quant.get(quantizer_mode, None)
diff -ruN marc_original/third_party/torchtune/torchtune/training/seed.py marc/third_party/torchtune/torchtune/training/seed.py
--- marc_original/third_party/torchtune/torchtune/training/seed.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/training/seed.py	2025-02-20 17:49:30.842026370 -0500
@@ -0,0 +1,83 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+import os
+
+import random
+from typing import Optional, Union
+
+import numpy as np
+import torch
+
+from torchtune.training._distributed import _broadcast_tensor, get_world_size_and_rank
+from torchtune.utils import get_logger
+
+_log: logging.Logger = get_logger()
+
+
+def set_seed(
+    seed: Optional[int] = None, debug_mode: Optional[Union[str, int]] = None
+) -> int:
+    """Function that sets seed for pseudo-random number generators across commonly used libraries.
+
+    This seeds PyTorch, NumPy, and the python.random module. For distributed jobs, each local process
+    sets its own seed, computed seed + rank.
+    For more details, see https://pytorch.org/docs/stable/notes/randomness.html.
+
+    Args:
+        seed (Optional[int]): the integer value seed. If `None`, a random seed will be generated and set.
+        debug_mode (Optional[Union[str, int]]): Controls debug_mode settings for deterministic operations within PyTorch.
+
+            * If `None`, don't set any PyTorch global values.
+            * If "default" or 0, don't error or warn on nondeterministic operations and additionally enable PyTorch CuDNN benchmark.
+            * If "warn" or 1, warn on nondeterministic operations and disable PyTorch CuDNN benchmark.
+            * If "error" or 2, error on nondeterministic operations and disable PyTorch CuDNN benchmark.
+            * For more details, see :func:`torch.set_deterministic_debug_mode` and
+              https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms.
+
+    Returns:
+        int: the current seed
+
+    Raises:
+        ValueError: If the input seed value is outside the required range.
+    """
+    world_size, rank = get_world_size_and_rank()
+    max_val = np.iinfo(np.uint32).max - world_size + 1
+    min_val = np.iinfo(np.uint32).min
+    if seed is None:
+        rand_seed = torch.randint(min_val, max_val, (1,))
+        seed = _broadcast_tensor(rand_seed, 0).item()  # sync seed across ranks
+    if seed < min_val or seed > max_val:
+        raise ValueError(
+            f"Invalid seed value provided: {seed}. Value must be in the range [{min_val}, {max_val}]"
+        )
+    local_seed = seed + rank
+    if rank == 0:
+        _log.debug(
+            f"Setting manual seed to local seed {local_seed}. Local seed is seed + rank = {seed} + {rank}"
+        )
+
+    torch.manual_seed(local_seed)
+    np.random.seed(local_seed)
+    random.seed(local_seed)
+
+    if debug_mode is not None:
+        _log.debug(f"Setting deterministic debug mode to {debug_mode}")
+        torch.set_deterministic_debug_mode(debug_mode)
+        deterministic_debug_mode = torch.get_deterministic_debug_mode()
+        if deterministic_debug_mode == 0:
+            _log.debug("Disabling cuDNN deterministic mode")
+            torch.backends.cudnn.deterministic = False
+            torch.backends.cudnn.benchmark = True
+        else:
+            _log.debug("Enabling cuDNN deterministic mode")
+            torch.backends.cudnn.deterministic = True
+            torch.backends.cudnn.benchmark = False
+            # reference: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
+            os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
+
+    return seed
diff -ruN marc_original/third_party/torchtune/torchtune/utils/_device.py marc/third_party/torchtune/torchtune/utils/_device.py
--- marc_original/third_party/torchtune/torchtune/utils/_device.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/utils/_device.py	2025-02-20 17:49:30.850026382 -0500
@@ -0,0 +1,158 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+from typing import Optional
+
+import torch
+
+from torchtune.utils._import_guard import _SUPPORTS_FLEX_ATTENTION
+
+if _SUPPORTS_FLEX_ATTENTION:
+    from torch.nn.attention.flex_attention import BlockMask
+else:
+    BlockMask = torch.Tensor
+
+
+def _get_local_rank() -> Optional[int]:
+    """Function that gets the local rank from the environment.
+
+    Returns:
+        local_rank int or None if not set.
+    """
+    local_rank = os.environ.get("LOCAL_RANK")
+    if local_rank is not None:
+        local_rank = int(local_rank)
+    return local_rank
+
+
+def _setup_cuda_device(device: torch.device) -> torch.device:
+    """Function that sets the CUDA device and infers the cuda
+    index if not set.
+
+    Args:
+        device (torch.device): The device to set.
+
+    Raises:
+        RuntimeError: If device index is not available.
+
+    Returns:
+        device
+    """
+    local_rank = _get_local_rank() or 0
+    if device.index is None:
+        device = torch.device(type="cuda", index=local_rank)
+
+    # Ensure index is available before setting device
+    if device.index >= torch.cuda.device_count():
+        raise RuntimeError(
+            "The local rank is larger than the number of available GPUs."
+        )
+
+    torch.cuda.set_device(device)
+    return device
+
+
+def _get_device_type_from_env() -> str:
+    """Function that gets the torch.device based on the current machine.
+
+    This currently only supports CPU, CUDA.
+
+    Returns:
+        device
+    """
+    if torch.cuda.is_available():
+        device = "cuda"
+    else:
+        device = "cpu"
+    return device
+
+
+def _validate_device_from_env(device: torch.device) -> None:
+    """Function that validates the device is correct given the current machine.
+    This will raise an error if the device is not available or doesn't match the
+    assigned process device on distributed runs.
+
+    Args:
+        device (torch.device): The device to validate.
+
+    Raises:
+        RuntimeError: If the device is not available or doesn't match the assigned process device.
+
+    Returns:
+        device
+    """
+    local_rank = _get_local_rank()
+
+    # Check if the device index is correct
+    if device.type == "cuda" and local_rank is not None:
+        # Ensure device index matches assigned index when distributed training
+        if device.index != local_rank:
+            raise RuntimeError(
+                f"You can't specify a device index when using distributed training. \
+                Device specified is {device} but was assigned cuda:{local_rank}"
+            )
+
+    # Check if the device is available on this machine
+    try:
+        torch.empty(0, device=device)
+    except RuntimeError as e:
+        raise RuntimeError(
+            f"The device {device} is not available on this machine."
+        ) from e
+
+
+def get_device(device: Optional[str] = None) -> torch.device:
+    """Function that takes an optional device string, verifies it's correct and available given the machine and
+    distributed settings, and returns a :func:`~torch.device`. If device string is not provided, this function will
+    infer the device based on the environment.
+
+    If CUDA is available and being used, this function also sets the CUDA device.
+
+    Args:
+        device (Optional[str]): The name of the device to use, e.g. "cuda" or "cpu".
+
+    Example:
+        >>> device = get_device("cuda")
+        >>> device
+        device(type='cuda', index=0)
+
+    Returns:
+        torch.device: Device
+    """
+    if device is None:
+        device = _get_device_type_from_env()
+    device = torch.device(device)
+    if device.type == "cuda":
+        device = _setup_cuda_device(device)
+    _validate_device_from_env(device)
+    return device
+
+
+def batch_to_device(batch: dict, device: torch.device) -> None:
+    """Function that takes a dictionary (or nested dictionary) of tensors and sets them
+    all to the same device. This utility is intended to be used for batches of data to be
+    moved to device, the update is inplace.
+
+    Args:
+        batch (dict): dict of Tensors or more nested dicts of tensors.
+        device (torch.device): torch device to move the tensor's too
+
+    Raises:
+        AttributeError: if batch dict contains anything other than tensors
+    """
+    for k, v in batch.items():
+        if isinstance(v, dict):
+            batch_to_device(v, device)
+        elif isinstance(v, torch.Tensor):
+            batch[k] = v.to(device)
+        elif _SUPPORTS_FLEX_ATTENTION and isinstance(v, BlockMask):
+            batch[k] = v.to(device)
+        else:
+            raise ValueError(
+                f"""To use batch_to_device, all elements in the batch must be a dict or Tensor.
+Got key "{k}" with value of type {type(v)}"""
+            )
diff -ruN marc_original/third_party/torchtune/torchtune/utils/_import_guard.py marc/third_party/torchtune/torchtune/utils/_import_guard.py
--- marc_original/third_party/torchtune/torchtune/utils/_import_guard.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/utils/_import_guard.py	2025-02-20 17:49:30.854026389 -0500
@@ -0,0 +1,23 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+import torchao
+from torchtune.utils._version import _is_fbcode, _nightly_version_ge, torch_version_ge
+
+# We can only use flex attention / BlockMask if torch version >= 2.5.0 and GPU is Turing / SM75 and above
+_SUPPORTS_FLEX_ATTENTION = (
+    torch_version_ge("2.5.0")
+    and torch.cuda.is_available()
+    and torch.cuda.get_device_capability() >= (7, 5)
+)
+
+torchao_version = torchao.__version__
+
+_USE_NEW_TENSOR_CORE_TILED_LAYOUT_API = not _is_fbcode() and (
+    ("dev" not in torchao_version and torchao_version >= "0.6.0")
+    or ("dev" in torchao_version and _nightly_version_ge(torchao_version, "2024-10-10"))
+)
diff -ruN marc_original/third_party/torchtune/torchtune/utils/__init__.py marc/third_party/torchtune/torchtune/utils/__init__.py
--- marc_original/third_party/torchtune/torchtune/utils/__init__.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/utils/__init__.py	2025-02-20 17:49:30.846026376 -0500
@@ -0,0 +1,17 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from ._device import batch_to_device, get_device
+from ._logging import get_logger
+
+from ._version import torch_version_ge
+
+__all__ = [
+    "batch_to_device",
+    "get_device",
+    "get_logger",
+    "torch_version_ge",
+]
diff -ruN marc_original/third_party/torchtune/torchtune/utils/_logging.py marc/third_party/torchtune/torchtune/utils/_logging.py
--- marc_original/third_party/torchtune/torchtune/utils/_logging.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/utils/_logging.py	2025-02-20 17:49:30.858026396 -0500
@@ -0,0 +1,101 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+import warnings
+from functools import lru_cache, wraps
+from typing import Callable, Optional, TypeVar
+
+from torch import distributed as dist
+
+T = TypeVar("T", bound=type)
+
+
+def get_logger(level: Optional[str] = None) -> logging.Logger:
+    """
+    Get a logger with a stream handler.
+
+    Args:
+        level (Optional[str]): The logging level. See https://docs.python.org/3/library/logging.html#levels for list of levels.
+
+    Example:
+        >>> logger = get_logger("INFO")
+        >>> logger.info("Hello world!")
+        INFO:torchtune.utils._logging:Hello world!
+
+    Returns:
+        logging.Logger: The logger.
+    """
+    logger = logging.getLogger(__name__)
+    if not logger.hasHandlers():
+        logger.addHandler(logging.StreamHandler())
+    if level is not None:
+        level = getattr(logging, level.upper())
+        logger.setLevel(level)
+    return logger
+
+
+@lru_cache(None)
+def log_once(logger: logging.Logger, msg: str, level: int = logging.INFO) -> None:
+    """
+    Logs a message only once. LRU cache is used to ensure a specific message is
+    logged only once, similar to how :func:`~warnings.warn` works when the ``once``
+    rule is set via command-line or environment variable.
+
+    Args:
+        logger (logging.Logger): The logger.
+        msg (str): The warning message.
+        level (int): The logging level. See https://docs.python.org/3/library/logging.html#levels for values.
+            Defaults to ``logging.INFO``.
+    """
+    log_rank_zero(logger=logger, msg=msg, level=level)
+
+
+def deprecated(msg: str = "") -> Callable[[T], T]:
+    """
+    Decorator to mark an object as deprecated and print additional message.
+
+    Args:
+        msg (str): additional information to print after warning.
+
+    Returns:
+        Callable[[T], T]: the decorated object.
+    """
+
+    @lru_cache(maxsize=1)
+    def warn(obj):
+        warnings.warn(
+            f"{obj.__name__} is deprecated and will be removed in future versions. "
+            + msg,
+            category=FutureWarning,
+            stacklevel=3,
+        )
+
+    def decorator(obj):
+        @wraps(obj)
+        def wrapper(*args, **kwargs):
+            warn(obj)
+            return obj(*args, **kwargs)
+
+        return wrapper
+
+    return decorator
+
+
+def log_rank_zero(logger: logging.Logger, msg: str, level: int = logging.INFO) -> None:
+    """
+    Logs a message only on rank zero.
+
+    Args:
+        logger (logging.Logger): The logger.
+        msg (str): The warning message.
+        level (int): The logging level. See https://docs.python.org/3/library/logging.html#levels for values.
+            Defaults to ``logging.INFO``.
+    """
+    rank = dist.get_rank() if dist.is_available() and dist.is_initialized() else 0
+    if rank != 0:
+        return
+    logger.log(level, msg)
Binary files marc_original/third_party/torchtune/torchtune/utils/__pycache__/_device.cpython-312.pyc and marc/third_party/torchtune/torchtune/utils/__pycache__/_device.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/utils/__pycache__/_import_guard.cpython-312.pyc and marc/third_party/torchtune/torchtune/utils/__pycache__/_import_guard.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/utils/__pycache__/__init__.cpython-312.pyc and marc/third_party/torchtune/torchtune/utils/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/utils/__pycache__/_logging.cpython-312.pyc and marc/third_party/torchtune/torchtune/utils/__pycache__/_logging.cpython-312.pyc differ
Binary files marc_original/third_party/torchtune/torchtune/utils/__pycache__/_version.cpython-312.pyc and marc/third_party/torchtune/torchtune/utils/__pycache__/_version.cpython-312.pyc differ
diff -ruN marc_original/third_party/torchtune/torchtune/utils/_version.py marc/third_party/torchtune/torchtune/utils/_version.py
--- marc_original/third_party/torchtune/torchtune/utils/_version.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune/utils/_version.py	2025-02-20 17:49:30.858026396 -0500
@@ -0,0 +1,46 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the BSD-style license found in the
+# LICENSE file in the root directory of this source tree.
+
+from datetime import datetime
+
+import torch
+
+
+def torch_version_ge(version: str) -> bool:
+    """
+    Check if torch version is greater than or equal to the given version.
+
+    Args:
+        version (str): The torch version to compare against
+
+    Returns:
+        bool: True if torch version is greater than or equal to the given version.
+
+    Example:
+        >>> print(torch.__version__)
+        2.4.0
+        >>> torch_version_ge("2.0")
+        True
+    """
+    return version in torch.__version__ or torch.__version__ >= version
+
+
+def _is_fbcode():
+    return not hasattr(torch.version, "git_version")
+
+
+def _nightly_version_ge(ao_version_str: str, date: str) -> bool:
+    """
+    Compare a torchao nightly version to a date of the form
+    %Y-%m-%d.
+
+    Returns True if the nightly version is greater than or equal to
+        the date, False otherwise
+    """
+    ao_datetime = datetime.strptime(
+        ao_version_str.split("+")[0].split("dev")[1], "%Y%m%d"
+    )
+    return ao_datetime >= datetime.strptime(date, "%Y-%m-%d")
diff -ruN marc_original/third_party/torchtune/torchtune.egg-info/dependency_links.txt marc/third_party/torchtune/torchtune.egg-info/dependency_links.txt
--- marc_original/third_party/torchtune/torchtune.egg-info/dependency_links.txt	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune.egg-info/dependency_links.txt	2025-02-26 02:22:48.057477654 -0500
@@ -0,0 +1 @@
+
diff -ruN marc_original/third_party/torchtune/torchtune.egg-info/entry_points.txt marc/third_party/torchtune/torchtune.egg-info/entry_points.txt
--- marc_original/third_party/torchtune/torchtune.egg-info/entry_points.txt	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune.egg-info/entry_points.txt	2025-02-26 02:22:48.057477654 -0500
@@ -0,0 +1,2 @@
+[console_scripts]
+tune = torchtune._cli.tune:main
diff -ruN marc_original/third_party/torchtune/torchtune.egg-info/PKG-INFO marc/third_party/torchtune/torchtune.egg-info/PKG-INFO
--- marc_original/third_party/torchtune/torchtune.egg-info/PKG-INFO	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune.egg-info/PKG-INFO	2025-02-26 02:22:48.041477645 -0500
@@ -0,0 +1,365 @@
+Metadata-Version: 2.2
+Name: torchtune
+Version: 0.0.0
+Summary: A native-PyTorch library for LLM fine-tuning
+Author-email: PyTorch Team <packages@pytorch.org>
+License: BSD 3-Clause License
+        
+        Copyright 2024 Meta
+        
+        Redistribution and use in source and binary forms, with or without modification,
+        are permitted provided that the following conditions are met:
+        
+        1. Redistributions of source code must retain the above copyright notice,this list
+        of conditions and the following disclaimer.
+        
+        2. Redistributions in binary form must reproduce the above copyright notice, this
+        list of conditions and the following disclaimer in the documentation
+        and/or other materials provided with the distribution.
+        
+        3. Neither the name of the copyright holder nor the names of its contributors may
+        be used to endorse or promote products derived from this software without specific
+        prior written permission.
+        
+        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY
+        EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+        OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT
+        SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+        INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+        TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+        BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+        CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+        ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+        DAMAGE.
+        
+Project-URL: GitHub, https://github.com/pytorch/torchtune
+Project-URL: Documentation, https://pytorch.org/torchtune/main/index.html
+Project-URL: Issues, https://github.com/pytorch/torchtune/issues
+Keywords: pytorch,finetuning,llm
+Requires-Python: >=3.9
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: datasets
+Requires-Dist: huggingface_hub
+Requires-Dist: safetensors
+Requires-Dist: sentencepiece
+Requires-Dist: tiktoken
+Requires-Dist: blobfile>=2
+Requires-Dist: numpy
+Requires-Dist: tqdm
+Requires-Dist: omegaconf
+Requires-Dist: psutil
+Requires-Dist: Pillow>=9.4.0
+Provides-Extra: dev
+Requires-Dist: bitsandbytes>=0.43.0; extra == "dev"
+Requires-Dist: comet_ml>=3.44.2; extra == "dev"
+Requires-Dist: pre-commit; extra == "dev"
+Requires-Dist: pytest==7.4.0; extra == "dev"
+Requires-Dist: pytest-cov; extra == "dev"
+Requires-Dist: pytest-mock; extra == "dev"
+Requires-Dist: pytest-integration; extra == "dev"
+Requires-Dist: tensorboard; extra == "dev"
+Requires-Dist: urllib3<2.0.0; extra == "dev"
+Requires-Dist: wandb; extra == "dev"
+Requires-Dist: expecttest; extra == "dev"
+
+
+
+
+# torchtune
+
+[![Unit Test](https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml/badge.svg?branch=main)](https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml)
+![Recipe Integration Test](https://github.com/pytorch/torchtune/actions/workflows/recipe_test.yaml/badge.svg)
+[![](https://dcbadge.vercel.app/api/server/4Xsdn8Rr9Q?style=flat)](https://discord.gg/4Xsdn8Rr9Q)
+
+[**Introduction**](#introduction) | [**Installation**](#installation) | [**Get Started**](#get-started) |  [**Documentation**](https://pytorch.org/torchtune/main/index.html) | [**Community**](#community) | [**License**](#license) | [**Citing torchtune**](#citing-torchtune)
+
+> [!IMPORTANT]
+> Update September 25, 2024: torchtune has support for **Llama 3.2 11B Vision**, **Llama 3.2 3B**, and **Llama 3.2 1B** models! Try them out by following our installation instructions [here](#Installation), then run any of the text configs [here](recipes/configs/llama3_2) or vision configs [here](recipes/configs/llama3_2_vision).
+
+
+&nbsp;
+
+## Introduction
+
+torchtune is a PyTorch library for easily authoring, finetuning and experimenting with LLMs.
+
+torchtune provides:
+
+- PyTorch implementations of popular LLMs from Llama, Gemma, Mistral, Phi, and Qwen model families
+- Hackable training recipes for full finetuning, LoRA, QLoRA, DPO, PPO, QAT, knowledge distillation, and more
+- Out-of-the-box memory efficiency, performance improvements, and scaling with the latest PyTorch APIs
+- YAML configs for easily configuring training, evaluation, quantization or inference recipes
+- Built-in support for many popular dataset formats and prompt templates
+
+
+&nbsp;
+
+### Models
+
+torchtune currently supports the following models.
+
+| Model                                         | Sizes     |
+|-----------------------------------------------|-----------|
+| [Llama3.2-Vision](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2#-llama-3.2-vision-models-(11b/90b)-)    | 11B [[models](torchtune/models/llama3_2_vision/_model_builders.py), [configs](recipes/configs/llama3_2_vision/)]        |
+| [Llama3.2](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2)    | 1B, 3B [[models](torchtune/models/llama3_2/_model_builders.py), [configs](recipes/configs/llama3_2/)]        |
+| [Llama3.1](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1)    | 8B, 70B, 405B [[models](torchtune/models/llama3_1/_model_builders.py), [configs](recipes/configs/llama3_1/)]        |
+| [Llama3](https://llama.meta.com/llama3)    | 8B, 70B [[models](torchtune/models/llama3/_model_builders.py), [configs](recipes/configs/llama3/)]        |
+| [Llama2](https://llama.meta.com/llama2/)   | 7B, 13B, 70B [[models](torchtune/models/llama2/_model_builders.py), [configs](recipes/configs/llama2/)]        |
+| [Code-Llama2](https://ai.meta.com/blog/code-llama-large-language-model-coding/)   | 7B, 13B, 70B [[models](torchtune/models/code_llama2/_model_builders.py), [configs](recipes/configs/code_llama2/)] |
+| [Mistral](https://huggingface.co/mistralai)   | 7B [[models](torchtune/models/mistral/_model_builders.py), [configs](recipes/configs/mistral/)] |
+| [Gemma](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b)   | 2B, 7B [[models](torchtune/models/gemma/_model_builders.py), [configs](recipes/configs/gemma/)] |
+| [Microsoft Phi3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3) | Mini [[models](torchtune/models/phi3/), [configs](recipes/configs/phi3/)]
+| [Qwen2](https://qwenlm.github.io/blog/qwen2/) | 0.5B, 1.5B, 7B [[models](torchtune/models/qwen2/), [configs](recipes/configs/qwen2/)]
+
+We're always adding new models, but feel free to [file an issue](https://github.com/pytorch/torchtune/issues/new) if there's a new one you would like to see in torchtune.
+
+&nbsp;
+
+### Finetuning recipes
+
+torchtune provides the following finetuning recipes for training on one or more devices.
+
+
+| Finetuning Method                          | Devices | Recipe  | Example Config(s) |
+|:-:|:-:|:-:|:-:|
+| Full Finetuning  | 1-8 | [full_finetune_single_device](recipes/full_finetune_single_device.py) <br> [full_finetune_distributed](recipes/full_finetune_distributed.py)| [Llama3.1 8B single-device](recipes/configs/llama3_1/8B_full_single_device.yaml) <br> [Llama 3.1 70B distributed](recipes/configs/llama3_1/70B_full.yaml)
+| LoRA Finetuning | 1-8  | [lora_finetune_single_device](recipes/lora_finetune_single_device.py) <br> [lora_finetune_distributed](recipes/lora_finetune_distributed.py) | [Qwen2 0.5B single-device](recipes/configs/qwen2/0.5B_lora_single_device.yaml) <br> [Gemma 7B distributed](recipes/configs/gemma/7B_lora.yaml)
+| QLoRA Finetuning | 1-8 | [lora_finetune_single_device](recipes/lora_finetune_single_device.py) <br> [lora_finetune_distributed](recipes/lora_finetune_distributed.py)| [Phi3 Mini single-device](recipes/configs/phi3/mini_qlora_single_device.yaml) <br> [Llama 3.1 405B distributed](recipes/configs/llama3_1/405B_qlora.yaml)
+| DoRA/QDoRA Finetuning | 1-8 | [lora_finetune_single_device](recipes/lora_finetune_single_device.py) <br> [lora_finetune_distributed](recipes/lora_finetune_distributed.py)| [Llama3 8B QDoRA single-device](recipes/configs/llama3/8B_qdora_single_device.yaml) <br> [Llama3 8B DoRA distributed](recipes/configs/llama3/8B_dora.yaml)
+| Quantization-Aware Training | 4-8 | [qat_distributed](recipes/qat_distributed.py)| [Llama3 8B QAT](recipes/configs/llama3/8B_qat_full.yaml)
+| Direct Preference Optimization |1-8 | [lora_dpo_single_device](recipes/lora_dpo_single_device.py) <br> [lora_dpo_distributed](recipes/lora_dpo_distributed.py) | [Llama2 7B single-device](recipes/configs/llama2/7B_lora_dpo_single_device.yaml) <br> [Llama2 7B distributed](recipes/configs/llama2/7B_lora_dpo.yaml)
+| Proximal Policy Optimization | 1 |  [ppo_full_finetune_single_device](recipes/ppo_full_finetune_single_device.py) | [Mistral 7B](recipes/configs/mistral/7B_full_ppo_low_memory.yaml)
+| Knowledge Distillation | 1 | [knowledge_distillation_single_device](recipes/knowledge_distillation_single_device.py) | [Qwen2 1.5B -> 0.5B](recipes/configs/qwen2/knowledge_distillation_single_device.yaml)
+
+
+The above configs are just examples to get you started. If you see a model above not listed here, we likely still support it. If you're unsure whether something is supported, please open an issue on the repo.
+
+&nbsp;
+
+### Memory and training speed
+
+Below is an example of the memory requirements and training speed for different Llama 3.1 models.
+
+> [!NOTE]
+> For ease of comparison, all the below numbers are provided for batch size 2 (without gradient accumulation), a dataset packed to sequence length 2048, and torch compile enabled.
+
+If you are interested in running on different hardware or with different models, check out our documentation on memory optimizations [here](https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html) to find the right setup for you.
+
+| Model | Finetuning Method | Runnable On | Peak Memory per GPU | Tokens/sec * |
+|:-:|:-:|:-:|:-:|:-:|
+| Llama 3.1 8B | Full finetune | 1x 4090 | 18.9 GiB | 1650 |
+| Llama 3.1 8B | Full finetune | 1x A6000 | 37.4 GiB |  2579|
+| Llama 3.1 8B | LoRA | 1x 4090 |  16.2 GiB | 3083 |
+| Llama 3.1 8B | LoRA | 1x A6000 | 30.3 GiB  | 4699 |
+| Llama 3.1 8B | QLoRA | 1x 4090 | 7.4 GiB | 2413  |
+| Llama 3.1 70B | Full finetune | 8x A100  | 13.9 GiB ** | 1568  |
+| Llama 3.1 70B | LoRA | 8x A100 | 27.6 GiB  | 3497  |
+| Llama 3.1 405B | QLoRA | 8x A100 | 44.8 GB  | 653  |
+
+*= Measured over one full training epoch
+
+**= Uses CPU offload with fused optimizer
+
+&nbsp;
+
+## Installation
+
+torchtune is tested with the latest stable PyTorch release as well as the preview nightly version. torchtune leverages
+torchvision for finetuning multimodal LLMs and torchao for the latest in quantization techniques; you should install these as well.
+
+&nbsp;
+
+### Install stable release
+
+```bash
+# Install stable PyTorch, torchvision, torchao stable releases
+pip install torch torchvision torchao
+pip install torchtune
+```
+
+&nbsp;
+
+### Install nightly release
+
+```bash
+# Install PyTorch, torchvision, torchao nightlies
+pip install --pre --upgrade torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/cu121 # full options are cpu/cu118/cu121/cu124
+pip install --pre --upgrade torchtune --extra-index-url https://download.pytorch.org/whl/nightly/cpu
+```
+
+You can also check out our [install documentation](https://pytorch.org/torchtune/main/install.html) for more information, including installing torchtune from source.
+
+&nbsp;
+
+To confirm that the package is installed correctly, you can run the following command:
+
+```bash
+tune --help
+```
+
+And should see the following output:
+
+```bash
+usage: tune [-h] {ls,cp,download,run,validate} ...
+
+Welcome to the torchtune CLI!
+
+options:
+  -h, --help            show this help message and exit
+
+...
+```
+
+&nbsp;
+
+## Get Started
+
+To get started with torchtune, see our [First Finetune Tutorial](https://pytorch.org/torchtune/main/tutorials/first_finetune_tutorial.html). Our [End-to-End Workflow Tutorial](https://pytorch.org/torchtune/main/tutorials/e2e_flow.html) will show you how to evaluate, quantize and run inference with a Llama model. The rest of this section will provide a quick overview of these steps with Llama3.1.
+
+
+### Downloading a model
+
+Follow the instructions on the official [`meta-llama`](https://huggingface.co/meta-llama) repository to ensure you have access to the official Llama model weights. Once you have confirmed access, you can run the following command to download the weights to your local machine. This will also download the tokenizer model and a responsible use guide.
+
+To download Llama3.1, you can run:
+
+```bash
+tune download meta-llama/Meta-Llama-3.1-8B-Instruct \
+--output-dir /tmp/Meta-Llama-3.1-8B-Instruct \
+--hf-token <HF_TOKEN> \
+```
+
+> [!Tip]
+> Set your environment variable `HF_TOKEN` or pass in `--hf-token` to the command in order to validate your access. You can find your token at https://huggingface.co/settings/tokens
+
+&nbsp;
+
+### Running finetuning recipes
+
+You can finetune Llama3.1 8B with LoRA on a single GPU using the following command:
+
+```bash
+tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device
+```
+
+For distributed training, tune CLI integrates with [torchrun](https://pytorch.org/docs/stable/elastic/run.html).
+To run a full finetune of Llama3.1 8B on two GPUs:
+
+```bash
+tune run --nproc_per_node 2 full_finetune_distributed --config llama3_1/8B_full
+```
+
+> [!Tip]
+> Make sure to place any torchrun commands **before** the recipe specification. Any CLI args after this will override the config and not impact distributed training.
+
+&nbsp;
+
+### Modify Configs
+
+There are two ways in which you can modify configs:
+
+**Config Overrides**
+
+You can directly overwrite config fields from the command line:
+
+```bash
+tune run lora_finetune_single_device \
+--config llama2/7B_lora_single_device \
+batch_size=8 \
+enable_activation_checkpointing=True \
+max_steps_per_epoch=128
+```
+
+**Update a Local Copy**
+
+You can also copy the config to your local directory and modify the contents directly:
+
+```bash
+tune cp llama3_1/8B_full ./my_custom_config.yaml
+Copied to ./my_custom_config.yaml
+```
+
+Then, you can run your custom recipe by directing the `tune run` command to your local files:
+
+```bash
+tune run full_finetune_distributed --config ./my_custom_config.yaml
+```
+
+&nbsp;
+
+Check out `tune --help` for all possible CLI commands and options. For more information on using and updating configs, take a look at our [config deep-dive](https://pytorch.org/torchtune/main/deep_dives/configs.html).
+
+&nbsp;
+
+### Custom Datasets
+
+torchtune supports finetuning on a variety of different datasets, including [instruct-style](https://pytorch.org/torchtune/main/basics/instruct_datasets.html), [chat-style](https://pytorch.org/torchtune/main/basics/chat_datasets.html), [preference datasets](https://pytorch.org/torchtune/main/basics/preference_datasets.html), and more. If you want to learn more about how to apply these components to finetune on your own custom dataset, please check out the provided links along with our [API docs](https://pytorch.org/torchtune/main/api_ref_datasets.html).
+
+&nbsp;
+
+## Community
+
+torchtune focuses on integrating with popular tools and libraries from the ecosystem. These are just a few examples, with more under development:
+
+- [Hugging Face Hub](https://huggingface.co/docs/hub/en/index) for [accessing model weights](torchtune/_cli/download.py)
+- [EleutherAI's LM Eval Harness](https://github.com/EleutherAI/lm-evaluation-harness) for [evaluating](recipes/eleuther_eval.py) trained models
+- [Hugging Face Datasets](https://huggingface.co/docs/datasets/en/index) for [access](torchtune/datasets/_instruct.py) to training and evaluation datasets
+- [PyTorch FSDP2](https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md) for distributed training
+- [torchao](https://github.com/pytorch-labs/ao) for lower precision dtypes and [post-training quantization](recipes/quantize.py) techniques
+- [Weights & Biases](https://wandb.ai/site) for [logging](https://pytorch.org/torchtune/main/deep_dives/wandb_logging.html) metrics and checkpoints, and tracking training progress
+- [Comet](https://www.comet.com/site/) as another option for [logging](https://pytorch.org/torchtune/main/deep_dives/comet_logging.html)
+- [ExecuTorch](https://pytorch.org/executorch-overview) for [on-device inference](https://github.com/pytorch/executorch/tree/main/examples/models/llama2#optional-finetuning) using finetuned models
+- [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index) for low memory optimizers for our [single-device recipes](recipes/configs/llama2/7B_full_low_memory.yaml)
+- [PEFT](https://github.com/huggingface/peft) for continued finetuning or inference with torchtune models in the Hugging Face ecosystem
+
+&nbsp;
+
+### Community Contributions
+
+We really value our community and the contributions made by our wonderful users. We'll use this section to call out some of these contributions. If you'd like to help out as well, please see the [CONTRIBUTING](CONTRIBUTING.md) guide.
+
+- [@SalmanMohammadi](https://github.com/salmanmohammadi) for adding a comprehensive end-to-end recipe for [Reinforcement Learning from Human Feedback (RLHF)](recipes/ppo_full_finetune_single_device.py) finetuning with PPO to torchtune
+- [@fyabc](https://github.com/fyabc) for adding Qwen2 models, tokenizer, and recipe integration to torchtune
+- [@solitude-alive](https://github.com/solitude-alive) for adding the [Gemma 2B model](torchtune/models/gemma/) to torchtune, including recipe changes, numeric validations of the models and recipe correctness
+- [@yechenzhi](https://github.com/yechenzhi) for adding [Direct Preference Optimization (DPO)](recipes/lora_dpo_single_device.py) to torchtune, including the recipe and config along with correctness checks
+
+
+&nbsp;
+
+## Acknowledgements
+
+The Llama2 code in this repository is inspired by the original [Llama2 code](https://github.com/meta-llama/llama/blob/main/llama/model.py).
+
+We want to give a huge shout-out to EleutherAI, Hugging Face and Weights & Biases for being wonderful collaborators and for working with us on some of these integrations within torchtune.
+
+We also want to acknowledge some awesome libraries and tools from the ecosystem:
+- [gpt-fast](https://github.com/pytorch-labs/gpt-fast) for performant LLM inference techniques which we've adopted out-of-the-box
+- [llama recipes](https://github.com/meta-llama/llama-recipes) for spring-boarding the llama2 community
+- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) for bringing several memory and performance based techniques to the PyTorch ecosystem
+- [@winglian](https://github.com/winglian/) and [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) for early feedback and brainstorming on torchtune's design and feature set.
+- [lit-gpt](https://github.com/Lightning-AI/litgpt) for pushing the LLM finetuning community forward.
+- [HF TRL](https://github.com/huggingface/trl) for making reward modeling more accessible to the PyTorch community.
+
+&nbsp;
+
+
+## License
+
+torchtune is released under the [BSD 3 license](./LICENSE). However you may have other legal obligations that govern your use of other content, such as the terms of service for third-party models.
+
+
+## Citing torchtune
+
+If you find the torchtune library useful, please cite it in your work as below.
+
+```bibtex
+@software{torchtune,
+  title = {torchtune: PyTorch's finetuning library},
+  author = {torchtune maintainers and contributors},
+  url = {https//github.com/pytorch/torchtune},
+  license = {BSD-3-Clause},
+  month = apr,
+  year = {2024}
+}
+```
diff -ruN marc_original/third_party/torchtune/torchtune.egg-info/requires.txt marc/third_party/torchtune/torchtune.egg-info/requires.txt
--- marc_original/third_party/torchtune/torchtune.egg-info/requires.txt	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune.egg-info/requires.txt	2025-02-26 02:22:48.061477657 -0500
@@ -0,0 +1,24 @@
+datasets
+huggingface_hub
+safetensors
+sentencepiece
+tiktoken
+blobfile>=2
+numpy
+tqdm
+omegaconf
+psutil
+Pillow>=9.4.0
+
+[dev]
+bitsandbytes>=0.43.0
+comet_ml>=3.44.2
+pre-commit
+pytest==7.4.0
+pytest-cov
+pytest-mock
+pytest-integration
+tensorboard
+urllib3<2.0.0
+wandb
+expecttest
diff -ruN marc_original/third_party/torchtune/torchtune.egg-info/SOURCES.txt marc/third_party/torchtune/torchtune.egg-info/SOURCES.txt
--- marc_original/third_party/torchtune/torchtune.egg-info/SOURCES.txt	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune.egg-info/SOURCES.txt	2025-02-26 02:22:48.237477756 -0500
@@ -0,0 +1,295 @@
+LICENSE
+MANIFEST.in
+README.md
+pyproject.toml
+recipes/__init__.py
+recipes/eleuther_eval.py
+recipes/full_finetune_distributed.py
+recipes/full_finetune_single_device.py
+recipes/generate.py
+recipes/knowledge_distillation_single_device.py
+recipes/lora_dpo_distributed.py
+recipes/lora_dpo_single_device.py
+recipes/lora_finetune_distributed.py
+recipes/lora_finetune_single_device.py
+recipes/ppo_full_finetune_single_device.py
+recipes/qat_distributed.py
+recipes/quantize.py
+recipes/configs/eleuther_evaluation.yaml
+recipes/configs/generation.yaml
+recipes/configs/quantization.yaml
+recipes/configs/code_llama2/7B_full_low_memory.yaml
+recipes/configs/code_llama2/7B_lora_single_device.yaml
+recipes/configs/code_llama2/7B_qlora_single_device.yaml
+recipes/configs/dev/8B_full_experimental.yaml
+recipes/configs/gemma/2B_full.yaml
+recipes/configs/gemma/2B_lora.yaml
+recipes/configs/gemma/2B_lora_single_device.yaml
+recipes/configs/gemma/2B_qlora_single_device.yaml
+recipes/configs/gemma/7B_full.yaml
+recipes/configs/gemma/7B_lora.yaml
+recipes/configs/gemma/7B_lora_single_device.yaml
+recipes/configs/gemma/7B_qlora_single_device.yaml
+recipes/configs/gemma/evaluation.yaml
+recipes/configs/llama2/13B_full.yaml
+recipes/configs/llama2/13B_lora.yaml
+recipes/configs/llama2/13B_qlora_single_device.yaml
+recipes/configs/llama2/70B_lora.yaml
+recipes/configs/llama2/70B_qlora.yaml
+recipes/configs/llama2/7B_full.yaml
+recipes/configs/llama2/7B_full_low_memory.yaml
+recipes/configs/llama2/7B_lora.yaml
+recipes/configs/llama2/7B_lora_dpo.yaml
+recipes/configs/llama2/7B_lora_dpo_single_device.yaml
+recipes/configs/llama2/7B_lora_single_device.yaml
+recipes/configs/llama2/7B_qat_full.yaml
+recipes/configs/llama2/7B_qlora.yaml
+recipes/configs/llama2/7B_qlora_single_device.yaml
+recipes/configs/llama2/generation_v2.yaml
+recipes/configs/llama3/70B_full.yaml
+recipes/configs/llama3/70B_lora.yaml
+recipes/configs/llama3/8B_dora.yaml
+recipes/configs/llama3/8B_dora_single_device.yaml
+recipes/configs/llama3/8B_full.yaml
+recipes/configs/llama3/8B_full_single_device.yaml
+recipes/configs/llama3/8B_lora.yaml
+recipes/configs/llama3/8B_lora_single_device.yaml
+recipes/configs/llama3/8B_qat_full.yaml
+recipes/configs/llama3/8B_qdora_single_device.yaml
+recipes/configs/llama3/8B_qlora_single_device.yaml
+recipes/configs/llama3_1/405B_qlora.yaml
+recipes/configs/llama3_1/70B_full.yaml
+recipes/configs/llama3_1/70B_lora.yaml
+recipes/configs/llama3_1/8B_full.yaml
+recipes/configs/llama3_1/8B_full_single_device.yaml
+recipes/configs/llama3_1/8B_lora.yaml
+recipes/configs/llama3_1/8B_lora_single_device.yaml
+recipes/configs/llama3_1/8B_qlora_single_device.yaml
+recipes/configs/llama3_2/1B_full.yaml
+recipes/configs/llama3_2/1B_full_single_device.yaml
+recipes/configs/llama3_2/1B_lora.yaml
+recipes/configs/llama3_2/1B_lora_single_device.yaml
+recipes/configs/llama3_2/1B_qlora_single_device.yaml
+recipes/configs/llama3_2/3B_full.yaml
+recipes/configs/llama3_2/3B_full_single_device.yaml
+recipes/configs/llama3_2/3B_lora.yaml
+recipes/configs/llama3_2/3B_lora_single_device.yaml
+recipes/configs/llama3_2/3B_qlora_single_device.yaml
+recipes/configs/llama3_2/knowledge_distillation_single_device.yaml
+recipes/configs/llama3_2_vision/11B_full.yaml
+recipes/configs/llama3_2_vision/11B_full_single_device.yaml
+recipes/configs/llama3_2_vision/11B_lora.yaml
+recipes/configs/llama3_2_vision/11B_lora_single_device.yaml
+recipes/configs/llama3_2_vision/evaluation.yaml
+recipes/configs/llama3_2_vision/generation_v2.yaml
+recipes/configs/mistral/7B_full.yaml
+recipes/configs/mistral/7B_full_low_memory.yaml
+recipes/configs/mistral/7B_full_ppo_low_memory.yaml
+recipes/configs/mistral/7B_lora.yaml
+recipes/configs/mistral/7B_lora_single_device.yaml
+recipes/configs/mistral/7B_qlora_single_device.yaml
+recipes/configs/mistral/evaluation.yaml
+recipes/configs/phi3/evaluation.yaml
+recipes/configs/phi3/mini_full.yaml
+recipes/configs/phi3/mini_full_low_memory.yaml
+recipes/configs/phi3/mini_lora.yaml
+recipes/configs/phi3/mini_lora_single_device.yaml
+recipes/configs/phi3/mini_qlora_single_device.yaml
+recipes/configs/qwen2/0.5B_full.yaml
+recipes/configs/qwen2/0.5B_full_single_device.yaml
+recipes/configs/qwen2/0.5B_lora.yaml
+recipes/configs/qwen2/0.5B_lora_single_device.yaml
+recipes/configs/qwen2/1.5B_full.yaml
+recipes/configs/qwen2/1.5B_full_single_device.yaml
+recipes/configs/qwen2/1.5B_lora.yaml
+recipes/configs/qwen2/1.5B_lora_single_device.yaml
+recipes/configs/qwen2/7B_full.yaml
+recipes/configs/qwen2/7B_full_single_device.yaml
+recipes/configs/qwen2/7B_lora.yaml
+recipes/configs/qwen2/7B_lora_single_device.yaml
+recipes/configs/qwen2/evaluation.yaml
+recipes/configs/qwen2/knowledge_distillation_single_device.yaml
+recipes/dev/generate_v2.py
+torchtune/__init__.py
+torchtune/_recipe_registry.py
+torchtune/recipe_interfaces.py
+torchtune.egg-info/PKG-INFO
+torchtune.egg-info/SOURCES.txt
+torchtune.egg-info/dependency_links.txt
+torchtune.egg-info/entry_points.txt
+torchtune.egg-info/requires.txt
+torchtune.egg-info/top_level.txt
+torchtune/_cli/__init__.py
+torchtune/_cli/cp.py
+torchtune/_cli/download.py
+torchtune/_cli/ls.py
+torchtune/_cli/run.py
+torchtune/_cli/subcommand.py
+torchtune/_cli/tune.py
+torchtune/_cli/validate.py
+torchtune/config/__init__.py
+torchtune/config/_errors.py
+torchtune/config/_instantiate.py
+torchtune/config/_parse.py
+torchtune/config/_utils.py
+torchtune/config/_validate.py
+torchtune/data/__init__.py
+torchtune/data/_chat_formats.py
+torchtune/data/_collate.py
+torchtune/data/_common.py
+torchtune/data/_converters.py
+torchtune/data/_instruct_templates.py
+torchtune/data/_messages.py
+torchtune/data/_prompt_templates.py
+torchtune/data/_utils.py
+torchtune/datasets/__init__.py
+torchtune/datasets/_alpaca.py
+torchtune/datasets/_arc.py
+torchtune/datasets/_chat.py
+torchtune/datasets/_cnn_dailymail.py
+torchtune/datasets/_concat.py
+torchtune/datasets/_grammar.py
+torchtune/datasets/_hh_rlhf_helpful.py
+torchtune/datasets/_instruct.py
+torchtune/datasets/_packed.py
+torchtune/datasets/_preference.py
+torchtune/datasets/_samsum.py
+torchtune/datasets/_sft.py
+torchtune/datasets/_slimorca.py
+torchtune/datasets/_stack_exchange_paired.py
+torchtune/datasets/_text_completion.py
+torchtune/datasets/_wikitext.py
+torchtune/datasets/multimodal/__init__.py
+torchtune/datasets/multimodal/_arc_multimodal.py
+torchtune/datasets/multimodal/_llava_instruct.py
+torchtune/datasets/multimodal/_multimodal.py
+torchtune/datasets/multimodal/_the_cauldron.py
+torchtune/generation/__init__.py
+torchtune/generation/_generation.py
+torchtune/models/__init__.py
+torchtune/models/convert_weights.py
+torchtune/models/clip/__init__.py
+torchtune/models/clip/_component_builders.py
+torchtune/models/clip/_model_builders.py
+torchtune/models/clip/_position_embeddings.py
+torchtune/models/clip/_transform.py
+torchtune/models/clip/inference/_transform.py
+torchtune/models/code_llama2/__init__.py
+torchtune/models/code_llama2/_model_builders.py
+torchtune/models/gemma/__init__.py
+torchtune/models/gemma/_component_builders.py
+torchtune/models/gemma/_model_builders.py
+torchtune/models/gemma/_tokenizer.py
+torchtune/models/gemma/gemma_norm_embedding.py
+torchtune/models/gemma/rms_norm.py
+torchtune/models/gemma/transformer.py
+torchtune/models/llama2/__init__.py
+torchtune/models/llama2/_component_builders.py
+torchtune/models/llama2/_model_builders.py
+torchtune/models/llama2/_model_utils.py
+torchtune/models/llama2/_prompt_template.py
+torchtune/models/llama2/_tokenizer.py
+torchtune/models/llama3/__init__.py
+torchtune/models/llama3/_component_builders.py
+torchtune/models/llama3/_model_builders.py
+torchtune/models/llama3/_model_utils.py
+torchtune/models/llama3/_tokenizer.py
+torchtune/models/llama3_1/__init__.py
+torchtune/models/llama3_1/_component_builders.py
+torchtune/models/llama3_1/_model_builders.py
+torchtune/models/llama3_1/_position_embeddings.py
+torchtune/models/llama3_2/__init__.py
+torchtune/models/llama3_2/_component_builders.py
+torchtune/models/llama3_2/_model_builders.py
+torchtune/models/llama3_2_vision/__init__.py
+torchtune/models/llama3_2_vision/_component_builders.py
+torchtune/models/llama3_2_vision/_convert_weights.py
+torchtune/models/llama3_2_vision/_encoder.py
+torchtune/models/llama3_2_vision/_model_builders.py
+torchtune/models/llama3_2_vision/_transform.py
+torchtune/models/mistral/__init__.py
+torchtune/models/mistral/_component_builders.py
+torchtune/models/mistral/_model_builders.py
+torchtune/models/mistral/_prompt_template.py
+torchtune/models/mistral/_tokenizer.py
+torchtune/models/phi3/__init__.py
+torchtune/models/phi3/_component_builders.py
+torchtune/models/phi3/_convert_weights.py
+torchtune/models/phi3/_model_builders.py
+torchtune/models/phi3/_position_embeddings.py
+torchtune/models/phi3/_tokenizer.py
+torchtune/models/qwen2/__init__.py
+torchtune/models/qwen2/_component_builders.py
+torchtune/models/qwen2/_convert_weights.py
+torchtune/models/qwen2/_model_builders.py
+torchtune/models/qwen2/_positional_embeddings.py
+torchtune/models/qwen2/_tokenizer.py
+torchtune/modules/__init__.py
+torchtune/modules/attention.py
+torchtune/modules/attention_utils.py
+torchtune/modules/common_utils.py
+torchtune/modules/feed_forward.py
+torchtune/modules/kv_cache.py
+torchtune/modules/layer_norm.py
+torchtune/modules/lr_schedulers.py
+torchtune/modules/position_embeddings.py
+torchtune/modules/rms_norm.py
+torchtune/modules/tanh_gate.py
+torchtune/modules/tied_linear.py
+torchtune/modules/transformer.py
+torchtune/modules/vision_transformer.py
+torchtune/modules/loss/__init__.py
+torchtune/modules/loss/ce_chunked_output_loss.py
+torchtune/modules/loss/kd_losses.py
+torchtune/modules/low_precision/__init__.py
+torchtune/modules/low_precision/_register_nf4_dispatch_ops.py
+torchtune/modules/low_precision/nf4_linear.py
+torchtune/modules/model_fusion/__init__.py
+torchtune/modules/model_fusion/_fusion.py
+torchtune/modules/model_fusion/_fusion_utils.py
+torchtune/modules/peft/__init__.py
+torchtune/modules/peft/_utils.py
+torchtune/modules/peft/dora.py
+torchtune/modules/peft/lora.py
+torchtune/modules/tokenizers/__init__.py
+torchtune/modules/tokenizers/_sentencepiece.py
+torchtune/modules/tokenizers/_tiktoken.py
+torchtune/modules/tokenizers/_utils.py
+torchtune/modules/transforms/__init__.py
+torchtune/modules/transforms/_transforms.py
+torchtune/modules/transforms/vision_utils/__init__.py
+torchtune/modules/transforms/vision_utils/get_canvas_best_fit.py
+torchtune/modules/transforms/vision_utils/get_inscribed_size.py
+torchtune/modules/transforms/vision_utils/pad_dim_to_size.py
+torchtune/modules/transforms/vision_utils/resize_with_pad.py
+torchtune/modules/transforms/vision_utils/tile_crop.py
+torchtune/rlhf/__init__.py
+torchtune/rlhf/_types.py
+torchtune/rlhf/rewards.py
+torchtune/rlhf/sequence_processing.py
+torchtune/rlhf/loss/__init__.py
+torchtune/rlhf/loss/dpo.py
+torchtune/rlhf/loss/ppo.py
+torchtune/rlhf/utils/__init__.py
+torchtune/rlhf/utils/_convert_weights.py
+torchtune/training/__init__.py
+torchtune/training/_activation_offloading.py
+torchtune/training/_compile.py
+torchtune/training/_distributed.py
+torchtune/training/_profiler.py
+torchtune/training/activations.py
+torchtune/training/lr_schedulers.py
+torchtune/training/memory.py
+torchtune/training/metric_logging.py
+torchtune/training/pooling.py
+torchtune/training/precision.py
+torchtune/training/quantization.py
+torchtune/training/seed.py
+torchtune/training/checkpointing/__init__.py
+torchtune/training/checkpointing/_checkpointer.py
+torchtune/training/checkpointing/_utils.py
+torchtune/utils/__init__.py
+torchtune/utils/_device.py
+torchtune/utils/_import_guard.py
+torchtune/utils/_logging.py
+torchtune/utils/_version.py
\ No newline at end of file
diff -ruN marc_original/third_party/torchtune/torchtune.egg-info/top_level.txt marc/third_party/torchtune/torchtune.egg-info/top_level.txt
--- marc_original/third_party/torchtune/torchtune.egg-info/top_level.txt	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/torchtune.egg-info/top_level.txt	2025-02-26 02:22:48.061477657 -0500
@@ -0,0 +1,2 @@
+recipes
+torchtune
diff -ruN marc_original/third_party/torchtune/version.txt marc/third_party/torchtune/version.txt
--- marc_original/third_party/torchtune/version.txt	1969-12-31 19:00:00.000000000 -0500
+++ marc/third_party/torchtune/version.txt	2025-02-20 17:49:30.862026402 -0500
@@ -0,0 +1 @@
+0.4.0
Binary files marc_original/ttt/__pycache__/preprocess.cpython-312.pyc and marc/ttt/__pycache__/preprocess.cpython-312.pyc differ
diff -ruN marc_original/ttt.sh marc/ttt.sh
--- marc_original/ttt.sh	1969-12-31 19:00:00.000000000 -0500
+++ marc/ttt.sh	2025-02-28 20:13:35.778320220 -0500
@@ -0,0 +1,38 @@
+# Specify data path
+data_file=../kaggle-arc-data/arc-agi_evaluation_challenges.json
+# Specify finetuned path
+base_checkpoint_dir=checkpoints/acc_rd_ttt-finetuned-1B-model
+# Specify where TTT adapters should be saved
+ttt_folder=experiments/ttt_models
+mkdir -p $ttt_folder
+
+
+# You need show an initial config file that is compatible with torchtune configs
+# This is provided in this repo
+lora_config_file=configs/ttt/1B_lora_single_device.yaml
+# lora_config_file=configs/ttt/8.1B_lora_single_device.yaml # for barc
+# But you can override some of the variables
+batch_size=2
+epochs=1
+learning_rate=5e-5
+lora_rank=128
+lora_alpha=16.0
+lora_to_output=False # should be False for Llama3.2 models for now (whether to apply lora to the output linear layer)
+unmask_outputs=True # True means use demonstration loss
+# You can specify how many tasks you want train for.
+num_tasks=100
+
+# You can run the main script
+python test_time_train.py --lora_config=$lora_config_file \
+--base_checkpoint_dir=$base_checkpoint_dir \
+--experiment_folder=$ttt_folder \
+--data_file=$data_file \
+--batch_size=$batch_size \
+--epochs=$epochs \
+--num_tasks=${num_tasks} \
+--lora_rank=$lora_rank \
+--lora_alpha=$lora_alpha \
+--lora_to_output=$lora_to_output \
+--unmask_outputs=$unmask_outputs \
+--new_format # use --barc_format for barc
+# --no_transform # no data augmentations
\ No newline at end of file
diff -ruN marc_original/upload_models.py marc/upload_models.py
--- marc_original/upload_models.py	1969-12-31 19:00:00.000000000 -0500
+++ marc/upload_models.py	2025-02-28 18:42:27.423154598 -0500
@@ -0,0 +1,26 @@
+"""Script to upload the checkpoints to the Hugging Face Hub."""
+from huggingface_hub import HfApi
+
+api = HfApi()
+
+# repo_id = "stewy33/acc_rd_ttt-Llama-3.2-1B-Instruct"
+# folder_path = "checkpoints/acc_rd_ttt-Llama-3.2-1B-Instruct"
+#repo_id = "stewy33/acc_rd_ttt-finetuned-1B-model"
+#folder_path = "checkpoints/acc_rd_ttt-finetuned-1B-model"
+repo_id = "stewy33/acc_rd_ttt-experiments_folder"
+folder_path = "experiments"
+
+# Create the repository first (if not exists)
+api.create_repo(
+    repo_id=repo_id,
+    repo_type="model",
+    exist_ok=True  # does nothing if already exists
+)
+
+# Now upload the folder
+api.upload_folder(
+    folder_path=folder_path,
+    path_in_repo=".",
+    repo_id=repo_id,
+    repo_type="model"
+)
Binary files marc_original/utils/__pycache__/__init__.cpython-312.pyc and marc/utils/__pycache__/__init__.cpython-312.pyc differ
Binary files marc_original/utils/__pycache__/np_cache.cpython-312.pyc and marc/utils/__pycache__/np_cache.cpython-312.pyc differ
